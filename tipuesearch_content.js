var tipuesearch = {"pages":[{"title":"Optimize Computational Efficiency of Skip-Gram with Negative Sampling","text":"The code snippet assumes Anaconda 5.2.0 version of Python virtual environment Acknowledgement The materials on this post are based the on four NLP papers, Distributed Representations of Words and Phrases and their Compositionality (Mikolov et al., 2013), word2vec Parameter Learning Explained (Rong, 2014), Improving Negative Sampling for Word Representation using Self-embedded Features (Chen et al., 2017) and word2vec Explained: Deriving Mikolov et al.'s Negative-Sampling Word-Embedding Method (Goldberg and Levy, 2014). Review on Word2Vec Skip-Gram In my previous post , I illustrated the neural network structure of Skip-Gram Word2Vec model that represents words in a vector space. Figure 1: Skip-Gram model structure I also derived the cost function of Skip-Gram in Derivation of Cost Function : $$J(\\theta) = - \\frac{1}{T} \\sum_{t=1}&#94;{T} \\sum_{c=1}&#94;{C} log \\frac{exp(W_{output_{(c)}} \\cdot h)}{\\sum&#94;V_{i=1}exp(W_{output_{(i)}} \\cdot h)} \\tag{1}$$ where $T$ is the size of training samples, $C$ is the window size , $V$ is the size of unique vocab in the corpus, and $W_{input}$, $W_{output}$ and $h$ are illustrated in figure 1 . I also noted that stochastic gradient descent (SGD) is used to mitigate computational burden — the size of $T$ in $\\frac{1}{T} \\sum&#94;T_{t=1}$ can be billions or more in NLP applications. The new cost function using SGD is: $$J(\\theta; w&#94;{(t)}) = - \\sum_{c=1}&#94;{C} log \\frac{exp(W_{output_{(c)}} \\cdot h)}{\\sum&#94;V_{i=1}exp(W_{output_{(i)}} \\cdot h)} \\tag{2}$$ Review on Softmax Softmax is a multinomial regression classifier. It means that it classifies multiple labels, such as predicting if an hand-written digit is $0,\\,1,\\,2,\\,...\\,8\\,$ or $9$. In case of binary classification (True or False), such as classifying fraud or not-fraud in bank transactions, binomial regression classifier called Sigmoid function is used. In eq (2) , the fraction inside the summation of log yields the probability distribution of all $V$-vocabs in the corpus, given the input word. In statistics, the conditional probability of $A$ given $B$ is denoted as $p(A|B)$. In Skip-Gram, we use the notation, $p(w_{context}| w_{center})$, to denote the conditional probability of observing a context word given a center word. It is obtained by using the softmax function: $$ p(w_{context}|w_{center}) = \\frac{exp(W_{output_{(context)}} \\cdot h)}{\\sum&#94;V_{i=1}exp(W_{output_{(i)}} \\cdot h)} \\in \\mathbb{R}&#94;{1} \\tag{3} $$ Exponentiation ensures that the transformed values are positive, and the normalization factor in the denominator ensures that the values have a range of $[0, 1)$ and their sum equals $1$. Figure 2: softmax function transformation The probability is computed $V$ times using eq (3) to obtain a conditional probability distribution of observing all $V$-unique vocabs in the corpus, given a center word ($w&#94;{(t)}$). $$ \\left[ \\begin{array}{c} p(w_{1}|w&#94;{(t)}) \\\\ p(w_{2}|w&#94;{(t)}) \\\\ p(w_{3}|w&#94;{(t)}) \\\\ \\vdots \\\\ p(w_{V}|w&#94;{(t)}) \\end{array} \\right] = \\frac{exp(W_{output} \\cdot h)}{\\sum&#94;V_{i=1}exp(W_{output_{(i)}} \\cdot h)} \\in \\mathbb{R}&#94;{V}\\tag{4} $$ Softmax is computationally very expensive There is an issue with the vanilla Skip-Gram — softmax is computationally very expensive, as it requires scanning through the entire output embedding matrix ($W_{output}$) to compute the probability distribution of all $V$ words, where $V$ can be millions or more. Figure 3: Algorithm complexity of vanilla Skip-Gram Furtheremore, the normalization factor in the denominator also requires $V$ iterations. In mathematical context, the normalization factor needs to be computed for each probability p($w_{context}| w_{center}$), making the alogrithm complexity = $O(V \\times V)$. However, when implemented on code, the normalization factor is computed only once and cached as a Python variable, making the alogrithm complexity = $O(V + V) \\approx O(V)$. This is possible because normalization factor is the same for all words. Due to this computational inefficiency, softmax is not used in most implementaions of Skip-Gram . Instead we use an alternative called negative sampling with sigmoid function , which rephrases the problem into a set of independent binary classification task of algorithm complexity = $O(K \\, + \\, 1)$, where $K$ typically has a range of $[5, 20]$. Skip-Gram Negative Sampling In Skip-Gram, assuming stochastic gradient descent , weight marices in the neural network are updated for each training sample to correctly predict output. Let's assume that the training corpus has 10,000 unique vocabs ($V$ = 10000) and the hidden layer is 300-dimensional ($N$ = 300). This means that there are 3,000,000 neurons in the output weight matrix ($W_{output}$) that need to be updated for each training sample (Notes: for the input weight matrix ($W_{input}$), only 300 neurons are updated for each training sample. This is illustrated in figure 18 of my previous post .) Since the size of the training corpus ($T$) is very large, updating 3M neurons for each training sample is unrealistic in terms of computational efficiency. Negative sampling addresses this issue by updating only a small fraction of the output weight neurons for each training sample. In negative sampling, $K$ negative samples are randomly drawn from a noise distribution . $K$ is a hyper-parameter that can be empirically tuned, with a typical range of $[5,\\, 20]$. For each training sample, you randomly draw $K$ number of negative samples from a noise distribution, $P_n(w)$, and the model will update $(k+1) \\times N$ neurons in the output weight matrix ($W_{output}$). $N$ is the dimension of the hidden layer ($h$), or the size of a word vector. $+1$ accounts for a positive sample . With the above assumption, if you set K=9 , the model will update (9 + 1) * 300 = 3000 neurons, which is only 0.1% of the 3M neurons in $W_{output}$. This is computationally much faster than the original Skip-Gram, and yet maintains a good quality of word vectors. The below figure has 3-dimensional hidden layer ($N=3$), 11 vocabs ($V=11$), and 3 negative samples ($K=3$). Figure 4: Skip-Gram model structure Notes: Choice of $K$ The paper (Mikolov et al., 2013) says that K=2 ~ 5 works for large data sets, and K=5 ~ 20 for small data sets. How does negative sampling work? With negative sampling, word vectors are no longer learned by predicting context words of a center word. Instead of using softmax to compute the $V$-dimensional probability distribution of observing an output word given an input word, $p(w_O|w_I)$, the model uses sigmoid function to learn to differentiate the actual context words ( positive ) from randomly drawn words ( negative ) from the noise distribution $P_n(w)$. Assume that the center word is \"regression\" . It is likely to observe \"regression\" + { \"logistic\" , \"machine\" , \"sigmoid\" , \"supervised\" , \"neural\" } pairs, but it is unlikely to observe \"regression\" + { \"zebra\" , \"pimples\" , \"Gangnam-Style\" , \"toothpaste\" , \"idiot\" }. The model maximizes the probability $p(D=1|w,c_{pos})$ of observing positive pairs, while minimizing the probability $p(D=0|w,c_{neg})$ of observing negative pairs. The idea is that if the model can distinguish between the positive pairs ($w$, $c_{pos}$) vs negative pairs ($w$, $c_{neg}$), good word vectors will be learned. Negative sampling converts multi-classification task into binary-classification task. The new objective is to predict, for any given word-context pair ($w,\\,c$), whether the word ($c$) is in the context window of the the center word ($w$) or not. Since the goal is to identify a given word as True ( positive , $D=1$) or False ( negative , $D=0$), we use sigmoid function instead of softmax function. The probability of a word ($c$) appearing within the context of the center word ($w$) can be defined as the following: $$ p(D=1|w,c;\\theta)=\\frac{1}{1+exp(-w_{output_{(j)}} \\cdot h)} \\in \\mathbb{R}&#94;{1} \\tag{5} $$ where $c$ is the word you want to know whether it came from the context window or the noise distribution. $w$ is the input (center) word, $h$ is the hidden layer, and $\\theta$ is the weight matrix passed into the model. $w_{output_{(j)}}$ is the word vector from the output weight matrix ($W_{output}$) of figure 1 . Eq (5) computes the probability that the given word ($c$) is a positive word ($D=1$). It only needs to be applied $K$ + 1 times instead of $V$ times for every word in the vocabulary, because $w_j$ comes from the concatenation of true context word ($c_{pos}$) and $K$ negative words ($W_{neg} = \\{ c_{neg, j}|j=1,\\cdots,K \\}$): $$ w_{output{(j)}} \\in \\{c_{pos}\\} \\cup W_{neg} \\tag{6} $$ This probability is computed $K + 1$ times to obtain a probability distribution of true context word and $K$ negative samples: $$ \\left[ \\begin{array}{c} p(D=1|w,c_{pos}) \\\\ p(D=1|w,c_{neg, 1}) \\\\ p(D=1|w,c_{neg, 2}) \\\\ p(D=1|w,c_{neg, 3}) \\\\ \\vdots \\\\ p(D=1|w,c_{neg, 4}) \\end{array} \\right] = \\frac{1}{1+exp(-(\\{c_{pos}\\} \\cup W_{neg}) \\cdot h)} \\in \\mathbb{R}&#94;{K+1}\\tag{7} $$ Compare this equation with eq (4) — you will notice that eq (7) is computationally much cheaper because $K$ is between 5 ~ 20, whereas $V$ can be millions. Moreover, no extra iterations are necessary to compute the normalization factor in the denominator of eq (4) , because sigmoid function is a binary regression classifier. The algorithm complexity for probability distribution of vanilla Skip-Gram is $O(V)$, whereas negative sampling's is $O(K+1)$. This shows why negative sampling saves a significant amount of computational cost per iteration. Figure 5: Choice of negative samples (Text source: Petrowiki ) For the purpose of illustration, consider the above paragraphs. Assume that our center word ($w$) is drilling , window size is $3$, and the number of negative samples ($K$) is $5$. With the window size of $3$, the contexts words are: \"engineer\" , \"traditionally\" , \"designs\" , \"fluids\" , \"with\" , and \"two\" . These context words are considered as positive labels ($D$ = 1). Our current context word ($c_{pos}$) is engineer . We also need negative words. We randomly pick $5$-words from the noise distribution $P_n(w)$ of the corpus for each context word, and consider them as negative samples ($D$ = 0). For the current context word, engineer , the 5 randomly drawn negative words ($c_{neg}$) are: \"minimized\" , \"primary\" , \"concerns\" , \"led\" , and \"page\" . The idea of negative sampling is that it is more likely to observe positive word pairs ($w$, $c_{pos}$) together than negative word pairs ($w$, $c_{neg}$) together in the corpus. The model attempts to maximize the the probability of observing positive pairs $p(c_{pos}|w) \\rightarrow 1$ and minimize the probability of observing negative pairs $p(c_{neg}|w) \\rightarrow 0$ simultaneously by iterating through the training samples and updating the weights ($\\theta$). Note that the sum of the probability distribution obtained by sigmoid function ( eq (7) ) does not need to equal $1$, unlike softmax ( eq (4) ). Figure 6: maximizing postive pairs and minimizing negative pairs By the time the output probability distribution is nearly one-hot-encoded as in $iter = 4$ of the above figure, weight matrices $\\theta$ are optimized and good word vectors are learned. This optimization can be done my maximizing the dot product of the input weight matrix and the hidden layer ($-w_{output_{(j)}} \\cdot h$) in eq (333) . Notes: Drawing random negative samples For each word-context pair ($w,\\,c$), $K$ new negative samples are randomly drawn from a noise distribution. In figure 5 , there are 6 positive context words ( \"engineer\" , \"traditionally\" , \"designs\" , \"fluids\" , \"with\" , and \"two\" ) for one center word ( \"drilling\" ), and $K$ is 5. This means that a total of 6 * 5 = 30 word vectors are updated for each center word. What is a noise distribution $P_n(w)$? Imagine a distribution of words based on how many times each word appeared in a corpus, $U(w)$ (this is called unigram distribution). For each word $w$, divide the number of times it appeared by a normalization factor $Z$ so that the distribution becomes a probability distribution of range $[0, 1)$ and sums up to $1$. Raise the normalized distribution to the power of $\\alpha$ so that the distribution is \"smoothed-out\". Then this becomes your noise distribution $P_n(w)$ — normalized frequency distribution of words raised to the power of $\\alpha$. Mathematically, it can be expressed as: $$ P_n(w) = \\left(\\frac{U(w)}{Z}\\right)&#94;{\\alpha} \\tag{8} $$ Raising the unigram distribution $U(w)$ to the power of $\\alpha$ has an effect of smoothing out the distribution. It attempts to combat the imbalance between common words and rare words by decreasing the probability of drawing common words, and increasing the probability drawing rare words. $\\alpha$ is a hyper-parameter that can be empirically tuned. The authors of the original Word2Vec paper claims that the unigram distribution $U(w)$ raised to the $3/4$rd power (i.e., $U(w)&#94;{3/4}/Z$) yielded the best result. Figure 7: Effect of raising power of unigram distribution $U(w)$ What is a positive word $c_{pos}$? Words that actually appear within the context window of the center word ($w$). After the model is optimized, the probability computed with eq (5) will output $\\approx$ 1 as shown in fig 6 . A word vector of a center word ($w$) will be more similar to a word vector of positive word ($c_{pos}$) than of randomly drawn negative words ($c_{neg}$). This is because words that frequently appear together show strong correlation with each other. What is a negative word $c_{neg}$? Words that are randomly drawn from a noise distribution $P_n(w)$. After the model is optimized, the probability computed with eq (5) will output $\\approx$ 0 as shown in fig 6 . When training an Word2Vec model, the vocab size ($V$) easily exceeds tens of thousands. When 5 ~ 20 negative samples are randomly drawn among the vocabs, it is unlikely to observe the random word with a center word together. Therefore, once the model is optimized: $p(D=1|w,c_{neg})\\approx0$. How are negative samples drawn? $K$-negative samples are randomly drawn from a noise distribution $P_n(w)$ using someting like np.random.choice . In [24]: import numpy words = [ 'apple' , 'bee' , 'desk' , 'chair' ] normalized_word_frequency = [ 0.023 , 0.12 , 0.34 , 0.517 ] np . random . choice ( words , size = 20 , p = normalized_word_frequency ) Out[24]: array(['chair', 'bee', 'chair', 'bee', 'chair', 'bee', 'chair', 'bee', 'desk', 'bee', 'chair', 'desk', 'desk', 'desk', 'chair', 'desk', 'chair', 'desk', 'chair', 'desk'], dtype=' \"chair\" appeared the most from the corpus, and has the highest probability ( 0.517 ) of being drawn as a negative sample. As a result, \"chair\" was drawn the most. The noise distribution is raised to the power of $3/4$rd to combat the imbalance between common vs rare words, as shown in figure 7 . Derivation of Cost Function in Negative Sampling Coming soon... import numpy as np import matplotlib.pyplot as plt from scipy import stats %matplotlib notebook # sample data generation data = sorted(stats.norm.rvs(size=1000) + 5) # fit normal distribution mean, std = stats.norm.fit(data, loc=0) pdf_norm = stats.norm.pdf(data, mean, std) temp = np.power(data, 3/4) temp_mean, temp_std = stats.norm.fit(temp, loc=0) temp_pdf_norm = stats.norm.pdf(temp, temp_mean, temp_std) fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(8, 4), sharey=True) ax1.set_title('$U(w)$') ax1.hist(temp, bins='auto', density=True) ax1.set_xticklabels(['apple', 'desk', 'cup', 'chair', 'zebra', 'room', 'pencil', 'water', 'coin']) ax1.plot(temp, temp_pdf_norm, label='norm') ax2.set_title('$U(w)&#94;{3/4}$') ax2.hist(data, bins='auto', density=True) ax2.plot(data, pdf_norm, label='norm') ax2.set_xticklabels(['apple', 'desk', 'cup', 'chair', 'zebra', 'room', 'pencil', 'water', 'coin'])","tags":"Natural Language Processing","url":"https://aegis4048.github.io/optimize_computational_efficiency_of_skip-gram_with_negative_sampling","loc":"https://aegis4048.github.io/optimize_computational_efficiency_of_skip-gram_with_negative_sampling"},{"title":"Demystifying Neural Network in Skip-Gram Language Modeling","text":"Acknowledgement The materials on this post are based the on two NLP papers, Distributed Representations of Words and Phrases and their Compositionality (Mikolov et al., 2013) and word2vec Parameter Learning Explained (Rong, 2014). Paradigm Shift in Word Embedding: Count-Based to Prediction-Based Up until 2013, the traditional models for NLP tasks were count-based models. They mainly involve computing a co-occurence matrix to capture meaningful relationships among words (If you are interested in how co-occurrence matrix is used for language modeling, check out Understanding Multi-Dimensionality in Vector Space Modeling ). For example: Document 1: \"all that glitters is not gold\" Document 2: \"all is well that ends well\" * START all that glitters is not gold well ends END START 0 2 0 0 0 0 0 0 0 0 all 2 0 1 0 1 0 0 0 0 0 that 0 1 0 1 0 0 0 1 1 0 glitters 0 0 1 0 1 0 0 0 0 0 is 0 1 0 1 0 1 0 1 0 0 not 0 0 0 0 1 0 1 0 0 0 gold 0 0 0 0 0 1 0 0 0 1 well 0 0 1 0 1 0 0 0 1 1 ends 0 0 1 0 0 0 0 1 0 0 END 0 0 0 0 0 0 1 1 0 0 Table 1: Co-Occurence Matrix Count-based language modeling is easy to comprehend — related words are observed (counted) together more often than unrelated words. Many attempts were made to improve the performance of the model to the state-of-art, using SVD, ramped window, and non-negative matrix factorization ( Rohde et al. ms., 2005 ), but the model did not do well in capturing complex relationships among words. Then, the paradigm started to change in 2013, when Thomas Mikolov proposed the prediction-based modeling technique, called Word2Vec, in his famous paper, Distributed Representations of Words and Phrases and their Compositionality . Unlike counting word co-occurrences, the model uses neural networks to learn intelligent representation of words in a vector space. Then, the paper submitted to ACL in 2014, Don't count, predict! A systematic comparison of context-counting vs. context-predicting semantic vectors , quantified & compared the performances of count-based vs prediction-based models. Figure 1: Performance comparison of models ( source ) The blue bars represent the count-based models, and the red bars are for prediction-based models. The full summary of the paper and more detailed description about the result graph can be found here . Long story short, prediction-based models outperformed count-based models by a large margin on various language tasks. Prediction-based word-embedding: Word2Vec Skip-Gram One of the prediction-based language model introduced by Mikolov is Skip-Gram: Figure 2: Original Skip-gram model architecture Figure 2 is a diagram presented in the original Word2Vec paper. It is essentially describing that the model uses a neural network of one hidden (projection) layer to correctly predict context words ($w(t-2)$, $w(t-1)$, $w(t+1)$, $w(t+2)$) of an input word ($w(t)$). In the other words, the model attempts to maximize the probability of observing all four context words together, given a center word. Mathematically, it can be denoted as eq (1) . The training objective is to learn word vector representations that are good at predicting the nearby words. Notes: CBOW and Skip-Gram There are two models for Word2Vec: Continous Bag Of Words (CBOW) and Skip-Gram . While Skip-Gram model predicts context words given a center word, CBOW model predicts a center word given context words. According to Mikolov: Skip-gram : works well with small amount of the training data, represents well even rare words or phrases CBOW : several times faster to train than the skip-gram, slightly better accuracy for the frequent words Skip-Gram model is a better choice most of the time due to its ability to predict infrequent words, but this comes at the price of increased computational cost. If training time is a big concern, and you have large enough data to overcome the issue of predicting infrequent words, CBOW model may be a more viable choice. The details of CBOW model won't be covered in this post. Why predict context words? A natural question is, why do we predict context words? One must understand that the ultimate goal of Skip-Gram model is not to predict context words, but to learn intelligent vector representation of words. It just happens that predicting context words inevitably results in good vector representations of words, because of the neural network structure of Skip-Gram. Neural network at its essence is just optimizing weight marices ($\\theta$) to correctly predict output. In Word2Vec Skip-Gram, the weight matrices are, in fact, the vector representations of words. Therefore, optimizing weight matrix = good vector representations of words. This is described in detail below . What is the application of vector representations of words? In Word2Vec, words are represented as vectors, and related words are placed closed to each other on a vector space. Mathematically, this means that the vector distance between related words are smaller than the vector distance between unrelated words. Figure 3: Vector distance between two words For example in figure 3 , correlation between \"success\" and \"achieve\" can be quantified by computing the vector distance between them (Notes: For illustration purpose, three-dimensional word vectors are assumed in the figure, because higher dimensional vectors can't be visualized. Also, distance annotated in the figure is Euclidean, but in real-life, we use Cosine distance to evaluate vector correlations). One interesting application of vector representaion of words is that it can be used to solve analogy tasks. Let's assume the following word vectors for \"Germany\" , \"capital\" , and \"Berlin\" . $$ \\begin{align*} vec(\\text{Germany}) & = [1.22 \\quad 0.34 \\quad -3.82] \\\\ vec(\\text{capital}) & = [3.02 \\quad -0.93 \\quad 1.82] \\\\ vec(\\text{Berlin}) & = [4.09 \\quad -0.58 \\quad 2.01] \\end{align*} $$ To find out the capital of Germany, the word vector of \"capital\" can be added to the word vector of \"Germany\" . $$ \\begin{align*} vec(\\text{Germany}) + vec(\\text{capital}) &= [1.22 \\quad 0.34 \\quad -3.82] + [3.02 \\quad -0.93 \\quad 1.82] \\\\ &= [4.24 \\quad -0.59 \\quad -2.00] \\end{align*} $$ Since the sum of the word vectors of \"Germany\" and \"capital\" is similar to the word vector of \"Berlin\" , the model may conclude that the capital of Germany is Berlin. $$ \\begin{align*} [4.24 \\quad -0.59 \\quad -2.00] & \\cong [4.09 \\quad -0.58 \\quad 2.01] \\\\ vec(\\text{Germany}) + vec(\\text{capital}) & \\cong vec(\\text{Berlin}) \\end{align*} $$ Notes: Analogy tasks don't always work Not all analogy tasks can be solved like this. The above illustration works like a magic, but there are many analogy problems that can't be solved with Word2Vec. Think of the above illustration as just one use case of Word2Vec. Derivation of Cost Function Skip-Gram model seeks to optimize the word weight (embedding) matrix by correctly predicting context words, given a center word. In the other words, the model wants to maximize the probability of correctly predicting all context words at the same time, given a center word. Maximizing the probability of predicting context words leads to optimizing the weight matrix ($\\theta$) that best represents words in a vector space. $\\theta$ is a concatenation of input and output weight matrices — $[W_{input} \\quad W_{output}]$, as described below . It is passed into the cost function ($J$) as a variable and optimized. Mathematically, it can be expressed as: $$ max \\, p(w_{1}, w_{2}, ... , w_{C}|w_{center}; \\, \\theta) \\tag{1} $$ where $C$ is the window size. Recall that in statistics, the probability of $A$ given $B$ is expressed as $P(A|B)$. Then, natural log is taken on eq (1) to simplify taking derivatives. $$ max \\, log \\, p(w_{1}, w_{2}, ... , w_{C}|w_{center}; \\, \\theta) \\tag{2} $$ Notes: Why take a natural log? In machine learning, it is a common practice to take a natural log to the objective function to simplify taking derivatives. For example, a multinomial regression classifer called Softmax (details explained below ) has the following probability function: $p(x_i) = \\frac{e&#94;{x_i}}{\\sum_{j=1}e&#94;{x_{j}}}$ Taking a log simplifies the function: $log \\, p(x_i) = x_i - log \\, {\\sum_{j=1}e&#94;{x_{j}}}$ Depending on a model, the argument ($x_i$) passed into the probability function ($p$) can be complicated, and simplifying the original softmax function helps with taking the derivatives in the future. Taking a log does not affect the optimized weights ($\\theta$), because natural log is a monotonically increasing function. This means that increasing the value of $x$-axis results in increasing the value of $y$-axis. This is important because it ensures that the maximum value of the original probability function occurs at the same point as the log probability function. Therefore: $max \\, p(x_i) = max \\, log \\, p(x_i)$ In Skip-Gram, softmax function is used for context words classfication. The details are explained below . Softmax in Skip-Gram has the following equation: $$ p(w_{context}|w_{center}; \\, \\theta) = \\frac{exp(W_{output_{(context)}} \\cdot h)}{\\sum&#94;V_{i=1}exp(W_{output_{(i)}} \\cdot h)} \\tag{3} $$ $W_{output_{(context)}}$ is a row vector for a context word from the output embedding matrix (see below ), and $h$ is the hidden (projection) layer word vector for a center word (see below ). Softmax function is then plugged into the eq (2) to yield a new objective function that maximizes the probability of observing all $C$ context words, given a center word: $$ max \\, log \\, \\prod_{c=1}&#94;{C} \\frac{exp(W_{output_{(c)}} \\cdot h)}{\\sum&#94;V_{i=1}exp(W_{output_{(i)}} \\cdot h)} \\tag{4} $$ Notes: Probability Product In statistics, probability of observing $C$ multiple events at the same time is computed by the product of each event's probability. $$p(x_{1}, x_{2} ... x_{C}) = p(x_{1}) \\times p(x_{2}) \\, \\times \\, ... \\, \\times \\, p(x_{C})$$ This can be shortened with a product notation: $$p(x_{1}, x_{2} ... x_{C}) = \\prod_{c=1}&#94;{C}p(x_{c})$$ However, in machine learning, the convention is to minimize the cost function, not to maximize it. To stick to the convention, we add a negative sign to eq (4) . This can be done because minimizing a negative log-likelihood is equivalent to maximizing a positive log-likelihood. Therefore, the cost function we want to minimize becomes: $$ J(\\theta; w&#94;{(t)}) = -log \\, \\prod_{c=1}&#94;{C} \\frac{exp(W_{output_{(c)}} \\cdot h)}{\\sum&#94;V_{i=1}exp(W_{output_{(i)}} \\cdot h)} \\tag{5} $$ where $c$ is the index of the context word around the center word ($w_{t}$). $t$ is the index of the center word within a corpus of size $T$. Using the property of log, it can be changed to: $$J(\\theta; w&#94;{(t)}) = - \\sum_{c=1}&#94;{C} log \\frac{exp(W_{output_{(c)}} \\cdot h)}{\\sum&#94;V_{i=1}exp(W_{output_{(i)}} \\cdot h)} \\tag{6}$$ Taking a log to the softmax function allows us to simplify the expression into simpler forms because we can split the fraction into addtion of the numerator and the denominator: $$ J(\\theta; w&#94;{(t)}) = - \\sum_{c=1}&#94;{C}(W_{output_{(c)}} \\cdot h) + C \\cdot log \\sum&#94;V_{i=1}exp(W_{output_{(i)}} \\cdot h) \\tag{7} $$ Different paper uses different notations for the cost function. To stick to the notation used in the Word2Vec original paper , some of the notations in eq (7) can be changed. However, they are all equivalent: $$J_{t}(\\theta;w&#94;{(t)}) = -\\sum_{-c\\leq j \\leq c,j\\neq 0} \\log p(w_{t+j} \\mid w_t ; \\, \\theta) \\tag{8}$$ Note that eq (7) and eq (8) are equivalent. They both assume stochastic gradient descent , which means that for each training sample ($w&#94;{(t)}$) in the corpus of size ($T$), one update is made to the weight matrix ($\\theta$). The cost function expressed in the paper shows batch gradient descent eq (9) , which means that only one update is made for all $T$ training samples: $$J(\\theta) = -\\frac{1}{T} \\sum&#94;T_{t=1} \\sum_{-c\\leq j \\leq c,j\\neq 0} \\log p(w_{t+j} \\mid w_t ;\\, \\theta) \\tag{9}$$ However, in Word2Vec, batch gradient descent is almost never used due to its high computational cost. The author of the paper stated that he used stochastic gradient descent for training. Read the below notes for more information about stochastic gradient descent. Window Size of Skip-Gram Softmax regression (or multinomial logistic regression) is a generalization of logistic regression to the case where we want to handle multiple classes. A general form of the softmax regression looks like this: $$J(\\theta) = -\\frac{1}{T} \\sum&#94;T_{t=1} \\sum&#94;K_{k=1} log \\frac {exp(\\theta&#94;{(k)\\top}x&#94;{(t)})} {\\sum&#94;K_{i=1} exp(\\theta&#94;{(i)\\top}x&#94;{(t)})} \\tag{10}$$ where $T$ is the number of training samples, and $K$ is the number of labels to classify. In NLP applications, $K = V$, because there are $V$ unique vocabulary we need to classify in a vector space. $V$ can easily exceed tens of thousands. Skip-Gram tweaks this a little, and replaces $K$ with a variable called window size ($m$). Window size is a hyper parameter of the model with a typical range of $[1, 10]$ (see figure 4 ). Recall that Skip-Gram is a model that attempts to predict neighboring words of a center word. It doesn't have to predict all $V$ vocab in the corpus that may be 100 or more words away from it, but instead predict only a few, 1~10 neighboring context words. This is also intuitive, considering how words that are far away carry less information about each another. Thus, the adapted form of the softmax regression equation for Skip-Gram becomes: $$J(\\theta) = -\\frac{1}{T} \\sum&#94;T_{t=1} \\sum_{-c\\leq j \\leq c,j\\neq 0} log \\frac {exp(\\theta&#94;{(t+j)\\top}x&#94;{(t)})} {\\sum&#94;K_{i=1} exp(\\theta&#94;{(i)\\top}x&#94;{(t)})} \\tag{11}$$ This is equivalent to eq (9) . Note that the $K$ in the denominator is still equal to $V$, because the denominator acts as a normalization factor, as described below . However, the size of $K$ in the denominator can still be reduced to smaller size using negative sampling . Neural Network Structure of Skip-Gram How is neural network used to minimize the cost functoin described in eq (11) ? One needs to look into the structure of the Skip-Gram model to gain insights about their correlation. For illustration purpose, let's assume that the entire corpus is composed of the quote from the Game of Thrones, \"The man who passes the sentence should swing the sword\" , by Ned Stark. There are 10 words ($T = 10$), and 8 unique words ($V = 8$). Note that in real life, the corpus is much bigger than just one sentence. The man who passes the sentence should swing the sword. - Ned Stark We will use window=1 , and assume that 'passes' is the current center word, making 'who' and 'the' context words. window is a hyper-parameter that can be empirically tuned. It typically has a range of $[1, 10]$. Figure 4: Training Window For illustration purpose, a three-dimensional neural net will be constructed. In *gensim*, this can be implemented by setting size=3 . This makes $N = 3$. Note that size is also a hyper-parameter that can be empirically tuned. In real life, a typical Word2Vec model has 200-600 neurons. from gensim.models import Word2Vec model = Word2Vec(corpus, size=3, window=1) This means that the input weight matrix ($W_{input}$) will have a size of $8 \\times 3$, and output weight matrix ($W_{output}&#94;T$) will have a size of $3 \\times 8$. Recall that the corpus, \"The man who passes the sentence should swing the sword\" , has 8 unique vocabularies ($V = 8$). Figure 5: Skip-Gram model structure Training: Forward Propagation The word embedding matrices ($W_{input}$, $W_{output}$) in Skip-Gram are optimized through forward and backward propagations. For each iteration of forward + backward propagations, the model learns to reduce prediction error by optimizing the weight matrix ($\\theta$), thus acquiring higher quality embedding matrices that better capture relationships among words. Forward propagation includes obtaining the probability distribution of words ($y_{pred}$ in figure 5 ) given a center word, and backward propagation includes calculating the prediction error, and updating the weight (embedding) matrices to minimize the prediction error. Input Layer ($x$) The input layer is a $V$-dim one-hot encoded vector. Every element in the vector is 0 except one element that corresponds to the center (input) word. Input vector is multiplied with the input weight matrix ($W_{input}$) of size $V \\times N$, and yields a hidden (projection) layer ($h$) of $N$-dim vector. Because the input layer is one-hot encoded, it makes the input weight matrix ($W_{input}$) to behave like a look-up table for the center word. Assuming epoch number of 1 ( iter=1 in gensim Word2Vec implementation) and stochastic gradient descent, the input vector is injected into the network $T$ times for every word in the corpus and makes $T$ updates to the weight matrix ($\\theta$) to learn from the training samples. Derivation of the stochasitc update equations are explained below . Figure 6: One-hot encoded input vector and parameter update Notes: Stochastic Gradient Descent The goal of any machine learning model is to find the optimal values of a weight matrix ($\\theta$) to minimize prediction error. A general update equation for weight matrix looks like the following: $\\theta&#94;{(new)}=\\theta&#94;{(old)}-\\eta\\cdot\\nabla_{J(\\theta)}$ $\\eta$ is learning rate, $\\nabla_{J(\\theta)}$ is gradient for the weight matrix, and $J(\\theta)$ is the cost function that has different forms for each model. The cost function for the Skip-Gram model proposed in the Word2Vec original paper has the following equation: $$J(\\theta) = -\\frac{1}{T} \\sum&#94;T_{t=1} \\sum_{-c\\leq j \\leq c,j\\neq 0} \\log p(w_{t+j} \\mid w_t ; \\theta)$$ Here, what gives us headache is the expression, $\\frac{1}{T} \\sum&#94;T_{t=1}$, because $T$ can be larger than billions or more in many NLP applications. It is basically telling us that billions of iterations need to be computed to make just one update to the weight matrix ($\\theta$). In order to mitigate this computational burden, the author of the paper states that Stochastic Gradient Descent (SGD) was used for parameter optimization. SGD removes the expression, $\\frac{1}{T} \\sum&#94;T_{t=1}$, from the cost function and performs parameter update for each training example, $w&#94;{(t)}$: $$J_{t}(\\theta;w&#94;{(t)}) = -\\sum_{-c\\leq j \\leq c,j\\neq 0} \\log p(w_{t+j} \\mid w_t ; \\theta)$$ Then, the new parameter update equation for SGD becomes: $\\theta&#94;{(new)}=\\theta&#94;{(old)}-\\eta\\cdot\\nabla_{J_{t}(\\theta;w&#94;{(t)})}$ The original vanilla graident descent makes $1$ parameter update for $T$ training samples, but the new update equation using SGD makes $T$ parameter update for $T$ training samples. However, this comes at the price of higher fluctuation (or variance) in minimizing prediction error. Input and Output Weight Matrix ($W_{input}$, $W_{output}$) Why does Skip-Gram model attempt to predict context words given a center word? How does predicting context words help with quantifying words and representing them in a vector space? In fact, the ultimate goal of the model is not to predict context words, but to construct the word embedding matrices ($W_{input}$, $W_{output}$) that best caputure relationship among words in a vector space. Skip-Gram achieves this by using a neural net — it optimizes the weight (embedding) matrices by adjusting the weight matrix to minimize the prediction error ($y_{pred} - y_{true}$). This will make more sense once you understand how the embedding matrix behaves like a look-up table . Each row in a word-embedding matrix is a word-vector for each word. Consider the following word-embedding matrix, $W_{input}$. Figure 7: Word-embedding matrix, $W_{input}$ The words of our interest are \"passes\" and \"should\" . \"passes\" has a word vector of $[0.1 \\quad 0.2 \\quad 0.7]$ and \"should\" has $[-2 \\quad 0.2 \\quad 0.8]$. Since we set the size of the weight matrix to be size=3 above , the matrix is three-dimensional, and can be visualized in a 3D vector space: Figure 8: 3D visualization of word vectors in embedding matrix Optimizing the embedding (weight) matrices ($\\theta$) results in representing words in a high quality vector space, and the model will be able to capture meaningful relationships among words. Notes: $\\theta$ in cost function There are two weight matrices that need to be optimized in Skip-Gram model: $W_{input}$ and $W_{output}$. Often times in neural net, the weights are expressed as $\\theta$. In Skip-Gram, $\\theta$ is a concatenation of input and output weight matrices — $[W_{input} \\quad W_{output}]$. $$ \\theta = [W_{input} \\quad W_{output}] = \\left[ \\begin{array}{l} u_{the} \\\\ u_{passes} \\\\ \\vdots \\\\ u_{who} \\\\ v_{the} \\\\ v_{passes} \\\\ \\vdots \\\\ v_{who} \\end{array} \\right] \\in \\mathbb{R}&#94;{2NV}$$ $\\theta$ has a size of $2V \\times N$, where $V$ is the number of unique vocab in a corpus, and $N$ is the dimension of word vectors in the embedding matrices. $2$ is multipled to $V$ because there are two weight matrices, $W_{input}$ and $W_{output}$. $u$ is a word vector from $W_{input}$ and $v$ is a word vector from $W_{output}$. Each word vectors are $N$-dim row vectors from input and output embedding matrices. Hidden (Projection) Layer ($h$) Skip-Gram uses a neural net with one hidden layer. In the context of natural language processing, hidden layer is often referred to as a projection layer, because $h$ is essentially an 1D vector projected by the one-hot encoded input vector. Figure 9: Computing projection layer $h$ is obtained by multiplying the input word embedding matrix with the $V$-dim input vector. $$h = W_{input}&#94;T \\cdot x \\in \\mathbb{R}&#94;{N} \\tag{12}$$ Softmax Output Layer ($y_{pred}$) The output layer is a $V$-dim probability distribution of all unique words in the corpus, given a center word. In statistics, the conditional probability of $A$ given $B$ is denoted as $p(A|B)$. In Skip-Gram, we use the notation, $p(w_{context}| w_{center})$, to denote the conditional probability of observing a context word given a center word. It is obtained by using the softmax function, $$ p(w_{context}|w_{center}) = \\frac{exp(W_{output_{(context)}} \\cdot h)}{\\sum&#94;V_{i=1}exp(W_{output_{(i)}} \\cdot h)} \\in \\mathbb{R}&#94;{1} \\tag{13} $$ where $W_{output_{(i)}}$ is the $i$-th row vector of size $1 \\times N$ from the output embedding matrix, $W_{output_{context}}$ is also a row vector of size $1 \\times N$ from the output embedding matrix corresponding to the context word. $V$ is the size of unique vocab in the corpus, and $h$ is the hidden (projection) layer of size ($N \\times 1$). The output is an $1 \\times 1$ scalar value of probability of range $[0, 1)$. This probability is computed $V$ times to obtain a conditional probability distribution of observing each unique vocabs in the corpus, given a center word. $$ \\left[ \\begin{array}{c} p(w_{1}|w_{center}) \\\\ p(w_{2}|w_{center}) \\\\ p(w_{3}|w_{center}) \\\\ \\vdots \\\\ p(w_{V}|w_{center}) \\end{array} \\right] = \\frac{exp(W_{output} \\cdot h)}{\\sum&#94;V_{i=1}exp(W_{output_{(i)}} \\cdot h)} \\in \\mathbb{R}&#94;{V}\\tag{14} $$ $W_{output}$ in the denominator of eq 13 has size $V \\times N$. Multiplying $W_{output}$ with $h$ of size $N \\times 1$ will yield a dot product vector of size $V \\times 1$. This dot product vector goes through the softmax function: Figure 10: softmax function transformation The exponentiation ensures that the transformed values are positive, and the normalization factor in the denominator ensures that the values have a range of $[0, 1)$. The result is a conditional probability distribution of observing each unique vocabs in the corpus, given a center word. Notes: Negative Sampling Softmax function in Skip-Gram has the following equation: $$ P = \\frac{exp(W_{output} \\cdot h)}{\\sum&#94;V_{i=1}exp(W_{output_{(i)}} \\cdot h)} \\in \\mathbb{R}&#94;{V}$$ There is an issue with softmax in Skip-Gram — it is computationally very expensive, as it requires scanning through the entire output embedding matrix ($W_{output}$) to compute the probability distribution of all $V$ words, where $V$ can be millions or more. Furtheremore, the normalization factor in the denominator also requires $V$ iterations. When implemented in codes, the normalization factor is computed only once and cached as a Python variable, making the alogrithm complexity = $O(V+V)\\approx O(V)$. Due to this computational inefficiency, softmax is not used in most implementaions of Skip-Gram . Instead we use an alternative called negative sampling with sigmoid function, which rephrases the problem into a set of independent binary classification task of algorithm complexity = $O(K+1)$, where $K$ typically has a range of $[5,20]$. Then, the new probability distribution is defined as: $$ P = \\frac{1}{1+exp(-(\\{c_{pos}\\} \\cup W_{neg}) \\cdot h)} \\in \\mathbb{R}&#94;{K+1}\\tag{7}$$ $K=20$ is used for small samples, and $K=5$ is used for big samples. Negative sampling is much cheaper than vanilla Skip-Gram with softmax, because $K$ is between 5 ~ 20, whereas $V$ can be millions. Moreover, no extra iterations are necessary to compute the normalization factor in the denominator, because sigmoid function is a binary regression classifier. The algorithm complexity of the probability distribution of vanilla Skip-Gram is $O(V)$, whereas negative sampling's is $O(K+1)$. This shows why negative sampling saves a significant amount of computational cost per iteration. In gensim , negative sampling is applied by default with Word2Vec(negative=5, ns_exponent=0.75) , where negative is the number of $K$-negative samples, and ns_exponent is a hyperparameter related to negative sampling, of range $(0, 1)$. The details of the methodology behind negative sampling deserves another fully devoted post, and as such, covered in a different post . Training: Backward Propagation Backward propagation involves computing prediction errors, and updating the weight matrix ($\\theta$) to optimize vector representation of words. Assuming stochastic gradient descent , we have the following general update equations for the weight matrix ($\\theta$): $$ \\theta_{new}=\\theta_{old}-\\eta\\cdot\\nabla_{J_{t}(\\theta;w&#94;{(t)}}) \\tag{15} $$ $\\eta$ is learning rate, $\\nabla_{J_{t}(\\theta;w&#94;{(t)})}$ is gradient for the weight matrix, and $J_{t}(\\theta;w&#94;{(t)})$ is the cost function defined in eq (6) . Since the $\\theta$ is a concatenation of input and output weight matrices ($[W_{input} \\quad W_{output}]$) as described above , there are two update equations for each embedding matrix: $$ W_{input}&#94;{(new)}=W_{input}&#94;{(old)}- \\eta \\cdot \\frac{\\partial J_{t}}{\\partial W_{input}} \\tag{16} $$ $$ W_{output}&#94;{(new)}=W_{output}&#94;{(old)}- \\eta \\cdot \\frac{\\partial J_{t}}{\\partial W_{output}} \\tag{17} $$ Mathematically, it can be shown that the gradients of $W_{input}$ $W_{output}$ have the following forms: $$ \\frac{\\partial J_{t}}{\\partial W_{input}} = x \\cdot (W_{output}&#94;T \\sum&#94;C_{c=1} e_c) \\tag{18}$$ $$ \\frac{\\partial J_{t}}{\\partial W_{output}} = h \\cdot \\sum&#94;C_{c=1} e_c \\tag{19}$$ The gradients can be substitued into eq (16) and eq (17) : $$ W_{input}&#94;{(new)}=W_{input}&#94;{(old)}- \\eta \\cdot x \\cdot (W_{output}&#94;T \\sum&#94;C_{c=1} e_c) \\tag{20} $$ $$ W_{output}&#94;{(new)}=W_{output}&#94;{(old)}- \\eta \\cdot h \\cdot \\sum&#94;C_{c=1} e_c \\tag{21} $$ $W_{input}$ is input weight matrix , $W_{output}$ is output weight matrix , $x$ is one-hot encoded input layer , $C$ is window size , and $e_{c}$ is prediction error for $c$-th context word in the window. Note that $h$ (hidden layer) is equivalent to $W_{input}&#94;T x$. Notes: Applying softmax Although eq (21) does not explicitly show it, softmax function is applied in the prediction error ($e_c$). Prediction error is the difference between the predicted and true probability ($y_{pred} - y_{true}$) as illustrated below . The predicted probability $y_{pred}$ is computed using softmax function using eq (13) . Prediction Error ($y_{pred} - y_{true}$) Skip-Gram model optimizes the weight matrix ($\\theta$) to reduce the prediction error. Prediction error is the difference between the probability distribution of words computed from the softmax output layer ($y_{pred}$) and the true probability distribution ($y_{true}$) of the $c$-th context word. Just like the input layer, $y_{true}$ is one-hot encoded vector, in which only one element in the vector that corresponds to the $c$-th context word is $1$, and the rest is all $0$. Figure 11: Prediction error window The figure has a window size of $2$, so two prediction errors were computed. Recall from the above notes about the window size that the original softmax regression classifier ( eq (10) ) has $K$ labels to classify, in which $K = V$ in NLP applications because there are $V$ words to classify. Employing window size transforms eq (10) into eq (11) and significantly reduces the algorithm complexity because the model only needs to compute prediction errors for $[1, 10]$ neighboring words, instead of computing all $V$-prediction errors for all vocabs that can be millions or more. Then, prediction errors for all $C$ context words are summed up to compute weight gradients to the update weight matrices. Figure 12: Sum of prediction errors Numerical Demonstration For the ease of illustration, screenshots from Excel will be used to demonstrate the concept of updating weight matrices through forward and backward propagations. Forward Propagation: Computing hiden (projection) layer Center word is \"passes\" . Window size is size=1 , making \"the\" and \"who\" context words. Hidden layer ($h$) is looked up from the input weight matrix. It is computed with eq (12) . Figure 13: Computing hidden (projection) layer **Forward Propagation: Softmax output layer** Output layer is a probability distribution of all words, given a center word. It is computed with eq (14) . Note that all context windows share the same output layer ($y_{pred}$). Only the errors ($e_c$) are different. Figure 14: Softmax output layer Backward Propagation: Sum of Prediction Errors $C$ different prediction errors are computed, then summed up. In this case, since we set window=1 above , only two errors are computed. Figure 15: Prediction errors of context words Backward Propagation: Computing $\\nabla W_{input}$ Gradients of input weight matrix ($\\frac{\\partial J_{t}}{\\partial W_{input}}$) are computed using eq (18) . Note that multiplying $W_{output}&#94;T \\sum&#94;C_{c=1} e_c$ with the one-hot-encoded input vector ($x$) makes the neural net to update only one word vector that corresponds to the input (center) word. Figure 16: Prediction errors of context words Backward Propagation: Computing $\\nabla W_{output}$ Gradients of output weight matrix ($\\frac{\\partial J_{t}}{\\partial W_{output}}$) are computed using eq (19) . Unlike the input weight matrix ($W_{input}$), all word vectors in the output weight matrix ($W_{output}$) are updated. Figure 17: Prediction errors of context words **Backward Propagation: Updating Weight matrices** Input and output weight matrices ($[W_{input} \\quad W_{output}]$) are updated using eq (20) and eq (21) . Figure 18: Updating $W_{input}$ Figure 19: Updating $W_{output}$ Note that for each iteration in the learning process, all weights in $W_{output}$ are updated, but only one row vector that corresponds to the center word is updated in $W_{input}$. When the model finishes updating both of the weight matrices, then one iteration is completed. The model then moves to the next iteration with the next center word. However, remember that this uses eq (8) as the cost function and assumes stochastic gradient descent . This means that one update is made for each training example. If eq (9) is used as a cost function instead (which is almost never the case), then one update is made for all $T$ training examples in the corpus.","tags":"Natural Language Processing","url":"https://aegis4048.github.io/demystifying_neural_network_in_skip_gram_language_modeling","loc":"https://aegis4048.github.io/demystifying_neural_network_in_skip_gram_language_modeling"},{"title":"Understanding Multi-Dimensionality in Vector Space Modeling","text":"The code snippet assumes Anaconda 5.2.0 version of Python virtual environment Acknowledgement Some materials on this post are from CS224n: Natural Language Processing with Deep Learning at the Stanford University. Check out the YouTube Lecture on Word Vectors and Word Senses taught by Dr. Chris Manning . One of the critical components in Natural Langauge Processing (NLP) is to encode text information in a numerical format that can be fed into an NLP model. Such technique, representing words in a numerical vector space, is called Vector Space Modeling . It is often synonymous to word embedding . A typical vector space model that haven't went through dimensional reduction has a dimension of $V \\times N$, where $V$ is a size of unique vocabulary, and $N$ varies based on the choice of modeling method (Notes: in document-to-word embedding model like TF-IDF, $V$ is a number of documents and and $N$ is a size of unique vocabulary). In this context, $V$ is basically a sample data size — the larger the $V$, the bigger the training data set. It is always good to have more training data than the otherwise. Therefore, our prime interest lies within the size of $N$, which affects the multi-dimensionality of a vector space model. Here is a simple illustration of how words look like in a high dimensional vector space. Figure 1: simple 10-dimensional word vectors Consider the two semantically similar words, \"Success\" and \"Achieve\". When converted into 10-dimensional word vectors using a vector space model of one's choice (Ex: Word2Vec), each word is a $1 \\times 10$ vector where each value in a vector represent the word's position in a 10D space. When projected on this high dimensional vector space, the similarity between words can be quantified by evaluating the similarity between these two word vectors. Observe that the vectors in the illustration above looks similar to each other: positions of non-zero values, and values of each cell. Similar word vectors will put similar words close to each other in a vector space, and as a result, \"Success\" and \"Achieve\" will have small Euclidean or Cosine Distance. One might experience difficulty in trying to visualize the Euclidean or Cosine distance of the word vectors in a 10D vector space. In fact, you can't visualize anything bigger then 3D. If one attempts to visualize the word vectors in a 2D or 3D space, he will have to represent the word vectors in 2D or 3D space first using dimensional reduction. Let's assume that such dimensional reduction was performed and the word vectors for \"Success\" and \"Achieve\" are reduced to 3D vectors. The word vectors will then look like this: Figure 2: Dimensional-reduced word vectors visualization in 3D Observe the dissimilarity between two word vectors and their positions within the 3D vector space. This is because 3 dimensions are not enough to capture all relationship among words and as a result fails to maintain the semantic relationship between two similar words, \"Success\" and \"Achieve\". Multi-dimensionality in vector space modeling has great significance because it directly affects the performance of any NLP model. In this post, the concept and effect of multi-dimensionality in NLP will be illustrated using mainly Co-Occurence Matrix and some Word2Vec models. Review on Vector Space Model Techniques Before we talk about the significance of the size of dimensions ($N$), let us review how text information is transformed into a numerical matrix. Please feel free to skip this part if you are already knowledgable about this topic. There are two types of methods for word embedding: Frequency-Based Methods and Prediction-Based Methods . The below table lists some options we have for each type of embedding method. Frequency-Based Methods Prediction-Based Methods Count Vector Continuous Bag of words TF-IDF Doc2Vec Co-Occurence Matrix Word2Vec Frequency-based methods are pretty straightforward to understand. It counts how many times each word appeared in each document, or how many times each word appeared together with each words. Co-Occurence Matrix is a type of frequency-based methods. Co-Occurence Matrix The value of $N$ for co-occurence matrix is the size of unique vocabulary. In the other words, co-occurence matrix is a square matrix of size $V \\times V$. Consider a co-occurence matrix with a fixed window size of $n=1$. Setting window size $n=1$ will tell the model to search adjacent context words that are positioned directly left or right of a center word. The matrix is contructed using the following two input documents: Document 1: \"all that glitters is not gold\" Document 2: \"all is well that ends well\" * START all that glitters is not gold well ends END START 0 2 0 0 0 0 0 0 0 0 all 2 0 1 0 1 0 0 0 0 0 that 0 1 0 1 0 0 0 1 1 0 glitters 0 0 1 0 1 0 0 0 0 0 is 0 1 0 1 0 1 0 1 0 0 not 0 0 0 0 1 0 1 0 0 0 gold 0 0 0 0 0 1 0 0 0 1 well 0 0 1 0 1 0 0 0 1 1 ends 0 0 1 0 0 0 0 1 0 0 END 0 0 0 0 0 0 1 1 0 0 Table 1: Co-Occurence Matrix Notes: START and END tokens In NLP, we often add START and END tokens to represent the beginning and end of sentences, paragraphs or documents. In thise case we imagine START and END tokens encapsulating each document, e.g., \"START All that glitters is not gold END\", and include these tokens in our co-occurrence counts. This co-occurence matrix is essentially a vector space model of $V$-dimensional ($V$ columns) matrix, in which $V = 10$. However, in most NLP tasks, this co-occurence matrix goes through PCA or SVD for dimensional reduction and decomposed into a new $k$-dimensional matrix. * $k_{1}$ $k_{2}$ START 0.705 0.484 all 0.705 -0.484 that 0.654 -0.783 glitters 0.52 0 is 1.027 0 not 0.654 0.783 gold 0.382 0.656 well 0.382 -0.656 ends 1.394 -1.061 END 1.394 1.061 Table 2: Dimension Reduced Co-Occurence Matrix The original matrix was 10D matrix (10 columns) — this can't be visualized. Humans can understand only up to 3D visualizations. However, dimensional reduction was performed with sklearn.decomposition.TruncatedSVD(n_components=2) , and the output table yielded a new matrix with reduced dimension of $k = 2$. This was because I set n_components = 2 . The word vectors can now be visualized in a 2D space. Further discussions about the choice of n_components and dimensional reduction will be followed in the later section of this post. Figure 3: Dimensional-reduced word vectors visualization in 2D Word2Vec Contrary to frequency-based methods, prediction-based methods are more difficult to understand. As the name 'prediction' implies, their methodologies are based on predicting context words given a center word ( Word2Vec Skip-Gram: $P(w_{context} \\mid w_{center})$), or a center word given context words ( Continuous Bag of Words: $P(w_{center} \\mid w_{context})$). Prediction-based methods use neural network algorithm, which means that we have to worry about the number of neurons (weights) in a network. In Word2Vec model, the model matrix has a dimension of $V \\times N$, where $V$ is the size of unique vocabulary and the size of $N$ is the number of neurons in a network. Figure 3: Skip-Gram algorithm structure for Word2Vec During the forward and back propagation process, the weights in Matrix $W$ ( Embedding matrix ) of size $V \\times N$ and Matrix $W'$ ( Context matrix ) of size $N \\times V$ are optimized to minimize a loss function. Recall that the number of neurons ($N$) is a hyper-parameter that needs to be empirically optimized. Choosing different values for $N$ will yield different output performances. $N = 300$ is a dimensional parameter known to work well with Word2Vec models. Note that a matrix with a dimension of 300 cannot be visualized. However, the dimension can be reduced down to 2D or 3D using t-distributed stochastic neighbor embedding (t-SNE) , or PCA. For NLP visualization purpose, T-SNE is often preferred over PCA or SVD due to its ability to reduce high dimensions to low dimensions while capturing complex relationships with neighboring wods. More comparison about PCA vs T-SNE will be illustrated later. Notes: Word2Vec algorithm The theory behind Word2Vec skip-gram algorithm maybe complex and difficult to understand for beginners. I encourage you to read the following two and articles that explain the theory. They do it quite well. Why is Multi-Dimensionality Important? First , high dimensionality leads to high computational cost. This is especially true in the case of co-occurence matrix, in which it has a dimension of $V \\times V$, where $V$ is the size of vocabulary in a corpus. The previous example shown in Table 1 had a corpus size of 12 words, and vocab size of 10 words. In real-life applications, corpus size easily exceeds 10's or 100 GB's. For example, Gensim 's pre-trained Word2Vec model trained from Google News had a vocab size of three million ( Github Source ). If we obtain a co-occurence matrix and feed it into an NLP model without dimensional reduction, we will be training our model with a matrix size of $3M \\times 3M$. This is unrealistic. We need to choose an optimal value for reduced dimension $k$ that will best describe the variability of the data while significantly cutting down the computational cost. On the other hand, computational cost due to high dimensionality is NOT a big concern with Word2Vec, because the most optimal dimension for Word2Vec are already known to be between 200 - 600 (Note: this doesn't mean that Word2Vec is superior to co-occurence matrix. Each has its own pros and cons). It is natural to think that high dimension would lead to higher accuracy, but this happens at the cost of increased computation time. One needs to find a \"sweet spot\" that optimizes the trade-off between accuracy vs. computation time. The recent paper submitted in Dec 2018 proposed a deterministic way to compute the optimal number of $k$-dimensions. It's code implementation is available on this Github repo . Second , dimension is a critical parameter in word embedding. Too low dimension leads to underfitting and makes your model not expressive enough to capture all possible word relations. On the other hand, too high dimension results in overfitting. However, in the recent paper , it was discovered that Word2Vec and GloVe models are not sensitive to overfitting. Figure 4: skip-gram Word2Vec: over-parametrization does not significantly hurt performance Figure 5: GloVe: over-parametrization does not significantly hurt performance The spikes of the skip gram Word2Vec and and GloVe models revealed the existence of the \"sweet spot\" in which a certain number of dimensions leads to the highest accuracy. The sharp rise in accuracy in the far left zone showed that too low number of dimensions results in underfitting. However, it is interesting to note that the flat right tail of the charts showed that indefinitely increasing the number of dimensions did not really result in overfitting. Often times in neural net applications, too high number of dimensions (neurons) results in overfitting, but in skip-gram Word2Vec and GloVe, this was not the case. A typical good NLP model trained on a wide variety of corpora has a dimension in the order of hundreds. The famous Mikolov et al. 2013 paper on skip-gram Word2Vec model suggests 300 neurons, and Rohde et al. ms., 2005 paper on co-occurence/correlation matrix suggests 600-800 dimensions (columns) as the optimum parameters for vector space modeling. Vector Space Modeling: Crude Oil News Figure 6: International news organization, Reuters Enough of theories. Now let's dive into the actual application of vector space modeling with Python code implementation. We will explore multi-dimensionality in NLP task using Reuters news articles on crude oil . Take a glance at the overall content of the crude oil articles with the following WordCloud visualizaiton. Figure 7: WordCloud of crude oil articles Imports Here is all the imports you need to follow with this tutorial. In [2]: import pandas as pd import numpy as np import nltk nltk . download ( 'reuters' ) from nltk.corpus import reuters from sklearn.decomposition import TruncatedSVD import matplotlib.pyplot as plt from mpl_toolkits.mplot3d import Axes3D % matplotlib notebook [nltk_data] Downloading package reuters to [nltk_data] C:\\Users\\EricKim\\AppData\\Roaming\\nltk_data... [nltk_data] Package reuters is already up-to-date! Sample Data Description First, you will have to download the articles. NLTK provides a nice api that allows us to download the digitized articles. In [3]: import nltk nltk . download ( 'reuters' ) from nltk.corpus import reuters [nltk_data] Downloading package reuters to [nltk_data] C:\\Users\\EricKim\\AppData\\Roaming\\nltk_data... [nltk_data] Package reuters is already up-to-date! In [4]: data = [] for fileid in reuters . fileids (): category = reuters . categories ( fileid ) text = reuters . raw ( fileid ) data . append ([ fileid , category , text ]) df_reuters = pd . DataFrame ( data , columns = [ 'File ID' , 'Category' , 'Text' ]) In [5]: df_reuters . head ( 10 ) Out[5]: File ID Category Text 0 test/14826 [trade] ASIAN EXPORTERS FEAR DAMAGE FROM U.S.-JAPAN RI... 1 test/14828 [grain] CHINA DAILY SAYS VERMIN EAT 7-12 PCT GRAIN STO... 2 test/14829 [crude, nat-gas] JAPAN TO REVISE LONG-TERM ENERGY DEMAND DOWNWA... 3 test/14832 [corn, grain, rice, rubber, sugar, tin, trade] THAI TRADE DEFICIT WIDENS IN FIRST QUARTER\\n ... 4 test/14833 [palm-oil, veg-oil] INDONESIA SEES CPO PRICE RISING SHARPLY\\n Ind... 5 test/14839 [ship] AUSTRALIAN FOREIGN SHIP BAN ENDS BUT NSW PORTS... 6 test/14840 [coffee, lumber, palm-oil, rubber, veg-oil] INDONESIAN COMMODITY EXCHANGE MAY EXPAND\\n Th... 7 test/14841 [grain, wheat] SRI LANKA GETS USDA APPROVAL FOR WHEAT PRICE\\n... 8 test/14842 [gold] WESTERN MINING TO OPEN NEW GOLD MINE IN AUSTRA... 9 test/14843 [acq] SUMITOMO BANK AIMS AT QUICK RECOVERY FROM MERG... In [6]: len ( df_reuters ) Out[6]: 10788 There are a total of 10,788 articles. To our convinience, the articles are already labeled with their respective categories. This helps us in a case we want to perform further analysis and run supervised learning. But we won't be running any supervised learning in this post. But instead, we will extract only the articles that are in the category of crude . In the other words, only the articles that talk about crude oil. In [7]: df_crude = df_reuters [ df_reuters [ 'Category' ] . apply ( lambda x : 'crude' in x )] df_crude . head ( 10 ) Out[7]: File ID Category Text 2 test/14829 [crude, nat-gas] JAPAN TO REVISE LONG-TERM ENERGY DEMAND DOWNWA... 123 test/15063 [acq, crude, earn, pet-chem] ENERGY/U.S. PETROCHEMICAL INDUSTRY\\n Cheap oi... 187 test/15200 [crude] TURKEY CALLS FOR DIALOGUE TO SOLVE DISPUTE\\n ... 205 test/15230 [crude] IRAQI TROOPS REPORTED PUSHING BACK IRANIANS\\n ... 209 test/15238 [crude, earn] UNION TEXAS OIL RESERVES DROPPED IN 1986\\n Un... 214 test/15244 [crude] GHANA TO BUY CRUDE OIL FROM IRAN\\n Ghana will... 257 test/15322 [crude, nat-gas] U.S.SENATE LIFTS SOME BANS ON NATURAL GAS\\n T... 266 test/15339 [crude, gas] EIA SAYS DISTILLATE STOCKS UNCHANGED, GASOLINE... 268 test/15344 [crude, gas] EIA SAYS DISTILLATE STOCKS UNCHANGED IN WEEK\\n... 272 test/15351 [crude, gas] RECENT U.S. OIL DEMAND OFF 2.6 PCT FROM YEAR A... In [8]: df_crude . shape Out[8]: (578, 3) There are 578 articles that talk about crude oil. Constructing Co-Occurence Matrix We will build co-occurence matrix that looks like Table 1 above. Before we build the matrix, we need to consider tokenization and window size . Tokenization Tokenization is a process of splitting sentences into separate words. Tokenization usually isn't as simple as splitting senteces based on space bars. It is usually a very complicated process that involves heavy regex manipulation, lemmatization, stemming, and some hard-coding with different rules for different cases. For example, converting 'unchanged' to 'unchange', 'stocks' to 'stock', 'pushing' to 'push', 'is, was, being', to 'be'. Or treating multi-words as one word, such as 'in spite of', 'no matter of', 'Panda Express', 'North America', or 'beat down'. Or dealing with punctuations, such as 'Parskin-disease', 'one-to-one' or 'O'reilly'. And many more. Tokenization won't be covered in detail in this post, because complex tokenization is not necessary for the purpose of explaining multi-dimensinoality in NLP. However, please do note that you will need advanced tokenization scheme in real-life applications. On the other hand, NLTK has a convinient package that allows you to check if a word is within the corpora of English. However, this does not remove punctuations. The basic use case looks like this: >>> from nltk.corpus import words >>> \"fine\" in words.words() True We will split the sentences using nltk.corpus.reuters.words() . In [52]: def read_corpus ( category = '' ): files = reuters . fileids ( category ) return [[ 'START' ] + [ w . lower () for w in list ( reuters . words ( f ))] + [ 'END' ] for f in files ] In [53]: corpus_crude = read_corpus ( category = [ 'crude' ]) In [54]: pd . DataFrame ( corpus_crude ) . head ( 10 ) Out[54]: 0 1 2 3 4 5 6 7 8 9 ... 991 992 993 994 995 996 997 998 999 1000 0 START japan to revise long - term energy demand downwards ... None None None None None None None None None None 1 START energy / u . s . petrochemical industry cheap ... . economy continues its modest rate of growth . END 2 START turkey calls for dialogue to solve dispute turkey said ... None None None None None None None None None None 3 START iraqi troops reported pushing back iranians iraq said today ... None None None None None None None None None None 4 START union texas oil reserves dropped in 1986 union texas ... None None None None None None None None None None 5 START ghana to buy crude oil from iran ghana will ... None None None None None None None None None None 6 START u . s . senate lifts some bans on ... None None None None None None None None None None 7 START eia says distillate stocks unchanged , gasoline off 200 ... None None None None None None None None None None 8 START eia says distillate stocks unchanged in week distillate fuel ... None None None None None None None None None None 9 START recent u . s . oil demand off 2 ... None None None None None None None None None None 10 rows × 1001 columns Choice of Window Size On of the critical variable in co-occurence matrix is window_size . The below illustration has a window size of 2. Figure 8: Illustration of window size A typical window size is chosen between 2-10. However, window size may exceed way over 10 in case of special circumstances, such as having too few data. Increasing window size may increase the accuracy of the model, but it comes at the price of computational cost, and sometimes loss in accuracy due to noise. The author of this paper claims that he observed decrease in model performance of his co-occurrence matrix when he used high window size with large corpus. However, he observed the opposite when high window size was used with smaller corpus. Figure 9: Effect of window size and corpus size Matrix Construction In [55]: def compute_co_occurrence_matrix ( corpus , window_size = 4 ): distinct_words = sorted ( list ( set ([ word for sentence in corpus for word in sentence ]))) num_words = len ( distinct_words ) word2Ind = { word : index for index , word in enumerate ( distinct_words )} M = np . zeros (( num_words , num_words )) for sentence in corpus : for i , word in enumerate ( sentence ): begin = max ( i - window_size , 0 ) end = min ( i + window_size , num_words ) context = sentence [ begin : end + 1 ] context . remove ( sentence [ i ]) current_row = word2Ind [ word ] for token in context : current_col = word2Ind [ token ] M [ current_row , current_col ] += 1 return M , word2Ind In [56]: reuters_corpus = read_corpus ( 'crude' ) M_co_occurrence , word2Ind_co_occurrence = compute_co_occurrence_matrix ( reuters_corpus , window_size = 5 ) In [57]: pd . DataFrame ( M_co_occurrence , index = word2Ind_co_occurrence . keys (), columns = word2Ind_co_occurrence . keys ()) . head ( 10 ) Out[57]: \" \"( \", \"... $ & ' ( ) )\" ... zinc zoete zollinger zone zones zubedei zubeidi zuheir zulia zverev \" 88.0 0.0 1.0 0.0 1.0 2.0 78.0 17.0 7.0 0.0 ... 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 \"( 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 ... 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 \", 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 ... 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 \"... 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 ... 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 $ 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 ... 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 & 2.0 0.0 0.0 0.0 0.0 12.0 31.0 8.0 4.0 0.0 ... 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 ' 78.0 0.0 0.0 0.0 0.0 31.0 22.0 8.0 5.0 0.0 ... 1.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 ( 17.0 0.0 0.0 0.0 0.0 8.0 8.0 6.0 220.0 1.0 ... 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 ) 7.0 1.0 0.0 0.0 0.0 4.0 5.0 220.0 2.0 0.0 ... 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 )\" 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 ... 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 10 rows × 8185 columns Notes: Effect of punctuatoins Be cautious when dealing with punctuations. Although the effect of punctuations in NLP is beyond the scope of this post, please note that punctuations often have their own roles to play in NLP applications. Ex: Parkinson-disease vs Parkinson's disease. Sometimes they may be simply removed, and sometimes they shouldn't be. Dimensional Reduction Dimensional reduction is necessary in most of the machine learning problems. It is used to reduce computational cost of optimization model, and to mitigate the issue of correlated features. For example, column 1 and column 2 are independent, but column 2, 3 and 4 may be correlated with one another by some relationship like: $$col_2 = log{(col_3 + col_4 ) / 2}$$ This kind of correlationship may not be visible to an engineer, and may cause problems when left unmitigated. Dimensional reduction is especially necessary in the case of co-occurence matrix due to the size of the matrix, as it was mentioned above . Recall that co-occurence matrix has a dimension of $V \\times V$, where $V$ is the size of unique vocabulary. Often times vocabulary size easily exceeds tens of thousand, and it is unrealistic to run a machine learning model on such a matrix due to time constraint. Moreover, co-occurrence matrix is a very sparse matrix. Take a look at the above Pandas DataFrame of the co-occurrence matrix. You will notice that most of the values are zero, which doesn't add much value to the matrix. Dimensional reduction will make the matrix more compact to convey \"most\" of the \"useful\" information. sklearn.decomposition.TruncatedSVD will be used for dimensional reduction. Notes: Two algorithms of TruncatedSVD TruncatedSVD has two options of algorithms. \"arpack\" and \"randomized\" . \"arpack\" has an algorithm complexity of $O(m \\times n \\times k)$, and \"randomized\" has $O(m \\times n \\times log(k))$. In theory, \"arpack\" is supposed to be better in terms of accuracy, but \"randomized\" is much faster by an order of magnitude, and claimed to be as nearly accurate as \"arpack\" . The default of Scipy implementation is \"randomized\" . In [15]: def reduce_to_k_dim ( M , n_components = 2 ): svd = TruncatedSVD ( n_components = n_components , n_iter = 10 , random_state = 42 ) M_reduced = svd . fit_transform ( M_co_occurrence ) print ( 'n_components =' , n_components ) print ( 'Explained Variance =' , round ( svd . explained_variance_ratio_ . sum (), 3 )) return M_reduced In [16]: reuters_corpus = read_corpus ( 'crude' ) M_co_occurrence , word2Ind_co_occurrence = compute_co_occurrence_matrix ( reuters_corpus , window_size = 5 ) M_reduced_2 = reduce_to_k_dim ( M_co_occurrence , n_components = 2 ) n_components = 2 Explained Variance = 0.906 In [17]: pd . DataFrame ( M_reduced_2 , columns = [ '$k_ {1} $' , '$k_ {2} $' ]) . head ( 10 ) Out[17]: $k_{1}$ $k_{2}$ 0 861.598711 -92.837976 1 1.523025 -0.235919 2 0.299002 0.054687 3 0.895219 -0.383224 4 1.626508 -0.125964 5 158.617460 -26.523809 6 975.388537 250.237303 7 211.345647 30.294191 8 162.468186 -12.582009 9 1.272724 -0.197958 The original $8185 \\times 8185$ co-occurence matrix was reduced into 2-dimensional matrix of $8185 \\times 2$, using dimensional reduction with n_components=2 . According to the explained_variance_ratio_ , the new reduced matrix captures 90.6% variability of the original data with window_size=5 . This may not be satisfactory, but good enough to obtain a reasonable word vector visualizaiton on a 2D space. We can also try different values of n_components . In [18]: M_reduced_3 = reduce_to_k_dim ( M_co_occurrence , n_components = 3 ) n_components = 3 Explained Variance = 0.923 In [19]: M_reduced_10 = reduce_to_k_dim ( M_co_occurrence , n_components = 10 ) n_components = 10 Explained Variance = 0.972 In [20]: M_reduced_50 = reduce_to_k_dim ( M_co_occurrence , n_components = 50 ) n_components = 50 Explained Variance = 0.993 In [21]: M_reduced_100 = reduce_to_k_dim ( M_co_occurrence , n_components = 100 ) n_components = 100 Explained Variance = 0.996 Notes: explained_variance_ratio_ Performing dimensional reduction inevitably results in loss of some data in the original matrix. It is a natural phenomenon considering how we are reducing $8185 \\times 8185$ matrix all the way down to $8185 \\times 2$ matrix. We can quantify the loss in data using explained_variance_ratio_ . I won't cover the details of this property, but long story short, it's good if explained_variance_ratio_ is close to 100%. Reducing the matrix to high $k$-dimension will result in smaller loss of data at the cost of high computational load, and vice versa. If you are interested in learning a deterministic way to decide the optimal value of $k$, take a look at this paper . Word Vectors Visualization The original word vector had 8185-dimension, but now its reduced down to 2-dimension and 3-dimension, which can be visualized on a 2D and 3D plane. Normalization Rescaling (normalization) needs to be done on rows to make each of them unit-length. Skipping this step will result in your visualization looking unbalanced. In [23]: # normalize M_lengths_2 = np . linalg . norm ( M_reduced_2 , axis = 1 ) M_normalized_2 = M_reduced_2 / M_lengths_2 [:, np . newaxis ] M_lengths_3 = np . linalg . norm ( M_reduced_3 , axis = 1 ) M_normalized_3 = M_reduced_3 / M_lengths_3 [:, np . newaxis ] Visualization In [24]: # Axes3D needs to be imported in case of plotting 3D visualizations from mpl_toolkits.mplot3d import Axes3D In [25]: def plot_embeddings ( M_reduced , word2Ind , words , ax ): dimension = M_reduced . shape [ 1 ] assert ( dimension == 3 or dimension == 2 ) for i , word in enumerate ( words ): index = word2Ind [ word ] embedding = M_reduced [ index ] if dimension == 3 : x , y , z = embedding [ 0 ], embedding [ 1 ], embedding [ 2 ] ax . scatter ( x , y , z , color = 'red' ) ax . text ( x , y , z , word ) else : x , y = embedding [ 0 ], embedding [ 1 ] ax . scatter ( x , y , marker = 'x' , color = 'red' ) ax . text ( x , y , word ) return fig , ax Choice of words to visualize Recall that there are 8185 unique vocabulary in our data. Visualizing all of them on a plot will won't be very informative, because readers won't be able to distinguish between words because they are too densely plotted. In [26]: # choose words to visualize words = [ 'bank' , 'barrels' , 'bpd' , 'ecuador' , 'energy' , 'industry' , 'oil' , 'petroleum' , 'output' , 'produce' , 'occidental' , 'mobil' , 'exxon' , 'electricity' , 'kilowatt' , 'china' , 'paris' , 'saudi' , 'norway' , 'blockading' , 'expert' , 'yen' , 'kuwaiti' , 'kuwait' , 'persian' , 'eia' , 'gulf' , 'bp' , 'uk' , 'gas' , 'europe' , 'allocated' , 'lacks' , 'militarily' , 'discouraged' , 'violations' , 'possibly' ] In [27]: fig = plt . figure () fig . suptitle ( 'Word Vector Visualizations' ) # First subplot ax1 = fig . add_subplot ( 1 , 2 , 1 ) ax1 = plot_embeddings ( M_normalized_2 , word2Ind_co_occurrence , words , ax1 ) # Second subplot ax2 = fig . add_subplot ( 1 , 2 , 2 , projection = '3d' ) ax2 = plot_embeddings ( M_normalized_3 , word2Ind_co_occurrence , words , ax2 ) Dimensionality Analysis on Visualizations 3D visualizations can be viewed from different angles. What will they look like in different angles? Let's take a look. Figure 10: 3D plot - angle 1 Figure 11: 3D plot - angle 2 Figure 12: 3D plot - angle 3 Interesting Observations: First , when viewed from a certain angle, the 3D plot looks exactly like the 2D plot. However, different patterns can be observed when viewed from a different angle. Second , the distance of \"violations\" and \"discouraged\" are different. They are close to each other on Figure 10 , but far away from each other on Figure 11 & 12 . Third , similar pattern was found between \"kilowatt\" and \"electricity\" . As you know, \"kilowatt\" is an unit of \"electricity\" . That is why they are close on Figure 11 & 12 , but are separated in Figure 10 . Observe how \"electricity\" is close, but \"kilowatt\" is not to the cluster of oil-producing entities in Figure 10 . A possible explanation is that \"electricity\" is not oil, but similar to oil in a sense that it is a \"type\" of energy consumed. On the other hand, although \"kilowatt\" is a unit of energy, but it is not a type of energy. Fourth , \"barrels\" and \" bpd \" are always close to one another. They seem very close on all three plots shown above, and indeed they look close on other angles as well. Due to time and space constraint, I can't take a screenshot of every possible angles and put it here, but I observed that they are always close on any angles. This makes sense, considering \"bpd\" stands for \"barrels per day\". Fifth , \"paris\" was group together with \"barrels\" and \"bpd\" . They have nothing in common, and yet they were grouped together. The model failed to project \"paris\" on a right vector space. Sixth , the co-occurrence vector space model was able to capture the cluster of oil producers. \"occidental\" is for Occidental Petroleum Corporation (or Oxy), and \"exxon\" is for Exxon Mobil, the two major oil-producing companies in the US. And \"gulf\" for the Gulf of Mexico, one of the biggest offshore oil-producing region in the world. Seventh , \"eia\" was not inside the cluster of oil producers in most of the angles. This is noteworthy because EIA (U.S. Energy Information Administration) provides energy statistics, such as oil & gas production, consumption, export and export rate. However, it is not a \"producing\" entity. The model was able to differentiate between an entity who \"talks\" about the oil-production, and the entities who actually \"produce\" oil. Eighth , the model was not able to differentiate nations (Kuwait, Norway, China, UK) from the other entities. A well trained Word2Vec model with 300-dimensions is able to distinguish among nations, cities, and companies, but this one couldn't. Warning! Previously I said that a good co-occurrence matrix model has a dimension of 600-800. However, the explained_variance_ratio_ was 92.3%, which is reasonably good, and the 3D visualizations seemed to capture the relationship among words pretty well. This happend because: 1. the purpose of this post was to explain dimensionality If you attempt to feed the co-occurrence matrix of 3-dimensions into a machine learning model, it will NOT perform well. It \"seems\" to work well because it was used only for visualizations. 2. the data had limited scope of topics: crude oil Having limited scope of topics really simplfies the complexity of problem. If the training data had all sorts of articles (Ex: toiles, water bottles, nuclear, desk, paper, insects, milk, gas, school, trash), this will not work well. Conclusion It is known that a well trained Word2Vec model has 300 dimensions. Think of each dimension in Word2Vec vector space as an entity that represents word relationships: man vs woman, objects vs living, humans vs animals, nation vs city, drink vs food, action vs concept, and many more. Assume that that there are 300 of these kinds of relationships among all words in this world, represented by a single dimension per relationship. The 3D visualization from the co-occurrence matrix model was able to capture the cluster of oil-producing entities ( Sixth observation). But it wasn't able to differentiate between nations and companies, because it simply didn't have a dimension that captures that kind of relationship ( Eighth observation). In Figure 10 , the model sensed the negative feelings that \"discouraged\" and \"violations\" conveyed, and put them in a close vector space ( Second observation). But when viewed from a different angle, or to put it better, viewed from a different dimension, the model put them far apart because they do not convey similar meanings ( Figure 11 & 12 ). On the other hand, the model completely failed to distinguish between \"paris\" vs \"barrels\" + \"bpd\" ( Fifth observation). Recall that our explained_variance_ratio_ for 3-dimension was 92.3% above . Some information about \"paris\" could've been lost during dimensional reduction and that might have caused the error in word vector projection on a 3D vector space. Or, we simply did not have sufficient training data. Having more dimensions allows a model to capture more complex relationships among words, and that's precisly why Google's Word2Vec model had 300 dimensions.","tags":"Natural Language Processing","url":"https://aegis4048.github.io/understanding_multi-dimensionality_in_vector_space_modeling","loc":"https://aegis4048.github.io/understanding_multi-dimensionality_in_vector_space_modeling"},{"title":"Transforming Non-Normal Distribution to Normal Distribution","text":"In the field of statistics, the assumption of normality is important because many statistical techniques perform calculations assuming that the data is normally distributed. The techniques that assume Gaussian or Gaussian-like distribution are listed below: Techniques That Assume Normality Hypothesis testing through t-test and z-test Analysis of variance (ANOVA) Sequential Gaussian simulation in spatial analysis Control limits in control chart Unfortunately, many real-life data are not normal. Permeability distribution of rock samples is lognormal. Time required to repair a malfunctioning component follows exponential distribution, and reliability analysis for machine performance with respect to time follows Weibull distribution. What should you do if your data fails a normality test, or is not Gaussian-like? You have three options: Use it as it is or fit non-normal distribution Try non-parametric method Transform the data into normal distribution 1. Use it as it is or fit non-normal distribution Altough your data is known to follow normal distribution, it is possible that your data does not look normal when plotted, because there are too few samples. For example, test scores of college students follow a normal distribution. If you know for certain that your data is normally distributed by nature, then according to the Central Limit Theorem, your data will eventually become normal when you obtain a greater number of sample data. This means that you can still use the famous standard deviation method to assign letter grades to students ( figure 1 ), even if your students' test scores do not look normally distributed. If you increase the number of students that takes your exam, the test score distribution will become more normal according to the Central Limit Theorem. Figure 1: assigning letter grades with standard deviation On the other hand, if you have plenty enough samples to represent the true population, you can fit different types of distributions to better describe your data. Different methods exist for different distributions and maybe you will be able to achieve your goal without using techniques that strictly require Gaussian distribution. The code snippet below fits three different distributions on the sample data: lognormal, normal, and Weibull distributions. Through a visual inspection, it can be observed that the sample data is the best represented by a lognormal distribution . Once we know that the sample data follows lognormal distribution, we can move forward by employing techniques that assume lognormal distribution. In [12]: import numpy as np import matplotlib.pyplot as plt from scipy import stats % matplotlib notebook In [4]: # sample data generation np . random . seed ( 42 ) data = sorted ( stats . lognorm . rvs ( s = 0.5 , loc = 1 , scale = 1000 , size = 1000 )) # fit lognormal distribution shape , loc , scale = stats . lognorm . fit ( data , loc = 0 ) pdf_lognorm = stats . lognorm . pdf ( data , shape , loc , scale ) # fit normal distribution mean , std = stats . norm . fit ( data , loc = 0 ) pdf_norm = stats . norm . pdf ( data , mean , std ) # fit weibull distribution shape , loc , scale = stats . weibull_min . fit ( data , loc = 0 ) pdf_weibull_min = stats . weibull_min . pdf ( data , shape , loc , scale ) In [5]: # visualize fig , ax = plt . subplots ( figsize = ( 8 , 4 )) ax . hist ( data , bins = 'auto' , density = True ) ax . plot ( data , pdf_lognorm , label = 'lognorm' ) ax . plot ( data , pdf_norm , label = 'normal' ) ax . plot ( data , pdf_weibull_min , label = 'Weibull_Min' ) ax . set_xlabel ( 'X values' ) ax . set_ylabel ( 'probability' ) ax . legend (); Notes Visual inspection is one option to assess the performance of the fitted distributions. The other option is to use hypothesis testing with Q-Q plots to numerically assess the performance of the fitted distribution. For example, if you want to numerically assess how well your data matches Gaussian distribution, you can test your hypothesis through D'Agostino-Pearson normality test, Anderson-Darling Test, or Shapiro-Wilk Test. In this post, normality test through scipy.stats.normaltest() will be covered. 2. Try non-parametric method There are pros and cons for using non-parametric methods. The biggest pros is that it does not assume anything about the distribution. They are distribution-free . You do not need to know distribution shape, mean, standard devation, skewness, kurtosis, etc... All you need is just a set of sample data that is representative of a population. The fact that it does not assume anything about the distribution has another implication when you have small number of data - there's no need for Central Limit Theorem to be applied. Recall that the Central Limit Theorem states that the data will become more and more Gaussian-distributed as the number of samples increases. Techniques that assume normality of a distribution expect the sample data to follow Central Limit Theorem. Non-parametric methods improves the performance of statistical calculation when there are too few number of samples that the Central Limit Theorem can't be applied. However, it is important that those few samples are reasonably representative of the true population. If they are not, your result will be biased. Non-parametric methods are geared toward hypothesis testing rather than estimation. Disadvantages of non-parametric methods include lack of power compared to more traditional approaches that require prior knowledge of a distribution. If you knew the distribution of your data with 100% certainty, there is no reason to use a non-parametric method. Doing so would be a waste of perfectly good prior knowledge. Another disadvantage is that many non-parametric methods are computation intensive. For example, Boostrapping is a non-parametric alternative that requires numerical iterations to calculate confidence interval of statistics. On the other hand, if the assumption of normality holds valid for your data, you can directly compute the confidence interval by using a simple equation. Notes Parametric methods are the type of methods that assume a certain shape of a distribution. For example, the following equation is used to calculate the confidence interval of a mean of a distribution: CI of mean = sample mean $\\pm$ $($distribution score $\\times$ Standard Error $)$ The variable in the equation, distribution score , depends on the type of the distribution. If you do not know the distribution shape of your data, it is very difficult to obtain the value of the distribution score. On the other hand, non-parametric methods do not assume anything about a distribution. A non-parametric alternative to calculate confidencer interval of mean is to use Bootstrapping . The following table lists non-parametric alternatives to techniques that assume normality of a distribution: Techniques That Assume Normality Non-Parametric Alternatives Confidence Interval with z-test Bootstrapping T-test Mann-Whitney test; Mood's median test; Kruskal-Wallis test ANOVA Mood's median test; Kruskal-Willis test Paired t-test One-sample sign test F-test; Bartlett's test Levene's test Individuals control chart Run Chart 3. Transform the data into normal distribution The data is actually normally distributed, but it might need transformation to reveal its normality. For example, lognormal distribution becomes normal distribution after taking a log on it. The two plots below are plotted using the same data, just visualized in different x-axis scale. Observe how lognormal distribution looks normal when log is taken on the x-axis. In [6]: import numpy as np import matplotlib.pyplot as plt from scipy import stats % matplotlib notebook In [7]: # sample data generation np . random . seed ( 42 ) data = sorted ( stats . lognorm . rvs ( s = 0.5 , loc = 1 , scale = 1000 , size = 1000 )) # fit lognormal distribution shape , loc , scale = stats . lognorm . fit ( data , loc = 0 ) pdf_lognorm = stats . lognorm . pdf ( data , shape , loc , scale ) In [9]: # visualize fig , ( ax1 , ax2 ) = plt . subplots ( 1 , 2 , figsize = ( 8 , 4 )) ax1 . hist ( data , bins = 'auto' , density = True ) ax1 . plot ( data , pdf_lognorm ) ax1 . set_ylabel ( 'probability' ) ax1 . set_title ( 'Linear Scale' ) ax2 . hist ( data , bins = 'auto' , density = True ) ax2 . plot ( data , pdf_lognorm ) ax2 . set_xscale ( 'log' ) ax2 . set_title ( 'Log Scale' ); Similar transformations can be done on the sample data to convert non-normal to normal distribution. Lognormal transformation is used convert rock permeability distributions to normal distribution, and square root transformation is used to analyze biological population growth, such as bacterial colonies per petri dish. These types of transformations - rescaling the distribution by taking exponents or log - are called Power Transformations. Box-Cox transformation is the most popular technique within the family of power transformations. Box-Cox Transformation: Theory Box-Cox Transformation is a type of power transformation to convert non-normal data to normal data by raising the distribution to a power of lambda ($\\lambda$). The algorithm can automatically decide the lambda ($\\lambda$) parameter that best transforms the distribution into normal distribution. Box-Cox transformation is a statistical technique known to have remedial effects on highly skewed data. Essentially it's just raising the distribution to a power of lambda ($\\lambda$) to transform non-normal distribution into normal distribution. The lambda ($\\lambda$) parameter for Box-Cox has a range of -5 < $\\lambda$ < 5 . If the lambda ($\\lambda$) parameter is determined to be 2, then the distribution will be raised to a power of 2 — $Y&#94;2$. The exception to this rule is when the lambda ($\\lambda$) parameter is 0 - log will be taken to the distribution — log($Y$). The below table shows how Box-Cox transformation raises the power of a distribution to different lambda ($\\lambda$) values: Lambda ($\\lambda$) Transformed Distribution ($Y&#94;{'}$) -2 $Y&#94;{'} = \\frac{1}{Y&#94;2}$ -1 $Y&#94;{'} = \\frac{1}{Y&#94;1}$ -0.5 $Y&#94;{'} = \\frac{1}{sqrt(Y)}$ 0 $Y&#94;{'} = log(Y)$ 0.5 $Y&#94;{1} = sqrt(Y)$ 1 $Y&#94;{'} = Y$ 2 $Y&#94;{'} = Y&#94;2$ Although in the table lambda ($\\lambda$) values of only -2 < $\\lambda$ < 2 were displayed, the actual algorithm has a range of -5 < $\\lambda$ < 5 . Also note that using a lambda ($\\lambda$) value of 1 does not do anything to the distribution. If the Box-Cox algorithm spits out $\\lambda = 1$, it probably means that your data is Gaussian-like or Gaussian enough to an extent that there is no need for transformation. All data to be positive and greater than 0 (Y > 0) Box-Cox transformation does not work if data is smaller than 0. This can easily be fixed by adding a constant ($C$) that will make all your data greater than zero. The transformation equation is then: $Y&#94;{'} = (Y + C)&#94;{\\lambda}$ Python Code Implementation The code implementation for Box-Cox transformation is very simple with the help of scipy.stats.boxcox() . from scipy import stats xt, lmbda = stats.boxcox(x) xt is the transformed data, and lmbda is the lambda ($\\lambda$) parameter. More detailed usage & analysis of Box-Cox will be covered in the next section. Box-Cox Transformation: Phone Call Duration - Gamma Distribution The distribution for phone call duration follows Erlang distribution, a member of a family of Gamma distribution. When the shape parameter of Gamma distribution has an integer value, the distribution is the Erlang disribution. Since power transformation is known to work well with Gamma distribution, we can try Box-Cox transformation to turn non-normal data into normal data. The below code snippet demonstrates how a typical Gamma distribution looks like when plotted: In [10]: from scipy import stats import matplotlib.pyplot as plt import numpy as np % matplotlib notebook In [11]: # random variable generation for gamma distribution def generate_gamma_dist ( shape ): dist_gamma = sorted ( stats . gamma . rvs ( shape , loc = 0 , scale = 1000 , size = 5000 )) shape , loc , scale = stats . gamma . fit ( dist_gamma , loc = 0 ) pdf_gamma = stats . gamma . pdf ( dist_gamma , shape , loc , scale ) return dist_gamma , pdf_gamma In [12]: # visualize fig , ax = plt . subplots ( figsize = ( 8 , 4 )) for i in range ( 1 , 5 ): x , y = generate_gamma_dist ( i ) ax . plot ( x , y , label = 'shape parameter = %s ' % i ) ax . set_xlabel ( 'X values' ) ax . set_ylabel ( 'probability' ) ax . set_ylim ( 0 , 0.0004 ) ax . set_xlim ( 0 , 10000 ) ax . set_title ( 'Gamma (Erlang) Distribution' ) ax . legend (); 1. Data Preparation 1.1. Sample Data Description We will use phone calls data from Enigma Public . Enigma Public is a website that processes & hosts various public data and allows people to obtain them through file downloads or API access. The sample data originally comes from the National Response Center (NRC). They receive phone calls from anyone witnessing an oil spill, chemical release or maritime security incident and record that data. For your convenience, I already downloaded the sample data and hosted it on this website. You can access the sample data directly by importing the file through requests . In [2]: import requests import io import pandas as pd In [3]: base_url = 'https://aegis4048.github.io/downloads/notebooks/sample_data/' filename = '08c32c03-9d88-42a9-b8a1-f493a644b919_NRCEventReporting-Calls-2010.csv' data = requests . get ( base_url + filename ) . content df = pd . read_csv ( io . StringIO ( data . decode ( 'utf-8' ))) df . head () Out[3]: seqnos date_time_received date_time_complete calltype responsible_company responsible_org_type responsible_city responsible_state responsible_zip source serialid 0 946479 2010-07-03T21:11:31+00:00 2010-07-03T21:19:57+00:00 INC NaN UNKNOWN NaN XX NaN TELEPHONE 15900 1 946480 2010-07-03T20:59:29+00:00 2010-07-03T21:16:22+00:00 INC CHEVRON PRIVATE ENTERPRISE NaN HI NaN WEB REPORT 15901 2 946481 2010-07-03T21:42:43+00:00 2010-07-03T21:53:07+00:00 INC BP PRIVATE ENTERPRISE NaN LA NaN TELEPHONE 15902 3 946482 2010-07-03T22:22:41+00:00 2010-07-03T22:34:07+00:00 INC CHEVRON PRIVATE ENTERPRISE SAN LUIS OBISPO CA 93401 TELEPHONE 15903 4 946483 2010-07-03T22:46:13+00:00 2010-07-03T22:50:24+00:00 INC NaN UNKNOWN NaN XX NaN TELEPHONE 15904 1.2. Sample Data Processing Since we are interested in the time ellapsed for each phone call, the primary columns of our interest are date_time_received and date_time_complete . However, the raw data is not in a numerical format that can be directly plotted on histogram; we will need to parse & process the time data. I chose .iloc[11000: 12000, :] because it would take too long time to process all ~30,000 rows of the original data. The data is then sorted by the timestamp column. Process DateTime In [4]: import datetime In [5]: def process_time ( row ): call_received = datetime . datetime . strptime ( row [ 'date_time_received' ] . split ( '+' )[ 0 ], '%Y-%m- %d T%H:%M:%S' ) call_ended = datetime . datetime . strptime ( row [ 'date_time_complete' ] . split ( '+' )[ 0 ], '%Y-%m- %d T%H:%M:%S' ) time_ellapsed = call_ended - call_received row [ 'Parsed Call Received' ] = str ( call_received ) row [ 'Parsed Call Ended' ] = str ( call_ended ) row [ 'Time Ellapsed' ] = str ( time_ellapsed ) row [ 'Time Ellapsed (minutes)' ] = round ( time_ellapsed . total_seconds () / 60 , 1 ) return row In [6]: # df was defined above parsed_df = df . iloc [ 11000 : 12000 , :] . apply ( process_time , axis = 1 ) . iloc [:, - 4 :] parsed_df [ 'Parsed Call Received' ] = pd . to_datetime ( parsed_df [ 'Parsed Call Received' ], format = '%Y-%m- %d %H:%M:%S' ) parsed_df [ 'Parsed Call Ended' ] = pd . to_datetime ( parsed_df [ 'Parsed Call Ended' ], format = '%Y-%m- %d %H:%M:%S' ) parsed_df = parsed_df . sort_values ( by = 'Parsed Call Received' ) parsed_df . head () Out[6]: Parsed Call Received Parsed Call Ended Time Ellapsed Time Ellapsed (minutes) 11000 2010-05-21 19:32:09 2010-05-21 19:43:35 0:11:26 11.4 11001 2010-05-21 19:54:40 2010-05-21 19:58:40 0:04:00 4.0 11002 2010-05-21 20:03:14 2010-05-21 20:09:11 0:05:57 6.0 11003 2010-05-21 20:04:26 2010-05-21 20:07:39 0:03:13 3.2 11004 2010-05-21 20:18:38 2010-05-21 20:27:45 0:09:07 9.1 Drop Duplicate Rows The original data does not record the time to a precision of microseconds ( 2010-07-03T21:11:31+00:00 ). Due to the imprecision of the recorded data, there will be rows with duplicate date_time_received and date_time_complete . The data itself is not duplicate, but the data in datetime column is duplicate. Leaving them as they are and plotting them might mess up your plots. Observe the change in the row number of the data frame. It's the indication that there are rows with the same date_time_received values. In [7]: parsed_df . shape Out[7]: (1000, 4) In [8]: new_parsed_df = parsed_df . drop_duplicates ( subset = [ 'Parsed Call Received' ], keep = False ) new_parsed_df . shape Out[8]: (998, 4) 2. Transformatoin 2.1. Sample Data Histogram It can be observed that the phone call duration data does not follow normal distribution. In [9]: time_duration_orig = new_parsed_df [ 'Time Ellapsed (minutes)' ] . values In [40]: fig , ax = plt . subplots ( figsize = ( 8 , 4 )) ax . hist ( time_duration_orig , bins = 'auto' , density = True ) ax . set_xlabel ( 'Call Duration (minutes)' ) ax . set_ylabel ( 'probability' ) ax . set_title ( 'Non-normal Distribution of Phone Call Duration' ); 2.2. Box-Cox Transformation The Python code implementation for Box-Cox is actually very simple. The below one-line code is it for transformation. More information about the Box-Cox function can be found in the scipy documentaion . In [10]: time_duration_trans , lmbda = stats . boxcox ( time_duration_orig ) How does Box-Cox determine the best transformation parameter to obtain a distribution that is close to normal? It calculates correlation coefficient for different lambda ($\\lambda$) values, and finds the one that maximizes the correlation coefficient. In our case, we find that the best lambda parameter is $\\lambda = -0.322$ In [13]: print ( 'Best lambda parameter = %s ' % round ( lmbda , 3 )) fig , ax = plt . subplots ( figsize = ( 8 , 4 )) prob = stats . boxcox_normplot ( time_duration_orig , - 20 , 20 , plot = ax ) ax . axvline ( lmbda , color = 'r' ); Best lambda parameter = -0.322 2.3. Visual Inspection by Fitting Gaussian Distribution One can visually inspect how good the transformation was by fitting a Gaussian distribution function. In [14]: # fit Gaussian distribution time_duration_trans . sort () mean , std = stats . norm . fit ( time_duration_trans , loc = 0 ) pdf_norm = stats . norm . pdf ( time_duration_trans , mean , std ) In [44]: # visual inspection fig , ax = plt . subplots ( figsize = ( 8 , 4 )) ax . hist ( time_duration_trans , bins = 'auto' , density = True ) ax . plot ( time_duration_trans , pdf_norm , label = 'Fitted normal distribution' ) ax . set_xlabel ( 'Call Duration (minutes)' ) ax . set_ylabel ( 'Transformed Probability' ) ax . set_title ( 'Box-Cox Transformed Distribution of Phone Call Duration' ) ax . legend (); Based on the transformed historgram and the respective fitted normal distribuion, it seems that our Box-Cox transformation with $\\lambda = -0.322$ worked well. 2.4. Visual Inspection with Q-Q Plots Visual inspection can be done in a different way with Q-Q plots. The red straight line is the fitted theoretical Gaussian distribution function. If the scatter plot is closer to the red straight line, it means that the data is very close to Gaussian distribution. Deviation from the red line indicates that the data is most likely not Gaussian. Recall that time_duration_orig is the original sample data, and time_duration_trans is the Box-Cox transformed data. In [26]: fig , ( ax1 , ax2 ) = plt . subplots ( 1 , 2 , figsize = ( 8 , 4 )) prob = stats . probplot ( time_duration_orig , dist = stats . norm , plot = ax1 ) prob = stats . probplot ( time_duration_trans , dist = stats . norm , plot = ax2 ) ax1 . set_title ( 'Original Data' ) ax1 . set_ylabel ( 'Call Duration (minutes)' ) ax2 . set_title ( 'Transforamed Data, λ = %s ' % - 0.322 ); ax2 . set_ylabel ( '' ); No significant deviation is observed in Q-Q plots for the transformed data. The transformed data seems to follow Gaussian distribution well. 2.5. Normality Test with Hypothesis Testing Sometimes one might prefer not to evaluate the normality of the transformed data with visual inspection. It is possible to run formal hypothesis testing and check normality in terms of statistical values with scipy.stats.normaltest . It is based on D'Agostino and Pearson's test that combines skew and kurtosis to produce an omnibus test of normality. scipy.stats.normaltest() returns a 2-tuple of the chi-squared statistic, and the associated p-value. Given the null hypothesis that x came from a normal distribution, if the p-value is very small, we reject the null hypothesis. It means that it is unlikely that the data came from a normal distribution. In [15]: k2 , p = stats . normaltest ( time_duration_trans ) print ( ' \\n Chi-squared statistic = %.3f , p = %.3f ' % ( k2 , p )) alpha = 0.05 if p > alpha : print ( ' \\n The transformed data is Gaussian (fails to reject the null hypothesis)' ) else : print ( ' \\n The transformed data does not look Gaussian (reject the null hypothesis)' ) Chi-squared statistic = 2.453, p = 0.293 The transformed data is Gaussian (fails to reject the null hypothesis) The traditional alpha value of 5% was assumed ($\\alpha = 0.05$). Based on the result of the hypothesis testing, it seems that the transformed data does not significantly deviate from a theoretical Gaussian distribution. 3. Back Transformation - Control Chart Analysis One might wonder why we ever want to transform data into something different. What's the point of running analysis on transformed data that significantly deviates from the original data? Let's say that you have a sample data for human's life expectancy, which ranges between 0 and 100. Let's say that the distribution is not Gaussian, so you raised it to a power of 2 to convert it to Gaussian, making the transformed range to be between 0 to 10000. You calculate the mean of the transformed data and find out that the mean is 4,900 years. It is unreasonble to think that average life span of humans is 4,900 years. $$ transformed \\space average \\space life \\space span = 4,900 \\space years $$ One must note that the whole point of data transformation is not to transform the data itself, but to use techniques that require a certain form of a distribution and acquire correct statistical values of your interest. This is where Back Transformation comes into play. You raised your sample data to a power of 2, and obtained the mean value of 4,900 years. Since you raised it to a power of 2, you will back transform it by lowering its power by 2. $$ original \\space average \\space life \\space span = \\sqrt{transformed \\space average \\space life \\space span} = \\sqrt{4,900 \\space years} = 70 \\space years $$ The concept of back transformation will be illustrated with control chart analysis. 3.1. Control Chart Basics If you don't have a good understanding of what control chart is, I recommend you to read this article . It is well written with clean, illustrative visualizations. The upper and lower control limits (UCL and LCL) in control charts are defined as values that are three standard deviations from a mean ($\\mu \\space \\pm \\space 3 \\sigma$). The control limits can be plotted on control chart with the following code snippet (note that new_parsed_df and time_duration_orig were defined above): In [29]: y = new_parsed_df [ 'Parsed Call Received' ] . values In [30]: mean = np . mean ( time_duration_orig ) std = np . std ( time_duration_orig ) upper_limit = mean + 3 * std lower_limit = mean - 3 * std In [31]: fig , ax = plt . subplots ( figsize = ( 8 , 4 )) ax . plot ( y , time_duration_orig ) ax . axhline ( mean , color = 'C1' ) ax . axhline ( upper_limit , color = 'r' ) ax . axhline ( lower_limit , color = 'r' ); ax . text ( y [ - 200 ], upper_limit + 3 , 'Upper Control Limit' , color = 'r' ) ax . text ( y [ - 200 ], lower_limit + 3 , 'Lower Control Limit' , color = 'r' ) ax . text ( y [ 3 ], mean + 3 , 'Mean' , color = 'C1' ) ax . set_ylabel ( 'Call duration (minutes)' ); ax . set_title ( 'Control Chart for Phone Call Duration - Original' ); A few interesting observations could be drawn from the control chart visualization. Intervals dense number of phone calls are from daytime, and intervals with sparse number of phone calls are made from night time. The peaks seem to happen quite regularly. This makes sense considering how some phone calls take much longer than the others due to special circumstances No phone call duration is smaller than 0, and yet the lower control limit is -10, because the traditional control limit computation assumes normality of data, when phone call duration is not normally distributed. 3.2. Why Is Transformation Necessary? The upper control limit plotted on the above visualization defines any phone calls that take longer than 26 minutes to be an outlier. But, are they really outliers? Some phone calls might take longer than 26 minutes due to some extreme circumstances. Moreover, those \"outliers\" seem to be happening too often to be considred outliers. This is happening because the calculation of control limits through plus/minus three standard deviation ($\\pm \\space 3 \\sigma$) assumes that the data is normally distributed. The standard deviation method fails because the assumption of normality is not valid for the phone call duration distribution. Box-Cox transformation is necessary. In [32]: time_duration_trans , lmbda = stats . boxcox ( time_duration_orig ) In [33]: mean_trans = np . mean ( time_duration_trans ) std_trans = np . std ( time_duration_trans ) upper_limit_trans = mean_trans + 3 * std_trans lower_limit_trans = mean_trans - 3 * std_trans In [34]: fig , ax = plt . subplots ( figsize = ( 8 , 4 )) ax . plot ( y , time_duration_trans ) ax . axhline ( mean_trans , color = 'C1' ) ax . axhline ( upper_limit_trans , color = 'r' ) ax . axhline ( lower_limit_trans , color = 'r' ); ax . text ( y [ - 200 ], upper_limit_trans - 0.15 , 'Upper Control Limit' , color = 'r' ) ax . text ( y [ - 200 ], lower_limit_trans + 0.15 , 'Lower Control Limit' , color = 'r' ) ax . text ( y [ 3 ], mean_trans + 0.1 , 'Mean' , color = 'C1' ) ax . set_ylabel ( 'Call duration (minutes)' ); ax . set_title ( 'Control Chart for Phone Call Duration - Transformed' ); A quick glance at the control chart of the transformed data tells us that the most of the phone calls were actually within the upper and lower control limit boundaries. $\\pm \\space 3 \\sigma$ standard deviation method is now working because the assumption of normality is satisfied. 3.3. Back Transforming Control Limits It is difficult for non-statisticians to understand that we are drawing conclusions from the transformed data. We need to back transform the calculated upper and lower control limits by taking the inverse of the lambda ($\\lambda$) parameter we applied for Box-Cox transformation. scipy.special.inv_boxcox will do the job. In [35]: from scipy.special import inv_boxcox In [36]: back_trans_upper_limits = inv_boxcox ( upper_limit_trans , lmbda ) back_trans_lower_limits = inv_boxcox ( lower_limit_trans , lmbda ) mean = np . mean ( time_duration_orig ) In [37]: fig , ax = plt . subplots ( figsize = ( 8 , 4 )) ax . plot ( y , time_duration_orig ) ax . axhline ( mean , color = 'C1' ) ax . axhline ( back_trans_upper_limits , color = 'r' ) ax . axhline ( back_trans_lower_limits , color = 'r' ); ax . text ( y [ - 200 ], back_trans_upper_limits + 3 , 'Upper Control Limit' , color = 'r' ) ax . text ( y [ - 200 ], back_trans_lower_limits + 3 , 'Lower Control Limit' , color = 'r' ) ax . text ( y [ 3 ], mean + 3 , 'Mean' , color = 'C1' ) ax . set_ylabel ( 'Call duration (minutes)' ); ax . set_title ( 'Control Chart for Phone Call Duration - Back Transformed' ); After back-transforming the Box-Cox transformed data, we can now draw a conclusion that all of the phone calls, except for one, made to the National Response Center between 2010-05-22 to 2010-06-01 were within the control limits.","tags":"Statistics","url":"https://aegis4048.github.io/transforming-non-normal-distribution-to-normal-distribution","loc":"https://aegis4048.github.io/transforming-non-normal-distribution-to-normal-distribution"},{"title":"Spatial Simulation 1: Basics of Variograms","text":"The code snippet assumes Anaconda 5.2.0 version of Python virtual environment Acknowledgement I would like to acknowledge Micahel Pyrcz , Associate Professor at the University of Texas at Austin in the Petroleum and Geosystems Engineering, for developing course materials that helped me write this article. Check out his Youtube Lecture on Variogram , and Variogram Excel numerical demo on his Github repo to help yourself better understand the statistical theories and concepts. Let's say that you are a spatial data analyst of a gold mining company, and want to know the distribution of gold percentage over 100m x 100m mining area. To understand the characteritics of the rock formations, you take 100 random rock samples from the mining area, but obviously these 100 data points are not enough to estimate gold percentage over every single spatial locations in the area. So you analyze the available data (100 rock samples from random locations) and simulate full 2D-surface plot for gold percentage over the mining area. This 2D surface simulation from sparse spatial data is a sequential process that involved a series of geostatistical techniques. Steps: Plot experimental variogram Fit variogram model Apply kriging Apply simulation on top of Kriging Run simulation multiple times and perform additioanl data analyses as needed In this post, the concepts, theory, and methodology of plotting a variogram will be covered. Experimental Variogram: Theory Variogram is a measure of dissimilarity over a distance. It shows how two data points are correlated from a spatial perspective, and provides useful insights when trying to estimate the value of an unknown location using collected sample data from other locations. Tobler's first law of geography states that \"everything is related to everything else, but near things are more related than distant things.\" Variogram shows the correlation between two spatial data points over distances. For example, terrains 1 km apart from each other are more likely to be similar than terrains 100 km apart from each other. Oil wells 500 ft apart from each other are more likely to show similar reservoir characteristics than oil wells 5000 ft apart from each other. Variogram is a function of variance over distance. It has the following equation and plot: $$\\gamma(h) = \\frac{1}{2N(h)}\\sum_{\\alpha =1}&#94;{N(h)}\\left ( z(u_{\\alpha })-z(u_{\\alpha} + h) \\right)&#94;2$$ Variables Explained $\\gamma(h)$ = a measure of dissimilarity vs distance. It is a spatial variance between two data points separated by the distance, $h$. $N(h)$ = number of all data point pairs separated by the distance, $h$. $h$ = lag distance. Separation between two data points. $u_{\\alpha }$ = data point on 2D or 3D space at the location, $\\alpha$. $u_{a} + h$ = data point separated from $u_{\\alpha }$ by the distance, $h$. $z(u_{\\alpha })$ = numerical value of data point, $u_{\\alpha }$ $z(u_{\\alpha} + h)$ = numerical value of data point, $u_{\\alpha} + h$ $\\sigma&#94;2$ = sill. Variance at lag distance, $h$, in which spatial data pairs lose correlation. Observation 1: $z(u_{\\alpha })$ - $z(u_{\\alpha} + h)$ There are two data points on the image: $z(u_{\\alpha })$ and $z(u_{\\alpha } + h)$. These two points are separated by the lag distance, $h$. The equation for variogram observes the difference between these two data points: $$z(u_{\\alpha })-z(u_{\\alpha} + h)$$ Observation 2: $N(h)$ $N(h)$ accounts for all data point pairs that are separated by lag distance $h$. Although only horizontal separation is shown in the image, separation between two data points can be horizontal, vertical, or diagonal. Variogram will calculate the difference between all pairs of data points, $z(u_{\\alpha })-z(u_{\\alpha} + h)$, that are separated lag distance, $h$. $$\\sum_{\\alpha =1}&#94;{N(h)}\\left ( z(u_{\\alpha })-z(u_{\\alpha} + h) \\right)&#94;2$$ Observation 3: $\\gamma (h)$ $\\gamma (h)$ denotes for variability of spatial data points at a lag distance, $h$. Recall that variogram accounts for all pairs separated by distance, $h$. It may seem very simple, but one little dot on a variogram plot is actually obtained after iterating for all pairs separated by $h$. $\\underline{ h = 1m }$ $\\underline{ h = 2m }$ $\\underline{ h = 3m }$ Observe how there were less data pairs connected by red lines for $h = 3m$. As the $h$ increases, there will be fewer number of pairs that are separated by $h$ due to spatial limitation. Observation 4: Sill ($\\sigma&#94;2$) Sill ($\\sigma&#94;2$) is the variance in which spatial data pairs lose correlation. As the distance between two data points increases, it will be less likely that those two data points are related to one another. You may assume that the oil wells separated by 100 ft exibit similar geologic characteristics, but you can't assume the same for a well in Texas and a well in California. Variogram works the similar way. Notes Spatial variance may never reach the sill if there is a trend. Ex: Area trend between well variability Observation 5: Range ($a$) Range is a distance in which the spatial variability reaches the sill ($\\sigma&#94;2$). Let's say that you are an exploration engineer for drilling a new oil well. You have drilled wells A, B, C, D that are each 100ft, 200ft, 300ft, and 400ft apart from the zone you want to drill a next new well, and want to know if you can use the data from the previously drilled wells. The geostatisticians in your team report that the geologic formation in the region has a range of 350 ft. This means that the rocks in the region lose geologic correlation with one another if they are more than 350 ft apart — you can't use data from well D because it is 400 ft apart. Observation 6: Nugget Effect ($c_{0}$) The nugget effect refers to the nonzero intercept of the variogram and is an overall estimate of error caused by measurement inaccuracy and environmental variability occurring at fine enough scales to be unresolved by the sampling interval. At distance $h = 0$, we would expect the spatial variance between pairs will be zero, but some variables seem to change in an abrupt manner in very short distance. The nugget effect is like the random noise. It's just the small scale variability that you can't estimate with your large scale variability model. However, if there is no expectation of high degree of discontinuity at distances shorter than the minimum data spacing, experts tend to ignore nugget effect ub geologic engineering. Summary In variogram, low variance ($\\gamma$) represents stronger correlation among data pairs. Spatial data pairs lose correlation with one another when variance ($\\gamma$) at lag distance ($h$) reaches the sill ($\\sigma&#94;2$). If there are variance poitns that exceeds the sill, it indicates the presence of trend, and needs to be detrended before variogram modeling. More information about trend will be discussed later. Experimental Variogram: Search Template Parameters Calculating variogram is challenging because real-life data are not as clean as the 2-D grid sample images shown above. Real-life data are often sparse data that are irregularly spaced. The sparse data you will get in real-life will have very few, or even no data point pairs that are EXACTLY $h$ distance apart from each other. Furthermore, the data points will not always be orthogonal or pararell to each other — there will be diagonalities. regular Spacing irregular Spacing So how do we get pairs separated by lag distance, $h$? We need to consider distance, tolerance, azimuth direction, azimuth tolerance, dip direction, dip tolerance, bandwith in horizontal plane, and bandwidth in vertical plane — through a set of guidelines called Variogram Search Template . variogram Search Template Choice of Azimuth (Directionality) ellipsoidal growth in orthogonal directions, 3D Natural process does not lead to omnidirectionality. Typically there is a direction of major continuity, such as statigraphic surface, and a direction minimum continuity such as perpendicular layers. The choice of azimuth is carefully decided after combining knowledge of geologists and geologic understanding of the region from previously sampled data. There are three orthogonal directions — horizontal major, horizontal minor, and vertical. The \"vertical\" directional is assumed to be the direction orthogonal to the horizontal plane; the vertical direction doesn't have to correspond to the Z-axis. The three mutually orthogonal directions grow in ellipsoidal shape, with the horizontal major axis constituting the longer radius the ellipse. Azimuth decides the direction of horizontal major axis. In typical geometry, an angle ($\\theta$) is measured from the positive X-axis, but in variogram, azimuth is measured from the positive Y-axis. Notes According to an article written by Jared Deutsch , in the absence of strong geologic evidence, a neutral isotropic model can be constructed to assist in determining a principle direction (in later code implementation, isotropy is established by setting azi_tol=90 . You are allowing for $\\pm$90 degrees of azimuth tolerance). Using ordinary kriging, a neutral model would be constructed with an isotropic, long range, high nugget effect variogram. You will check if you can observe any directionality in this neutral model, and combine it with other geologic knowledge to assist in determinng the principle direction. Choice of Azimuth Tolerance Azimuth tolerance should be chosen such that maximum number of pairs are found in the search template and exclude unreasonable associations. A common choice of azimuth tolerance is 22.5°. However, it can be changed to make a decision about whether to increase precision, or stability. Reducing angle tolerance will give precise variogram ( figure 1 ), and increasing angle tolerance will give stable variogram ( figure 2 ). figure 1: smaller azimuth tol -> precise figure 2: bigger azimuth tol -> stable Choice of Lag Distance A variogram should span less than the maximum length of the field. For example, if the field A has a dimension of 100km x 100km, the maximum lag distance ($h$) of the calculated variogram should be less than 50 km. Calculating longer lags results in pairing samples from the edges of the field with each other. Due to spatial limitation of the field (recall that field A is only 100km long), there will be fewer pairs that are 80, 90, 100km apart than pairs that are 5, 10, 20km apart from each other. These variogram points will be much less informed than shorter distance pairs, with fewer pairs supporting the calculation. Observe how the figure 4 shows misleading information about the region. The figure tells us that the correlation becomes stronger for pairs separated by $h > 50$ as their distance increases. This is inconsistent with natural process — we expect the spatial correlation among pairs to decrease as they are further apart from each other. figure 3: sample Data figure 4: total lag = field length figure 5: total lag = field length / 2 Increasing the number of lag distance will result in fewer lag bins, which means that there will be fewer points on the calculated variogram ( figure 6 ). There will be a trade-off between stability and precision, and an engineer must carefully choose the parameters that is the best representative of the geology of the region. figure 6: stable, but imprecise figure 7: precise, but unstable Choice of Lag Tolerance Lag tolerance is usually half of the lag distance ( figure 8 ). Choosing lag tolerance smaller than 1/2 lag distance will result in missing out data pairs that are not within the lag tolerance ( figure 9 ). Choosing lag tolerance bigger than 1/2 lag distance will result in overlapping of data pairs ( figure 10 ). figure 8: lag tol. = 1/2 lag dist. figure 9: lag tol. < 1/2 lag dist. figure 10: lag tol. > 1/2 lag dist. Although the lag tolerance is usually half of the lag distance, in cases of erratic variograms, we may choose to overlap calculations by setting lag tolerance that is greater than half of the lag distance to increase stability in a variogram. Overlapping calculations has an effect of smoothing out and reducing noise in the calculated variogram ( figure 12 ). Smoothing out the calculated variogram maybe helpful in fitting a variogram model later. However, be careful not to smooth out too much, as it will result in imprecise variogram model that is not representative of the regional geology. A decision between stability and precision must be made. figure 11: noisy figure 12: smooth Choice of Bandwidth Just like azimuth tolerance, bandwidth should be chosen such that maximum number of pairs are found in the search template and exclude unreasonable associations. According to Jared Deutsch , bandwidths are infrequently used as they seldom improve the stability of the calculaed variogram. Instead, a carefully chosen set of angle tolerances (azimuth and dip) are applied. Experimental Variogram: Python Implementation In [ ]: In [ ]:","tags":"Geostatistics","url":"https://aegis4048.github.io/spatial-simulation-1-basics-of-variograms","loc":"https://aegis4048.github.io/spatial-simulation-1-basics-of-variograms"},{"title":"Parse PDF Files While Retaining Structure with Tabula-py","text":"If you've ever tried to do anything with data provided to you in PDFs, you know how painful it is — it's hard to copy-and-paste rows of data out of PDF files. It's especially hard if you want to retain the formats of the data in PDF file while extracting text. Most of the open source PDF parsers available are good at extracting text. But when it comes to retaining the the file's structure, eh, not really. Try tabula-py to extract data into a CSV or Excel spreadsheet using a simple, easy-to-use interface. One look is worth a thousand words. Take a look at the demo screenshot. Installations This installation tutorial assumes that you are using Windows. However, according to the offical tabula-py documentation , it was confirmed that tabula-py works on macOS and Ubuntu. 1. Download Java Tabula-py is a wrapper for tabula-java, which translates Python commands to Java commands. As the name \"tabula-java\" suggests, it requires Java. You can download Java here . 2. Set environment PATH variable (Windows) One thing that I don't like about Windows is that it's difficult to use a new program I downloaded in a console environment like Python or CMD window. But oh well, if you are a Windows user, you have to go through this extra step to allow Python to use Java. If you are a macOS or Ubuntu user, you probably don't need this step. Find where Java is installed, and go to Control Panel > System and Security > System > Advanced system settings > Advanced > Environment Variables... to set environment PATH variable for Java. Make sure you have Java\\jdk1.8.0_201\\bin and Java\\jre1.8.0_201\\bin in the environment path variable. Then, type java -version on CMD window. If you successfully installed Java and configured the environment variable, you should see something like this: java -version java version \"1.8.0_201\" Java(TM) SE Runtime Environment (build 1.8.0_201-b09) Java HotSpot(TM) 64-Bit Server VM (build 25.201-b09, mixed mode) If you don't see something like this, it means that you didn't properly configure environment PATH variable for Java. 3. Re-start Your Command Prompt Any program invoked from the command prompt will be given the environment variables that was at the time the command prompt was invoked. If you launched your Python console or Jupyter Notebook before you updated your environment PATH variable, you need to re-start again. Otherwise the change in the environment variable will not be reflected. If you are experiencing FileNotFoundError or 'java' is not recognized as an internal or external command, operable program or batch file inside Jupyter or Python console, it's the issue of environment variable. Either you set it wrong, or your command prompt is not reflecting the change you made in the environment variable. To check if the change in the environment variable was reflected, run the following code in Jupyter or Python console: import os s = os.environ[\"PATH\"].split(';') for item in s: print(item) Something like these must be in the output if everything is working fine: C:\\Program Files\\Java\\jdk1.8.0_201\\bin C:\\Program Files\\Java\\jre1.8.0_201\\bin 4. Install Tabula-py This is the last step: pip install tabula-py More detailed instructions are provided in the github repo of tabula-py Tabula Web Application Tabula supports web application to parse PDF files. You do not need this to use tabula-py, but from my personal experience I strongly recommend you to use this tool because it really helps you debugging issues when using tabula-py. For example, I was tring to parse 100s of PDF files at once, and for some reason tabula-py would return an NoneType object instead of pd.DataFrame object (by default, tabula-py extracts tables in dataframe) for one PDF file. There was nothing wrong with my codes, and yet it would just not parse the file. So I tried opening it on the tabula web-app, and realized that it was actually a scanned PDF file and that tabula is unable to parse scanned PDFs. Long story short, if it can be parsed with tabula web-app, you can replicate it with tabula-py. If tabula web-app can't, you should probably look for a different tool. Installations If you already configured the environment PATH variable for Java, all you need to do is downloading the .zip file here and running tabula.exe . That's it. Tabula has really nice web UI that allows you to parse tables from PDFs by just clicking buttons. Note The web-app will automatically open in your browser with 127.0.0.1:8080 local host. If port 8080 is already being used by another process, you will need to shut it down. But normally you don't have to worry about this. Screenshots This is what you will see when you launch tabula.exe . Browse... the PDF file you want to parse, and import . You can either use Autodetect Tables or drag your mouse to choose the area of your interest. If the PDF file has a complicated structure, it is usually better to manually choose the area of your interest. Also, note the option Repeat to All Pages . Selecting this option will apply the area you chose for all pages. Here's the output. More explanation about Lattice and Stream options will be discussed in detail later. Template JSON Files Tabula web-app accepts the user's drag & click as input and translates it into Java arguments that are actually used behind the scenes to parse PDF files. The translated Java arguments are accessible to users in a JSON format. Select the area you want to parse, and click Save Selections as Template . Then, Download the translated Java arguments in a text JSON file. These arguments are useful when coding arguments for tabula.read_pdf() later. template.json { \"page\": 2, \"extraction_method\": \"guess\", \"x1\": 24.785995330810547, \"x2\": 589.3559953308105, \"y1\": 390.5325, \"y2\": 695.0025, \"width\": 564.57, \"height\": 304.47 } Running Tabula-py Tabula-py enables you to extract tables from PDFs into DataFrame and JSON. It can also extract tables from PDFs and save files as CSV, TSV or JSON. Some basic code examples are as follows: import tabula # Read pdf into DataFrame df = tabula.read_pdf(\"test.pdf\", options) # Read remote pdf into DataFrame df2 = tabula.read_pdf(\"https://github.com/tabulapdf/tabula-java/raw/master/src/test/resources/technology/tabula/arabic.pdf\") # convert PDF into CSV tabula.convert_into(\"test.pdf\", \"output.csv\", output_format=\"csv\") # convert all PDFs in a directory tabula.convert_into_by_batch(\"input_directory\", output_format='csv') Area Selection You can select portions of PDFs you want to analyze by setting area (top,left,bottom,right) option in tabula.read_pdf() . This is equivalent to dragging your mouse and setting the area of your interest in tabula web-app as it was mentioned above. Default is the entire page. Also note that you can choose the page, or pages you want to parse with pages option. The sample PDF file can be downloaded from here . In [1]: import tabula import pandas as pd In [37]: file = 'pdf_parsing/lattice-timelog-multiple-pages.pdf' df = tabula . read_pdf ( file , lattice = True , pages = 2 , area = ( 406 , 24 , 695 , 589 )) df Out[37]: Start Date End Date (hr) Activity Activity Detail Operation Com 0 12/13/2014\\r06:00 12/13/2014\\r09:00 3.0 SURF-DRILL DRILL SURFACE DRL Rotate from 1600' to 1859' (259' @ 8 fph). WOB... 1 12/13/2014\\r09:00 12/13/2014\\r11:00 2.0 SURF-CIRC CIRCULATE CIRC Pump 2- 50 bbl hi vis sweep; Circulate to surface 2 12/13/2014\\r11:00 12/13/2014\\r14:00 3.0 SURF-TRIP TOOH TRIP TOOH (Slick off bottom) f/1859' to 108' (SLM)... 3 12/13/2014\\r14:00 12/13/2014\\r16:00 2.0 PLAN EQUIP BHA PJSA - Break bit & L/D directional BHA, clean ... 4 12/13/2014\\r16:00 12/13/2014\\r17:30 1.5 PLAN DRLG CSG PTJSA / R/U Pipe Pros.Csg. tools / PTJSA on ru... 5 12/13/2014\\r17:30 12/13/2014\\r18:00 0.5 PLAN DRLG CSG Make up 13 3/8 Gemco PDC drillable float shoe;... 6 12/13/2014\\r18:00 12/13/2014\\r18:30 0.5 PLAN PERS SFTY HJSM with Morning tour crew, Pipe Pro casing c... 7 12/13/2014\\r18:30 12/13/2014\\r23:30 5.0 PLAN DRLG CSG Make up 13 /8\" PDC drillable float collar onto... 8 12/13/2014\\r23:30 12/14/2014\\r01:30 2.0 SURF-CIRC CIRCULATE CIRC HJSM on Hoisting personal; Make up Swedge in ... 9 12/14/2014\\r01:30 12/14/2014\\r03:30 2.0 PLAN DRLG CSG Run 13 3/8\"J-55 54.5 BTC f/ 1,639' to 1,819';... 10 12/14/2014\\r03:30 12/14/2014\\r04:30 1.0 SURF-CIRC CIRCULATE CIRC Circulate Bttms up while Rigging down csg crew... 11 12/14/2014\\r04:30 12/14/2014\\r06:00 1.5 SURF-CMT CEMENT SURFACE\\rCASING CMT HJSM w/ Basic Cementer, H&P rig crew & PNR; D... Alternatively, you can set area with percentage scale by setting relative_area=True . For this specific PDF file, the below area=(50, 5, 92, 100), relative_area=True option is equivalent to area=(406, 24, 695, 589) above. In [38]: file = 'pdf_parsing/lattice-timelog-multiple-pages.pdf' df = tabula . read_pdf ( file , lattice = True , pages = 2 , area = ( 50 , 5 , 92 , 100 ), relative_area = True ) df Out[38]: Start Date End Date Dur (hr) Activity Activity Detail Operation Com 0 2/13/2014\\r6:00 12/13/2014\\r09:00 3.0 SURF-DRILL DRILL SURFACE DRL Rotate from 1600' to 1859' (259' @ 8 fph). WOB... 1 2/13/2014\\r9:00 12/13/2014\\r11:00 2.0 SURF-CIRC CIRCULATE CIRC Pump 2- 50 bbl hi vis sweep; Circulate to surface 2 2/13/2014\\r1:00 12/13/2014\\r14:00 3.0 SURF-TRIP TOOH TRIP TOOH (Slick off bottom) f/1859' to 108' (SLM)... 3 2/13/2014\\r4:00 12/13/2014\\r16:00 2.0 PLAN EQUIP BHA PJSA - Break bit & L/D directional BHA, clean ... 4 2/13/2014\\r6:00 12/13/2014\\r17:30 1.5 PLAN DRLG CSG PTJSA / R/U Pipe Pros.Csg. tools / PTJSA on ru... 5 2/13/2014\\r7:30 12/13/2014\\r18:00 0.5 PLAN DRLG CSG Make up 13 3/8 Gemco PDC drillable float shoe;... 6 2/13/2014\\r8:00 12/13/2014\\r18:30 0.5 PLAN PERS SFTY HJSM with Morning tour crew, Pipe Pro casing c... 7 2/13/2014\\r8:30 12/13/2014\\r23:30 5.0 PLAN DRLG CSG Make up 13 /8\" PDC drillable float collar onto... 8 2/13/2014\\r3:30 12/14/2014\\r01:30 2.0 SURF-CIRC CIRCULATE CIRC HJSM on Hoisting personal; Make up Swedge in ... 9 2/14/2014\\r1:30 12/14/2014\\r03:30 2.0 PLAN DRLG CSG Run 13 3/8\"J-55 54.5 BTC f/ 1,639' to 1,819';... 10 2/14/2014\\r3:30 12/14/2014\\r04:30 1.0 SURF-CIRC CIRCULATE CIRC Circulate Bttms up while Rigging down csg crew... 11 2/14/2014\\r4:30 12/14/2014\\r06:00 1.5 SURF-CMT CEMENT SURFACE\\rCASING CMT HJSM w/ Basic Cementer, H&P rig crew & PNR; D... Notes on Escape Characters When used as lattice mode, tabula replaces abnormally large spacing between texts and newline within a cell with \\r . This can be fixed with a simple regex manipulation. In [41]: clean_df = df . replace ( ' \\r ' , ' ' , regex = True ) clean_df Out[41]: Start Date End Date Dur (hr) Activity Activity Detail Operation Com 0 2/13/2014 6:00 12/13/2014 09:00 3.0 SURF-DRILL DRILL SURFACE DRL Rotate from 1600' to 1859' (259' @ 8 fph). WOB... 1 2/13/2014 9:00 12/13/2014 11:00 2.0 SURF-CIRC CIRCULATE CIRC Pump 2- 50 bbl hi vis sweep; Circulate to surface 2 2/13/2014 1:00 12/13/2014 14:00 3.0 SURF-TRIP TOOH TRIP TOOH (Slick off bottom) f/1859' to 108' (SLM)... 3 2/13/2014 4:00 12/13/2014 16:00 2.0 PLAN EQUIP BHA PJSA - Break bit & L/D directional BHA, clean ... 4 2/13/2014 6:00 12/13/2014 17:30 1.5 PLAN DRLG CSG PTJSA / R/U Pipe Pros.Csg. tools / PTJSA on ru... 5 2/13/2014 7:30 12/13/2014 18:00 0.5 PLAN DRLG CSG Make up 13 3/8 Gemco PDC drillable float shoe;... 6 2/13/2014 8:00 12/13/2014 18:30 0.5 PLAN PERS SFTY HJSM with Morning tour crew, Pipe Pro casing c... 7 2/13/2014 8:30 12/13/2014 23:30 5.0 PLAN DRLG CSG Make up 13 /8\" PDC drillable float collar onto... 8 2/13/2014 3:30 12/14/2014 01:30 2.0 SURF-CIRC CIRCULATE CIRC HJSM on Hoisting personal; Make up Swedge in ... 9 2/14/2014 1:30 12/14/2014 03:30 2.0 PLAN DRLG CSG Run 13 3/8\"J-55 54.5 BTC f/ 1,639' to 1,819';... 10 2/14/2014 3:30 12/14/2014 04:30 1.0 SURF-CIRC CIRCULATE CIRC Circulate Bttms up while Rigging down csg crew... 11 2/14/2014 4:30 12/14/2014 06:00 1.5 SURF-CMT CEMENT SURFACE CASING CMT HJSM w/ Basic Cementer, H&P rig crew & PNR; D... Lattice Mode vs Stream Mode Tabula supports two primary modes of table extraction — Lattice mode and Stream mode. Lattice Mode lattice=True forces PDFs to be extracted using lattice-mode extraction. It recognizes each cells based on ruling lines, or borders of each cell. Stream Mode stream=True forces PDFs to be extracted using stream-mode extraction. This mode is used when there are no ruling lines to differentiate one cell from the other. Instead, it uses spacings among each cells to recognize each cell. PDF File 1 : Lattice mode recommended PDF file 2 : Stream mode recommended How would it look like if PDF File 1 and PDF file 2 are each extracted in both stream mode and lattice mode? In [51]: # PDF File 1: lattice mode file = 'pdf_parsing/lattice-timelog-multiple-pages.pdf' df = tabula . read_pdf ( file , lattice = True , pages = 2 , area = ( 406 , 24 , 695 , 589 )) df . head () Out[51]: Start Date End Date (hr) Activity Activity Detail Operation Com 0 12/13/2014\\r06:00 12/13/2014\\r09:00 3.0 SURF-DRILL DRILL SURFACE DRL Rotate from 1600' to 1859' (259' @ 8 fph). WOB... 1 12/13/2014\\r09:00 12/13/2014\\r11:00 2.0 SURF-CIRC CIRCULATE CIRC Pump 2- 50 bbl hi vis sweep; Circulate to surface 2 12/13/2014\\r11:00 12/13/2014\\r14:00 3.0 SURF-TRIP TOOH TRIP TOOH (Slick off bottom) f/1859' to 108' (SLM)... 3 12/13/2014\\r14:00 12/13/2014\\r16:00 2.0 PLAN EQUIP BHA PJSA - Break bit & L/D directional BHA, clean ... 4 12/13/2014\\r16:00 12/13/2014\\r17:30 1.5 PLAN DRLG CSG PTJSA / R/U Pipe Pros.Csg. tools / PTJSA on ru... In [57]: # PDF File 1: stream mode file = 'pdf_parsing/lattice-timelog-multiple-pages.pdf' df = tabula . read_pdf ( file , stream = True , guess = False , pages = 2 , area = ( 406 , 24 , 695 , 589 )) df . head ( 11 ) Out[57]: Start Date End Date (hr) Activity Activity Detail Operation Com 0 12/13/2014 12/13/2014 3.0 SURF-DRILL DRILL SURFACE DRL Rotate from 1600' to 1859' (259' @ 8 fph). WOB... 1 06:00 09:00 NaN NaN NaN NaN SPP 2300, motor diff 650, 800 GPM, torque 18k. 2 NaN NaN NaN NaN NaN NaN (T.D. Surface @ 09:00 12-13-14) 3 12/13/2014 12/13/2014 2.0 SURF-CIRC CIRCULATE CIRC Pump 2- 50 bbl hi vis sweep; Circulate to surface 4 09:00 11:00 NaN NaN NaN NaN NaN 5 12/13/2014 12/13/2014 3.0 SURF-TRIP TOOH TRIP TOOH (Slick off bottom) f/1859' to 108' (SLM)... 6 11:00 14:00 NaN NaN NaN NaN Hole taking correct fill 7 12/13/2014 12/13/2014 2.0 PLAN EQUIP BHA PJSA - Break bit & L/D directional BHA, clean ... 8 14:00 16:00 NaN NaN NaN NaN ext. 9 12/13/2014 12/13/2014 1.5 PLAN DRLG CSG PTJSA / R/U Pipe Pros.Csg. tools / PTJSA on ru... 10 16:00 17:30 NaN NaN NaN NaN csg. In [62]: # PDF File 2: lattice mode file = 'pdf_parsing/stream-railroad-pages-1-4.pdf' df = tabula . read_pdf ( file , lattice = True , pages = 1 , area = ( 209 , 12.5 , 387.3 , 386 )) df Out[62]: WELL INFORMATION In [64]: # PDF File 2: stream mode file = 'pdf_parsing/stream-railroad-pages-1-4.pdf' df = tabula . read_pdf ( file , stream = True , guess = False , pages = 1 , area = ( 209 , 12.5 , 387.3 , 386 )) df Out[64]: Unnamed: 0 WELL INFORMATION 0 API No.: 42-003-46352 County: A 1 Well No.:22H RRC Distri 2 Lease Name: UNIVERSITY \"7-43\" Field Name 3 RRC Lease No.: 40532 Field No.: 4 Location: Section: 35, Block: 7, Survey: UN... NaN 5 Latitude: Longitude: 6 This well is located 17.2 miles in a SE 7 direction from ANDREWS, NaN 8 which is the nearest town in the county. NaN Observe how lattice mode extraction for PDF file 2 was able to extract only \"WELL INFORMATION\" string. This is not an error. Recall that lattice mode identifies cells by ruling lines. Notes About guess option According to the offical documentation , guess is known to make a conflict between stream option. If you feel something strange with your result, try setting guess=False . For example, for PDF File 1 , if stream mode is used without setting guess=False , it would look like this: In [66]: # PDF File 1: stream mode, guess=True file = 'pdf_parsing/lattice-timelog-multiple-pages.pdf' df = tabula . read_pdf ( file , stream = True , pages = 2 , area = ( 406 , 24 , 695 , 589 )) df . head ( 11 ) Out[66]: Report #:3 Daily Operation:12/13/2014 06:00 - 12/14/2014 06:00 Unnamed: 2 Unnamed: 3 Unnamed: 4 Unnamed: 5 Unnamed: 6 0 Job Category NaN Primary Job Type NaN NaN AFE Number NaN 1 ORIG DRILLING NaN ODR NaN NaN 033402 NaN 2 Days From Spud (days) Days on Location (days) End Depth (ftKB) End Depth (TVD) (ftKB) Dens Last Mud (lb/gal) Rig NaN NaN 3 1 3 1,859.0 1,858.5 8.60 H & P, 637 NaN NaN 4 Operations Summary NaN NaN NaN NaN NaN NaN 5 Drld. Surface f/1600' to 1859' T.D. @ 09:00... NaN NaN NaN NaN NaN NaN 6 S/M, R/U csg running equip & Run 45 jts. of... NaN NaN NaN NaN NaN NaN 7 Surface Float shoe @ 1,857.5' NaN NaN NaN NaN NaN Surface 8 Float collar @ 1,817.9' NaN NaN NaN NaN NaN NaN 9 Remarks NaN NaN NaN NaN NaN NaN 10 Rig (H&P 637), Well (University 7-43 # 22H) NaN NaN NaN NaN NaN NaN Pandas Option Pandas arguments can be passed into tabula.read_pdf() as a dictionary object. In [74]: file = 'pdf_parsing/lattice-timelog-multiple-pages.pdf' df = tabula . read_pdf ( file , lattice = True , pages = 2 , area = ( 406 , 24 , 695 , 589 ), pandas_options = { 'header' : None }) df . head () Out[74]: 0 1 2 3 4 5 6 0 Start Date End Date (hr) Activity Activity Detail Operation Com 1 12/13/2014\\r06:00 12/13/2014\\r09:00 3 SURF-DRILL DRILL SURFACE DRL Rotate from 1600' to 1859' (259' @ 8 fph). WOB... 2 12/13/2014\\r09:00 12/13/2014\\r11:00 2 SURF-CIRC CIRCULATE CIRC Pump 2- 50 bbl hi vis sweep; Circulate to surface 3 12/13/2014\\r11:00 12/13/2014\\r14:00 3 SURF-TRIP TOOH TRIP TOOH (Slick off bottom) f/1859' to 108' (SLM)... 4 12/13/2014\\r14:00 12/13/2014\\r16:00 2 PLAN EQUIP BHA PJSA - Break bit & L/D directional BHA, clean ... More Documentation Further instructions about tabula-py can be found on its official github repo .","tags":"Others","url":"https://aegis4048.github.io/parse-pdf-files-while-retaining-structure-with-tabula-py","loc":"https://aegis4048.github.io/parse-pdf-files-while-retaining-structure-with-tabula-py"},{"title":"Creating a Jupyter Notebook-Powered Data Science Blog with Pelican","text":"What powers this blog, Pythonic Excursions? - Pelican. - Me Pelican is a static site genertor, written in Python. It is a Python library used to auto-generate HTML elements that are used to run websites. Pelican-powered blogs are light and easy to host with no scaling concerns. It pre-generates HTML files and responds with the existing files during a typical HTTP request-response cycle. So, why should you use Pelican? It's GREAT for Blogging You can write your content directly with the editor of your choice in reStructuredText or Markdown formats. Select a favorite theme of your choice from a collection of Pelican-themes , and write articles. The CSS and Javascript contained in the theme will handle the rest and output your article nice and clean. One look is worth a thousand words. Take a look at this markdown file that is used to render this article through Github Pages by Jake VanderPlas . His blog is made with Pelican, after modifying some codes in Octopress theme (you can also write your articles in other formats, such as Jupyter Notebook, which powers this blog). Minimal Learning Curves Going through the official documenations, user-made tutorials, or YouTube videos can be painful. Using Pelican will minimize wasting your time dealing with the learning curves. One thing that makes it very easy to learn & modify is that there already are lots blogs that run on Pelican, and their source codes are open to public. Pythonic Excursions -- source code , Aegis-Jupyter Theme by Me onCrash = 'reboot();' -- source code , Elegant Theme by Talha Mansoor Pythonic Perambulations -- source code , Adapted Octopress Theme by Jake VanderPlas ... and many more If you don't want to learn Pelican from scratch, you can download these open source repos and start from there. You will only need to learn how to tweak some settings to meet your needs. Completely Static Output Is Easy To Host Anywhere The output of Pelican is all HTML. You don't have to worry about configuring a complicated database and optimizing connections. Let's take a look at how Pelican works. Most Pelican blogs have the following directory tree. blog content articles article_1.md article_2.md article_3.md figures images ... output category images figures index.html archives.html article_1.html article_2.html article_3.html plugins themes custom_theme static templates Makefile pelicanconf.py publishconf.py output directory is the folder where Pelican stores the auto-generated HTML files, and those existing files are returned to the user who sent an HTTP request to view your website. The other directories are tools and templates used to generate the HTML files in the output folder. You do not need to configure a SQL database, or execute any codes on the server. All outputs are completely static. Can Pelican Be Used In Dynamic Websites Too? Yes, it can. Although Pelican is a static site generator, that does not mean that you can't have dynamic backend features on your website. You can pre-generate the output HTML files with Pelican, and just wrap it around with the backend framework of your choice. Let's say that you are developing a web-app with Django, and you want part of your website to be a static blog. You have a Pelican-generated output HTML file called article_1.html . In Django, you can render your Pelican-generated HTML file using a basic Class-Based-View like this: views.py from django.views.generic import TemplateView class PelicanView(TemplateView): template_name = 'article_1.html' urls.py from django.urls import re_path from your_awesome_app import views app_name = 'your_awesome_app' urlpatterns = [ re_path('&#94;$', views.PelicanView.as_view(), name='pelican'), ] And that's all it takes to integrate Pelican with Django. Part of your website can be static pages where it doesn't need to execute any code on a server, but the other part of your website can be dynamic pages where you can send queries to your server. Of course, the methodology to combine Pelican with dynamic backend will differ for each backend framework of your choice, but you get the idea. Here is the point: Pelican is a static site generator, but that does not mean that Pelican can't be used in dynamic websites. And Pelican is GREAT for blogging. Introducing Aegis-Jupyter Theme Aegis-Jupyter theme is a custom Pelican theme I made to easily host & maintain a Jupyter Notebook powered data science blog. I borrowed some CSS design of the articles from Jake VanderPlas , and improved the rendering of Jupyter Notebook files by adding custom CSS & JS codes. Every articles you see in archives page is rendered using Jupyter Notebook .ipynb files, even this very article you are reading right now! There are several reason's you why you might wanna consider using Aegis-Jupyter theme. Jupyter-Notebook-Based Articles First and foremost, your articles are be rendered by Jupyter Notebook. The question of \"Why would you want to use Aegis-Jupyter theme?\" is synonymous to \"Why would you want to use Jupyter Notebook?\" - it allows you to create and share documents that contain live code, equations, visualizations and narrative text. You don't have to go through the hassel of writing Python input codes in HTML, save the output visualizations in jpg or png files, and then render it on a browser using an image tag like this: &ltimg src=\"your_awesome_visualization.png\"> No, don't do this. This is bad. You don't want to be keep doing this for every single output of your code block. This is too much work. Simply write codes in Jupyter, save it, and render your article. Aegis-Jupyter theme was built for that purpose. Mobile Device Friendly The theme renders very nicely on all resolutions, screenwidth, and mobile devices. Try viewing this website on your phone. If you are on PC, try stretching & collapsing the browser size and see how it responsively re-aligns itself. Google Analytics Support If you own any kind of website, not just a data science blog, at some point in your life you would be wondering about the behaviors of the viewers. How many people visit my website every week? How many of them are unique visitors? From what region do I get the most number of visitors? On average, how many minutes do people stay on my website? Which post was the most popular? From what social media platform do I get the most number of visitors from? These kinds of questions can be answered by leveraging the power of Google Analytics, FOR FREE . All you need to do is to create a Google Analytics account, get a tracking ID, and put that on publishconf.py file. For example, if your Google Analytics tracking ID is UA-1XXXXXXXX-1 , then you set GOOGLE_ANALYTICS variable liks this: publishconf.py GOOGLE_ANALYTICS = \"UA-1XXXXXXXX-1\" That's it. Aegis-Jupyter theme will take care of the rest. More detailed tutorials on how to create Google Analytics account and tracking ID will come later. Easy to Manage Your Articles Meta properties of your article can easily be managed my changing attributes in markdown files. The below markdown is the actual .md file that renders this article . non-parametric-confidence-interval-with-bootstrap.md Title: Non-Parametric Confidence Interval with Bootstrap Tags: non-parametric, confidence-interval, bootstrap(stats), statistics Date: 2019-01-04 09:00 Slug: non-parametric-confidence-interval-with-bootstrap Subtitle: Keywords: Featured_Image: images/featured_images/bootstrap.png Social_Media_Description: Bootstrapping can calculate uncertainty in any confidence interval of any kind of distribution. It's great because it is distribution-free. Summary: {% notebook downloads/notebooks/Non-ParametricConfidenceIntervalswithBootstrap.ipynb cells[1:2] %} {% notebook downloads/notebooks/Non-ParametricConfidenceIntervalswithBootstrap.ipynb cells[:] %} The below screenshot is the preview of the article on the landing page of this blog. Observe how each attribute in the markdown file is used to render the output preview page. You can declare additional attributes as much as you want. Share Your Posts on Social Media Aegis-Jupyter theme leverages the power of Open Graph Meta Tags and renders the preview of your website nicely when shared on social media. You can set a preview image by declaring Featured_Image and set preview descriptions by declaring Social_Media_Description for each article in its respective markdown files. If you do not specify Featured_Image attribute in the markdown file, a default featured image will show up when shared on social media. Default featured image can be set up in pelicanconf.py file. This is what I have for my blog: pelicanconf.py FEATURED_IMAGE = SITEURL + '/theme/img/logo_icon_background.png' Search Box Most static websites do not support search box functionality out of the box. However, Pelican supports Tipue Search , a jQuery site search plugin. Talha Mansoor made a pelican plugin that allows Pelican to leverage the power of Tipue Search, and Aegis-Jupyter integrated it to work with articles written in Jupyter Notebook. Take a look at it with your own eyes by scrolling up and actually using the search box in this blog! Disqus Comment Box Being able to communicate with the audiences is a quintessential component of a blog. Create an account in Disqus and get your Disqus Website Name here . Then, declare DISQUS_SITENAME variable in publishconf.py . That's all it takes to have a comment box feature for your blog. Aegis-Jupyter will handle the rest. And, of course, IT'S FREE publishconf.py DISQUS_SITENAME = \"pythonic-excursions\" In [ ]:","tags":"Others","url":"https://aegis4048.github.io/creating-a-jupyternotebook-powered-data-science-blog-with-pelican","loc":"https://aegis4048.github.io/creating-a-jupyternotebook-powered-data-science-blog-with-pelican"},{"title":"Non-Parametric Confidence Interval with Bootstrap","text":"The code snippet assumes Anaconda 5.2.0 version of Python virtual environment Acknowledgement I would like to acknowledge Micahel Pyrcz , Associate Professor at the University of Texas at Austin in the Petroleum and Geosystems Engineering, for developing course materials that helped me write this article. Check out his Youtube Lecture on Bootstrap , and Boostrap Excel numerical demo on his Github repo to help yourself better understand the statistical theories and concepts. Bootstrap is a non-parametric statistical technique to resample from known samples to estimate uncertainty in summary statistics. When there are small, limited number of samples, it gives a more accurate forecast model than directly obtaining a forecast model from the limited sample pool (assuming that the sample set of data is reasonable representation of the population). It is non-parametric because it does not require any prior knowledge of the distribution (shape, mean, standard devation, etc..). Advantages of Bootstrap One great thing about Bootstrapping is that it is distribution-free . You do not need to know distribution shape, mean, standard devation, skewness, kurtosis, etc... All you need is just a set of sample data that is representative of a population. The fact that Bootstrapping does not depend on a type of distribution leads to another great advantage - It can calculate uncertainty in any confidence interval of any kind of distribution . For example, the analytical solution to calculate a confidence interval in any statistics of a distribution is as follows: CI of mean = stats of interest $\\pm$ $($distribution score $\\times$ Standard Error $)$ There are three problems with analytically solving for confidence interval of a statistic. First, the variable in the equation, distribution score , depends on the type of the distribution. If you do not know the distribution shape of your population, it is very difficult to calculate the confidence interval of a statistic. Second, not all statistics have a formula to calculate its Standard Error . For example, there exists an equation to calculate the standard error of a mean: Standard Error = $\\sigma_{sample} \\ \\mathbin{/} \\ \\sqrt{N}$ But there is no equation to calculate the standard error of a median. If you want to obtain confidence intervals for other statistics (ex: skewness, kurtosis, IQR, etc...), it will be very difficult to do so, simply because there are no equations for them. Third, some statistics have analytical solutions for its standard error calculation, but it is so convoluted that Bootstrapping is simpler. A classic example is obtaining a CI for the correlation coefficient given a sample from a bivariate normal distribution. Bootstrapping calculates confidence intervals for summary statistics numerically, not analytically , and this is why it can calculate ANY summary stats for ANY distribution. Methodology One goal of inferential statistics is to determine the value of a parameter of an entire population. It is typically too expensive or even impossible to measure this directly. So we use statistical sampling. We sample a population, measure a statistic of this sample, and then use this statistic to say something about the corresponding parameter of the population. Bootstrapping is a type of resampling method to save time and money taking measurements. From a sample pool of size N, it picks a random value N times with replacement , and create M number of new Bootstrapped-sample pools. The term with replacement here means that you put back the sample you drew to the original sample pool after adding it to a new Bootstrapped-sample pool. Think of it this way: you randomly choose a file from a folder in your PC, and you copy and paste the randomly-chosen file into a new folder. You do not cut and paste the file, but you copy and paste the file into a new folder. You will have M number of folders (M is an arbitrary number of your choice), each containing N number of files. Bootstrapping resamples the original sample pool to generate multiple smaller population of the true population. Each Bootstrap simulation is done by selecting a random value from the sample pool. For example, lets assume that you have the following sample pool of integers: Sample Integers = [12, 433, 533, 14, 65, 42, 64] From the sample pool of size N=7, you choose a random value N=7 times, and create a new sample pool of size N=7. In Bootstrap, each newly created sample pool is called a realization . You generate many of these realizations, and use them to calculate uncertainties in summary stats. Realization 1 = [12, 533, 533, 533, 12, 14, 42] Realization 2 = [65, 14, 14, 65, 433, 64, 14] Realization 3 = [433, 64, 533, 14, 14, 64, 12] Realization 4 = [14, 65, 65, 433, 533, 42, 12] Notice the duplicate data in the realizations (Ex: 533, 533, 533). Duplicates in realizations exist because each data in realization is randomly chosen from the original sample pool with replacement . Warning! It is extremly important that the N size for each Bootstrap realization matches the N size of the original sample pool. We use Bootstrap to numerically estimate the confidence interval (CI). It's an alternative tool to analytically solve for CI. Observing how CI is analytically calculated may help one to understand why the value of N is important. Let's take the CI of a mean for example. Recall that the CI of a mean represents how far a sample mean can deviate from the true population mean. In case of a Gaussian, or Gaussian-like distribution (ex: student-t), the equation to analytically solve for confidence interval of a mean is as follows: CI of mean = sample mean $\\pm$ $($z-score $\\times$ Standard Error $)$ Standard Error = $\\sigma_{sample} \\ \\mathbin{/} \\ \\sqrt{N}$ where $N$ is the number of measured samples. If you increase the number of samples, the standard error of a mean decreases. This logically makes sense, because the more samples you have, the more accurate the estimation of the true population mean becomes. The size of each Bootstrap realization, N, works the similar way, except that the random sample in each realization is not from the true population, but from a measured sample pool. Increasing the N-value will falsely make you to calculate smaller confidence interval. It can be observed that the CI obtained by using a wrong N-value for Bootstrap generates narrower CI. As a result, the CI of the sample mean does not cover the true population mean, returning a misleading estimation. In summary, Bootstrapping is used for three reasons: Bootstrap can obtain confidence interval in any statistics. Bootstrap does not assume anything about a distribution. Bootstrap helps when there are too few number of samples. Imports In [2]: import pandas as pd import numpy as np import scipy.stats import matplotlib.pyplot as plt import matplotlib.gridspec as gridspec % matplotlib notebook 1.A. Confidence Intervals in Summary Stats: US Male Height - Gaussian Distribution Bootstrap simulation can be run to obtain confidence intervals in various population parameters: mean, stdev, variance, min, or max. In this example, we will work with the height distribution of the US Male population, which tends to be Gaussian. However, the fact that the distribution Gaussian is totally unrelated to Bootstrap simulation, because it does not assume anything about the distribution. Bootstrapping can give us confidence intervals in any summary statistics like the following: By 95% chance, the following statistics will fall within the range of: Mean : 75.2 ~ 86.2, with 80.0 being the average Standard Deviation : 2.3 ~ 3.4 with 2.9 being the average Min : 54.3 ~ 57.2, with 55.2 being the average Max : 77.8 ~ 82.4, with 79.8 being the average Skew : -0.053 ~ 0.323, with 0.023 being the average 1.A.0. Bootstrap Scripts Bootstrap Simulator In [3]: def bootstrap_simulation ( sample_data , num_realizations ): n = sample_data . shape [ 0 ] boot = [] for i in range ( num_realizations ): real = np . random . choice ( sample_data . values . flatten (), size = n ) boot . append ( real ) columns = [ 'Real ' + str ( i + 1 ) for i in range ( num_realizations )] return pd . DataFrame ( boot , index = columns ) . T Summary Statistics Calculator In [4]: def calc_sum_stats ( boot_df ): sum_stats = boot_df . describe () . T [[ 'mean' , 'std' , 'min' , 'max' ]] sum_stats [ 'median' ] = boot_df . median () sum_stats [ 'skew' ] = boot_df . skew () sum_stats [ 'kurtosis' ] = boot_df . kurtosis () sum_stats [ 'IQR' ] = boot_df . quantile ( 0.75 ) - boot_df . quantile ( 0.25 ) return sum_stats . T Visualization Script In [5]: def visualize_distribution ( dataframe , ax_ ): dataframe = dataframe . apply ( lambda x : x . sort_values () . values ) for col , label in zip ( dataframe , dataframe . columns ): fit = scipy . stats . norm . pdf ( dataframe [ col ], np . mean ( dataframe [ col ]), np . std ( dataframe [ col ])) ax_ . plot ( dataframe [ col ], fit ) ax_ . set_ylabel ( 'Probability' ) Generate Confidence Intervals In [6]: def calc_bounds ( conf_level ): assert ( conf_level < 1 ), \"Confidence level must be smaller than 1\" margin = ( 1 - conf_level ) / 2 upper = conf_level + margin lower = margin return margin , upper , lower def calc_confidence_interval ( df_sum_stats , conf_level ): margin , upper , lower = calc_bounds ( conf_level ) conf_int_df = df_sum_stats . T . describe ( percentiles = [ lower , 0.5 , upper ]) . iloc [ 4 : 7 , :] . T conf_int_df . columns = [ 'P' + str ( round ( lower * 100 , 1 )), 'P50' , 'P' + str ( round ( upper * 100 , 1 ))] return conf_int_df def print_confidence_interval ( conf_df , conf_level ): print ( 'By {}% c hance, the following statistics will fall within the range of: \\n ' . format ( round ( conf_level * 100 , 1 ))) margin , upper , lower = calc_bounds ( conf_level ) upper_str = 'P' + str ( round ( upper * 100 , 1 )) lower_str = 'P' + str ( round ( lower * 100 , 1 )) for stat in conf_df . T . columns : lower_bound = round ( conf_df [ lower_str ] . T [ stat ], 1 ) upper_bound = round ( conf_df [ upper_str ] . T [ stat ], 1 ) mean = round ( conf_df [ 'P50' ] . T [ stat ], 1 ) print ( \" {0:<10} : {1:>10} ~ {2:>10} , AVG = {3:>5} \" . format ( stat , lower_bound , upper_bound , mean )) 1.A.1 Sample Data Description 100 samples of US male height data is provided in my Github Repo - sample_data/US_Male_Height.csv . Summary statistics of the sample data can be calculated. Your goal is to calculate the confidence intervals for the summary stats. In [7]: # height data height_data = pd . read_csv ( 'sample_data/US_Male_Height.csv' ) height_data . index = [ 'Male ' + str ( i + 1 ) for i in range ( height_data . shape [ 0 ])] height_data . round ( 1 ) . T Out[7]: Male 1 Male 2 Male 3 Male 4 Male 5 Male 6 Male 7 Male 8 Male 9 Male 10 ... Male 91 Male 92 Male 93 Male 94 Male 95 Male 96 Male 97 Male 98 Male 99 Male 100 Height (in) 70.8 72.8 72.5 67.3 72.7 73.6 65.0 67.1 70.8 70.6 ... 71.7 66.4 72.9 74.5 73.5 70.5 73.1 63.6 68.7 73.0 1 rows × 100 columns In [8]: height_summary_stats = calc_sum_stats ( height_data ) height_summary_stats Out[8]: Height (in) mean 69.881971 std 3.169548 min 63.143732 max 77.762886 median 69.894434 skew -0.059779 kurtosis -0.700743 IQR 5.154145 Visualization In [9]: fig , ax = plt . subplots ( figsize = ( 8 , 4 )) ax . set_xlabel ( 'Height (inches)' ); fig . suptitle ( 'Original Sample Data Distribution: Gaussian Distribution' ) visualize_distribution ( height_data , ax ); Based on the distribution plot of the original sample data, we can observe that the distribution indeed looks Gaussian. However, the fact that it looks like Gaussian does not matter at all when Bootstrapping, because Bootstrapping does not assume anything about the distribution. 1.A.2 Resampling From the Sample Data Each Bootstrap resampling (realization) can be done in one-line with numpy.random.choice() . Each realization is an array of size N, where N is the length of the original sample data. There are M number of realizations, where M is an arbitrary number of your choice. Results In [10]: M = 100 # number of realizations - arbitrary bootstrap_data = bootstrap_simulation ( height_data , M ) bootstrap_data . round ( 1 ) . head ( 10 ) Out[10]: Real 1 Real 2 Real 3 Real 4 Real 5 Real 6 Real 7 Real 8 Real 9 Real 10 ... Real 91 Real 92 Real 93 Real 94 Real 95 Real 96 Real 97 Real 98 Real 99 Real 100 0 72.7 67.9 65.0 69.2 70.4 64.9 70.3 66.3 67.6 72.2 ... 74.1 66.3 73.0 68.5 65.3 72.5 72.7 69.2 66.4 72.3 1 68.4 70.5 65.3 64.5 71.8 69.2 70.5 71.6 65.3 67.9 ... 72.5 68.6 66.1 71.7 67.3 74.1 67.9 71.3 72.9 65.0 2 71.3 74.1 72.8 72.9 68.3 67.9 73.1 65.0 73.6 72.8 ... 69.5 72.5 72.5 73.3 69.2 74.1 73.0 65.5 67.9 63.1 3 72.5 73.0 71.4 68.5 73.3 70.5 70.5 70.6 68.5 69.0 ... 64.5 69.2 66.0 69.5 72.5 70.3 67.9 68.3 73.6 73.5 4 67.2 73.5 73.6 67.2 64.5 72.9 72.8 66.4 69.2 66.8 ... 66.3 73.6 71.3 73.1 71.6 72.2 64.9 69.0 71.7 70.2 5 69.1 66.0 65.5 69.1 71.7 70.6 66.0 73.0 72.2 69.9 ... 73.0 66.3 69.0 67.9 69.4 69.9 69.5 68.7 72.4 67.3 6 72.2 68.5 72.9 63.1 73.6 73.1 70.8 75.5 69.9 70.6 ... 68.4 65.0 68.5 68.8 67.2 72.2 65.5 70.6 72.2 66.8 7 73.1 72.5 69.0 72.5 71.6 68.7 73.5 66.2 71.6 74.4 ... 73.6 68.5 72.2 73.1 72.3 72.1 66.8 77.8 72.8 69.5 8 67.3 69.5 74.5 66.8 69.1 65.0 69.9 70.6 65.0 73.1 ... 67.1 72.8 70.5 70.8 73.6 72.5 71.8 67.9 67.2 70.6 9 66.3 72.9 65.5 72.5 72.4 70.6 73.1 69.5 67.2 68.7 ... 71.6 69.2 65.0 71.3 71.4 69.1 73.3 70.2 66.1 67.6 10 rows × 100 columns In [11]: boot_sum_stats = calc_sum_stats ( bootstrap_data ) boot_sum_stats . round ( 1 ) Out[11]: Real 1 Real 2 Real 3 Real 4 Real 5 Real 6 Real 7 Real 8 Real 9 Real 10 ... Real 91 Real 92 Real 93 Real 94 Real 95 Real 96 Real 97 Real 98 Real 99 Real 100 mean 69.4 70.0 69.9 70.1 70.0 69.5 70.2 70.2 69.7 70.5 ... 69.7 69.9 69.7 69.9 70.1 70.0 70.0 69.5 70.1 69.4 std 3.1 3.1 3.2 3.3 3.3 3.1 3.1 3.2 2.9 3.0 ... 3.1 3.3 2.8 3.0 3.2 2.8 3.2 3.2 3.2 3.3 min 63.1 63.6 64.1 63.1 63.1 63.1 63.1 63.1 63.6 63.1 ... 63.1 63.1 63.1 63.6 63.6 64.5 64.1 63.1 63.1 63.1 max 75.5 77.8 76.1 77.8 76.1 76.1 76.1 77.8 77.8 77.8 ... 75.5 77.8 77.8 75.5 77.8 76.1 77.8 77.8 77.8 76.1 median 69.1 70.3 70.5 70.4 70.9 69.2 70.5 70.6 69.5 70.7 ... 69.9 69.5 69.5 69.9 69.9 69.9 70.4 69.2 70.6 69.1 skew -0.1 -0.0 -0.2 -0.0 -0.4 -0.0 -0.3 -0.3 0.2 -0.2 ... -0.4 -0.0 -0.0 -0.3 -0.1 0.1 0.2 -0.1 -0.2 -0.0 kurtosis -1.1 -0.8 -1.1 -0.4 -0.9 -0.7 -0.9 -0.6 -0.4 -0.5 ... -0.9 -0.7 -0.3 -0.9 -0.8 -0.9 -0.6 -0.7 -0.8 -1.0 IQR 5.4 5.2 5.9 4.2 5.2 4.9 5.5 5.0 4.9 4.2 ... 5.2 5.4 4.9 4.7 4.8 4.4 5.1 4.8 5.3 5.7 8 rows × 100 columns Visualize In [13]: fig , ax = plt . subplots ( figsize = ( 8 , 4 )) ax . set_xlabel ( 'Height (inches)' ); fig . suptitle ( 'Distribution of Bootstrap-Simulated Data: Gaussian' ) visualize_distribution ( bootstrap_data , ax ); Each line in the plot represents one Bootstrap realization. There are 100 realizations, each having 100 random samples. 1.A.3 Uncertainty Models in Summary Statistics with Blox Plots In [16]: f = plt . figure () plt . suptitle ( 'Uncertainty Models for Various Statistics: US Male Height - Gaussian' ) gs = gridspec . GridSpec ( 2 , 4 ) ax1 = plt . subplot ( gs [ 0 , 0 : 4 ]) ax2 = plt . subplot ( gs [ 1 , 0 ]) ax3 = plt . subplot ( gs [ 1 , 1 ]) ax4 = plt . subplot ( gs [ 1 , 2 ]) ax5 = plt . subplot ( gs [ 1 , 3 ]) boot_sum_stats . T [[ 'mean' , 'min' , 'max' , 'median' ]] . boxplot ( ax = ax1 ) boot_sum_stats . T [[ 'std' ]] . boxplot ( ax = ax2 ) boot_sum_stats . T [[ 'IQR' ]] . boxplot ( ax = ax3 ) boot_sum_stats . T [[ 'skew' ]] . boxplot ( ax = ax4 ) boot_sum_stats . T [[ 'kurtosis' ]] . boxplot ( ax = ax5 ) ax5 . set_ylim ([ - 3 , 3 ]); 1.A.4 Confidence Interval in Summary Statistics Confidence intervals of summary statistics usually have a confidence level of 90%, 95%, or 99%. In this case, we will choose 95% confidence level . In [17]: confidence_level = 0.95 conf_int = calc_confidence_interval ( boot_sum_stats , confidence_level ) conf_int . round ( 1 ) Out[17]: P2.5 P50 P97.5 mean 69.4 69.9 70.5 std 2.8 3.2 3.4 min 63.1 63.1 64.5 max 75.4 77.8 77.8 median 69.1 70.0 71.1 skew -0.5 -0.1 0.3 kurtosis -1.1 -0.7 -0.1 IQR 4.0 4.9 5.8 In [18]: print_confidence_interval ( conf_int , confidence_level ) By 95.0% chance, the following statistics will fall within the range of: mean : 69.4 ~ 70.5 , AVG = 69.9 std : 2.8 ~ 3.4 , AVG = 3.2 min : 63.1 ~ 64.5 , AVG = 63.1 max : 75.4 ~ 77.8 , AVG = 77.8 median : 69.1 ~ 71.1 , AVG = 70.0 skew : -0.5 ~ 0.3 , AVG = -0.1 kurtosis : -1.1 ~ -0.1 , AVG = -0.7 IQR : 4.0 ~ 5.8 , AVG = 4.9 1.B. Confidence Intervals in Summary Stats: Rock Permeability - Lognormal Distribution It was previously stated that Bootstrapping does not assume anything about the distribution. Is that really true? The previous example of the US Male Height distribution was a Gaussian distribution. But what if the distribution of our interest is not Gaussian? In this example, rock pearmeability, which has a lognormal distribution , will be used to show that Bootstrap does not depend on the type of the distribution. 1.B.0. Bootstrap Scripts The sample scripts used for US Male Height example will be used for Bootstrap simulation. Same scripts can be used for both Gaussian and lognormal distribution because Bootstrapping does not assume anything about the distribution. 1.B.1. Sample Data Description 105 samples of permeability data is provided in Github Repo - sample_data/PoroPermSampleData.xlsx . Permeability data is taken at many times at different depth of a wellbore. Summary statistics of the sample data can be calculated. Your goal is to calculate the confidence intervals for the summary stats. In [19]: # permeability data perm_depth_data = pd . read_excel ( 'sample_data/PoroPermSampleData.xlsx' , sheet_name = 'Sheet1' )[[ 'Depth' , 'Permeability (mD)' ]] perm_data = perm_depth_data [ 'Permeability (mD)' ] . to_frame () # visualize fig = plt . figure () ax = plt . axes () ax . plot ( perm_depth_data [ 'Permeability (mD)' ], perm_depth_data [ 'Depth' ]); ax . invert_yaxis () ax . set_title ( 'Permeability Along A Wellbore' ) ax . set_xlabel ( 'Permeability (mD)' ) ax . set_ylabel ( 'Depth (ft)' ); In [20]: perm_summary_stats = calc_sum_stats ( perm_data ) perm_summary_stats Out[20]: Permeability (mD) mean 161.008972 std 80.900128 min 43.534147 max 573.461883 median 144.329837 skew 1.625086 kurtosis 5.498080 IQR 102.580432 Visualization In [21]: fig , ax = plt . subplots ( figsize = ( 8 , 4 )) ax . set_xlabel ( 'Permeability (mD)' ) fig . suptitle ( 'Original Sample Data Distribution: Lognormal Distribution' ) visualize_distribution ( perm_data , ax ); Based on the distribution of the original sample data, we can observe that the distribution looks lognormal. The uncertainty in summary statistics can be calculated using Bootstrap the same way it was done for the US Male Height (Gaussian) distribution, because Bootstrap does not depend on the shape of the distribution. Warning! Outlier removal on rock permeability cannot be done directly, as this is a lognormal distribution. Recall that the typical outlier removal method assumes the distribution to be Gaussian. If you want to detect outliers for non-Gaussian distributions, you have to first transform the distribution into Gaussian. 1.B.2 Resampling From the Sample Data Each Bootstrap resampling (realization) can be done in one-line with numpy.random.choice() . Each realization is an array of size N, where N is the length of the original sample data. There are M number of realizations, where M is an arbitrary number of your choice. Results In [22]: M = 100 # number of realizations - arbitrary boot_perm_data = bootstrap_simulation ( perm_data , M ) boot_perm_data . round ( 1 ) . head ( 10 ) Out[22]: Real 1 Real 2 Real 3 Real 4 Real 5 Real 6 Real 7 Real 8 Real 9 Real 10 ... Real 91 Real 92 Real 93 Real 94 Real 95 Real 96 Real 97 Real 98 Real 99 Real 100 0 61.9 258.6 138.8 61.9 285.3 156.1 179.7 125.9 58.8 89.6 ... 151.0 227.4 59.1 573.5 258.6 56.3 240.6 89.6 132.7 182.6 1 170.5 61.0 143.7 214.1 264.2 244.1 144.7 160.5 83.9 258.6 ... 58.8 279.8 244.1 92.1 213.7 160.5 240.6 146.0 141.8 138.8 2 143.7 86.8 117.6 92.7 83.9 104.0 187.9 138.8 162.3 132.1 ... 258.4 380.1 89.6 89.6 123.3 77.5 102.7 193.1 133.2 234.5 3 166.9 151.0 240.6 265.5 183.1 65.6 59.1 305.1 103.5 131.6 ... 214.9 128.9 210.8 108.6 193.1 125.9 77.5 151.5 112.3 58.8 4 161.0 146.0 89.6 84.9 129.1 43.5 170.5 97.7 190.9 197.8 ... 56.3 85.0 53.4 79.4 58.8 92.7 102.7 190.9 126.3 161.0 5 104.0 132.1 129.1 144.4 184.8 263.5 151.0 170.5 162.9 311.6 ... 61.0 156.1 170.5 264.2 244.1 85.0 112.3 117.6 224.4 265.5 6 305.1 77.5 213.7 84.9 240.6 58.8 224.4 234.5 128.9 193.1 ... 66.9 138.8 240.6 66.9 166.9 84.7 305.1 80.4 53.4 264.2 7 286.8 171.4 92.1 84.9 116.9 245.7 141.8 135.8 206.6 116.9 ... 162.3 244.1 187.9 151.0 84.9 85.0 573.5 170.5 83.3 117.6 8 142.7 187.9 131.6 117.6 244.1 214.1 182.6 134.7 132.7 132.7 ... 123.3 104.0 65.6 86.8 84.9 193.1 56.3 136.9 156.1 311.6 9 58.8 132.1 380.1 136.9 65.6 244.1 134.7 77.8 321.1 79.4 ... 128.9 83.9 182.6 132.0 117.6 234.5 227.4 187.9 80.4 138.8 10 rows × 100 columns In [23]: boot_perm_sum_stats = calc_sum_stats ( boot_perm_data ) boot_perm_sum_stats . round ( 1 ) Out[23]: Real 1 Real 2 Real 3 Real 4 Real 5 Real 6 Real 7 Real 8 Real 9 Real 10 ... Real 91 Real 92 Real 93 Real 94 Real 95 Real 96 Real 97 Real 98 Real 99 Real 100 mean 159.5 152.6 146.9 155.7 160.3 161.7 161.5 164.4 146.6 162.7 ... 143.9 150.0 166.7 155.6 155.2 151.5 164.7 171.3 143.8 148.1 std 80.0 64.1 72.6 77.2 72.1 98.0 88.0 90.7 65.3 73.3 ... 78.4 68.0 64.7 78.0 68.8 74.8 106.0 96.3 63.1 62.0 min 56.3 43.5 43.5 43.5 53.4 43.5 43.5 43.5 43.5 43.5 ... 43.5 43.5 53.4 43.5 43.5 53.4 53.4 43.5 43.5 43.5 max 573.5 305.1 380.1 573.5 380.1 573.5 573.5 573.5 321.1 321.1 ... 573.5 380.1 380.1 573.5 380.1 321.1 573.5 573.5 380.1 380.1 median 143.7 138.8 132.1 144.4 138.8 143.7 142.7 138.8 136.9 145.6 ... 132.0 133.2 151.0 141.8 138.8 133.2 133.2 144.7 132.7 138.8 skew 1.8 0.6 1.1 1.7 0.7 1.6 2.2 2.0 0.6 0.4 ... 2.1 0.8 0.6 1.8 0.7 0.6 1.9 2.2 0.9 0.8 kurtosis 6.1 -0.5 1.0 6.9 -0.3 4.3 7.7 6.4 -0.2 -0.9 ... 8.1 0.5 0.3 6.7 0.2 -0.7 4.6 7.0 1.0 1.1 IQR 79.3 80.8 92.7 113.0 102.4 129.2 102.4 86.7 95.2 114.0 ... 77.3 98.2 78.7 102.6 86.7 112.7 132.3 79.2 82.1 80.8 8 rows × 100 columns Visualize In [29]: fig , ax = plt . subplots ( figsize = ( 8 , 4 )) fig . suptitle ( 'Distribution of Bootstrap-Simulated Data: Lognormal' ) ax . set_xlabel ( 'Permeability (mD)' ) visualize_distribution ( boot_perm_data , ax ); 1.B.3 Uncertainty Models in Summary Statistics with Blox Plots In [25]: f = plt . figure () plt . suptitle ( 'Uncertainty Models for Various Statistics: Rock Permeability - Lognormal' ) gs = gridspec . GridSpec ( 2 , 4 ) ax1 = plt . subplot ( gs [ 0 , 0 : 4 ]) ax2 = plt . subplot ( gs [ 1 , 0 ]) ax3 = plt . subplot ( gs [ 1 , 1 ]) ax4 = plt . subplot ( gs [ 1 , 2 ]) ax5 = plt . subplot ( gs [ 1 , 3 ]) boot_perm_sum_stats . T [[ 'mean' , 'min' , 'max' , 'median' ]] . boxplot ( ax = ax1 ) boot_perm_sum_stats . T [[ 'std' ]] . boxplot ( ax = ax2 ) boot_perm_sum_stats . T [[ 'IQR' ]] . boxplot ( ax = ax3 ) boot_perm_sum_stats . T [[ 'skew' ]] . boxplot ( ax = ax4 ) boot_perm_sum_stats . T [[ 'kurtosis' ]] . boxplot ( ax = ax5 ) ax4 . set_ylim ([ - 3 , 3 ]) ax5 . set_ylim ([ - 10 , 10 ]); Observe the positive skewness in the boxplot summary statistics. This is consistent with the left-justified lognormal distribution of the permeability plot. 1.B.4 Confidence Interval in Summary Statistics Confidence intervals of summary statistics usually have a confidence level of 90%, 95%, or 99%. In this case, we will choose 90% confidence level . In [26]: confidence_level = 0.9 conf_int_perm = calc_confidence_interval ( boot_perm_sum_stats , confidence_level ) conf_int_perm . round ( 1 ) Out[26]: P5.0 P50 P95.0 mean 146.1 160.0 171.3 std 64.4 77.4 96.4 min 43.5 43.5 54.3 max 321.1 573.5 573.5 median 132.7 144.3 156.1 skew 0.4 1.4 2.2 kurtosis -0.6 4.0 8.6 IQR 78.4 98.1 120.9 In [27]: print_confidence_interval ( conf_int_perm , confidence_level ) By 90.0% chance, the following statistics will fall within the range of: mean : 146.1 ~ 171.3 , AVG = 160.0 std : 64.4 ~ 96.4 , AVG = 77.4 min : 43.5 ~ 54.3 , AVG = 43.5 max : 321.1 ~ 573.5 , AVG = 573.5 median : 132.7 ~ 156.1 , AVG = 144.3 skew : 0.4 ~ 2.2 , AVG = 1.4 kurtosis : -0.6 ~ 8.6 , AVG = 4.0 IQR : 78.4 ~ 120.9 , AVG = 98.1","tags":"Statistics","url":"https://aegis4048.github.io/non-parametric-confidence-interval-with-bootstrap","loc":"https://aegis4048.github.io/non-parametric-confidence-interval-with-bootstrap"},{"title":"Uncertainty Modeling with Monte-Carlo Simulation","text":"The code snippet assumes Anaconda 5.2.0 version of Python virtual environment Acknowledgement I would like to acknowledge Micahel Pyrcz , Associate Professor at the University of Texas at Austin in the Petroleum and Geosystems Engineering, for developing course materials that helped me write this article. Check out his Youtube Lecture on Monte-Carlo Simulation to help yourself better understand the statistical theories and concepts. Monte Carlo Simulation is a random sampling method to model uncertainty of a population estimation. When given only population parameters (mean, standard deviation, degrees of freedom, etc..), but not the sample data itself, it generates random samples based on the distribution parameters to create a sample pool that is representative of the true population. Uncertainty models can be created from the newly generated sample pool. Based on historical data, expertise in the field, or past experience, you might know the typical values of population mean, standard deviation and degrees of freedom. While these parameters are useful for developing a model, they do not tell you the uncertainties in a population. In a financial market, you might know the distribution of possible values through the mean and standard deviation of returns. By using a range of possible values, instead of a single guess, you can create a more realistic picture of what might happen in the future. Let's assume that your consultant recommended you a certain investment program that has a mean return rate of 10% and a standard deviation of 1% However, You do not have an access to the actual sample data that is used to obtain the mean, and standard deviation . You made 100 investments through this program, but your 100 investments had an average rate of return of 3%. Did the consultant lie to you, or is it one of the possible corner cases that you can have if you are unlucky? What is the P10, P50, P90 value of this investment program? What are the the most plausible range of rate of return? Does your 3% rate of return fall within that range ? In order to answer these questions, you need sample data that is representative of the population. Monte-Carlo simulation takes population parameters as arguments, and generates series of random samples to investigate a range of possible outcomes . Methodology Monte-Carlo simulation is one of the random sampling method that generates a new set of random samples from statistic parameters of a population. It assumes a certain distribution shape, and population parameters as input and returns a random sample based on the distribution shape and parameters. The most simple examples are as follows: Excel Gaussian: NORM.INV(RAND(), mean, stdev) Lognormal: LOGNORM.INV(RAND(), mean, stdev) Chi-Square: CHISQ.INV(RAND(), degree_freedom) F-distribution: F_INV(RAND(), degree_freedom_numerator, degree_freedom_denominator) Python Gaussian: np.random.normal(mean, stdev) Lognormal: np.random.lognormal(mean, stdev) Chi-Square: np.random.chisquare(degree_freedom) F-distribution: np.random.f(degree_freedom_numerator, degree_freedom_denominator) These examples are the most simple cases of generating random samples vis Monte-Carlo simulation. Random samples can be generated as many times as desired. Based on the N-number of random samples generated, you can draw a CDF or boxplot to model uncertainty in prediction. Warning! In order to use Monte-Carlo simulation, you must know the distribution shape (normal, lognormal, chi-square, etc..) and distribution parameters (mean, standard deviation, degrees of freedom, etc..) of the data. If you do not have enough samples to draw an uncertainty model, or do not know the distribution shape and parameters, Bootstrap simulation may address your issue. Random samples of interest can can be created via an applied form of Monte-Carlo simulation. For example, the Casino Dice Roll Example simulates a game 1,000 times for a single player, and calculates a player's final fund at the end. The final fund of a single player is one random Monte-Carlo sample. The process is repeated 100 times to account for 100 players' final fund, and now we have 100 random Monte-Carlo samples. Total Thickness of Two Formations Example generates two sets of Monte-Carlo formation samples N times (where N is arbitrary number of your choice) to account for Formation A, and Formation B. The two sets of Monte-Carlo formation data are then added together to obtain Monte-Carlo data for total thickness. 1. Casino Dice Roll Example How do casinos earn money? The answer is simple - the longer you play, the bigger the chance of you losing money. Let's assume an imaginary dice roll game between a casino house and a player. The rules are simple. Dice Roll Game Rules There is an imaginary dice that rolls between 1 to 100. If a player rolls between 1 to 51, the house wins. If a player rolls between 52 to 100, the player wins. A player can bet as many times as he wants. With the above rules, the house has 2% higher chance of winning over a player . As a financial analyst of the house, upper management wants you to create a Dice Roll game profit forecast model. Question : If a certain game is configured so that the house has 2% higher chance of winning over a player , what is the expected profit forecast model for the game? Monte-Carlo simulation can be used to simulate the possible outcomes of dice roll game, and generate a forecast model. 1.0 Game Simulator Scripts Imports In [2]: import random import scipy import matplotlib.pyplot as plt import pandas as pd import numpy as np % matplotlib notebook Dice Roll Simulation In [3]: def rolldice (): dice = random . randint ( 1 , 100 ) if dice <= 51 : # Player loses return False elif dice > 51 & dice <= 100 : # Player wins return True Single Game Simulation In [4]: def play ( total_funds , wager_amount , total_plays , final_fund ): play_num = [] # x-axis of the plot funds = [] # y-axis of the plot play = 1 while play <= total_plays : if rolldice (): # Player wins total_funds = total_funds + wager_amount # updates current total funds play_num . append ( play ) funds . append ( total_funds ) else : # Player loses total_funds = total_funds - wager_amount play_num . append ( play ) funds . append ( total_funds ) play = play + 1 final_fund . append ( funds [ - 1 ]) # final_fund contains the ending fund of all players return final_fund , play_num , funds Results Visualization In [5]: def simulate_visualize ( init_money , bet , num_bet , num_players = 1 ): # simulates and generates a plot f , ax = plt . subplots () count = 1 ending_fund_all_players = [] while count <= num_players : ending_fund_all_players , num_play , funds_record = play ( init_money , bet , num_bet , ending_fund_all_players ) ax . plot ( num_play , funds_record ) count += 1 ax . set_title ( str ( num_players ) + ' Player(s): ' + 'Change in Total Fund with Each Game' ) ax . set_ylabel ( 'Player \\' s Fund ($)' ) ax . set_xlabel ( 'Number of Bets' ) return ending_fund_all_players In [6]: def simulate ( init_money , bet , num_bet , num_players = 1 ): # simulates, but does not plot count = 1 ending_fund_all_players = [] while count <= num_players : ending_fund_all_players , num_play , funds_record = play ( init_money , bet , num_bet , ending_fund_all_players ) count += 1 return ending_fund_all_players 1.1 Monte-Carlo Simulation: 1 Player Let's say than an imaginary player, 'Eric', visits the house and wants to play the Dice Roll Game. A Monte-Carlo simulation can be run to simulate the result of Eric's game. The simulation will be run with the following conditions: Eric starts with \\$10,000 Eric bets \\$100 each time Eric plays the game 1,000 times In [148]: simulate_visualize ( init_money = 10000 , bet = 100 , num_bet = 1000 , num_players = 1 ) plt . axhline ( 10000 , color = \"red\" , linewidth = 3 ) plt . text ( 780 , 10200 , 'Starting Money $10,000' , color = 'red' ); Eric started with 10,000 dollars. To your surprise, Eric actually ended up earning money from the house by 2,500 dollars after 1,000 games . According to the configuration of the game, the house has 2% higher chance of winning over Eric. Therefore, with such a high number of games, like a thousand, the house was supposed to earn money from the player. But it was not the case here. Was the configuration of the game wrong, or was Eric just really lucky? 1.1 Monte-Carlo Simulation: 100 Players Eric earned $2,500 dollars after running 1,000 games. However, if hundred other players play the Dice Roll game for thousand times each, would the result be different? From the house's perspective, what is the expected profit from the Dice Roll game? To get more accurate estimation of the expected profit, multiple Monte-Carlo simulation will be run. In this case, hundred. The simulation will be run with the following conditions: Hundred players each start with \\$10,000 Hundred players bet \\$100 each time Hundred players play the game 1,000 times In [7]: simulate_visualize ( init_money = 10000 , bet = 100 , num_bet = 1000 , num_players = 100 ) plt . axhline ( 10000 , color = \"white\" , linewidth = 3 ) plt . text ( 100 , 10400 , 'Starting Money $10,000' , color = 'white' , weight = 'bold' ); As it can be shown on the plots, Eric's earning 2,500 dollars after 1,000 games was a plausible outcome. There was even a player who eanred ended up with 16,500 dollars, which means that he earned 6,500 dollars ! However, this does not mean that the house will earn negative profit. The plot clearly indicates overall trend in the house earning money over the players as the number of bets increases. 1.3 Uncertainty Modeling The previous simulation results represent the outcome of 100 players each playing 1,000 games . One hundred Monte-Carlo simulations were run, and now we have one hundred samples of 1,000 game simulations data. To obtain more accurate uncertainty model for the Dice Roll game, further simulations will be run for 1,000 players each playing 100, 1,000, 10,000, and 100,000 games . In [6]: df = pd . DataFrame () for num_games in [ 100 , 1000 , 5000 , 10000 ]: result = simulate ( init_money = 10000 , bet = 100 , num_bet = num_games , num_players = 1000 ) col_name = str ( num_games ) + ' Games ($)' df [ col_name ] = result In [7]: df . index . name = 'Player Number' df . head ( 10 ) Out[7]: 100 Games ($) 1000 Games ($) 5000 Games ($) 10000 Games ($) Player Number 0 8400 7000 5800 -15600 1 8600 8000 16000 -34800 2 9600 7600 -400 5000 3 9400 10400 -6600 -11200 4 9400 10600 -400 0 5 8600 7200 -200 -19600 6 10800 7800 5600 -14000 7 9800 12400 -4000 -6200 8 10600 7600 24600 -9400 9 7400 7400 1800 -19200 In [10]: ax = df . boxplot ( grid = False ) ax . set_title ( 'Uncertainty Model for Dice Roll Game Profit: 1000 Players' ) ax . set_ylabel ( 'Player \\' s Fund ($)' ) ax . axhline ( 10000 , color = \"red\" , linewidth = 3 ); ax . text ( 3.5 , 11500 , 'Starting Money $10,000' , color = 'red' ); The generated box plot is the forecast model for the Dice Roll game profit generation. It tells you the most likely range of profit expected for N number of games played for each player. Based on the box plot uncertainty model, you can confirm that the longer you play, the bigger chance of you losing money. Although some lucky players may double, or even triple their money at the casino, far bigger population of the players will end up losing money to the casino. Recall that the Dice Roll game was configured so that the Casino has 2% higher chance of winning the game over a player. Summary: A player starts with 10,000 dollars and bets 100 dollar for each game. If a player plays 100 games, he will most likely end up between 12,500 to 6800 dollars If a player plays 1000 games, he will most likely end up between 15,800 to $-$360 dollars If a player plays 5,000 games, he will most likely end up between 19,200 to $-$18,900 dollars If a player plays 10,000 games, he will most likely end up between 15,200 to $-$36,000 dollars 1.4 Outlier Removal and Mean of the Prediction The uncertainty model generated by Monte-Carlo simulations gives you a range of possible outcome. But what if you want a single value of the outcome? One simple way to address this question is to just calculate the average of the simulated data. Means of simulated data BEFORE outlier removal In [192]: raw_mean = pd . DataFrame ( df . describe () . T [ 'mean' ]) . T raw_mean . rename ( index = { 'mean' : 'original mean' }, inplace = True ) raw_mean Out[192]: 100 Games ($) 1000 Games ($) 5000 Games ($) 10000 Games ($) original mean 9812.4 7930.8 214.0 -9865.2 But as it can be observed in the boxplot, the simulated data contains outliers (circled points). One might want to remove these outliers before calculating the average of the data to improve accuracy. The traditional IQR outlier detection method can be implemented. IQR = P75 - P25 Lower Fence = P25 - 1.5 $\\times$ IQR Upper Fence = P75 + 1.5 $\\times$ IQR In [1]: def get_outlier_params ( orig_data ): iqr_params = orig_data . describe () . T [[ '25%' , '75%' ]] iqr_params [ 'IQR' ] = iqr_params [ '75%' ] - iqr_params [ '25%' ] iqr_params [ 'Lower Fence' ] = iqr_params [ '25%' ] - 1.5 * iqr_params [ 'IQR' ] iqr_params [ 'Upper Fence' ] = iqr_params [ '75%' ] + 1.5 * iqr_params [ 'IQR' ] return iqr_params In [194]: iqr_params = get_outlier_params ( df ) iqr_params Out[194]: 25% 75% IQR Lower Fence Upper Fence 100 Games ($) 9200.0 10600.0 1400.0 7100.0 12700.0 1000 Games ($) 6000.0 10200.0 4200.0 -300.0 16500.0 5000 Games ($) -4450.0 5200.0 9650.0 -18925.0 19675.0 10000 Games ($) -16600.0 -3150.0 13450.0 -36775.0 17025.0 Means of simulated data AFTER outlier removal In [195]: def remove_outliers ( outlier_params , data ): outlier_removed_df = pd . DataFrame () for column in data . columns : outlier_removed_df [ column ] = data [ column ] . apply ( lambda x : x if x > outlier_params [ 'Lower Fence' ][ column ] else np . nan ) outlier_removed_df [ column ] = data [ column ] . apply ( lambda x : x if x < outlier_params [ 'Upper Fence' ][ column ] else np . nan ) return outlier_removed_df In [196]: new_df = remove_outliers ( iqr_params , df ) new_mean = pd . DataFrame ( new_df . describe () . round ( 1 ) . T [ 'mean' ]) . T new_mean . rename ( index = { 'mean' : 'outlier-removed mean' }, inplace = True ) pd . concat ([ raw_mean , new_mean ]) Out[196]: 100 Games ($) 1000 Games ($) 5000 Games ($) 10000 Games ($) original mean 9812.4 7930.8 214.0 -9865.2 outlier-removed mean 9800.2 7892.8 172.7 -9950.1 Based on the simulated mean of each players Dice Roll game result, it can be observed that a player will lose ~20,000 dollars if he plays the 10,000 games , betting 100 dollars each game. 2. Oil Field Example: Total Thickness of Two Formations Your company is about to drill into two formations: formation A and formation B . From the previous experiences within the asset, you know the the distribution of each formation's thickness (which is rarely the case...). In order to develop production / facility plans, you need to draw an uncertainty model for the total thickness of formation A + formation B . 2.0.1 Assumptions Before Monte-Carlo simulation is run to develop the uncertainty model, a few assumptions will be made. The formation thickness in the asset has Gaussian distribution Formation A has a mean value of 10 ft, and standard deviation of 2 ft. Formation B has a mean value of 24 ft, and standard deviation of 4 ft. The mean and standard deviation were calculated from large enough samples, and their values are reliable. We are not given any sample data set. We are only given mean and standard deviations. In [18]: assumptions = pd . DataFrame ( data = [[ 10 , 24 ],[ 2 , 4 ]], columns = [ 'Formation A (ft)' , 'Formation B (ft)' ], index = [ 'mean' , 'stdev' ]) assumptions Out[18]: Formation A (ft) Formation B (ft) mean 10 24 stdev 2 4 Recall that Monte-Carlo simulation requires the distribution shape and distribution parameters of the population. If we know the distribution shape, but do not have large enough samples to estimate reasonable values for the mean and the standard deviation of the population, Monte-Carlo simulation for Gaussian distribution may return inaccurate results. This can't really be helped since we just don't have enough samples. Furthurmore, if we have reasonably large enough samples, but do not know the distribution shape, Monte-Carlo simulation cannot be run . Recall that when generating random samples, it assumes a certain form of a distribution. (Ex: numpy.random.normal() , numpy.random.lognormal() , numpy.random.chiquare() ). Notes If Monte-Carlo simulation cannot be run because the distribution shape is unknown, non-parametric Bootstrap simulation can be used to generate random samples. 2.0.2 Why Use Monte-Carlo Simulation? One might ask why Monte-Carlo simulation is needed for this task. Why can't we just add the provided means of the two formations and use it for our thickness model? Total Thickness = Form. A Mean Thickness + Form. B Mean Thickness Total Thickness = 10 ft + 24 ft = 34 ft However, this simple forecast model does not give any information about the uncertainty in the total thickness of the formation. That is, we only know the overall mean thickness, but nothing about the possible range of thickness of the formations. Ideally we want to formulate something like the following: The total formation thickness will fall within the range of 27 ~ 41 ft by 80% chance, with 34 ft being the mean of the distribution. When we are given only the estimated mean and standard deviation of the population, uncertainty model cannot be formulated without some kind of random sampling method. Monte-Carlo simulation can be used to generate a pool of random samples. 2.1 Monte-Carlo Simulation for Gaussian Distribution Steps Using the provided mean and standard deviation, generate a random Gaussian distribution of Formation A and B thickness. Recall that we assumed the thickness distribution to be Gaussian. Generate random thickness values N times. Add the randomly generated thickness values for Formation A and B. Generate visualizations (CDF, boxplot, etc...) The distribution is Gaussian, and therefore np.random.normal() will be used to generate random normal distribution of formation thickness. If the distribution was assumed to be non-Gaussian, other function will be used to create random samples. For more information, check the numpy documentation of random sampling for various distributions . In [19]: mean_A = assumptions [ 'Formation A (ft)' ][ 'mean' ] mean_B = assumptions [ 'Formation B (ft)' ][ 'mean' ] std_A = assumptions [ 'Formation A (ft)' ][ 'stdev' ] std_B = assumptions [ 'Formation B (ft)' ][ 'stdev' ] iteration = 1000 monte_A = np . random . normal ( mean_A , std_A , iteration ) monte_B = np . random . normal ( mean_B , std_B , iteration ) total_thic = monte_A + monte_B df_thic = pd . DataFrame ([ monte_A , monte_B , total_thic ], index = [ 'Formation A (ft)' , 'Formation B (ft)' , 'Total Thickness (ft)' ]) . T df_thic . index . name = 'Iteration' df_thic . round ( 1 ) . head ( 10 ) Out[19]: Formation A (ft) Formation B (ft) Total Thickness (ft) Iteration 0 7.3 29.4 36.6 1 9.4 28.7 38.1 2 7.7 18.5 26.1 3 11.0 33.1 44.1 4 8.1 21.8 29.9 5 10.0 23.2 33.2 6 10.2 26.5 36.7 7 10.8 25.5 36.3 8 10.8 22.7 33.4 9 10.1 23.1 33.3 Visualizations Cumulative probablity function (CDF) and boxplot can be used to visualize the simulation result. In [33]: def visualize_distribution ( dataframe , ax_ ): dataframe = dataframe . apply ( lambda x : x . sort_values () . values ) for col , label in zip ( dataframe , dataframe . columns ): fit = scipy . stats . norm . pdf ( dataframe [ col ], np . mean ( dataframe [ col ]), np . std ( dataframe [ col ])) ax_ . plot ( dataframe [ col ], fit ) ax_ . set_ylabel ( 'Probability' ) In [34]: fig , ( ax1 , ax2 ) = plt . subplots ( 1 , 2 , figsize = ( 8 , 4 )) fig . suptitle ( 'Uncertainty Models for Total Formation Thickness' ) visualize_distribution ( df_thic [ 'Total Thickness (ft)' ] . to_frame (), ax1 ) ax1 . set_title ( 'Probability Distribution Function' ) ax1 . set_ylabel ( 'Probability' ) ax1 . set_xlabel ( 'Total Thickness (ft)' ) ax2 . boxplot ( df_thic [ 'Total Thickness (ft)' ]) ax2 . set_title ( 'Boxplot' ) ax2 . set_ylabel ( 'Total Thickness (ft)' ); ax2 . set_xticklabels ([]); Business Decision on P10, P50, and P90 Statistics Many of the business decisions are made on P10, P50, and P90 values. When reporting your statistical analysis to the management, you want to provide them the most likely range of outcome. In [36]: pd . DataFrame ( df_thic [ 'Total Thickness (ft)' ] . describe ( percentiles = [ 0.1 , 0.9 ])) . T . iloc [:, 4 : 7 ] . round ( 1 ) Out[36]: 10% 50% 90% Total Thickness (ft) 28.6 34.2 39.9 Based on the obtained P10, P50, and P90 values, the following forcast can be constructed: The total formation thickness will fall within the range of 28.6 ~ 39.9 ft by 80% chance, with 34.2 ft being the mean of the distribution.","tags":"Statistics","url":"https://aegis4048.github.io/uncertainty-modeling-with-monte-carlo-simulation","loc":"https://aegis4048.github.io/uncertainty-modeling-with-monte-carlo-simulation"}]};