var tipuesearch = {"pages":[{"title":"Predicting VRU Economics For Declining Wells","text":"Many producers, when sizing vapor recovery units (VRUs) or signing long-term contracts for them, tend to overlook an important factor: the inevitable decline of flash gas rates over time. Flash gas rates at heater treaters (HT), vapor recovery towers (VRT), and tanks follow a similar decline pattern to well production, yet it's a common misconception that these rates will remain steady or only decrease marginally. Working at a compressor company, I've seen firsthand how producers expect flash gas rates to hold relatively flat or marginally decline; the flash gas rates go down much quicker than they think. While the oil decline rate plays the largest role in this reduction, many still fail to account for it when planning for long-term operations. It's not necessarily that they don't know about the decline, but rather that they simply forget to think about it. Downsizing is often not done in a timely manner because it's overlooked, and as a result, VRUs quickly become oversized and uneconomical. It's also common to see a well experience more than a 50% drop in production within the first few months, making this issue even more critical. KEY DELIVERABLES OF THIS ARTICLE: 1. Process simulation results for predicting low-pressure flash gas rates at each monthly decline. 2. Evidence demonstrating that flash gas rate declines are primarily driven by oil decline, with sales gas decline having a minor impact in comparison. 3. An Excel sheet for calculating VRU economics alongside simulation results. Notes: Clarifications on Terms Low Pressure Vessels: Refers to vessels such as low-pressure separators, heater treaters (HT), vapor recovery towers (VRT), or tanks. These vessels are downstream of the primary separator and operate at pressures lower than the sales line pressure. Sales (Produced) Gas: High-pressure gas from the primary separator that operates around the sales line pressure, typically lean gas with a BTU content of 1050-1600 Btu/scf. Flash Gas: Gas volumes coming from low-pressure vessels operating below sales line pressure. These gases are richer, typically ranging from 1400-3000 Btu/scf, and lack sufficient pressure to enter the sales line. Flash gas can either be flared or recovered by VRUs that compress the gas to above sales line pressure. Flash gas is often distinguished from sales gas. VRU Flowrate: For the purpose of this article, VRU flowrate is synonymous to flash gas. VRU: Vapor Recovery Unit, VRU, is one application of a compressor. The compressor type can be rotary vane, rotary screw, or reciprocating. Contents 0. Introduction Notes: Clarifications on Terms 1. Industry Practices for VRU Sizing 2. Predicting lash gas flowrates with process simulations 2.1. Site (operational) setups 2.2. Simulation scenarios 3. Simulation results 4. VRU Economics 5. Conclusion/Summary 1. Industry Practices for VRU Sizing When a new facility is being built, engineers typically input the expected initial production (IP) flowrates into process simulation software to estimate the expected flash gas flowrates at low-pressure vessels. In the absence of process simulation software, many producers and vendors rely on ballpark estimates using flat scf/bbl numbers based on regional experience to predict flash gas volumes. While quick, this scf/bbl method doesn't account for the operating pressure of the vessels, leading to inaccuracies. These projected rates are then provided to VRU vendors, who size the equipment and provide quotes based on the high IP flowrates. Since these flowrates are often large, the VRU is sized to handle peak capacity, resulting in a costly investment. However, as the well declines, these high flowrates no longer match reality, and the oversized VRU quickly becomes uneconomical. Unfortunately, this issue is often realized too late, as VRUs are not always the primary concern for producers. Some keen producers who pay attention to VRUs predict monthly VRU flowrate declines using process simulations or by applying an oil decline curve to the IP flash gas rates. For example, if oil is expected to drop by 50% in the first 4 months and the IP flash gas rate is initially calculated to be 100 MCFD, the flash gas rate will decrease to around 50 MCFD in 4 months (It's important to note that downstream flash gas rates are not significantly impacted by sales gas decline. See section 3.1). The large and expensive compressor sized for peak initial flowrate will either be downsized or moved to a different location with similar production capacity to ensure its continued economic viability. 2. Predicting lash gas flowrates with process simulations The purpose of this section is to demonstrate that the declining flash gas rates at low-pressure vessels closely follow the oil decline curve and to highlight that gas decline has a minor impact compared to oil decline. BR&E Promax software is used for the simulations. 2.1. Site (operational) setups A generic upstream facility structure is used for simulations. Reservoir fluid flows up the production tubing to the wellhead, then to the first-stage 2-phase separator, followed by a second-stage 2-phase separator operating 5 psi lower than the first stage, a heater treater, VRT, and finally to atmospheric storage tanks. The purpose of the second-stage separator is to separate the liquids entrained in the gas stream from the first-stage separator. Figure 1 shows the facility model created in Promax for IP flowrates. This model serves as the base, accepting monthly declining oil, gas, and water rates ( Section 2.2 ) as inputs, and returning declining rates of downstream flash gases as outputs ( Table ??? ). Figure 1: Generic upstream facility modeling done in BR&E Promax. IP rates of 9 MMSCFD (gas), 1800 BOPD (oil), and 3700 BWPD (water) are assumed. Numbers in the red boxes show produced and flash gas rates. Assumptions Generic upstream oil and gas facility with wellhead. 2 stage 2-phase separators operating at 140 and 135 psig. VRU discharge pressure set equal to 1st separator pressure: 140 psig. Details of VRU components (ex: cooler, knockout scrubbers) are omitted for simplicity. Wellhead temperature: 120°F. 3-phase heater treater operating at 50 psig, 140°F. Atmospheric oil tank operating at 0.4 psig, 120°F 48° API oil. Sales gas energy content: around 1420 Btu/scf Facility Initial production rates: 9 MMSCFD (gas), 1800 BOPD (oil), and 3700 BWPD (water). Future declining production rates are predicted by Figure 2 BR&E Promax solver is used to solve for flowrates at wellhead. Solver adjusts inlet (wellhead) flowrates to match production rates at measurement points (sales gas meter/LACT/trucking/etc) leaving the system. Realistic oil and gas compositions used. 2.2. Simulation scenarios To observe the impact of well decline on downstream flash gas flowrates, decline curve analysis (DCA) parameters are set up to simulate declining oil, gas, and water flowrates. Oil and water decline rates (with water having no impact on flash gas volumes) are fixed to isolate the impact of gas declines. Three scenarios are considered: 1) gas declining faster than oil, 2) gas declining slower than oil, and 3) gas declining at the same rate as oil. The DCA parameters shown in Figure 2 are used to create hypothetical declining well production rates, as shown in Figures 3, 4, and 5 . These rates are then fed into the Promax model to predict flash gas rates at low-pressure vessels. Figure 2: DCA parameters used to simulate declining well flowrates. Oil and water decline rates are fixed to observe the sole impact of produced gas decline on flash gas declines. Note that the b-factor is set the same for both oil and gas in all scenarios for ease of decline curve comparisons. Figure 3: Well decline curve prediction from the DCA parameters in scenario A of Figure 2 . Source Code For Figure (3) import pandas as pd import numpy as np import matplotlib.pyplot as plt def general_decline(t, qi, Di, b): \"\"\" Arps' decline equation for production decline modeling. Args: t (array-like): Time values. qi (float): Initial production rate. Di (float): Initial decline rate. b (float): Decline exponent. Returns: array-like: Production rate at time t. \"\"\" return qi * (1 + b * Di * t) ** (-1 / b) params = { 'Oil': {'qi': 1800, 'Di': 0.3, 'b': .7}, 'Gas': {'qi': 9000, 'Di': 0.5, 'b': 0.7}, 'Water': {'qi': 3700, 'Di': 0.5, 'b': 1} } x = np.arange(0, 61) fits = { 'Oil': [], 'Gas': [], 'Water': [], } for key in params.keys(): target_params = list(params[key].values()) fits[key] = general_decline(x, *target_params) df = pd.DataFrame.from_dict(fits) df['Count'] = x fig, ax = plt.subplots(figsize=(8, 4.5)) oc = 'green' gc = 'red' wc = 'blue' ax.plot(df['Count'], df['Oil'], c=oc, label='Oil') ax.plot(df['Count'], df['Gas'], c=gc, label='Gas') ax.plot(df['Count'], df['Water'], c=wc, label='Water') ax.set_xlabel('Months') ax.set_ylabel('BOPD, BWPD, MCFD') ax.grid(True) ax.spines['top'].set_visible(False) ax.spines['right'].set_visible(False) #ax.minorticks_on() ax.grid(axis='both', which='minor', color='grey', linestyle='--', alpha=0.2) ax.legend(ncol=6, loc='upper right') ymax = 10000 ymin = 10 xmax = ax.get_xlim()[1] xmin = 0 - xmax * 0.05 ax.set_yscale('log') ax.set_ylim(20, None) ax.set_xlim(xmin, None) ax.text(0.05, 0.15, 'Gas: ' + str(params['Gas']), fontsize=10, ha='left', transform=ax.transAxes, color=gc, alpha=0.7) ax.text(0.05, 0.1, 'Oil: ' + str(params['Oil']), fontsize=10, ha='left', transform=ax.transAxes, color=oc, alpha=0.7) ax.text(0.05, 0.05, 'Water: ' + str(params['Water']), fontsize=10, ha='left', transform=ax.transAxes, color=wc, alpha=0.7) arps_equation = r\"$q_t = q_i \\left( 1 + b D_i t \\right)&#94;{-\\frac{1}{b}}$\" ax.text( 0.05, 0.35, arps_equation, fontsize=11, ha='left', va='top', transform=ax.transAxes, color='black', bbox=dict(facecolor='white', edgecolor='black', boxstyle='round,pad=0.3') ) ax.text( 0.95, 0.8, 'Scenario: A', fontsize=17, ha='right', va='top', transform=ax.transAxes, color='black', bbox=dict(facecolor='white', edgecolor='black', boxstyle='round,pad=0.3') ) def setbold(txt): return ' '.join([r\"$\\bf{\" + item + \"}$\" for item in txt.split(' ')]) bold_txt = setbold('Scenario A: Gas Decline Faster Than Oil, ') plain_txt = 'from hypothetical DCA parameters' fig.suptitle(bold_txt + plain_txt, verticalalignment='top', x=0, horizontalalignment='left', fontsize=11) yloc = 0.9 ax.annotate('', xy=(0.01, yloc + 0.01), xycoords='figure fraction', xytext=(1.02, yloc + 0.01), arrowprops=dict(arrowstyle=\"-\", color='k', lw=0.7)) ax.text(0.98, 0.08, 'aegis4048.github.io', fontsize=10, ha='right', transform=ax.transAxes, color='grey', alpha=0.5) fig.tight_layout() Figure 4: Well decline curve prediction from the DCA parameters in scenario B of Figure 2 . Source Code For Figure (4) import pandas as pd import numpy as np import matplotlib.pyplot as plt def general_decline(t, qi, Di, b): \"\"\" Arps' decline equation for production decline modeling. Args: t (array-like): Time values. qi (float): Initial production rate. Di (float): Initial decline rate. b (float): Decline exponent. Returns: array-like: Production rate at time t. \"\"\" return qi * (1 + b * Di * t) ** (-1 / b) params = { 'Oil': {'qi': 1800, 'Di': 0.3, 'b': .7}, 'Gas': {'qi': 9000, 'Di': 0.1, 'b': 0.7}, 'Water': {'qi': 3700, 'Di': 0.5, 'b': 1} } x = np.arange(0, 61) fits = { 'Oil': [], 'Gas': [], 'Water': [], } for key in params.keys(): target_params = list(params[key].values()) fits[key] = general_decline(x, *target_params) df = pd.DataFrame.from_dict(fits) df['Count'] = x fig, ax = plt.subplots(figsize=(8, 4.5)) oc = 'green' gc = 'red' wc = 'blue' ax.plot(df['Count'], df['Oil'], c=oc, label='Oil') ax.plot(df['Count'], df['Gas'], c=gc, label='Gas') ax.plot(df['Count'], df['Water'], c=wc, label='Water') ax.set_xlabel('Months') ax.set_ylabel('BOPD, BWPD, MCFD') ax.grid(True) ax.spines['top'].set_visible(False) ax.spines['right'].set_visible(False) #ax.minorticks_on() ax.grid(axis='both', which='minor', color='grey', linestyle='--', alpha=0.2) ax.legend(ncol=6, loc='upper right') ymax = 10000 ymin = 10 xmax = ax.get_xlim()[1] xmin = 0 - xmax * 0.05 ax.set_yscale('log') ax.set_ylim(20, None) ax.set_xlim(xmin, None) ax.text(0.05, 0.15, 'Gas: ' + str(params['Gas']), fontsize=10, ha='left', transform=ax.transAxes, color=gc, alpha=0.7) ax.text(0.05, 0.1, 'Oil: ' + str(params['Oil']), fontsize=10, ha='left', transform=ax.transAxes, color=oc, alpha=0.7) ax.text(0.05, 0.05, 'Water: ' + str(params['Water']), fontsize=10, ha='left', transform=ax.transAxes, color=wc, alpha=0.7) arps_equation = r\"$q_t = q_i \\left( 1 + b D_i t \\right)&#94;{-\\frac{1}{b}}$\" ax.text( 0.05, 0.35, arps_equation, fontsize=11, ha='left', va='top', transform=ax.transAxes, color='black', bbox=dict(facecolor='white', edgecolor='black', boxstyle='round,pad=0.3') ) ax.text( 0.95, 0.8, 'Scenario: B', fontsize=17, ha='right', va='top', transform=ax.transAxes, color='black', bbox=dict(facecolor='white', edgecolor='black', boxstyle='round,pad=0.3') ) def setbold(txt): return ' '.join([r\"$\\bf{\" + item + \"}$\" for item in txt.split(' ')]) bold_txt = setbold('Scenario B: Gas Decline Faster Than Oil, ') plain_txt = 'from hypothetical DCA parameters' fig.suptitle(bold_txt + plain_txt, verticalalignment='top', x=0, horizontalalignment='left', fontsize=11) yloc = 0.9 ax.annotate('', xy=(0.01, yloc + 0.01), xycoords='figure fraction', xytext=(1.02, yloc + 0.01), arrowprops=dict(arrowstyle=\"-\", color='k', lw=0.7)) ax.text(0.98, 0.08, 'aegis4048.github.io', fontsize=10, ha='right', transform=ax.transAxes, color='grey', alpha=0.5) fig.tight_layout() Figure 5: Well decline curve prediction from the DCA parameters in scenario C of Figure 2 . Source Code For Figure (5) import pandas as pd import numpy as np import matplotlib.pyplot as plt def general_decline(t, qi, Di, b): \"\"\" Arps' decline equation for production decline modeling. Args: t (array-like): Time values. qi (float): Initial production rate. Di (float): Initial decline rate. b (float): Decline exponent. Returns: array-like: Production rate at time t. \"\"\" return qi * (1 + b * Di * t) ** (-1 / b) params = { 'Oil': {'qi': 1800, 'Di': 0.3, 'b': .7}, 'Gas': {'qi': 9000, 'Di': 0.3, 'b': 0.7}, 'Water': {'qi': 3700, 'Di': 0.5, 'b': 1} } x = np.arange(0, 61) fits = { 'Oil': [], 'Gas': [], 'Water': [], } for key in params.keys(): target_params = list(params[key].values()) fits[key] = general_decline(x, *target_params) df = pd.DataFrame.from_dict(fits) df['Count'] = x fig, ax = plt.subplots(figsize=(8, 4.5)) oc = 'green' gc = 'red' wc = 'blue' ax.plot(df['Count'], df['Oil'], c=oc, label='Oil') ax.plot(df['Count'], df['Gas'], c=gc, label='Gas') ax.plot(df['Count'], df['Water'], c=wc, label='Water') ax.set_xlabel('Months') ax.set_ylabel('BOPD, BWPD, MCFD') ax.grid(True) ax.spines['top'].set_visible(False) ax.spines['right'].set_visible(False) #ax.minorticks_on() ax.grid(axis='both', which='minor', color='grey', linestyle='--', alpha=0.2) ax.legend(ncol=6, loc='upper right') ymax = 10000 ymin = 10 xmax = ax.get_xlim()[1] xmin = 0 - xmax * 0.05 ax.set_yscale('log') ax.set_ylim(20, None) ax.set_xlim(xmin, None) ax.text(0.05, 0.15, 'Gas: ' + str(params['Gas']), fontsize=10, ha='left', transform=ax.transAxes, color=gc, alpha=0.7) ax.text(0.05, 0.1, 'Oil: ' + str(params['Oil']), fontsize=10, ha='left', transform=ax.transAxes, color=oc, alpha=0.7) ax.text(0.05, 0.05, 'Water: ' + str(params['Water']), fontsize=10, ha='left', transform=ax.transAxes, color=wc, alpha=0.7) arps_equation = r\"$q_t = q_i \\left( 1 + b D_i t \\right)&#94;{-\\frac{1}{b}}$\" ax.text( 0.05, 0.35, arps_equation, fontsize=11, ha='left', va='top', transform=ax.transAxes, color='black', bbox=dict(facecolor='white', edgecolor='black', boxstyle='round,pad=0.3') ) ax.text( 0.95, 0.8, 'Scenario: C', fontsize=17, ha='right', va='top', transform=ax.transAxes, color='black', bbox=dict(facecolor='white', edgecolor='black', boxstyle='round,pad=0.3') ) def setbold(txt): return ' '.join([r\"$\\bf{\" + item + \"}$\" for item in txt.split(' ')]) bold_txt = setbold('Scenario C: Gas Decline Faster Than Oil, ') plain_txt = 'from hypothetical DCA parameters' fig.suptitle(bold_txt + plain_txt, verticalalignment='top', x=0, horizontalalignment='left', fontsize=11) yloc = 0.9 ax.annotate('', xy=(0.01, yloc + 0.01), xycoords='figure fraction', xytext=(1.02, yloc + 0.01), arrowprops=dict(arrowstyle=\"-\", color='k', lw=0.7)) ax.text(0.98, 0.08, 'aegis4048.github.io', fontsize=10, ha='right', transform=ax.transAxes, color='grey', alpha=0.5) fig.tight_layout() 3. Simulation results BR&E Promax is used to simulate flash gas declines at the heater and VRT. The base model shown in Figure 2 was fed with the three declining scenarios shown in Figures 3, 4, and 5 . Declines up to 60 months are simulated, though not all months are shown for brevity. You can download the full Simulation Results Data (VRU_Declining_Economics-Sim_Results.xlsx) to review the complete Promax simulation results in Excel. Refer to Tabs 1a, 1b, and 1c. Note that the DCA parameters shown on the right are realistic parameters that you can see in the field. Results Summary Flash gas declines closely follow the oil decline, with gas declines having a relatively minor impact in comparison. The primary driver of downstream flash gas rate declines is the oil production rate. Do not predict flash gas decline from the produced (sales) gas decline. VRU flowrates can decrease by up to 86% compared to IP flowrates in 12 months, or more than 50% in just 3 months. They decline faster than expected, much like oil production. Table 1: Simulation results using the base model shown in Figure 2 and declining flowrate inputs shown in Figures 3, 4, and 5 . The DCA parameters shown on the right represents declining scenarios used to simulate well decline. The percentages represent how much production declines compared to the initial production. Actual flowrate numbers are shown in Table 2 below. The first rows in each scenario are emtpy because there's no decline at month=0. Table 2: Simulation results using the base model shown in Figure 2 and declining flowrate inputs shown in Figures 3, 4, and 5 . The DCA parameters shown on the right represents declining scenarios used to simulate well decline. 4. VRU Economics VRU economics are calculated based on the simulation results from scenario C in Table 2 for both the heater treater and VRT. The calculation tables are provided below in Table 3 for the VRU installation on the VRT and Table 4 for the VRU installation on the heater. Note that the energy content of the flash gas from the VRT (2690 Btu/scf) is higher than that from the heater (1930 Btu/scf). This is because lighter hydrocarbons like methane and ethane flash off at higher pressures, while heavier hydrocarbons do not flash until the pressure is significantly lower. Recall from the site setups in Section 2.1 that the heater operates at 50 psig, while the VRT operates at 3 psig. You can download the full Simulation Results Data (VRU_Declining_Economics-Sim_Results.xlsx) to review the economics calculation formulas in Excel. Refer to Tabs \"Gas Economics Calcualtor - HT\", and \"Gas Economics Calcualtor - VRT\". Assumptions Oklahoma 7% sales gas tax. $0.75/MCF gathering fee, typical for Oklahoma. Separate VRUs are used for heater and VRT each. This is because compressor inlet cannot take gases from two different pressures (the compressor company I work for has \"Multi-stream\" technology that enables dual inlet for two pressures, but it's not the focus of this article. Contact me if you are interested). Same VRU sizing for both VRT and heater (although the heater may require smaller sizing due to its lower flash gas flowrate and higher operating pressure. The higher the gas pressure at the compressor inlet, the greater the compressor's flowrate capacity). Generic $3,500 monthly VRU rental cost. This can vary depending on application and vendors. The operator is paid based on energy content (MMBtu) rather than volume. This may not be an option for some regions. 30 days assumed for all months for calculation simplicity. Economics Summary VRU installation on the heater becomes uneconomical after month 9. VRU installation on the VRT becomes uneconomical after month 16. The heater application becomes uneconomical much faster than the VRT due to its lower flowrate (22% lower) and lower energy content (39% lower). Table 3: VRU economics calculation table for VRU installed on scenario C's VRT gas rates. The VRU becomes uneconomical after month 15. Table 4: VRU economics calculation table for VRU installed on scenario C's heater gas rates. The VRU becomes uneconomical after month 8. VRU installation on heater becomes uneconomical much faster than VRT application due to heater gas having lower flowrate and lower energy content. 5. Conclusion/Summary 1. Flash gas rates for VRU applications may decline quickly, quicker than people think. They follow the oil decline curve. 2. Many people in the industry tend to forget about the fact that VRU flowrates decline quickly. I've witnessed this first-hand working at a compressor company. Downsizing is often not done in a timely manner. 3. Process simulations are performed to show the impact of oil and gas decline on downstream flash gases from low pressure vessels. Simulation site setups are described in Section 2.1 . Well declining rates are simulated with 3 sets of DCA parameters described in Section 2.2 . 4. Simulation results in Table 1 indicate that flash gas rates at low pressure vessels (heater, VRT) follow oil decline. The primary sales gas has some impact, but negligible compared to oil decline's influence. 5. When forecasting VRU decline, use the decline rate of oil production. Do not predict off the primary produced gas decline. 6. Flash gas rates decline quickly after IP, just like oil. In Table 1 , flash gas off the heater became halved in 3 months, and quartered in 6 months. 7. Due to fast decline after the initial production, VRU may become quickly uneconomical. In Table 4 that shows VRU economics for heater, VRU installation on heater flash gas became uneconomical 8 months after IP. Failure to downsize the unit in a timely manner, or being stuck with a long-term contract (unless the unit can be moved to another pad) can be detrimental to lease operating expense (LOE). 8. Be careful when signing a long-term contract for VRUs. Be prepared to predict VRU economics and move the long-term contracted VRU to new locations on a timely manner to avoid economic loss. 9. Energy content coming from low pressure vessel (VRT @3#: 2690 Btu/scf) is higher than the one coming from higher pressure vessel (Heater @50#: 1930 Btu/scf). This is because lighter hydrocarbons like methane and ethane flash off at higher pressures, while heavier hydrocarbons do not flash until the pressure is significantly lower. The difference in energy content has a huge impact on VRU economics. Process simulations are required for accurate prediction of flash gas energy content at different vessels. Contact me via email (aegis4048@gmail.com) or leave a comment below for any professional inquiries regarding compressor applications or process simulations.","tags":"Oil and Gas","url":"https://aegis4048.github.io/predicting_vru_economics_for_declining_wells","loc":"https://aegis4048.github.io/predicting_vru_economics_for_declining_wells"},{"title":"Impact of Seasonal Changes on Storage Tank Emissions","text":"Concerns over global warming have led to stricter regulations, pushing oil and gas operators to produce \"green\" oil. One of the primary sources of emissions in the industry is from crude oil storage tanks. When hydrocarbon liquids are transferred into storage tanks from upstream separators, the crude oil is not fully stabilized yet and still contains some dissolved volatile gases, which vaporize inside the tank. These vapors accumulate, increasing pressure within the tank, requiring a pressure relief system to prevent explosions. Such relief mechanism can be either vapor recovery systems with compressors to capture vapors and send them to pipelines for sales, which are more environmentally friendly, or venting to air. Vapor generation within a tank is influenced by two main factors: 1) the pressure drop from the upstream separator to the storage tank, and 2) the tank's operating temperature. This article focuses on the latter—how the operating temperature of the tank, impacted by seasonal changes, affects emission volumes. Here's a quick takeaway for skimmers: Emission volumes are 22% (Texas) to 56% (North Dakota) higher in summer than in winter due to increased air temperatures. The summer-winter discrepancy is more pronounced in colder regions because of the greater seasonal temperature difference. See Table 2 below for a simulation summary across various scenarios. Note that this result is specific to the site simulation setups detailed below and may need fine tuning for different facility setups. However, the concept should still be generally applicable for most upstream facilities. Figure 0: Flash volume simulation at atmospheric storage tanks using BR&E Promax. The setup assumes a liquid line from a heater operating at 30 psig and 120°F, dumping into an atmospheric tank at a rate of 3028 STBBL/D (34,600 lb/h) year-round. The plot shows increased emission rate at summer compared to winter due to increased ambient air temperature Figure 1: Flash volume simulation at atmospheric storage tanks using BR&E Promax. The setup assumes a liquid line from a heater operating at 30 psig and 120°F, dumping into an atmospheric tank at a rate of 3028 STBBL/D (34,600 lb/h) year-round. The red line represents vapor emission volumes from the tank. All conditions are held constant over 12 months, except for monthly ambient air temperature to isolate the effect of seasonal temperature on emissions. Williston, North Dakota, seasonal temperatures are used. Results show 47 MCFD emissions during winter and 73 MCFD during summer—a 56% increase. Elevated summer temperatures lead to higher emission volumes inside tanks. Refer to Figure 6 below for a process flow diagram of the site. Source Code For Figure (1) import pandas as pd import numpy as np import matplotlib.pyplot as plt import matplotlib.ticker as ticker df = pd.read_excel('https://aegis4048.github.io/downloads/notebooks/sample_data/seasonal_impact_data.xlsx', sheet_name='Sim Format 1') df = df[df['Location'] == 'Williston, ND'] months = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec'] x = np.array([i for i in range(12)]) fig, ax = plt.subplots(figsize=(8, 4)) ax.plot(x, df['AVG Air T (F)'], color='purple', label='Avg Air T [°F]', marker='v', markersize=8, linestyle='-') ax.plot(x, df['Tank T (F)'], color='darkslateblue', label='Tank T [°F]', marker='x', markersize=8, linestyle='-') ax.plot(x, df['Gas (MCFD)'], color='red', label='Tank Vapor [MCFD]', marker='o', markersize=8, linestyle='-', markerfacecolor='white') ax.axhline(y=120, ls='--', color='green', label='Inlet Liquid T [°F]') ax.set_ylim(0, 140) ax.set_xlim(-0.3, 11.3) ax.set_xticks(range(len(months))) ax.set_xticklabels(months) ax.yaxis.set_minor_locator(ticker.AutoMinorLocator()) ax.grid(axis='y', which='minor', linestyle='--', alpha=0.2) # Minor grid lines ax.grid(axis='both', which='major', linestyle='-', alpha=0.3) # Major grid lines ax.yaxis.get_major_ticks()[-1].gridline.set_visible(False) ax.spines['right'].set_visible(False) ax.spines.top.set_visible(False) def annotate_arrows(data_y, num, tick_spacing, ymax, x_start=1, ox=0.2, fontsize=14, flip_y=False): ''' data_y = y coordinate of the datapoint of interest num = index of the datapoint of interest tick_spacing = spacing between two major ticks ox = offset of x ''' head_padding = ymax * 0.04 # extra space between the datapoint of interest and arrowhead oy = ymax * 0.08 # offset of y sx = x_start + (tick_spacing / 2) * num sy = data_y + head_padding if flip_y: oy = -ymax * 0.15 sy = data_y - head_padding ax.arrow(sx + ox, sy + oy, -ox, -oy, head_width=0.1, head_length=ymax * 0.0333, fc='k', ec='k', lw=1) t1 = ax.text(sx + ox, sy + oy, str(int(data_y)) + ' MCFD', fontsize=fontsize, color='k', ha='center') t1.set_bbox(dict(facecolor='white', alpha=1, edgecolor='k', pad=3, lw=1)) xytext = (sx + ox, sy + oy) return xytext ymax = 160 fs=9 yloc=110 lw=1 spacing = abs(ax.get_xticks()[1] - ax.get_xticks()[2]) y1 = df['Gas (MCFD)'].values[0] y2 = df['Gas (MCFD)'].values[6] _1 = -2 _2 = 10 textxy1 = annotate_arrows(y1, _1 + 0.1, spacing, ymax, fontsize=fs) textxy2 = annotate_arrows(y2, _2 + 0.1, spacing, ymax, fontsize=fs) ax.plot([textxy1[0], textxy1[0]], [textxy1[1], yloc], lw=lw, color='k', ls='--') ax.plot([textxy2[0], textxy2[0]], [textxy2[1], yloc], lw=lw, color='k', ls='--') ax.plot([textxy1[0], textxy2[0]], [yloc, yloc], lw=lw, color='k', ls='--') diff_percentage = '+' + str(int((y2 - y1)/y1 * 100)) + '%' t1 = ax.text((textxy1[0] + textxy2[0])/2, yloc, diff_percentage, fontsize=11, color='red', ha='center', fontweight='bold') t1.set_bbox(dict(facecolor='white', alpha=1, edgecolor='k', pad=5, lw=lw)) ax.legend(fontsize=10, ncol=4, loc='upper left', framealpha=1) ax.set_ylabel('Tmp. [°F], Vapor Rate [MCFD]', fontsize=11) ax.text(0.4, 0.2, 'Upstream liquid line from heater', fontsize=10, ha='left', transform=ax.transAxes, color='k') ax.text(0.4, 0.14, '@30 psig, 120°F', fontsize=10, ha='left', transform=ax.transAxes, color='k') ax.text(0.98, 0.08, 'aegis4048.github.io', fontsize=10, ha='right', transform=ax.transAxes, color='grey', alpha=0.5) def setbold(txt): return ' '.join([r\"$\\bf{\" + item + \"}$\" for item in txt.split(' ')]) bold_txt = setbold('Impact of Seasonal Changes on Tank Emissions, ') plain_txt = 'Bakken Shale' fig.suptitle(bold_txt + plain_txt, verticalalignment='top', x=0, horizontalalignment='left', fontsize=11) yloc = 0.9 ax.annotate('', xy=(0.01, yloc + 0.01), xycoords='figure fraction', xytext=(1.02, yloc + 0.01), arrowprops=dict(arrowstyle=\"-\", color='k', lw=0.7)) fig.tight_layout() Contents 0. Introduction 1. Theories: Impact of temperature changes at separator vessels 1.1. Flash volume increase 1.2. Heavier separated gas 1.3. Higher energy content 1.4. Higher dew point temperature 1.5. More stabilized crude oil (RVP) 2. Simulation Setups 2.1. Site (operational) setups Notes: Flash, Breathing, Working, and Loading Losses 2.2. Software (Promax) & calculation methods (AP42 Chap 7) Notes: API 12F Tanks 2.3. Regional ambient air temperatures data 3. Simulation results 4. Conclusion 1. Theories: Impact of temperature changes at separator vessels Altering the operating temperature of a separator vessel impacts the phase behavior of fluids within it. This section provides a theoretical explanation of the effects of increasing temperature from low to high. Figure 2: Simulation of flash behavior at low (100°F) vs high (150°F) temperatures, modeled with BR&E Promax. At higher operating temperatures in a separator vessel, the separated gas volume increases, with higher specific gravity, greater energy content (Btu/scf), and the separated crude becomes more stable, as indicated by a lower Reid Vapor Pressure value. 1.1. Flash volume increase Increasing the operating temperature of a separator vessel (or storage tanks) raises the fluid mole fraction that escapes into the gas line. This phenomenon can be explained by phase envelopes, which show phase behavior changes under different pressures and temperatures. Assume Figure 3 represents a phase envelope of a sample obtained from the liquid line of a separator operating at 200 psig. The blue circle on the figure sits on the 100% liquid bubble point curve, showing that by definition, the liquid is at its bubble point when it leaves any separator. This 200 psig liquid then flows into a heater operating at 50 psig and 120°F, causing the operating condition of the fluid to shift, represented by the red dashed circle. Flash gas is generated from the pressure drop from 200 psig to 50 psig, where 50% of the inlet liquid vaporizes and separates into the gas line of a heater. Now assume the operator increases the heater temperature from 120°F to 150°F. The operating condition of the fluid in the heater shifts horizontally, and now sits on the 5% liquid (95% vapor) curve, shown by the solid red circle. The vapor fraction rises from 50% to 95% due to the temperature increase. This demonstrates why raising temperature leads to higher flash volumes in any vessel, as the fluid's operating condition moves closer to the 100% vapor dew point curve, increasing the vapor mole fraction. The same phase behavior applies to atmospheric tanks. When tank temperatures rise in summer due to hotter ambient air, more vapors are generated, as the fluid's operating condition moves toward a higher vapor fraction curve. Figure 3: \"Fake\" phase envelope of a sample from the outlet liquid line of a separator operating at 200 psig, 115°F. This envelope is an educational illustration and is not derived from an actual sample. The original condition of the sample is marked by the blue circle. The liquid from the 200# separator flows into a heater operating at 50#, 120°F, represented by the dashed red circle. When the heater temperature is raised from 120°F to 150°F, the condition shifts horizontally toward the 100% vapor dew point curve. The envelope shows the varying vapor fractions of the fluid inside the heater at different operating conditions. 1.2. Heavier separated gas Heavier hydrocarbon molecules vaporize more readily at elevated temperatures than at low temperatures, increasing the mole fraction of heavy-ends in the gas line and, consequently, the overall gas gravity. In Figure 2 , the gas specific gravity (SG) rose from 0.816 to 0.956, and molecular weight (MW) from 23.6 to 27.7 lb/lbmol after a 50°F increase in vessel temperature. 1.3. Higher energy content Heavier hydrocarbon molecules have higher energy content (Btu/scf). In Figure 2 , gas energy content increased from 1396 Btu/scf to 1567 Btu/scf after a 50°F increase in vessel temperature. Light-ends like methane have a MW of 16.04 lb/lbmol and an energy content of 1010 Btu/scf, whereas heavier-ends like n-pentane have an MW and energy content of 72.15 lb/lbmol and 4008.7 Btu/scf. In fact, gas molecular weight and energy content exhibit a nearly perfect linear correlation for paraffinic hydrocarbons (e.g., methane, ethane, butane, pentane, hexane), as shown in Figure 4 . Figure 4: Scatter plot of known compounds upto n-hexadecane (C16H34) listed in the GPA 2145 table. Source Code For Figure (4) import pandas as pd import matplotlib.pyplot as plt df = pd.read_csv('https://aegis4048.github.io/downloads/notebooks/sample_data/GPA 2145-16_Compound_Properties_Table_English_Truncated_and_PNA_Identified.csv') # Labeling for displaying texts labels = ['methane', 'propane', 'n-butane', 'n-heptane', 'n-octane', 'n-decane', 'cyclohexane', 'cyclopentane', 'ethane', 'n-dodecane','n-tetradecane','n-hexadecane', 'methanol', 'ethanol', 'naphthalene', 'isobutylcyclopentane', 'hydrogen', 'sulfur dioxide', 'hydrogen sulfide', 'toluene', 'benzene', 'm-xylene', 'pentylbenzene', 'hexylbenzene', 'propylene', '1-butene' ] df['Display Text'] = df['Compound'].isin(labels) BTEX_idx = df[df['Is BTEX'] == True].index aromatic_idx = df[df['Is Aromatic'] == True].index non_HC_idx = df[df['Is Hydrocarbon'] == False].index hydroxyl_idx = df[df['Is Hydroxyl'] == True].index paraffinic_idx = df[df['Is Paraffinic'] == True].index naphethenic_idx = df[df['Is Naphthenic'] == True].index other_idx = df[df['Others'] == True].index whole_idx = list(df.index) x = df['Molar Mass [g/mol]'] y = df['Gross Heating Value Ideal Gas [Btu/ft&#94;3]'] ##################################### Plotting ####################################### fig, ax = plt.subplots(figsize=(8, 4.5)) alpha = 1 _1 = ax.scatter(x.loc[paraffinic_idx], y.loc[paraffinic_idx], s=50, edgecolor='k', alpha=alpha, label='Paraffinic') _2 = ax.scatter(x.loc[naphethenic_idx], y.loc[naphethenic_idx], s=50, edgecolor='k', alpha=alpha, label='Naphthenic') _3 = ax.scatter(x.loc[aromatic_idx], y.loc[aromatic_idx], s=50, edgecolor='k', alpha=alpha, label='Aromatic/BTEX') _4 = ax.scatter(x.loc[hydroxyl_idx], y.loc[hydroxyl_idx], s=50, edgecolor='k', alpha=alpha, label='Hydroxylic') _5 = ax.scatter(x.loc[non_HC_idx], y.loc[non_HC_idx], s=50, edgecolor='k', alpha=alpha, label='Non-HCs') _6 = ax.scatter(x.loc[other_idx], y.loc[other_idx], s=50, edgecolor='k', alpha=alpha, label='Other-HCs') c1 = _1.get_facecolor()[0] c2 = _2.get_facecolor()[0] c3 = _3.get_facecolor()[0] c4 = _4.get_facecolor()[0] c5 = _5.get_facecolor()[0] c6 = _6.get_facecolor()[0] ax.legend(fontsize=9, ncol=3) texts = df['Compound'] for i, txt in enumerate(texts): if df['Display Text'].loc[i]: c = c5 ha ='left' va = 'top' if df['Is Paraffinic'].loc[i]: c = c1 ha ='right' va = 'bottom' if df['Is Naphthenic'].loc[i]: c = c2 ha ='right' va = 'bottom' if df['Is Aromatic'].loc[i]: c = c3 va = 'top' ha = 'left' if df['Is Hydroxyl'].loc[i]: c = c4 va = 'bottom' ha = 'left' if df['Others'].loc[i]: c = c6 va = 'top' ha = 'left' if ha == 'left': icr = 3 else: icr= -3 ax.annotate(txt, (x.loc[i] + icr, y.iloc[i]), fontsize=10, c=c, ha=ha, va=va) ax.minorticks_on() ax.grid(axis='y', which='major', linestyle='--', color='grey', alpha=0.5) #ax.grid(axis='y', which='minor', linestyle='--', color='grey', alpha=0.2) #ax.grid(axis='x', which='minor', color='grey', linestyle='--', alpha=0.2) ax.grid(axis='x', which='major', color='grey', linestyle='--', alpha=0.5) ax.spines['top'].set_visible(False) ax.spines['right'].set_visible(False) ax.set_xlabel('Molecular Weight', fontsize=11) ax.set_ylabel('Gross Heating Value [Btu/scf]', fontsize=11) ax.text(0.99, 0.1, 'aegis4048.github.io', fontsize=12, ha='right', va='center', transform=ax.transAxes, color='grey', alpha=0.5) #ax.set_xlim(0, 1) #ax.set_ylim(10000, 30000) def setbold(txt): return ' '.join([r\"$\\bf{\" + item + \"}$\" for item in txt.split(' ')]) bold_txt = setbold('GHV_{gas} vs. MW') plain_txt = ', for different compound groups' fig.suptitle(bold_txt + plain_txt, verticalalignment='top', x=0, horizontalalignment='left', fontsize=12, y=0.96) yloc = 0.88 ax.annotate('', xy=(0.01, yloc + 0.01), xycoords='figure fraction', xytext=(1.02, yloc + 0.01), arrowprops=dict(arrowstyle=\"-\", color='k', lw=0.7)) ax.annotate('Data source: GPA 2145-16', xy=(-0.11, -.12), xycoords='axes fraction', fontsize=9) fig.tight_layout() 1.4. Higher dew point temperature Gas dew point temperatures are critical for compressor applications. In high-pressure setups, like those using reciprocating compressors, accurate dew point calculations help define design parameters for compression chambers and ensure liquid knockout scrubbers are properly sized. For lower-pressure applications (below 300 psig), such as oil-flooded screw compressors, dew point control is essential to prevent liquid condensation during compression, which can lead to lube oil contamination. The specific gravity of a mixture affects its dew and bubble points. While the relationship isn't perfectly linear, heavier mixtures generally exhibit higher dew points at the same pressure than lighter ones. Figure 5 illustrates this effect with vapor samples from the same tank in different seasons. The summer sample, with a higher specific gravity, has a dew point temperature of 222°F at 600 psig, while the lighter winter sample has a dew point of 184°F—a significant difference that could impact operations. In this 600 psig scenario, which exceeds the operating capacity of screw compressors, reciprocating compressors are required. For reciprocating compressors, scrubber sizing for liquid knockout is heavily influenced by dew point temperature. A scrubber sized based on a winter sample may be undersized for summer operations, risking liquid carryover during higher-temperature seasons. Figure 5: Phase envelopes of tank vapor samples under summer and winter conditions. The dashed line represents the winter sample, while the solid line represents the summer sample. The winter sample has an energy content of 2021 Btu/scf, whereas the summer sample is richer at 2317 Btu/scf. Yellow diamonds mark the dew point temperatures at 600 psig, showing the summer sample's higher dew point of 222°F compared to the winter sample's 184°F. Source Code For Figure (5) import pandas as pd import numpy as np import matplotlib.pyplot as plt df = pd.read_excel('https://aegis4048.github.io/downloads/notebooks/sample_data/seasonal_impact_data.xlsx', sheet_name='Bakken Envelope') df.sort_values(by='Month') fig, ax = plt.subplots(figsize=(8, 4.5)) months = sorted(list(set(df['Month']))) lss = ['--', '-'] for ls, month in zip(lss, months): df_cur = df[df['Month'] == month] x_crit = df_cur['Critical T [°F]'].values[0] y_crit = df_cur['Critical P [psig]'].values[0] df_cur = df_cur[df_cur['Pressure [psig]'] - y_crit < 20] # filter to remove outliers in sim results x_dew = df_cur[df_cur['Curve Type'] == 'Dew']['Temperature [°F]'].values y_dew = df_cur[df_cur['Curve Type'] == 'Dew']['Pressure [psig]'].values x_bubble = df_cur[df_cur['Curve Type'] == 'Bubble']['Temperature [°F]'].values y_bubble = df_cur[df_cur['Curve Type'] == 'Bubble']['Pressure [psig]'].values ax.plot(x_bubble, y_bubble, label=month + ': Bubble Point', ls=ls, color='red') ax.plot(x_dew, y_dew, label=month + ': Dew Point', ls=ls, color='blue') ax.scatter(x_crit, y_crit, s=60, edgecolor='k', fc='white', zorder=3, label='Critical Point') ax.set_ylim(0, 1199) ax.set_xlabel('Temperature [°F]') ax.set_ylabel('Pressure [psig]') ax.grid(True) ax.spines['top'].set_visible(False) ax.spines['right'].set_visible(False) ax.minorticks_on() ax.grid(axis='both', which='minor', color='grey', linestyle='--', alpha=0.2) ax.scatter(184, 600, marker='D', s=60, fc='yellow', edgecolor='purple', zorder=10, label='Dew Point T @600 psig') ax.scatter(222, 600, marker='D', s=60, fc='yellow', edgecolor='purple', zorder=10) ax.text(184, 610, 'Dew T=184F°', fontsize=10, ha='right', va='bottom', color='purple') ax.text(107, 550, '(Winter)', fontsize=10, ha='left', va='bottom', color='purple') ax.text(230, 610, 'Dew T=222F°', fontsize=10, ha='left', va='bottom', color='purple') ax.text(230, 550, '(Summer)', fontsize=10, ha='left', va='bottom', color='purple') handles, labels = ax.get_legend_handles_labels() handles.pop(-2) labels.pop(-2) ax.legend(handles[:], labels[:], ncol=2) ax.text(0.98, 0.08, 'aegis4048.github.io', fontsize=10, ha='right', transform=ax.transAxes, color='grey', alpha=0.5) def setbold(txt): return ' '.join([r\"$\\bf{\" + item + \"}$\" for item in txt.split(' ')]) bold_txt = setbold('Dew Point T. Difference In Summer vs. Winter') plain_txt = ', for tank vapor line' fig.suptitle(bold_txt + plain_txt, verticalalignment='top', x=0, horizontalalignment='left', fontsize=11) yloc = 0.9 ax.annotate('', xy=(0.01, yloc + 0.01), xycoords='figure fraction', xytext=(1.02, yloc + 0.01), arrowprops=dict(arrowstyle=\"-\", color='k', lw=0.7)) fig.tight_layout() 1.5. More stabilized crude oil (RVP) Increased vapor generation rate inside a tank means that the stored liquid becomes more stabilized, with less volatiles because those dissolved volatiles in the crude escaped the system through a vent line (vented to air, captured, or flared). This can be seen in Figure 2 , in which the Reid Vapor Pressure (RVP) for higher temperature vessel (150F) is lower (RVP=8.84 psi) than the lower T vessel (100F, RVP=9.74 psi). Lower RVP means more stable crude. EPA Gasoline Reid Vapor Pressure requires storage gasoline to have RVP value lower than 9 to 7.8 psi to reduce emissions of volatile organic compounds (VOCs). 2. Simulation Setups This section details the simulation setups and assumptions used to investigate the impact of seasonal temperature changes on storage tank emissions. 2.1. Site (operational) setups A generic upstream facility structure is used for simulations. Reservoir fluid flows up the production tubing to the wellhead, then to a 2-phase separator, a heater treater, and finally to atmospheric storage tanks. Figure 6 shows facility modeling done with Promax. Figure 6: Generic upstream facility modeling done in BR&E Promax. This simulation assumes an ambient air temperature of 15.5°F in Williston, ND, in December (see Table 1 below for regional seasonal temperature data). All operating parameters are held constant except timeframe (month) and ambient air temperature across various locations to observe the impact of seasonal temperature changes on tank emission volumes. Assumptions Generic upstream oil and gas facility with wellhead. 2-phase separator operating at 150 psig. VRU discharge pressure set equal to separator pressure: 150 psig. Wellhead temperature: 140°F. 3-phase heater treater operating at 30 psig, 120°F. Atmospheric oil tank operating at 0.6 psig, with temperature affected by ambient air and incoming fluid temperature. 47° API oil. Sales gas energy content: 1203 Btu/scf, specific gravity: 0.69. Facility production rates: 15 MMSCFD (gas), 3000 BOPD (oil), and 2000 BWPD (water). Four standard 750 BBL API 12F oil storage tanks (15.5' x 24', tan-colored). Water tank sizing omitted for simplicity; water tank temperature assumed to match oil tank temperature. Timeframe (month) and location are the only variables that change for each simulation run. Compression boost for heater treater gas to salesline is omitted for simplicity, as it doesn't affect downstream tank conditions. Common handling strategies for HT gas include: 1) Installing an additional booster compressor dedicated to HT gas, 2) Routing the gas line to tank inlets so HT gas can be handled by the tank VRU along with tank vapors, and 3) Using a 3-way control system to enable a single compressor to handle both the tank gas line (0.6 psig) and HT gas line (30 psig). Notes: Flash, Breathing, Working, and Loading Losses AP-42 calculation methods (see below ) for atmospheric storage tanks provide calculation methods for four vaporization types: flash, working, breathing, and loading. Flash: Rapid release of dissolved gases from liquid due to a pressure drop from high to low pressure Working: Occurs during tank filling operations. As the liquid level rises, vapor space pressure increases, expelling vapors through the roof vent. Although not included in AP-42 methods, working loss can also involve vaporization from turbulence and vapor/liquid contact due to splash loading. Downcomers (submerged loading) help reduce splash losses. Breathing: Vaporization caused by day (hot) vs night (cold) temperature differences. Loading: Results from inefficiencies during truck-loading operations. 2.2. Software (Promax) & calculation methods (AP42 Chap 7) BR&E Promax is a process simulation software widely used in the oil and gas industry. One of it's features include implementation storage tank emission calculation methods outlined in AP42 Chapter 7: Liquid Storage Tanks . The chapter explains emission mechanism for fixed and floating roof tanks for various tank dimensions, structures, paint color, location, month of year, etc. The site simulation setup for this article assumes upstream facility, which traditionally use standardized API 12F tanks. Some of the key inputs used in AP42 can be taken from standard API 12F tank specs such as dimensions (15.5' x 24' for 750 BBL capacity), design pressure (16 oz vent and 1 oz vacuum safety), and tank geometry (fixed roof vertical cylinder). Notes: API 12F Tanks API 12F tanks are standardized storage tanks primarily used in upstream oil and gas production facilities to store crude oil and produced water at atmospheric pressure. Designed and built to the specifications set by the American Petroleum Institute (API), these tanks are widely used due to their reliability, uniformity, and compatibility with industry standards. Standardized Design: API 12F tanks follow strict specifications for dimensions, shape, thickness, reinforcement details, and materials (typically carbon steel) with welded construction to ensure durability and strength. Standardized capacities range from 90 to 1,000 BBL. Design Pressure: Tanks are equipped with a pressure relief mechanism to keep operating pressure below 16 oz (1 psig) to prevent explosion, and a vacuum relief mechanism to stay above 1 oz, preventing implosion. Compliance and Safety: For facilities near commercial or residential areas, API guidelines recommend an additional pressure/vacuum relief mechanism for emergency scenarios, set to release at 24 oz for overpressure and 1.5 oz for vacuum. Most remote wellsites in the field do not require this extra installation. Vent Line Connection Sizing: The guidelines specify minimum pipe diameters for roof vent lines, especially critical when not venting directly to air. This sizing ensures minimal pressure drop between the vent line and emission control system (e.g., VRU or flare). Typical tank operating pressure is below 0.6 psig, allowing only a small margin to ensure vapor flow to the VRU or flare. Even minor pressure drops from friction can restrict flow, leading to overpressurization and triggering relief valves on the tanks, which results in unwanted emissions. Pipe diameters must be large enough to prevent any pressure drop due to friction that could block flow. Figure A: API 12F tank standard specifications at different nominal capacities, screenshot from API 12F 13th edition, 2019. 2.3. Regional ambient air temperatures data AP-42 Chapter 7 Table 7.1-7. Meteorological Data for Selected U.S. Locations provides average monthly temperatures for all 12 months across various locations. Key oil and gas regions' average temperatures are summarized in Table 1 . Notably, the Bakken region shows the largest seasonal temperature difference between summer and winter, leading to the highest seasonal variation in emission volumes (56%), compared to regions like Eagle Ford (22%), as summarized in Table 2 . Table 1: Seasonal temperature data for key U.S. locations, selected from AP-42 Chapter 7 Table 7.1-7 , matched to corresponding basins. Rows are sorted by locations with the highest to lowest temperature differences between January and July. 3. Simulation results Table 2 shows storage tank simulation results for quantifying the seasonal contrast for summer vs winter for tank vapor rates and other properties. The colder the region, such as in Bakken, Powder River, and Marcellus, the greater was the differences due to greater seosonal T difference than warmer regions in Texas. Bakken region showed the greated seasonal tank flash rate increase of 56% in summer compared to winter, while Eagleford showed the lowest increase of 22%. My simulation result table can be downloaded from here: Storage Tank Emissions Simulation Data (seasonal_impact_data.xlsx) Table 2: Storage tank simulation result table contrasting seasonal differences for summer vs. winter for tank vapor line with assumed setups detailed in Simulation Setups above. Air T (°F) is regional seasonal ambient air temperature taken from Table 1 . Tank T (°F) is the operating temperature of the storage tank, affected by upstream heater treater temperature (120F in this setup), and ambient air temperature. RVP (psia), Reid Vapor Pressure, is taken from \"Prod. Oil\" stream in Figure 6 . Dew T (°F) taken from \"VRU Discharge\" stream in Figure 6 . This is the dew point temperature of the compressed gas at 150 psig. The rest of the properties, Gas (MCFD), SG, and GHV (Btu/scf) are taken from stream \"Total Vapor\" . The biggest seasonal contrast occur between Jan vs. July due to the greatest air T difference. WARNING: Important Assumption Regarding \"Regional\" Data It's important to clarify that the simulation results for each region are based solely on variations in temperature across different time frames and locations. All other parameters—such as reservoir-specific compositions, pressure, and temperature conditions of vessels—are held constant. In other words, when referencing results for \"Bakken Shale,\" this does not imply that sample compositional data is obtained from Bakken wells. Only the regional temperature data is adjusted. The purpose of keeping all other variables static, aside from temperature, is to isolate and observe the sole impact of seasonal temperature variations across regions. This provides a clearer idea of how much more flash volume can be expected in summer compared to winter at different key regions. WARNING: Dew Point Sensitivity To Compositions Dew point temperature (which is part of phase envelope) is highly sensitive to composition. The fact that it showed maximum dew point T difference of only 12F (168F vs 190.7F at winter vs. summer) for Bakken in the sim result table should not be assumed to be similar for other similar applications too. For example in Figure 5 , dew point temperature difference at 150 psig is approximately 40 degrees for winter vs summer. However, other parameters are relatively less senstive to composition variations, and can be referenced for general applications. 4. Conclusion 1. When an upstream liquid line flows into a separator vessel, it enters as 100% liquid, with separation occurring inside the vessel. Changing the vessel's operating temperature or pressure shifts the fluid's position on a phase envelope, altering its vapor fraction inside a vessel (see 1.1. Flash volume increase and Figure 3 above), and compositions of the separated outlet lines. This same phenomenon applies to atmospheric applications like storage tanks. 2. Increasing a vessel's temperature raises flash volume, producing a heavier (richer) separated gas with higher energy content and a higher dew point temperature at a given pressure. For the oil line, higher temperature lowers the crude oil's Reid Vapor Pressure (RVP), meaning the crude is more stabilized as more volatiles have separated into the gas line. 3. Emission handling systems (flares or vapor recovery units) should be sized to manage increased gas volume and gravity during summer compared to winter. 4. Summer flash volume increases ranged from 20–30% in Texas regions (Eagle Ford, Delaware, Midland) and from 50–60% in North Dakota (Bakken), compared to winter. 5. Colder regions have larger temperature differences between summer and winter, resulting in a greater seasonal variation in tank vapor emissions. 6. See Table 2 for a summary of simulation results showing the impact of monthly air temperature on tank vapor emissions by region.","tags":"Oil and Gas","url":"https://aegis4048.github.io/impact-of-seasonal-changes-on-storage-tank-emissions","loc":"https://aegis4048.github.io/impact-of-seasonal-changes-on-storage-tank-emissions"},{"title":"H₂S Levels: Why High-Pressure Samples Don't Tell the Full Story","text":"H₂S (hydrogen sulfide) measurement is crucial in process engineering for several reasons: 1) regulatory compliance, 2) personnel safety, and 3) maintaining equipment integrity. Different safety and environmental standards protect both people and the environment, and costs associated with equipment damage vary based on the ppm levels of H₂S, making accurate measurements essential. One problem I've noticed in the energy industry is the assumption that gas sample lab analysis from one pressure point remains representative at other pressure points. For example, operators often take the 20 ppm H₂S value measured from the primary pressurized separator (typically operating between 100–1000 psig), and re-use it for vapor recovery applications, which usually operate at near 0 psig. At this lower pressure, the H₂S ppm can rise to 63 ppm due to phase behavior changes. A model simulating such a facility is shown in Figure 1 below. Figure 1: A model of a typical upstream oil and gas facility, simulated with BR&E Promax. The heater treater is omitted for simplicity (as it doesn't significantly affect downstream conditions, with temperatures high enough to break emulsions in this case). The middle-left vessel (HP Sep #500) is the primary separator, operating at 500 psig and 141°F. The flash gas line (HP Sep. Gas) from this separator reports 20 ppm H₂S, which is higher than the typical 16 ppm midstream requirement and may require sour gas treatment. However, the gas line (Oil Tank Flash) from the oil tank, operating at 0.4 psig, shows 63.4 ppm H₂S. The H₂S value triples at the lower pressure, demonstrating that reusing the 20 ppm value measured at the HP separator for VRU or combustor applications would be inaccurate. This issue is even more severe for the water tank vapor (Water Tank Flash), which reports 302 ppm H₂S. In Oklahoma, according to subsection (c) Storage tank provision of Okla. Admin. Code § 165:10-3-16 , this requires installation of a warning sign, wind indicator, and fencing. Additionally a flare, vapor recovery, or H₂S stripping system is required. Contents 1. Why does H₂S ppm levels matter? 1.1. Regulatory Compliance 1.2. Equipment Damage 1.3. Personnel Safety 2. H₂S measurement practices in upstream Oil and Gas 3. Understanding phase behavior of H₂S and hydrocarbons 3.1. Flash Point Pressure 3.2. H₂S ppm level changes in vapor vs liquid 4. H₂S sim results at various production rates 4.1. Simulation screenshots 4.2. Simulation results summary 5. Conclusion 1. Why does H₂S ppm levels matter? 1.1 Regulatory Compliance In Oklahoma, Okla. Admin. Code § 165:10-3-15 - Venting and Flaring provides guidelines on when operators can vent or flare gas, depending on the situation. However, in all cases, if H₂S levels exceed 100 ppm, venting is prohibited, and the gas must be flared. It cannot be vented through storage tank thief hatches. This rule applies to scenarios such as initial flowback periods for newly completed or recompleted wells, exploration wells, etc.—all must be flared. The only exception is when the gas to be flared is not of pipeline quality, and this exception can only last for up to 45 days. Okla. Admin. Code § 165:10-3-16 - Operation in hydrogen sulfide areas applies to upstream facilities with H₂S levels above 20 ppm. Notably, Subsection (c) requires measuring H₂S concentration 1-ft above the open thief hatches of atmospheric storage tanks. If levels exceed 100 ppm, warning signs, wind indicators, and fencing are required. Additionally, a flare, vapor recovery, or H₂S stripping system must be installed. In Figure 1 above, the H₂S ppm level is triple of the 100 ppm threshold, and will most likely need to be monitored. Also, subsection (h) mandates that flaring equipment in public areas be made from hydrogen sulfide stress cracking (HSSC) resistant materials. Material selection guidelines for HSSC resistance can be found in NACE MR-0175/ISO 15156. In Texas, the Texas Railroad Commission (RRC) considers oil and gas operations with H₂S levels exceeding 100 ppm as sour gas operations and imposes safety and regulatory requirements ( RRC Statewide Rule 36 ). On the other hand, the Texas Commission on Environmental Quality (TCEQ) defines sour gas as above 24 ppm and imposes additional restrictions beyond those required by the RRC. The RRC focuses on the safety and operational integrity of oil and gas facilities, while the TCEQ emphasizes air emissions control. 1.2 Equipment Damage Natural gas pipeline companies require H₂S levels to be below 4–16 ppm (depending on the operator) and impose fees if these levels are exceeded to protect their pipelines from damage. Similarly, compressor rental companies also set limits on H₂S concentrations to prevent corrosion and protect their equipment. These thresholds are typically outlined in pipeline or equipment rental specifications. 1.3 Personnel Safety Occupational Safety and Health Administration (OSHA) sets three standards based on ppm limits (note that OSHA ppm measurements are based on air sampling in the work environment, not inside a process stream): 1. Permissible Exposure Limit (PEL) A ceiling of 20 ppm during an 8-hour work shift. Workers should not be exposed to concentrations exceeding this level at any time during the shift, except under specific short-term conditions. 2. Short-Term Exposure Limit (STEL) Workers can be exposed to up to 50 ppm of H₂S, but only for a maximum of 10 minutes. Beyond this, protective measures are mandatory to prevent prolonged exposure. 3. Immediately Dangerous to Life or Health (IDLH) Exposure above 100 ppm is immediately dangerous to life and health. Immediate evacuation or the use of self-contained breathing apparatus (SCBA) is required for worker safety. 2. H₂S measurement practices in upstream Oil and Gas H₂S measurements in upstream production facilities are rarely done using air detection devices. Typically, a gas sample is taken from a primary pressurized separator, usually operating between 100~1000 psig. The main purpose of this sample is to ensure compliance with gas pipeline specifications (such as water content, H₂S, CO₂, and N₂ levels) and to determine the fair price of the gas, with higher energy content leading to higher value. The sample is sent to a lab for compositional analysis. If the operation engineer has concerns about H₂S levels, based on offset well experiences, they may request the lab to specifically report H₂S concentrations at an extra cost. Some labs include this for free, while others charge for it. Often, H₂S is not reported in the standard lab analysis. When asked to provide H₂S ppm numbers, sometime they get a nearby offset well sample analysis that includes H₂S numbers, and report it instead of the actual well they are working on and call it good (which can be, of course, largely inaccurate). Figure 2 shows a real lab sample where H₂S composition is reported, and Figure 3 illustrates a case where H₂S was omitted. When H₂S ppm levels are reported by the lab, operators use this information to comply with regulations and meet pipeline and equipment rental specifications. However, issues arise when H₂S ppm values obtained from a pressurized vessel (operating between 100–1000 psig) are applied to atmospheric applications like storage tanks or vapor recovery systems. H₂S concentrations at near-atmospheric pressures can be significantly higher than those measured in high-pressure vessels. Figure 1 above illustrates this issue, showing a case where 20 ppm H₂S is reported at a 500 psig separator gas line, but 63.4 ppm and 302 ppm are reported in the gas lines of oil and water storage tanks at 0.4 psig, respectively. I witnessed a similar issue at a gas compressor rental company. Operators provided H₂S measurements of 10–15 ppm, but our rental equipment suffered damage consistent with much higher concentrations. Initially, our account managers suspected the customer was either lying or mistaken. However, as this problem occurred repeatedly, they asked me to model the facility and simulate H₂S at the compressor inlet. My analysis revealed that H₂S ppm values measured at high-pressure separators are not representative of atmospheric applications. As pressure decreases, lighter hydrocarbons like methane and ethane preferentially flash off more quickly than H₂S, leading to an increase in H₂S concentration as the lighter components escape first. Detailed explanation of this phenomenon is provided below . Figure 2: Gas sample analysis from VRU pulling flash gas from upstream atmospheric oil tanks, reporting 0.01% mol frac = 100 ppm H₂S. This is right on the 100 ppm H₂S limit set by Okla. Admin. Code § 165:10-3-15 and cannot be vented. Figure 3: Example of a gas analysis from a primary HP separator operating at 959 psia. Although the lab offers an option for H₂S analysis, it was not measured as the customer opted not to pay extra for it. 3. Understanding phase behavior of H₂S and hydrocarbons During oil and gas phase separation and distillation processes, compounds with smaller molecular weights, known as \"light-ends,\" tend to flash off into the gas phase before the \"heavy-ends.\" The exact classification of light-ends versus heavy-ends can vary depending on the context, but for this article, I will use these terms to compare the relative weights of compounds. Figure 4 below shows a simulation of reservoir fluid being sequentially separated into gas and liquid phases through 3-stage separation (500 psig, 200 psig, and 0 psig). The fraction of methane, the lightest compound, in each vessel's gas line sequentially decreases as the 2-phase fluid moves through lower-pressure vessels. Light-ends flash off into the gas phase much faster than heavy-ends. By the time the process stream pressure reaches atmospheric level (0 psig), a significant portion of the light-ends has already escaped in the previous separators, leaving behind a higher concentration of heavy-ends. This also increases the concentration of H₂S because H₂S is heavier than methane and other light-ends have already flashed off. Table 1 shows the compositional changes in percentage between the 500# flash gas and the 0# flash gas. It also highlights that not only does the absolute fraction of heavy-ends increase relative to light-ends, but the magnitude of such increase is more pronounced for heavier compounds. For example, ethane (MW = 30.07) fraction increased by 376.4%, whereas pentane (MW = 72.15) saw a dramatic 1932% increase. Figure 4: Simulation of an upstream facility where reservoir fluid is separated through a series of 3 vessels, modeled using BR&E Promax. The reservoir fluid composition reflects real well fluid properties. Initially, the fluid enters the primary separator at 500 psig, with methane making up 87.3% of the gas line, decreasing to 70.3% and 25.5% in downstream separators as pressure drops. Meanwhile, the concentration of heavier compounds increases due to the reduction of methane. Note the rising energy content (Btu/ft³) and H₂S ppm levels as separator pressures decrease. The H₂S level increased from 18.8 ppm at the high-pressure separator to 129 ppm (a 6.86-fold increase) in the atmospheric tank. This demonstrates that H₂S ppm levels measured at high pressure cannot be directly applied to low-pressure applications. Table 1: Comparison of compositional changes at the 500 psig separator vs. the 0 psig atmospheric tank gas line. Methane fraction decreased as it flashed off first, increasing the concentration of heavier-ends. The magnitude of the change correlates with each compound's molecular weight, with heavier compounds showing greater changes. 3.1. Flash Point Pressure Why do light-ends flash to gas before heavy-ends? It's because the flash point pressure of light-ends is higher than that of heavy-ends. Figure 5 shows the flash point curves of six pure compounds. If the pressure and temperature are to the left of a compound's curve, it exists as a liquid; if to the right, it exists as vapor. Take ethane (orange) as an example. At 80°F, the flash point pressure of ethane is 618 psig. At the same temperature, propane's (red) flash point pressure is 131 psig, which is lower than ethane. Now, assume these two compounds are in the same system (but exist as independent pure compounds) at the same temperature of 80°F (illustrated by the vertical dashed grey line), and the pressure changes from 620 psig to 300 psig and then 100 psig. At 620 psig, both compounds are in liquid form. As pressure drops to 300 psig, ethane will vaporize while propane remains liquid. When pressure drops further to 100 psig, pressure is below both compounds' flash curves, and both will vaporize. This occurs because ethane's flash point curve is higher than propane's, meaning ethane vaporizes first. Looking at the figure, you'll notice that the lighter the compound, the higher its curve, illustrating why lighter compounds preferentially flash to vapor more easily than heavy-ends. The flash point curve of H₂S is located below (or to the right of) methane and ethane, which explains why methane and ethane vaporize more easily than H₂S. Figure 5: Flash point curves of pure compounds, calculated and generated using Python. Circular markers indicate critical points. Note that the critical point of H₂S extends far outside the plot at 1285 psig and 213°F (not shown), showing a different pattern from the hydrocarbon compounds. This is because H₂S does not contain a carbon atom and, therefore, does not fully follow the characteristics of hydrocarbons. The grey dashed line shows the 80°F isothermal temperature. Downward-triangular markers indicate flash point pressures of various compounds along the 80°F line. However, the 80°F isothermal is located far right of methane's flash curve, meaning it does not intersect. This indicates that methane will always be in a vapor phase, regardless of the pressure, until the pressure is below its critical pressure, after which methane enters a critical phase. Source Code For Figure (5) import CoolProp.CoolProp as CP import matplotlib.pyplot as plt import numpy as np def Pa_to_psi(pa): return pa / 6894.745 def K_to_F(K): return (K - 273.15) * 1.8 + 32 components = ['Methane', 'Ethane', 'H2S', 'Propane', 'Butane', 'Pentane'] comp_dict = {comp: {} for comp in components} for comp in components: HEOS = CP.AbstractState('HEOS', comp) HEOS.set_mole_fractions([1]) HEOS.build_phase_envelope(\"dummy\") PE = HEOS.get_phase_envelope_data() comp_dict[comp]['Ts'] = np.array(sorted([K_to_F(item) for item in PE.T])) comp_dict[comp]['Ps'] = np.array(sorted([Pa_to_psi(item) - 14.7 for item in PE.p])) T_unit = 'F' P_unit ='psig' fig, ax = plt.subplots(figsize=(8, 4.5)) for comp in components: ax.plot(comp_dict[comp]['Ts'], comp_dict[comp]['Ps'], label=comp) ax.scatter(comp_dict[comp]['Ts'][-1], comp_dict[comp]['Ps'][-1], s=40, edgecolor='k', fc='white', zorder=3) ax.scatter(comp_dict[comp]['Ts'][-1], comp_dict[comp]['Ps'][-1], s=40, edgecolor='k', fc='white', zorder=3) ax.axvline(x=80, color='grey', linestyle='--') ax.set_xlabel('Temperature [%s]' % T_unit) ax.set_ylabel('Pressure [%s]' % P_unit) ax.grid(True) ax.spines['top'].set_visible(False) ax.spines['right'].set_visible(False) ax.set_xlim(-300, 400) ax.set_ylim(0, 799) ax.legend(ncol=3, fontsize=9, loc='upper left') x_increment = 5 ax.text(73, 799, '80°F Line', fontsize=10, ha='center', va='top', color='grey', rotation=90) ax.text(80 + x_increment, 617.64, '618# (Ethane)', ha='left', c=plt.rcParams['axes.prop_cycle'].by_key()['color'][1]) ax.text(80 + x_increment, 291.29, '291# (Hydrogen Sulfide)', ha='left', c=plt.rcParams['axes.prop_cycle'].by_key()['color'][2]) ax.text(80 + x_increment, 130.64, '131# (Propane)', ha='left', c=plt.rcParams['axes.prop_cycle'].by_key()['color'][3]) ax.text(80 + x_increment, 22.52, '23# (n-Butane)', ha='left', c=plt.rcParams['axes.prop_cycle'].by_key()['color'][4]) ax.scatter(80, 617.64, marker='v', c=plt.rcParams['axes.prop_cycle'].by_key()['color'][1]) ax.scatter(80, 291.29, marker='v', c=plt.rcParams['axes.prop_cycle'].by_key()['color'][2]) ax.scatter(80, 130.64, marker='v', c=plt.rcParams['axes.prop_cycle'].by_key()['color'][3]) ax.scatter(80, 22.52, marker='v', c=plt.rcParams['axes.prop_cycle'].by_key()['color'][4]) ax.minorticks_on() ax.grid(axis='both', which='minor', color='grey', linestyle='--', alpha=0.2) ax.text(0.98, 0.08, 'aegis4048.github.io', fontsize=10, ha='right', transform=ax.transAxes, color='grey', alpha=0.5) def setbold(txt): return ' '.join([r\"$\\bf{\" + item + \"}$\" for item in txt.split(' ')]) bold_txt = setbold('Pure Compound Flash Points at 80°F, ') plain_txt = 'Phase Diagram' fig.suptitle(bold_txt + plain_txt, verticalalignment='top', x=0, horizontalalignment='left', fontsize=11) yloc = 0.9 ax.annotate('', xy=(0.01, yloc + 0.01), xycoords='figure fraction', xytext=(1.02, yloc + 0.01), arrowprops=dict(arrowstyle=\"-\", color='k', lw=0.7)) fig.tight_layout() Notes: Mixture behavior Assume a 50:50 mol fraction binary mixture of ethane and propane at 80°F. In this case, they are mixed, not existing as independent pure compounds. The mixture starts at 1000 psig as 100% liquid. Now, drop the pressure to 300 psig, which is below ethane's flash point but above propane's. Would the mixture be a 2-phase fluid? Yes. However, does this mean the vapor fraction is composed of 100% ethane? No. When compounds are mixed and the pressure drops, it doesn't work as if all of the lighter compounds vaporize first, and then, once the pressure drops further, the heavier ones vaporize later. It doesn't work like that. Both compounds vaporize together, but the lighter compound preferentially vaporizes more than the heavier one. Running such a simulation in BR&E Promax, one can find that at 80°F and 300 psig, the mixture forms a 2-phase fluid with 35.4% vapor and 64.6% liquid. The vapor phase (35.4%) is composed of 64.8% ethane and 35.2% propane. The liquid phase (64.6%) is composed of 41.9% ethane and 58.1% propane. This distinction needs to be clarified. 3.2. H₂S ppm level changes in vapor vs liquid H₂S ppm levels are typically more problematic in the gas phase because concentrations are higher there than in the liquid phase. As shown in Figure 6 , H₂S levels in the gas phase are consistently higher than in the liquid phase. Additionally, as the reservoir fluid undergoes separation at decreasing pressures, H₂S ppm in the gas phase rises, while it drops in the liquid phase. This trend holds in both 2-phase and 3-phase separations. Why does this happen? The answer lies in H₂S's high flash point pressure of 291 psig. At 80°F and 0 psig, H₂S primarily exists in the gas phase. In simpler terms, H₂S prefers the gas phase, although not as much as lighter gases like methane or ethane. In contrast, heavier compounds like pentane, hexane, and heptane, which usually end up in crude oil tanks, have much lower flash points. At 80°F, pentane's flash point is -4.2 psig, hexane's is -11.7 psig, and heptane's is -13.8 psig. At atmospheric pressure (0 psig), these compounds mostly stay in the liquid phase. Most of the H₂S ends up in the gas phase because of its lower molecular weight and tendency to remain in gas form as pressure decreases. As pressure drops, H₂S flashes into the gas phase more easily than it dissolves in the liquid. Since H₂S has lower solubility in the liquid phase compared to heavier hydrocarbons, it concentrates in the gas phase as pressure decreases, while its presence in the liquid phase lessens. Figure 6: Simulation of an upstream facility where reservoir fluid is separated through 3 vessels, modeled using BR&E Promax. The reservoir fluid composition represents real well fluid properties. The simulation shows two key results: 1) H₂S levels are higher in the gas phase than in the liquid phase, and 2) H₂S concentrations increase in the gas phase as separation progresses, while they decrease in the liquid phase. This phenomenon is due to H₂S's high flash point pressure and its preference to flash into the gas phase rather than dissolve into the liquid phase. 4. H₂S sim results at various production rates In Figure 1 above, H₂S increased from 20 ppm to 60 ppm at oil tank flash, and 300 ppm at water tank flash. This scenario was assuming 1000 BOPD, 500 BWPD, 5 MMSCFD production. How would the ppm values change at different production rates? I provide Promax sim result screenshots in this section for readers to observe and learn. For all scenarios, oil prod is fixed at 1000 BOPD. Water rate ranges between 100 to 2000 BWPD, and gas rate ranges from 2 MMSCFD to 20 MMSCFD. Oil gravity is assumed to be 40° API (impact of API gravity didn't seem to be much after running some simulations). 4.1. Simulation screenshots For summary, navigate down to 4.2. Simulation results summary. 4.1.1 100 BWPD 2 MMSCFD 5 MMSCFD 10 MMSCFD 20 MMSCFD 4.1.2 1000 BWPD 2 MMSCFD 5 MMSCFD 10 MMSCFD 20 MMSCFD 4.1.3 2000 BWPD 2 MMSCFD 5 MMSCFD 10 MMSCFD 17 MMSCFD * Simulated failed and could not be converged beyond 17 MMSCFD mark, so 17 MMSCFD is reported here instead of 20. 4.2. Simulation results summary Table 2 shows the simulated H₂S ppm levels at various production rates across different process streams. Table 3 is a normalized version of Table 2, allowing for easier comparison of how H₂S ppm levels measured at high-pressure conditions change at lower-pressure conditions. Normalization was necessary because H₂S ppm levels at the 500# separator's gas side fluctuate as production rates (BOPD, BWPD, MMSCFD) change. Four key conclusions can be drawn from Table 3 : 1. H₂S ppm levels measured at high-pressure applications should not be used for low-pressure applications due to significant discrepancies. 2. These discrepancies are more pronounced in water tank flash gas than in oil tank flash gas. For oil tank flash gas, H₂S levels were 2.5 times higher than in the separator gas, whereas for water tank flash gas, the difference was 13–24 times greater. Stricter standards should be applied for water tank flash lines. 3. Increasing the Gas-Oil-Ratio (GOR) raises ppm levels to a certain extent in lower-pressure applications, increasing the deviation from high-pressure separator measurements. However, high GOR results in only ~10% higher H₂S ppm compared to low GOR, and the effect of GOR on H₂S ppm is minimized in high watercut applications. 4. An increase in water cut slightly reduces H₂S levels, but the effect is minimal. Table 2: H₂S ppm levels reported at various process streams and production rates, taken from simulation results shown in 4.1. Simulation screenshots above. Table 3: Normalized version of Table 2 . All simulated ppm values were multiplied by a normalizing factor so that the H2S ppm levels taken at the 500# separator's gas side equals 20 ppm in all scenarios. 5. Conclusion 1. H₂S ppm levels are important to comply with regulations, prevent equipment damage, and ensure personnel safety. 2. H₂S ppm levels measured in high-pressure applications should not be applied to low-pressure environments, as the compositions change during separation at lower pressures. 3. H₂S ppm levels significantly increase at low pressure. For example, in Figure 1 , 20 ppm measured at the 500# separator rises to 63.4 ppm and 302 ppm at the atmospheric oil and water tank flash lines, respectively. 4. H₂S levels increase as lighter ends (methane and ethane) flash off before (and more) H₂S enters the gas phase. This occurs because methane (MW=16.04) and ethane (MW=30.07) are lighter than H₂S (MW=34.08), giving them higher flash point pressures, as shown in Figure 5 . However, methane is the main driver, as it flashes significantly easier than ethane (shown in Figure 4 and Table 1 ), leaving a higher concentration of H₂S in lower-pressure vessels. 5. H₂S levels are much higher in gas streams than in liquid streams because H₂S has a high flash point pressure of 291 psig (at 80°F) and prefers the gas phase. As pressure drops, H₂S flashes into the gas phase more readily than it dissolves into the liquid. 6. Discrepancies between high- and low-pressure ppm levels are more pronounced in water tank flash gas than in oil tank flash gas. For oil tank flash gas, H₂S levels were 2.5 times higher than in the 500 psig separator gas, whereas for water tank flash gas, the difference was 13–24 times greater (shown in Table 3 ). Stricter safety and corrosion standards should be applied for water tank flash lines. 7. The impact of GOR and water cut is minimal. The main factor is the compositional changes due to pressure drop and flashing.","tags":"Oil and Gas","url":"https://aegis4048.github.io/h2s-levels-why-high-pressure-samples-dont-tell-the-full-story","loc":"https://aegis4048.github.io/h2s-levels-why-high-pressure-samples-dont-tell-the-full-story"},{"title":"Robust Linear Regressions In Python","text":"Contents 1. Why is OLS vulnerable to outliers? 1.1. Convergence to mean 1.2. Squared term magnifies impact of outliers 1.3. Understanding the L-norms in cartesian space 2. Robust Models 2.1. RANSAC regressor 2.1.1. Visual demonstrations 2.1.2. Median Absolute Deviation (MAD) threshold 2.1.3. Required iteration calculation 2.1.4. More detailed visual demonstrations 2.1.5. RANSAC code snippets 2.2. Huber regressor 2.2.1. Huber loss function 2.2.2. Motivation 2.2.3. Parameter tuning 2.2.4. Huber code snippets 2.3. Theil-Sen regressor 2.3.1. Sample size and model robustness 2.3.2. Spatial median 2.3.3. Why use median instead of mean with outliers? 2.3.3.1. Effect of the squared term 2.3.3.2. Measure of central tendency 2.3.4. Theil-Sen code snippets 2.4. Summary 3. Extension to 3D+ multivariate linear regressions 3.1. Visual demonstrations In regression analysis, understanding and mitigating the impact of outliers is crucial for accurate model predictions. Common approaches like Ordinary Least Squares (OLS) often fall short when outliers are present, leading to skewed results. This discussion will explore how various regression techniques, including OLS, RANSAC, Huber, and Theil-Sen regressors, each handle outliers differently. By examining these methods through both theoretical insights and practical demonstrations using Python, I aim to highlight their unique responses to outlier influences, thereby guiding the selection of the most appropriate regression model for datasets with varying outlier characteristics. Figure 1: In scenarios involving outliers, the RANSAC regressor is often the preferred choice as it identifies and excludes outliers before fitting the model. Other robust regressors, like Huber and Theil-Sen, aim to dampen the impact of outliers rather than exclude them. This approach is beneficial if the so-called 'outliers' are actually integral parts of the dataset, needing consideration rather than exclusion. In terms of robustness to outliers, the hierarchy typically follows: RANSAC > Theil-Sen > Huber > OLS. Note that the points labeled with the 'x' markers are outliers detected by the RANSAC model. Source Code For Figure (1) import random import numpy as np import matplotlib.pyplot as plt from sklearn import linear_model from sklearn.linear_model import RANSACRegressor, HuberRegressor, TheilSenRegressor ###################################### data ###################################### X = np.array([ 0.15, -0.34, 0.32, 0.43, -0.4, -0.04, -0.51, 0.3, 0.47, 0.12, 0.08, 0.04, -0.08, -0.23, 0.08, -0.03, 0.03, 0.04, 0.01, 0.06, 0.03, 0., -0.04, -0.18, -0.19, -0.06, -0.26, -0.16, 0.13, 0.09, 0.03, -0.03, 0.04, 0.14, -0.01, 0.4, -0.06, 0.15, 0.08, 0.05, -0.15, -0.09, -0.15, -0.11, -0.07, -0.19, -0.06, 0.17, 0.23, 0.18]).reshape(-1, 1) y = np.array([17.44, 25.46, 18.61, 26.07, 24.96, -1.22, 26.45, 26.5, 20.57, 3.08, 35.9 , 32.47, 20.84, 13.37, 42.44, 27.23, 35.65, 29.51, 31.28, 41.34, 32.19, 33.67, 25.64, 9.3, 14.63, 25.1, 4.69, 14.42, 47.53, 33.82, 32.2 , 24.81, 32.64, 45.11, 26.76, 68.01, 23.39, 43.49, 37.88, 36.01, 16.32, 19.77, 16.34, 19.57, 29.28, 16.62, 24.39, 43.77, 50.46, 47.09]) ################################### Model fits ################################### # Ordinary Least Squares ols = linear_model.LinearRegression().fit(X, y) y_pred_ols = ols.predict(X) coefs_ols = ols.coef_ intercept_ols = ols.intercept_ # RANSAC ransac = RANSACRegressor(random_state=1).fit(X, y) y_pred_ransac = ransac.predict(X) coefs_ransac = ransac.estimator_.coef_ intercept_ransac = ransac.estimator_.intercept_ # Huber huber = HuberRegressor().fit(X, y) y_pred_huber = huber.predict(X) coefs_huber = huber.coef_ intercept_huber = huber.intercept_ # TheilSen TS = TheilSenRegressor().fit(X, y) y_pred_TS = TS.predict(X) coefs_TS = TS.coef_ intercept_TS = TS.intercept_ ########################## Outliers detected - RANSAC ############################ X_outlier_ransac = X[~ransac.inlier_mask_ ] y_outlier_ransac = y[~ransac.inlier_mask_ ] #################################### Plotting #################################### fig, ax = plt.subplots(figsize=(8, 4.5)) ax.scatter(X_outlier_ransac, y_outlier_ransac, s=100, c='#ff7f0e', marker='x') ax.scatter(X, y, s=100, fc='grey', lw=1, edgecolors='k', alpha=0.3) ax.plot(X, y_pred_ols, label='OLS') ax.plot(X, y_pred_ransac, label='RANSAC') ax.plot(X, y_pred_huber, label='Huber') ax.plot(X, y_pred_TS, label='TheilSen') ax.legend(fontsize=12, ncol=4) ax.grid(axis='y', linestyle='--', color='#acacac', alpha=0.5) ax.spines['top'].set_visible(False) ax.spines['right'].set_visible(False) ax.set_xlabel('X', fontsize=13) ax.set_ylabel('y', fontsize=13) ax.text(0.99, 0.1, 'aegis4048.github.io', fontsize=12, ha='right', va='center', transform=ax.transAxes, color='grey', alpha=0.5) def setbold(txt): return ' '.join([r\"$\\bf{\" + item + \"}$\" for item in txt.split(' ')]) bold_txt = setbold('Robust Linear Regression') plain_txt = r', performance comprisons of OLS vs. robust models' fig.suptitle(bold_txt + plain_txt, verticalalignment='top', x=0, horizontalalignment='left', fontsize=13, y=0.96) yloc = 0.88 ax.annotate('', xy=(0.01, yloc + 0.01), xycoords='figure fraction', xytext=(1.02, yloc + 0.01), arrowprops=dict(arrowstyle=\"-\", color='k', lw=0.7)) fig.tight_layout() 1. Why is OLS vulnerable to outliers? A simple 2D Ordinary Least Squares (OLS) solves for arguments (slope $m$ and intercept $b$) that minimize the following objective (loss) function: $$L_{2}&#94;{2} = \\underset{m, b}{\\text{argmin}} \\sum_{i=1}&#94;n [y_i - (mx_i + b)]&#94;2 \\tag{1}$$ Intuitively the objective function is finding the best slope and intercept that will minimize the squared prediction error (residuals) of your model prediction $\\hat{y} = mX + b$ against the observation $y$. Eq-1 is often referred to as $L_{2}$-Norm squared . It has some interesting properties that make it vulnerable to the outliers: 1.1. Convergence to mean The equation has mathematical properties that make it converge to mean. While I'm not gonna bore the readers explaining the mathematical proof (I don't understand them anyway), I can show you in Python that it indeed converges to the mean. Consider the following code snippet: In [1]: import numpy as np from scipy.optimize import minimize # Implemenation of eq-1 above def L2_norm_squared ( point , _x ): return np . sum (( _x - point [ 0 ]) ** 2 ) x = np . array ([ 3 , 9 , 21 , 25 ]) x0 = 0 # initial guess, this can be anything L2 = minimize ( L2_norm_squared , x0 , args = ( x ), method = 'Nelder-Mead' ) print ( \"L2-squared minimized:\" , L2 . x ) print ( \"Mean of x:\" , [ np . mean ( x )]) L2-squared minimized: [14.5] Mean of x: [14.5] Observe that the parameter 14.5 , which minimizes the objective function, is also the sample mean. This is because the objective function converges towards the mean. However, the problem with the mean is that it's not the best measure of central tendency in the presence of outliers that induce skewness. Consider the below figure: Figure 2: Three measures of central tendency in presence of outliers: mode, median and mean (image adpated from here). In an ideal normal distribution without outliers, the mode, median, and mean coincide. However, outliers skew the distribution, pulling the mean away from where most data points lie. The median, less affected by outliers, provides a better measure of central tendency in such cases, leading to the preference for median-based estimators like the $L_{1}$-Norm absolute loss over squared loss. Figures 18 and 19 below shows visual demonstrations of superior performance of median-based estimators in presence of outliers. 1.2. Squared term magnifies impact of outliers The OLS loss function aims to minimize the squared residuals, which escalates the influence of outliers because the loss increases exponentially with larger residuals . This can lead to a poor fit. Less severe loss functions, like the regular $L_{2}$-Norm (square root of sum of squared residuals): $$L_2 = \\underset{m, b}{\\text{argmin}} \\sqrt{\\sum_{i=1}&#94;n [y_i - (mx_i + b)]&#94;2} \\tag{2}$$ or $L_{1}$-Norm (absolute residuals): $$L_1 = \\underset{m, b}{\\text{argmin}} \\sum_{i=1}&#94;n |y_i - (mx_i + b)| \\tag{3}$$ can be more resilient to outliers, dampening their effects. Eq-2 has the squared term, but the effect of squaring is mitigated by taking the squared root, making it more robust than $L_{2}$-Norm squared . Eq-3 on the other hand, has no squared term at all, making it the most robust choice among the three. 1.3. Understanding the L-norms in cartesian space Interesting observations arise when the norms are adapted in a Cartesian space, and these can be used to illustrate why the OLS is vulnerable to outliers. Consider four data points in a 2D Cartesian space along the x-axis and y-axis. Minimizing the $L_{2}$-Norm squared ( eq-1 , which is the objective function of the OLS) for the distance from these four points identifies the centroid (center of mass) coordinate that minimizes the sum of the squares of distances to each point. This point, having the smallest average distance to each of the four points, can be found through a simple formula: its x and y coordinates are the averages of the x's and y's of the four points, respectively. This is possible because the $L_{2}$-Norm squared has the mathematical property of converging to the mean, as demonstrated above. On the other hand, minimizing the $L_{2}$-Norm eq-2 identifies a point that minimizes the sum of Euclidean distances to each point, known as the spatial (geometric) median. Note that the $L_{2}$-Norm essentially generalizes the Pythagorean theorem for n-dimensional spaces. (Minimizing the $L_{1}$-Norm absolute loss eq-3 reduces the Manhattan distances from all points, a process without any specific Cartesian property associated with it) Figure 3 below demonstrates how outliers affect the mean (centroid, $L_{2}$-Norm squared ) compared to the median (spatial median, $L_{2}$-Norm ). When an outlier with an extreme y-value is introduced, it shifts the mean upwards more significantly than the median. This difference arises because the median's objective function computes the square root of squared residuals, thereby reducing the impact of squared errors and assigning less weight to this outlier than the mean-based estimator. Given that such outliers are typically undesirable in regression model fitting, methods that either mitigate (like Theil-Sen and Huber regressors) or eliminate (like the RANSAC regressor) the influence of these outliers are preferred. Figure 3: The graph illustrates the robustness of median-based estimators compared to mean-based estimators in the presence of outliers. The lef plot shows a dataset without outliers. The mean (green plus) represents the centroid (a point that minimizes the average distances) of the four points and the median (orange star) shows the spatial median (a point that minimizes the sum of Euclidean distances). On the right, the introduction of an outlier (marked with a red 'x') significantly shifts the mean upwards, whereas the median remains relatively stable, highlighting its robustness. This contrast is due to the squared error in the mean calculation magnifying the effect of outliers, whereas the median—equivalent to the square root of the squared error—dampens their impact, thus remaining more stable in their presence. Source Code For Figure (3) import matplotlib.pyplot as plt import numpy as np from scipy.optimize import minimize x = [3, 9, 21, 25] y = [12, 35, 16, 28] x_outlier = [3, 9, 15, 21, 25] y_outlier =[12, 35, 100, 16, 28] ys = [y, y_outlier] xs = [x, x_outlier] # L2 loss - euclidean distance def L2_objective_func(point, _x, _y): return np.sum(np.sqrt((point[0] - _x)**2 + (point[1] - _y)**2)) # L2 squared loss def L2_squared_objective_func(point, _x, _y): return np.sum((_x - point[0])**2 + (_y - point[1])**2) s = 150 init_guess = [0, 0] fig, axes = plt.subplots(1, 2, figsize=(9, 4)) for i, (ax, x, y) in enumerate(zip(axes, xs, ys)): # calculates L2 euclidean loss. This results in a spatial median result_L2 = minimize(L2_objective_func, init_guess, args=(x, y), method='Nelder-Mead') # calculatse L2 squared loss. This results in a centroid (center of mass) result_L2_2 = minimize(L2_squared_objective_func, init_guess, args=(x, y), method='Nelder-Mead') # Proves that minimizing L2 squared loss results in mean assert np.allclose(result_L2_2.x, [np.mean(x), np.mean(y)], atol=1e-4) ax.scatter(x, y, s=s, edgecolor='blue', fc=(0, 0, 1, 0.05)) _s1 = ax.scatter(result_L2.x[0], result_L2.x[1], s=s, label=r'Median: $\\underset{x, y}{\\mathrm{argmin}} \\sum&#94;{n}_{i=1}\\sqrt{(x_{i} - \\hat{x}_{i})&#94;{2} + (y_{i} - \\hat{y}_{i})&#94;2}$', marker='*') _s2 = ax.scatter(result_L2_2.x[0], result_L2_2.x[1], s=s, label=r'Mean: $\\underset{x, y}{\\mathrm{argmin}} \\sum&#94;{n}_{i=1}[(x_{i} - \\hat{x}_{i})&#94;{2} + (y_{i} - \\hat{y}_{i})&#94;2]$', marker='+', lw=3) xmax = 30 ymax = 110 ax.set_xlim(0 - 0.05 * xmax, xmax) ax.set_ylim(0 - 0.05 * ymax, ymax) ax.grid(axis='both', linestyle='--', color='#acacac', alpha=0.5) ax.spines['top'].set_visible(False) ax.spines['right'].set_visible(False) if i == 0: x_increment = 1.2 else: x_increment = 1.5 ax.text(result_L2_2.x[0] - x_increment, result_L2_2.x[1] + 6, '(%d, %d)' % (result_L2_2.x[0], result_L2_2.x[1]), color=_s2.get_facecolor()[0], ha='center') ax.text(result_L2.x[0] + x_increment, result_L2.x[1] + 6, '(%d, %d)' % (result_L2.x[0], result_L2.x[1]), color=_s1.get_facecolor()[0], ha='center') ax.text(0.98, 0.08, 'aegis4048.github.io', fontsize=10, ha='right', transform=ax.transAxes, color='grey', alpha=0.5) ax.text(result_L2_2.x[0] - x_increment, result_L2_2.x[1] + 14, 'Mean', color=_s2.get_facecolor()[0], ha='center') ax.text(result_L2.x[0] + x_increment, result_L2.x[1] + 14, 'Median', color=_s1.get_facecolor()[0], ha='center') ax.set_ylabel('Y') ax.set_xlabel('X') axes[1].scatter(x[2], y_outlier[2], s=100, marker='x') axes[1].text(x[2], y_outlier[2] - 10, 'outlier', color='red', ha='center') handles, labels = [], [] for ax in axes: for h, l in zip(*ax.get_legend_handles_labels()): handles.append(h) labels.append(l) break fig.legend(handles, labels, fontsize=10.5, ncol=2, loc='lower center', bbox_to_anchor=(0.5, -0.14), frameon=True) def setbold(txt): return ' '.join([r\"$\\bf{\" + item + \"}$\" for item in txt.split(' ')]) bold_txt = setbold('Mean vs Median, ') plain_txt = r'effect of outliers on median- vs mean-based estimators' fig.suptitle(bold_txt + plain_txt, verticalalignment='top', x=0, horizontalalignment='left', fontsize=12, y=0.96) yloc = 1.035 ax.annotate('', xy=(0.01, yloc + 0.01), xycoords='figure fraction', xytext=(1.02, yloc + 0.01), arrowprops=dict(arrowstyle=\"-\", color='k', lw=0.7)) fig.tight_layout() 2. Robust Models 2.1. RANSAC regressor RAN dom SA mple C onsensus (RANSAC) is an iterative algorithm that separates inliers vs. outliers, and fits a regression model only using the separated inliers. (Note that these steps are based on sklearn's implementation of RANSAC regressor. ) Steps Start the 1st iteration. Randomly select $m$ ( min_samples ) data points from the whole data set. $m$ = number of features + 1. Therefore, for 2D linear regression, $m=2$. No duplicates. Fit a linear regression model on the selected $m$ data points. The objective function being minimized is the $L_{1}$-Norm absolute loss eq-3 ( loss='absolute_error by default. Alternatively you could try loss='squared_error' ). When $m=2$, the slope generalizes to a simple $(y_{j}−y_{i})/(x_{j}−x_{i})$. Classify a point as an inlier if it falls within a certain threshold value ( residual_threshold ) from the fitted regression line. By default, the threshold is the Median Absolute Deviation (MAD) (auto-computed) of the whole data. Repeat this for every data point in the original data set. Compute the value of $N$ ( required number of iterations ). Start the 2nd iteration. Repeat steps 1-4. If the newly fitted model from the 2nd iteration contains more inliers, replace the previous model with the new one. Otherwise keep the old model from the 1st iteration. If both the old and new models have the same number of inliers, pick the one with a better $R&#94;2$ score. Compare the old and new $N$ values, and pick a smaller one: min(old_N, new_N) Repeat steps 1-7 until the $N$-th iteration. $N$ can be indirectly controlled by tweaking stop_probability and min_samples arguments. Iterations are finished. Fit a new regression model using only the final inliers. Return the fitted model. 2.1.1. Visual demonstrations Figure 4: Simplified visual demonstration of RANSAC algorithm $m=2$ ( min_samples=2 ). 2.1.2. Median Absolute Deviation (MAD) threshold The default threshold for RANSAC employs the Median Absolute Deviation (MAD), a value calculated once before the iterative process begins. Just as how the standard deviation serves as a measure of spread around the mean, MAD represents variability around the median. This choice is motivated by the robustness of the median over the mean as a measure of central tendency as explained above . MAD can be computed with the following codes: In [5]: import numpy as np X = np . array ([ 3 , 6 , 9 , 10 , 12 , 15 , 17 , 18 , 20 , 24 , 25 , 26 , 27 ]) . reshape ( - 1 , 1 ) y = np . array ([ 8 , 12 , 15.5 , 13.5 , 17 , 20 , 18 , 24 , 24.5 , 8 , 6 , 9 , 7 ]) MAD = np . median ( np . abs ( y - np . median ( y ))) MAD Out[5]: 5.5 Figure 5: Median Absolute Deviation (MAD) is a measure centered around the median, akin to how the standard deviation ($\\pm\\sigma$) around the mean in a normal distribution encompasses 68.2% of the population. Importantly, the median provides a more accurate representation of central tendency in the presence of skewness or outliers, compared to the mean. Source Code For Figure (5) import numpy as np import matplotlib.pyplot as plt from scipy.stats import gumbel_r, iqr data = gumbel_r.rvs(size=100000, random_state=1) x = np.linspace(min(data), max(data), 100000) pdf = gumbel_r.pdf(x) M = np.median(data) MAD = np.median(np.abs(data - M)) Mean = np.mean(data) perc_left = gumbel_r.cdf(M - MAD) perc_right = 1 - gumbel_r.cdf(M + MAD) perc_center = 1 - perc_left - perc_right def y_finder(y_arr, x_arr, x_val, idx_increment=0): idx = np.argmin(np.abs(np.array(x_arr) - x_val)) return y_arr[idx + idx_increment] fig, ax = plt.subplots(figsize=(8, 3)) ax.plot(x, pdf, 'k-', lw=2, zorder=99) ax.fill_between(x, pdf, where=((x >= M - MAD) & (x <= M + MAD)), color='skyblue', alpha=0.3) ax.vlines(x=M - MAD, ymax=y_finder(pdf, x, M - MAD), ymin=0, ls='--') ax.vlines(x=M + MAD, ymax=y_finder(pdf, x, M + MAD), ymin=0, ls='--') ax.vlines(x=M, ymax=y_finder(pdf, x, M), ymin=0, color='r', ls='--') ax.text(M - MAD - 0.1, 0.01, '%.1f%s' % (perc_left * 100, r'%'), ha='right', va='bottom') ax.text(M + MAD + 0.1, 0.01, '%.1f%s' % (perc_right * 100, r'%'), ha='left', va='bottom') ax.text(M, 0.15, '%.1f%s' % (perc_center * 100, r'%'), ha='center', va='center', fontsize=20) ax.text(M - MAD - 0.1, y_finder(pdf, x, M - MAD), 'Median-MAD', ha='right', va='center', color='#1f77b4', fontsize=13) ax.text(M + MAD + 0.1, y_finder(pdf, x, M + MAD), 'Median+MAD', ha='left', va='center', color='#1f77b4', fontsize=13) ax.text(M + 0.1, y_finder(pdf, x, M), 'Median', ha='left', va='center', color='r', fontsize=13) ax.text(0.01, 0.15, 'aegis4048.github.io', fontsize=10, ha='left', transform=ax.transAxes, color='grey', alpha=0.5) ax.set_ylim(0, None) ax.set_xlim(-2.5, 6) ax.spines['top'].set_visible(False) ax.spines['right'].set_visible(False) def setbold(txt): return ' '.join([r\"$\\bf{\" + item + \"}$\" for item in txt.split(' ')]) bold_txt = setbold('Mean Absolute Deviation (MAD), ') plain_txt = r'measure of central tendency using median; Gumbel distribution' fig.suptitle(bold_txt + plain_txt, verticalalignment='top', x=0, horizontalalignment='left', fontsize=11, y=0.95) yloc = 0.85 ax.annotate('', xy=(0.01, yloc + 0.01), xycoords='figure fraction', xytext=(1.02, yloc + 0.01), arrowprops=dict(arrowstyle=\"-\", color='k', lw=0.7)) fig.tight_layout() 2.1.3. Required iteration calculation Another important component of the RANSAC is the required iteration ($N$). $$ N >= \\frac{log(1-p)}{log(1-e&#94;m)} \\tag{4}$$ where $N$ : required number of iterations. $p$ : probability (confidence) that one outlier-free sample is generated. (default=0.99). $e$ : inlier ratio (# of inliers / # of entire data points). This may change every iteration. $m$ : minimum number of samples chosen randomly from origianl data (default=2). $N$ is rounded up to the nearest integer. Note that you can indirectly control the value of $N$ by changing stop_probability and min_samples in the sklearn package. It can be computed with the following codes: In [6]: import numpy as np p = 0.99 # sklearn default m = 2 # sklearn default for 2D linear regression w = 0.6 # this may change every iteration N = abs ( float ( np . ceil ( np . log ( 1 - p ) / np . log ( 1 - w ** m )))) N Out[6]: 11.0 2.1.4. More detailed visual demonstrations Figure 6: First iteration in RANSAC. Left plot shows randomly selected 2 data points (blue dots) for regression. The right plot displays residuals, with the dashed red line indicating MAD. Points with residuals exceeding MAD are marked as outliers. Initially, $N$ is set to 100 max_trials=100 . After the first iteration, $N$ is computed to be 20 using eq-4 . Since 20 is smaller than 100, $N$ is updated to 20. A total of 6 inliers are found in this round. Figure 7: Second iteration. The new model has more inliers (9) then the previous model (6). Model is updated. The inlier ratio ($w$ in eq-4 ) changes. $N$ is computed to be 8 in this iteration. Since 8 is smaller than 20 from the previous iteration, $N$ is updated to 8. Figure 8: Third iteration. Although the number of inliers equals the previous iteration, a visual inspection of the left plot reveals that the current model is incorrect. To decide whether an update is necessary, we compare the $R&#94;2$ scores. Since the previous model has a higher $R&#94;2$ (0.92) than the current one (0.62), no model update is performed. This process continues until the $N$-th (8th) iteration, assuming that $N$ remains constant. Once all iterations are completed, the final model is fitted using the selected inliers. Source Code For Figures (6-8) import numpy as np import matplotlib.pyplot as plt from sklearn import linear_model from sklearn.linear_model import RANSACRegressor X = np.array([3, 6, 9, 10, 12, 15, 17, 18, 20, 24, 25, 26, 27]).reshape(-1, 1) y = np.array([8, 12, 15.5, 13.5, 17, 20, 18, 24, 24.5, 8, 6, 9, 7]) MAD = np.median(np.abs(y - np.median(y))) N_PREV = 100 def fit_and_plot(X, y, MAD, random_state, selected_idx, stage, update_status): global N_PREV X_selected = X[selected_idx] y_selected = y[selected_idx] ols = linear_model.LinearRegression() model = ols.fit(X_selected, y_selected) y_pred = model.predict(X) residuals = np.abs(y - y_pred) ransac = RANSACRegressor(max_trials=1, random_state=random_state).fit(X, y) # deal with rs=11 y_pred_ransac = ransac.predict(X) num_inliers = np.sum(ransac.inlier_mask_) R2 = ransac.score(X[ransac.inlier_mask_], y[ransac.inlier_mask_]) p = 0.99 m = 2 w = num_inliers / len(y) N = abs(float(np.ceil(np.ceil(np.log(1 - p) / np.log(1 - w ** m))))) #################################################################################### fig, axes = plt.subplots(1, 2, figsize=(10, 4)) axes[0].scatter(X, y, s=100, color='#70ad47', label='Inliers') axes[0].scatter(X[~ransac.inlier_mask_], y[~ransac.inlier_mask_], s=100, color='red', label='Outliers') axes[0].scatter(X_selected, y_selected, s=100, color='#4472c4', label='Random') axes[0].axline(xy1=(X_selected[0][0], y_selected[0]), xy2=(X_selected[-1][0], y_selected[-1]), color='#7030a0', label='Regression on Random') axes[0].legend(loc='upper left', ncol=2, fontsize=9) axes[0].set_title('RANSAC: %s iteration' % stage, fontsize=10) scatter_dict = {'s':100, 'fc':'none', 'linewidth':1.5} axes[1].scatter(X[ransac.inlier_mask_], residuals[ransac.inlier_mask_], edgecolors='k', label='Inliers', **scatter_dict) axes[1].scatter(X[~ransac.inlier_mask_], residuals[~ransac.inlier_mask_], edgecolors='r', label='Outliers', **scatter_dict) axes[1].scatter(X[selected_idx], residuals[selected_idx], edgecolors='b', label='Random', **scatter_dict) axes[1].axhline(y=MAD, color='r', linestyle='--') axes[1].legend(loc='upper left', ncol=3, fontsize=9) axes[1].text(-0.5, MAD + 0.5, 'Threshold (MAD) = 5.50', fontsize=9, ha='left', va='bottom', color='r', alpha=1) axes[1].text(0, 25, '# of Inliers = %d' % num_inliers, ha='left', weight='bold', fontsize=12) axes[1].text(0, 22.75, update_status, ha='left', fontsize=10) axes[1].text(0, 20.5, 'min($N_{old}$, $N_{new}$)= min(%d, %d)= %d' % (N_PREV, N, N), ha='left', fontsize=10) axes[1].text(0, 18.25, '$R&#94;2$ = %.2f' % round(R2, 2), ha='left', fontsize=10) axes[1].set_title('Residuals of regression', fontsize=10) xmax = 30 ymax = 30 for ax in axes: ax.spines.top.set_visible(False) ax.spines.right.set_visible(False) ax.set_xlim(0 - 0.05 * xmax, xmax + 0.05 * xmax) ax.set_ylim(0 - 0.05 * ymax, ymax + 0.05 * ymax) N_PREV = N fit_and_plot(X, y, MAD, random_state=3, selected_idx=[4, 6], stage='1st', update_status='model updated (initial)') fit_and_plot(X, y, MAD, random_state=11, selected_idx=[0, 6], stage='2nd', update_status='model updated') fit_and_plot(X, y, MAD, random_state=29, selected_idx=[-9, -1], stage='3rd', update_status='model NOT updated') 2.1.5. RANSAC code snippets For quick copy-paste, replace X and y with your own data. Make sure to reshape your X so that it is a 2D numpy.ndarray object with shape like (13, 1) . In [7]: import numpy as np from sklearn.linear_model import RANSACRegressor from sklearn import linear_model import matplotlib.pyplot as plt # sample data X = np . array ([ 0.15 , - 0.34 , 0.32 , 0.43 , - 0.4 , - 0.04 , - 0.51 , 0.3 , 0.47 , 0.12 , 0.08 , 0.04 , - 0.08 , - 0.23 , 0.08 , - 0.03 , 0.03 , 0.04 , 0.01 , 0.06 , 0.03 , 0. , - 0.04 , - 0.18 , - 0.19 , - 0.06 , - 0.26 , - 0.16 , 0.13 , 0.09 , 0.03 , - 0.03 , 0.04 , 0.14 , - 0.01 , 0.4 , - 0.06 , 0.15 , 0.08 , 0.05 , - 0.15 , - 0.09 , - 0.15 , - 0.11 , - 0.07 , - 0.19 , - 0.06 , 0.17 , 0.23 , 0.18 ]) . reshape ( - 1 , 1 ) y = np . array ([ 17.44 , 25.46 , 18.61 , 26.07 , 24.96 , - 1.22 , 26.45 , 26.5 , 20.57 , 3.08 , 35.9 , 32.47 , 20.84 , 13.37 , 42.44 , 27.23 , 35.65 , 29.51 , 31.28 , 41.34 , 32.19 , 33.67 , 25.64 , 9.3 , 14.63 , 25.1 , 4.69 , 14.42 , 47.53 , 33.82 , 32.2 , 24.81 , 32.64 , 45.11 , 26.76 , 68.01 , 23.39 , 43.49 , 37.88 , 36.01 , 16.32 , 19.77 , 16.34 , 19.57 , 29.28 , 16.62 , 24.39 , 43.77 , 50.46 , 47.09 ]) # fit and predict: RANSAC ransac = RANSACRegressor ( random_state = 1 ) . fit ( X , y ) y_pred_ransac = ransac . predict ( X ) # retrieve the fitted parameters coefs = ransac . estimator_ . coef_ # RANSAC fits many regression models. The \".estimator_\" attribute intercept = ransac . estimator_ . intercept_ # has the final model fitted. # seperate into inliers vs. outliers X_inlier = X [ ransac . inlier_mask_ ] y_inlier = y [ ransac . inlier_mask_ ] X_outlier = X [ ~ ransac . inlier_mask_ ] y_outlier = y [ ~ ransac . inlier_mask_ ] print ( \"RANSAC ----------------------------------- \\n \" ) print ( \"Coefficients :\" , coefs ) print ( \"Intercept :\" , intercept ) print ( \"# of inliers :\" , sum ( ransac . inlier_mask_ )) print ( \"Fraction of inliers :\" , sum ( ransac . inlier_mask_ ) / len ( y )) print ( \" \\n ------------------------------------------\" ) # fit and predict: Ordinary Least Squares ols = linear_model . LinearRegression () . fit ( X , y ) y_pred_ols = ols . predict ( X ) # plot plt . title ( 'RANSAC Regressor' , fontsize = 14 ) plt . scatter ( X_inlier , y_inlier , s = 100 , c = 'green' , label = 'Inliers' ) plt . scatter ( X_outlier , y_outlier , s = 100 , c = 'red' , label = 'Outliers' ) plt . plot ( X , y_pred_ols , label = 'OLS' ) plt . plot ( X , y_pred_ransac , label = 'RANSAC' ) plt . legend (); RANSAC ----------------------------------- Coefficients : [92.50723098] Intercept : 30.331391269710938 # of inliers : 40 Fraction of inliers : 0.8 ------------------------------------------ WARNING! RANSAC relies on random sampling, which means that with a larger dataset (exceeding 50 points), your regression outcomes can differ due to this inherent randomness. This approach offers the advantage of flexibility for iterative adjustments (as observed when running the same code multiple times without a fixed random seed) when turning the model. However, it also leads to variability in outcomes, which can be problematic for replicating results or sharing code. To achieve consistent results when you are done tuning and trying to report, it's advisable to set a random seed, as in: model = RANSACRegressor(random_state=3) . 2.2. Huber regressor The Huber regressor doesn't eliminate outliers but mitigates their effect, making it a preferred choice when you have reason to believe that so-called outliers may have relevance in the regression analysis. In contrast to other robust regression techniques like RANSAC or Theil-Sen, which involve substantial structural modifications in their algorithms compared to default linear regression, such as subsampling and fitting regressions on these subsets, the Huber regressor differes only in its choice of loss function while retaining the fundamental structure of default linear regression. 2.2.1. Huber loss function The original Huber loss is a piece-wise function shown in eq-5 . This is the most generic search result you will see if you google Huber regressor. Note that the term, $y - \\hat{y}$, are often referred to as residuals (prediction error). $$ L_{\\delta} (y - \\hat{y})= \\left\\{ \\begin{array}{ll} \\frac{1}{2}(y - \\hat{y})&#94;{2} & \\text{for } |y - \\hat{y}| \\leq \\delta \\text{ (inlier)}, \\\\ \\delta \\cdot (|y - \\hat{y}| - \\frac{1}{2}\\delta) & \\text{otherwise (outlier)}. \\\\ \\end{array} \\right. \\tag{5}$$ where $y$ : observation, original data point $\\hat{y}$ : prediction $\\delta$ : absolute parameter that controls the number of outliers $L$ : loss function HOWEVER, a challenge with the equation eq-5 is that the $\\delta$ parameter is in absolute scale. For instance, if 95% of your residuals fall within the range of (45, 60), setting $\\delta=1.35$ equates to a 2.25% to 3.00% residual tolerance, which works well. But when dealing with a different dataset where the residuals range from (4355, 13205), applying the same $\\delta=1.35$ results in an impractical ~0.001% residual tolerance. Therefore, a unique $\\delta$ value would be needed for each dataset's residual range. To address this, scikit-learn modifies the original Huber loss by using the scale parameter $\\sigma$, allowing for a consistent threshold parameter that operates on a relative scale instead of an absolute one. The loss function that HuberRegressor minimizes is given by: $$ \\begin{align} \\underset{w, \\sigma}{\\text{argmin}}\\left[\\sum&#94;{n}_{i=1}\\left(\\sigma + L_{\\epsilon}\\left(\\frac{X_{i}w - y_{i}}{\\sigma}\\right)\\sigma\\right) + \\alpha ||w||_{2}&#94;{2}\\right] \\tag{6} \\end{align} $$ where $$ L_{\\epsilon}(z)= \\left\\{ \\begin{array}{ll} z&#94;{2} & \\text{for } z \\leq \\epsilon \\text{ (inlier)}, \\\\ 2\\epsilon |z| - \\epsilon&#94;{2} & \\text{otherwise (outlier)}. \\\\ \\end{array} \\right. \\tag{7}$$ where $w$ : linear regression arguments. For 2D liear regression, it's (slope, intercept) $\\sigma$ : scaling parameter. This allows the residual threshold to be on a relative scale so it can work for all residual ranges. $i$ : index of a data point $n$ : number of samples. $L$ : Huber loss function $\\epsilon$ : relative epsilon parameter that controls the number of outliers. $X$ : independent variable. Note that $X_{i}w$ is equivalent to prediction $\\hat{y}_{i}$ $y$ : observation, original data point $\\alpha$ : learning rate $z$ : residual divided by $\\sigma$ The loss function maybe a bit challenging to interpret for beginners. Figure 9 below shows how the equation breaks down. Recall that $L_{\\epsilon}$ is a piece-wise function that applies different formula depending on whether the data point is identified as an outlier or not by the $\\epsilon$ parameter, which is set to be 1.35 (recommended) by scikit learn. Figure 9: Explanation of eq-9 . 2.2.2. Motivation The two most commonly used loss functions are squared loss, $L(y - \\hat{y}) = (y - \\hat{y})&#94;2$ and absolute loss, $L(y - \\hat{y}) = |y - \\hat{y}|$. While the squared loss is more accurate, it has the disadvantage that it has the tendency to be dominated by outliers. This susceptibility arises because squared residuals magnify the effect of outliers, as the residuals are squared, leading to an exponential increase in loss as residuals grow. In contrast, absolute loss grows linearly with residuals, rendering it more robust to outliers. The Huber regressor offers a compelling compromise, leveraging the strengths of squared and absolute loss functions while mitigating their weaknesses. It employs squared loss for small-residual data points (inliers) and absolute loss for large-residual data points (outliers). The distinction between inliers and outliers is governed by the $\\delta$ parameter, set to 1.35 by default in scikit-learn. 2.2.3. Parameter tuning ($\\delta$) The $\\delta$ parameter controls the residual threshold for determining whether squared loss or Huber loss is applied. As shown in Figure 10 , smaller $\\delta$ values lead to a more robust approach by applying Huber loss to a greater number of data points, emphasizing absolute loss. Conversely, larger $\\delta$ values result in squared loss being more prevalent, making the method more susceptible to outliers. Notably, for very large $\\delta$ values, Huber loss converges to squared loss (susceptible to outliers), as described in eq-8 : $$ \\lim_{\\delta\\to \\infty}: \\text{Huber Loss} \\approx \\text{Squared Loss} \\tag{8}$$ Note that in the scikit-learn implementation , the residuals are divided by the scale parameter sigma |(y - Xw - c) / sigma| to ensure that one does not need to rescale epsilon to achieve the same robustness. The default value of $\\delta$ in scikit-learn is 1.35. Figure 10: $\\delta$ controls the residual thresholds used to determine whether to compute squared loss or Huber loss. Small value of $\\delta$ (left plot) allows Huber loss to be applied for wider residual ranges of data, making it more robust to outliers. Observe that the blue line (Huber Loss) in the left plot applies to greater residual ranges with smaller $\\delta$(=1.35). Source Code For Figure (10) import numpy as np import matplotlib.pyplot as plt error_range = np.linspace(-3.3, 3.3, 100) theta_values = [1.35, 2.35] huber_color = 'blue' squared_color = 'green' fig, axes = plt.subplots(1, len(theta_values), figsize=(9, 4.5)) for i, theta in enumerate(theta_values): huber_loss = np.where(np.abs(error_range) <= theta, 0.5 * error_range ** 2, theta * (np.abs(error_range) - 0.5 * theta)) squared_error_loss = 0.5 * error_range ** 2 ax = axes[i] # Set the range for which the line styles and alphas will change x_range = (-theta, theta) # Plot Huber Loss line huber_loss_segment = np.where((error_range >= x_range[0]) & (error_range <= x_range[1]), huber_loss, np.nan) ax.plot(error_range, huber_loss_segment, linewidth=2, zorder=3, alpha=0.3, linestyle='dashed', color=huber_color) huber_loss_segment = np.where((error_range < x_range[0]) | (error_range > x_range[1]), huber_loss, np.nan) ax.plot(error_range, huber_loss_segment, label='Huber Loss', linewidth=2, zorder=3, alpha=1, linestyle='-', color=huber_color) # Plot Squared Loss line squared_loss_segment = np.where((error_range >= x_range[0]) & (error_range <= x_range[1]), squared_error_loss, np.nan) ax.plot(error_range, squared_loss_segment, label='Squared Loss', linewidth=2, zorder=3, alpha=1, linestyle='-', color=squared_color) squared_loss_segment = np.where((error_range < x_range[0]) | (error_range > x_range[1]), squared_error_loss, np.nan) ax.plot(error_range, squared_loss_segment, linewidth=2, zorder=3, alpha=0.3, linestyle='dashed', color=squared_color) # Fill the area between the axvlines fill_alpha = 0.03 ax.axvspan(-5, -theta, alpha=fill_alpha, color=huber_color, zorder=-8, label='Outliers') ax.axvspan(theta, 5, alpha=fill_alpha, color=huber_color, zorder=-9) ax.axvspan(-theta, theta, alpha=fill_alpha, color=squared_color, zorder=-9, label='Inliers') ax.set_xlabel('Residuals ($y - \\\\hat{y}$)', fontsize=13) ax.set_ylabel('Loss', fontsize=13) ax.grid(True, alpha=0.3) lg = ax.legend(loc='upper center', ncol=2) for i, lh in enumerate(lg.legendHandles): if i > 1: lh.set_alpha(0.4) ax.axvline(x=theta, color='r', linestyle=\"dotted\", alpha=0.7) ax.axvline(x=-theta, color='r', linestyle=\"dotted\", alpha=0.7) ax.spines['top'].set_visible(False) ax.spines['right'].set_visible(False) ax.set_xlim(-3.3, 3.3) ax.text(0, 3.5, '$\\delta = %.2f$' % theta, fontsize=13, ha='center', va='top', color='r', alpha=0.7, rotation=0) ax.text(0.05, 0.1, 'aegis4048.github.io', fontsize=10, ha='left', va='center', transform=ax.transAxes, color='grey', alpha=0.5) axes[0].text(3, 0.1, 'more robust', ha='right', fontsize=13, bbox=dict( facecolor='white')) axes[1].text(3, 0.1, 'less robust', ha='right', fontsize=13, bbox=dict( facecolor='white')) def setbold(txt): return ' '.join([r\"$\\bf{\" + item + \"}$\" for item in txt.split(' ')]) bold_txt = setbold('Huber Loss, ') plain_txt = r'effect of $\\delta$-parameter on inlier vs. outlier detection' fig.suptitle(bold_txt + plain_txt, verticalalignment='top', x=0, horizontalalignment='left', fontsize=11) yloc = 0.9 ax.annotate('', xy=(0.01, yloc + 0.01), xycoords='figure fraction', xytext=(1.02, yloc + 0.01), arrowprops=dict(arrowstyle=\"-\", color='k', lw=0.7)) fig.tight_layout() 2.2.4. Huber code snippets For quick copy-paste, replace X and y with your own data. Make sure to reshape your X so that it is a 2D numpy.ndarray object with shape like (13, 1) . In [8]: import numpy as np from sklearn.linear_model import HuberRegressor from sklearn import linear_model import matplotlib.pyplot as plt # sample data X = np . array ([ 0.15 , - 0.34 , 0.32 , 0.43 , - 0.4 , - 0.04 , - 0.51 , 0.3 , 0.47 , 0.12 , 0.08 , 0.04 , - 0.08 , - 0.23 , 0.08 , - 0.03 , 0.03 , 0.04 , 0.01 , 0.06 , 0.03 , 0. , - 0.04 , - 0.18 , - 0.19 , - 0.06 , - 0.26 , - 0.16 , 0.13 , 0.09 , 0.03 , - 0.03 , 0.04 , 0.14 , - 0.01 , 0.4 , - 0.06 , 0.15 , 0.08 , 0.05 , - 0.15 , - 0.09 , - 0.15 , - 0.11 , - 0.07 , - 0.19 , - 0.06 , 0.17 , 0.23 , 0.18 ]) . reshape ( - 1 , 1 ) y = np . array ([ 17.44 , 25.46 , 18.61 , 26.07 , 24.96 , - 1.22 , 26.45 , 26.5 , 20.57 , 3.08 , 35.9 , 32.47 , 20.84 , 13.37 , 42.44 , 27.23 , 35.65 , 29.51 , 31.28 , 41.34 , 32.19 , 33.67 , 25.64 , 9.3 , 14.63 , 25.1 , 4.69 , 14.42 , 47.53 , 33.82 , 32.2 , 24.81 , 32.64 , 45.11 , 26.76 , 68.01 , 23.39 , 43.49 , 37.88 , 36.01 , 16.32 , 19.77 , 16.34 , 19.57 , 29.28 , 16.62 , 24.39 , 43.77 , 50.46 , 47.09 ]) # fit and predict: Huber huber = HuberRegressor ( epsilon = 1.35 ) . fit ( X , y ) # 1.35 is the default. You can try different epsilon values. y_pred_huber = huber . predict ( X ) # retrieve the fitted parameters coefs = huber . coef_ intercept = huber . intercept_ # seperate into inliers vs. outliers X_inlier = X [ ~ huber . outliers_ ] y_inlier = y [ ~ huber . outliers_ ] X_outlier = X [ huber . outliers_ ] y_outlier = y [ huber . outliers_ ] print ( \"Huber ----------------------------------- \\n \" ) print ( \"Coefficients :\" , coefs ) print ( \"Intercept :\" , intercept ) print ( \"# of inliers :\" , sum ( ~ huber . outliers_ )) print ( \"Fraction of inliers :\" , sum ( ~ huber . outliers_ ) / len ( y )) print ( \" \\n ------------------------------------------\" ) # fit and predict: Ordinary Least Squares ols = linear_model . LinearRegression () . fit ( X , y ) y_pred_ols = ols . predict ( X ) # plot plt . title ( 'Huber Regressor' , fontsize = 14 ) plt . scatter ( X_inlier , y_inlier , s = 100 , c = 'green' , label = 'Inliers' ) plt . scatter ( X_outlier , y_outlier , s = 100 , c = 'red' , label = 'Outliers' ) plt . plot ( X , y_pred_ols , label = 'OLS' ) plt . plot ( X , y_pred_huber , label = 'Huber' ) plt . legend (); Huber ----------------------------------- Coefficients : [50.32948547] Intercept : 29.03041669256188 # of inliers : 31 Fraction of inliers : 0.62 ------------------------------------------ WARNING! Despite the application of the robust Huber regressor, the regression fit in the plot above (Figure ?) is still notably influenced by the presence of outliers, albeit to a lesser extent. This influence persists because, instead of completely excluding outliers during the fitting process, Huber aims to attenuate their impact by employing absolute loss rather than squared loss. Therefore, unless there is a compelling reason to retain and account for the so-called \"outliers\" within your data, using the Huber regressor is not typically recommended. 2.3. Theil-Sen regressor The Theil-Sen Regressor calculates the slope by examining all possible combinations of subsets from the dataset and taking the median of these slopes. This approach is based on the \"$n$ choose $k$\" $\\binom{n}{k}$ method, where $n$ is the number of data points and $k$ is the subset size. For example, with $k=2$ (standard for 2D linear regression) and $n=50$, the algorithm computes $\\binom{50}{2} = 1225$ combinations, each with a subset size of $k$. The median of the slopes from these subsets is used as the final slope for the regression line. This method can be adapted for different values of $k$, leading to a varying number of combinations. It is important to note that the TheilSen regressor does not explicitly identify outliers from inliers, unlike RANSAC or Huber. (Note that these steps are based on sklearn's implementation of TheilSen regressor. ) Steps Consider an example dataset of size $n=6$: $A(1, 3)$, $B(2,2)$, $C(3,6)$, $D(4,5)$, $E(5,7)$, $F(6,5)$ Figure 11: Initial dataset of size $n=6$. Source Code For Figure (11) import numpy as np import matplotlib.pyplot as plt ##################################### sample data ##################################### X = np.array([1, 2, 3, 4, 5, 6]).reshape(-1, 1) y = np.array([3, 2, 6, 5, 7, 5]) ###################################### plotting ####################################### fig, ax = plt.subplots(figsize=(8, 4.5)) ax.scatter(X, y, label='Original Data') points = ['A','B','C','D','E','F'] for x_, y_, point in zip(X, y, points): ax.text(x_, y_ + 1, '%s(%d,%d)' % (point, x_, y_), ha='left') ax.legend(loc='upper left') ax.grid(axis='y', linestyle='--', color='#acacac', alpha=0.5) ax.spines['top'].set_visible(False) ax.spines['right'].set_visible(False) ymax = 15 ax.set_ylim(0 - 0.05 * ymax, ymax) ax.set_xlabel('X', fontsize=13) ax.set_ylabel('y', fontsize=13) ax.text(0.99, 0.1, 'aegis4048.github.io', fontsize=10, ha='right', va='center', transform=ax.transAxes, color='grey', alpha=0.5) def setbold(txt): return ' '.join([r\"$\\bf{\" + item + \"}$\" for item in txt.split(' ')]) bold_txt = setbold('Initial Data') plain_txt = r', $n=6$' fig.suptitle(bold_txt + plain_txt, verticalalignment='top', x=0, horizontalalignment='left', fontsize=13, y=0.96) yloc = 0.88 ax.annotate('', xy=(0.01, yloc + 0.01), xycoords='figure fraction', xytext=(1.02, yloc + 0.01), arrowprops=dict(arrowstyle=\"-\", color='k', lw=0.7)) fig.tight_layout() Let $k=2$ (default for 2D linear regression). $\\binom{6}{2}=15$ samples are generated. Consider the first sample: $A(1, 3)$ and $B(2,2)$. The slope can be obtained with a simple $(y_{j}−y_{i})/(x_{j}−x_{i})$. Obtain intercept from the slope. Figure 12: Linear regression on the first sample, composed of points $A$ and $B$. Source Code For Figure (12) import numpy as np import matplotlib.pyplot as plt ##################################### sample data ##################################### X = np.array([1, 2, 3, 4, 5, 6]).reshape(-1, 1) y = np.array([3, 2, 6, 5, 7, 5]) ########################## slope and intercept calc# ################################## slope = (y[0] - y[1]) / (X[0] - X[1]) intercept = y[0] - slope * X[0] y_pred = X * slope + intercept ###################################### plotting ####################################### fig, ax = plt.subplots(figsize=(8, 4.5)) ax.scatter(X, y, label='Original Data') ax.scatter(X[:2], y[:2], label='Current sample') ax.plot(X, y_pred, label='Linear model for (A, B): y = %.1fx + %.1f' % (slope, intercept), color='#ff7f0e', ls='-.') points = ['A','B','C','D','E','F'] for x_, y_, point in zip(X, y, points): ax.text(x_, y_ + 1, '%s(%d,%d)' % (point, x_, y_), ha='left') ax.legend(loc='upper left') ax.grid(axis='y', linestyle='--', color='#acacac', alpha=0.5) ax.spines['top'].set_visible(False) ax.spines['right'].set_visible(False) ymax = 15 ax.set_ylim(0 - 0.05 * ymax, ymax) ax.set_xlabel('X', fontsize=13) ax.set_ylabel('y', fontsize=13) ax.text(0.99, 0.1, 'aegis4048.github.io', fontsize=10, ha='right', va='center', transform=ax.transAxes, color='grey', alpha=0.5) def setbold(txt): return ' '.join([r\"$\\bf{\" + item + \"}$\" for item in txt.split(' ')]) bold_txt = setbold('First Sample Slope Calculation') plain_txt = r', $k=2$ ($A$ and $B$)' fig.suptitle(bold_txt + plain_txt, verticalalignment='top', x=0, horizontalalignment='left', fontsize=13, y=0.96) yloc = 0.88 ax.annotate('', xy=(0.01, yloc + 0.01), xycoords='figure fraction', xytext=(1.02, yloc + 0.01), arrowprops=dict(arrowstyle=\"-\", color='k', lw=0.7)) fig.tight_layout() (Optional 3.A). When more than two data points $k>2$ are present, the least-squares method, or OLS eq-1 , is used for parameter fitting. This method is necessary as the simple slope formula $(y_{j}−y_{i})/(x_{j}−x_{i})$ is only valid for two data points. As the number of data points $k$ approaches the total number of observations $n$, the Theil-Sen regressor converges towards OLS results, which is not robust. Figure 13: Because there are $k=3$ points, least-squares (OLS) method is necessary to fit a slope and an intercept. Note that the model robustness decreases as $k$ increases. Source Code For Figure (13) import numpy as np import matplotlib.pyplot as plt from sklearn import linear_model ##################################### sample data ##################################### X = np.array([1, 2, 3, 4, 5, 6]).reshape(-1, 1) y = np.array([3, 2, 6, 5, 7, 5]) ########################## OLS on the current sample ################################## # Ordinary Least Squares ols = linear_model.LinearRegression().fit(X[:3], y[:3]) y_pred_ols = ols.predict(X) coefs_ols = ols.coef_ intercept_ols = ols.intercept_ ###################################### plotting ####################################### fig, ax = plt.subplots(figsize=(8, 4.5)) ax.scatter(X, y, label='Original Data') ax.scatter(X[:3], y[:3], label='Current sample') ax.plot(X, y_pred_ols, label='OLS (A, B, C): y = %.1fx + %.1f' % (coefs_ols[0], intercept_ols), color='#ff7f0e', ls='-.') points = ['A','B','C','D','E','F'] for x_, y_, point in zip(X, y, points): ax.text(x_, y_ + 1, '%s(%d,%d)' % (point, x_, y_), ha='left') ax.legend(loc='upper left') ax.grid(axis='y', linestyle='--', color='#acacac', alpha=0.5) ax.spines['top'].set_visible(False) ax.spines['right'].set_visible(False) ymax = 15 ax.set_ylim(0 - 0.05 * ymax, ymax) ax.set_xlabel('X', fontsize=13) ax.set_ylabel('y', fontsize=13) ax.text(0.99, 0.1, 'aegis4048.github.io', fontsize=10, ha='right', va='center', transform=ax.transAxes, color='grey', alpha=0.5) def setbold(txt): return ' '.join([r\"$\\bf{\" + item + \"}$\" for item in txt.split(' ')]) bold_txt = setbold('First Sample Slope Calculation') plain_txt = r', $k=3$ ($A$, $B$ and $C$)' fig.suptitle(bold_txt + plain_txt, verticalalignment='top', x=0, horizontalalignment='left', fontsize=13, y=0.96) yloc = 0.88 ax.annotate('', xy=(0.01, yloc + 0.01), xycoords='figure fraction', xytext=(1.02, yloc + 0.01), arrowprops=dict(arrowstyle=\"-\", color='k', lw=0.7)) fig.tight_layout() 4. Repeat step 3 for all 15 samples. A 2x15 matrix is obtained for 15 slopes and 15 intercepts. This is equivalent to fitting 15 straight lines. Figure 14: Regression fitted on all of the 15 samples, each of size $k=2$. Observe that all straight lines pass through $k=2$ points. Source Code For Figure (14) from itertools import combinations import numpy as np from sklearn.linear_model import LinearRegression, TheilSenRegressor from scipy.optimize import minimize import matplotlib.pyplot as plt ##################################### sample data ##################################### X = np.array([1, 2, 3, 4, 5, 6]).reshape(-1, 1) y = np.array([3, 2, 6, 5, 7, 5]) ############################# parameter optimization ################################## n_samples, n_features = X.shape n_subsamples = 2 fit_intercept = True # \"n choose k\" -> n Combination k number of samples. indices = np.array(list(combinations(range(n_samples), n_subsamples))) parameters = [] for subset in indices: X_subset = X[subset] y_subset = y[subset] model = LinearRegression(fit_intercept=fit_intercept) model.fit(X_subset, y_subset) parameters.append([model.intercept_, model.coef_[0]]) parameters = np.vstack(parameters) slopes = parameters[:, 1] intercepts = parameters[:, 0] ###################################### plotting ####################################### fig, ax = plt.subplots(figsize=(8, 4.5)) ax.scatter(X, y, label='Original Data') for slope, intercept in zip(slopes, intercepts): y_pred_sample = slope * X + intercept ax.plot(X, y_pred_sample, zorder=-99, color='silver') ax.plot(X, y_pred_sample, zorder=-99, color='lightgrey', label='Sample fit, $k=2$') points = ['A','B','C','D','E','F'] for x_, y_, point in zip(X, y, points): ax.text(x_, y_ + 1, '%s(%d,%d)' % (point, x_, y_), ha='left') ymax = 15 ax.set_ylim(0 - 0.05 * ymax, ymax) ax.legend(loc='upper left') ax.grid(axis='y', linestyle='--', color='#acacac', alpha=0.5) ax.spines['top'].set_visible(False) ax.spines['right'].set_visible(False) ax.set_xlabel('X', fontsize=13) ax.set_ylabel('y', fontsize=13) ax.text(0.99, 0.1, 'aegis4048.github.io', fontsize=10, ha='right', va='center', transform=ax.transAxes, color='grey', alpha=0.5) def setbold(txt): return ' '.join([r\"$\\bf{\" + item + \"}$\" for item in txt.split(' ')]) bold_txt = setbold('Fitting 6C2 = 15 Samples') plain_txt = r', 15 regression models fitted' fig.suptitle(bold_txt + plain_txt, verticalalignment='top', x=0, horizontalalignment='left', fontsize=13, y=0.96) yloc = 0.88 ax.annotate('', xy=(0.01, yloc + 0.01), xycoords='figure fraction', xytext=(1.02, yloc + 0.01), arrowprops=dict(arrowstyle=\"-\", color='k', lw=0.7)) fig.tight_layout() 5. A spatial median of the 15 slopes and 15 intercepts is computed. Note that the spatial median is not the simple median of each slopes and and intercepts independently. Figure 15: This plot displays regression parameters of a 2D linear model, with intercepts casted on the x-axis and slopes casted on the y-axis. For a model with three parameters, such visualization would extend into 3D space. The spatial median is determined to be slope = 0.67 and interecept = 2.33. Source Code For Figure (15) from itertools import combinations import numpy as np from sklearn.linear_model import LinearRegression, TheilSenRegressor from scipy.optimize import minimize import matplotlib.pyplot as plt ##################################### sample data ##################################### X = np.array([1, 2, 3, 4, 5, 6]).reshape(-1, 1) y = np.array([3, 2, 6, 5, 7, 5]) ############################# parameter optimization ################################## n_samples, n_features = X.shape n_subsamples = 2 fit_intercept = True # \"n choose k\" -> n Combination k number of samples. indices = np.array(list(combinations(range(n_samples), n_subsamples))) parameters = [] for subset in indices: X_subset = X[subset] y_subset = y[subset] model = LinearRegression(fit_intercept=fit_intercept) model.fit(X_subset, y_subset) parameters.append([model.intercept_, model.coef_[0]]) parameters = np.vstack(parameters) ########################## spatial median approximation ############################### # L2 loss - euclidean distance def L2_objective_func(point, _x, _y): return np.sum(np.sqrt((point[0] - _x)**2 + (point[1] - _y)**2)) intercepts = parameters[:, 0] slopes = parameters[:, 1] init_guess = [np.mean(intercepts), np.mean(slopes)] # starting with mean is a good guess to reduce computational cost result_L2 = minimize(L2_objective_func, init_guess, args=(intercepts, slopes), method='Nelder-Mead') ################################### result validation ################################# # Checks that the implemented codes here agree with the sklearn implementation TS = TheilSenRegressor().fit(X, y) y_pred_TS = TS.predict(X) np.testing.assert_almost_equal(result_L2.x[1], TS.coef_[0], decimal=2) np.testing.assert_almost_equal(result_L2.x[0], TS.intercept_, decimal=2) ###################################### plotting ####################################### fig, ax = plt.subplots(figsize=(7, 4)) ax.scatter(intercepts, slopes, s=150, edgecolor='blue', fc=(0, 0, 1, 0.05)) _s1 = ax.scatter(result_L2.x[0], result_L2.x[1], s=400, marker='*', label=r'Median: $\\underset{x, y}{\\mathrm{argmin}} \\sum&#94;{n}_{i=1}\\sqrt{(x_{i} - \\hat{x}_{i})&#94;{2} + (y_{i} - \\hat{y}_{i})&#94;2}$') ax.grid(axis='both', linestyle='--', color='#acacac', alpha=0.5) ax.spines['top'].set_visible(False) ax.spines['right'].set_visible(False) ax.set_xlabel('Intercepts') ax.set_ylabel('Slopes') ax.text(result_L2.x[0] + 0.5, result_L2.x[1] + 0.15, '(%.2f, %.2f)' % (result_L2.x[0], result_L2.x[1]), color=_s1.get_facecolor()[0], ha='left') ax.text(result_L2.x[0] + 0.5, result_L2.x[1] + 0.5, 'Spatial Median', color=_s1.get_facecolor()[0], ha='left') ax.text(0.98, 0.08, 'aegis4048.github.io', fontsize=10, ha='right', transform=ax.transAxes, color='grey', alpha=0.5) def setbold(txt): return ' '.join([r\"$\\bf{\" + item + \"}$\" for item in txt.split(' ')]) bold_txt = setbold('Spatial Median of Slopes and Intercepts, ') plain_txt = r'obtained by minimizing the $L_{2}$ norm.' fig.suptitle(bold_txt + plain_txt, verticalalignment='top', x=0, horizontalalignment='left', fontsize=11, y=0.95) yloc = 0.87 ax.annotate('', xy=(0.01, yloc + 0.01), xycoords='figure fraction', xytext=(1.02, yloc + 0.01), arrowprops=dict(arrowstyle=\"-\", color='k', lw=0.7)) fig.tight_layout() 6. Return the final model. Figure 16: Straight line drawn from the spatial median of (intercepts, slopes) from step 5. Source Code For Figure (16) from itertools import combinations import numpy as np from sklearn.linear_model import LinearRegression, TheilSenRegressor from scipy.optimize import minimize import matplotlib.pyplot as plt ##################################### sample data ##################################### X = np.array([1, 2, 3, 4, 5, 6]).reshape(-1, 1) y = np.array([3, 2, 6, 5, 7, 5]) ############################# parameter optimization ################################## n_samples, n_features = X.shape n_subsamples = 2 fit_intercept = True # \"n choose k\" -> n Combination k number of samples. indices = np.array(list(combinations(range(n_samples), n_subsamples))) parameters = [] for subset in indices: X_subset = X[subset] y_subset = y[subset] model = LinearRegression(fit_intercept=fit_intercept) model.fit(X_subset, y_subset) parameters.append([model.intercept_, model.coef_[0]]) parameters = np.vstack(parameters) ########################## spatial median approximation ############################### # L2 loss - euclidean distance def L2_objective_func(point, _x, _y): return np.sum(np.sqrt((point[0] - _x)**2 + (point[1] - _y)**2)) intercepts = parameters[:, 0] slopes = parameters[:, 1] init_guess = [np.mean(intercepts), np.mean(slopes)] # starting with mean is a good guess to reduce computational cost result_L2 = minimize(L2_objective_func, init_guess, args=(intercepts, slopes), method='Nelder-Mead') final_slope = result_L2.x[1] final_intercept = result_L2.x[0] y_pred = X * final_slope + final_intercept ################################### result validation ################################# # Checks that the implemented codes here agree with the sklearn implementation TS = TheilSenRegressor().fit(X, y) y_pred_TS = TS.predict(X) np.testing.assert_almost_equal(result_L2.x[1], TS.coef_[0], decimal=2) np.testing.assert_almost_equal(result_L2.x[0], TS.intercept_, decimal=2) ###################################### plotting ####################################### fig, ax = plt.subplots(figsize=(8, 4.5)) ax.scatter(X, y, label='Original Data') ax.plot(X, y_pred, label='Final TheilSen model: y = %.1fx + %.1f' % (final_slope, final_intercept), color='#ff7f0e', ls='-.') points = ['A','B','C','D','E','F'] for x_, y_, point in zip(X, y, points): ax.text(x_, y_ + 1, '%s(%d,%d)' % (point, x_, y_), ha='left') ax.legend(loc='upper left') ax.grid(axis='y', linestyle='--', color='#acacac', alpha=0.5) ax.spines['top'].set_visible(False) ax.spines['right'].set_visible(False) ymax = 15 ax.set_ylim(0 - 0.05 * ymax, ymax) ax.set_xlabel('X', fontsize=13) ax.set_ylabel('y', fontsize=13) ax.text(0.99, 0.1, 'aegis4048.github.io', fontsize=10, ha='right', va='center', transform=ax.transAxes, color='grey', alpha=0.5) def setbold(txt): return ' '.join([r\"$\\bf{\" + item + \"}$\" for item in txt.split(' ')]) bold_txt = setbold('Final Slope and Intercept') plain_txt = r', TheilSen regressor for $n=6$ and $k=2$' fig.suptitle(bold_txt + plain_txt, verticalalignment='top', x=0, horizontalalignment='left', fontsize=13, y=0.96) yloc = 0.88 ax.annotate('', xy=(0.01, yloc + 0.01), xycoords='figure fraction', xytext=(1.02, yloc + 0.01), arrowprops=dict(arrowstyle=\"-\", color='k', lw=0.7)) fig.tight_layout() 2.3.1. Sample size and model robustness When a sample size $k> 2$, indicating more than two data points in each sample, the Theil-Sen regressor employs the OLS method for each sample generated. This method uses the $L_2$-norm squared eq-1 as its objective function, which, while effective, is sensitive to outliers because it places greater emphasis on larger residuals due to the squared term. As $k$ approaches $n$, the robustness of the Theil-Sen regressor decreases. This decrease in robustness occurs because at $k=n$, the model is essentially fitting an OLS model to the entire dataset as one single sample ($\\binom{n}{k} = 1$). In a 3D regression context, $k$ is at leat 3, factoring in the number of features plus 1 if fit_intercept=True (default). The sklearn package sets $k$ to its minimum value by default to maximize robustness. Adjusting $k$ is not recommended unless there's a clear understanding of how it affects the model's robustness. 2.3.2. Spatial median The spatial median is the point minimizing the sum of Euclidean distances from all points in a given space. This concept extends the 2D Pythagorean theorem to n-dimensional space and is mathematically defined as the point minimizing the $L_2$-norm eq-2 . For clarity, consider a scenario with five oil and gas wells and a central refinery. To minimize pipeline construction costs, the refinery should be located at the spatial median, thereby reducing the total length of required pipelines. It's crucial to note that the spatial median is not simply the median of x-axis and y-axis values considered separately, but a combined evaluation of all dimension, as highlighted in Figure 17 . Figure 17: Taking the median of the x-axis and y-axis separately yields different results from the spatial median. Source Code For Figure (17) import matplotlib.pyplot as plt import numpy as np from scipy.optimize import minimize x = [3, 9, 21, 25] y = [12, 35, 16, 28] x_outlier = [3, 9, 15, 21, 25] y_outlier =[12, 35, 100, 16, 28] ys = [y, y_outlier] xs = [x, x_outlier] # L2 loss - euclidean distance def L2_objective_func(point, _x, _y): return np.sum(np.sqrt((point[0] - _x)**2 + (point[1] - _y)**2)) s = 150 init_guess = [0, 0] fig, axes = plt.subplots(1, 2, figsize=(9, 4)) for i, (ax, x, y) in enumerate(zip(axes, xs, ys)): # calculates L2 euclidean loss. This results in a spatial median result_L2 = minimize(L2_objective_func, init_guess, args=(x, y), method='Nelder-Mead') ax.scatter(x, y, s=s, edgecolor='blue', fc=(0, 0, 1, 0.05)) _s1 = ax.scatter(result_L2.x[0], result_L2.x[1], s=s, label=r'Spatial Median', marker='*') _s2 = ax.scatter(np.median(x), np.median(y), s=s, label=r'Separate Medians', marker='+', lw=3) xmax = 30 ymax = 110 ax.set_xlim(0 - 0.05 * xmax, xmax) ax.set_ylim(0 - 0.05 * ymax, ymax) ax.grid(axis='both', linestyle='--', color='#acacac', alpha=0.5) ax.spines['top'].set_visible(False) ax.spines['right'].set_visible(False) ax.text(np.median(x), np.median(y) + 6, '(%d, %d)' % (np.median(x), np.median(y)), color=_s2.get_facecolor()[0], ha='right') ax.text(result_L2.x[0], result_L2.x[1] + 6, '(%d, %d)' % (result_L2.x[0], result_L2.x[1]), color=_s1.get_facecolor()[0], ha='left') ax.text(0.98, 0.08, 'aegis4048.github.io', fontsize=10, ha='right', transform=ax.transAxes, color='grey', alpha=0.5) ax.text(np.median(x), np.median(y) + 14, 'Separate Medians', color=_s2.get_facecolor()[0], ha='right') ax.text(result_L2.x[0], result_L2.x[1] + 14, 'Spatial Median', color=_s1.get_facecolor()[0], ha='left') ax.set_ylabel('Y') ax.set_xlabel('X') axes[1].scatter(x[2], y_outlier[2], s=100, marker='x') axes[1].text(x[2], y_outlier[2] - 10, 'outlier', color='red', ha='center') def setbold(txt): return ' '.join([r\"$\\bf{\" + item + \"}$\" for item in txt.split(' ')]) bold_txt = setbold('Separate Medians vs Spatial Medians, ') plain_txt = r'' fig.suptitle(bold_txt + plain_txt, verticalalignment='top', x=0, horizontalalignment='left', fontsize=12, y=0.96) yloc = 0.88 ax.annotate('', xy=(0.01, yloc + 0.01), xycoords='figure fraction', xytext=(1.02, yloc + 0.01), arrowprops=dict(arrowstyle=\"-\", color='k', lw=0.7)) fig.tight_layout() A convenient library for calculating spatial median is hdmedians . This library features a Cython implementation that enables faster computation. An example usage is as the following: In [12]: import hdmedians as hd x = np . array ([ 3 , 9 , 21 , 25 ], dtype = float ) # dtype=float is necessary to avoid TypeError due to conflicts with Cython y = np . array ([ 12 , 35 , 16 , 28 ], dtype = float ) data = np . array ([ x , y ]) np . array ( hd . geomedian ( data )) Out[12]: array([17.06557376, 22.22950821]) 2.3.3. Why use median instead of mean with outliers? In Figure 15 , where 15 slopes and intercepts were fitted, the TheilSen regressor uses the spatial median of these parameters rather than their mean. This choice is informed by the median's superior resilience to outliers, which can be understood deeper from two perspectives: 2.3.3.1. Effect of the squared term The squared term in the mean calculation, stemming from the least-squares method (which converges to the mean as previously demonstrated above ), amplifies the influence of extreme data points. In a 2D or n-dimensional space, the mean, or centroid, is the point that minimizes the squared sum of distances from each point, effectively minimizing the $L_2$-norm squared eq-1 . This minimization is akin to reducing the average distances from all points. Due to the squaring of distances, outliers significantly impact the mean, as illustrated in Figure 3 . In contrast, the spatial median employs the $L_2$-norm eq-2 . While both equations incorporate a squared term, the spatial median involves taking the square root of the sum of these squared distances, which lessens the impact of extreme values. Note that for 1D array ($k=1$, not applicable to 2D regression), the spatial median is equivalent to 1D median because eq-2 equates to eq-1 . 2.3.3.2. Measure of central tendency Median is a better measure of central tendency than mean in presence of outliers. For simple illustration, consider an array $x = [1, 2, 3, 4, 5]$, where both the mean and median are 3. Introducing an outlier to form $x&#94;{'} = [1,2,3,4,5, 5000]$ shifts the median only slightly to 3.5, while the mean soars to 836. This is not a good representation of the point in which most data points cluster. For a detailed demonstration, consider a Theil-Sen regression with the same dataset as in the previous RANSAC example above , featuring $n=50$ points. The dataset, generated from the model $y = 92x + 30$ with some outliers, leads to $\\binom{50}{2} = 1225$ unique samples, each yielding a slope and an intercept. The distribution of these fitted parameters is depicted in Figure 18 . The distribution of slopes (left plot) exhibits rightward skewness. The median slope, calculated as 74.6, aligns more closely with the true model parameter (92) used to generate the data. This aligns with the theory that the median is less influenced by outliers than the mean, providing a better representation of central tendency, as also illustrated in Figure 2 above. Figure 18: Displays the distribution of fitted parameters from the 1225 samples. The model's true parameters are defined by $y=92x + 30$. The left plot, representing the slope distribution, highlights the median as a more accurate representation of the central clustering of data points, underscoring the median's superiority over the mean in outlier-affected scenarios. Conversely, in the intercept distribution (right plot), the mean and median are nearly identical, a result of the intercepts' normal (symmetric) distribution. It's crucial to recognize that the medians depicted here are one-dimensional, used solely for illustrative purposes, and do not represent spatial medians. Source Code For Figure (18) from itertools import combinations import random import numpy as np import matplotlib.pyplot as plt from sklearn.linear_model import LinearRegression ###################################### data ###################################### X = np.array([ 0.15, -0.34, 0.32, 0.43, -0.4, -0.04, -0.51, 0.3, 0.47, 0.12, 0.08, 0.04, -0.08, -0.23, 0.08, -0.03, 0.03, 0.04, 0.01, 0.06, 0.03, 0., -0.04, -0.18, -0.19, -0.06, -0.26, -0.16, 0.13, 0.09, 0.03, -0.03, 0.04, 0.14, -0.01, 0.4, -0.06, 0.15, 0.08, 0.05, -0.15, -0.09, -0.15, -0.11, -0.07, -0.19, -0.06, 0.17, 0.23, 0.18]).reshape(-1, 1) y = np.array([17.44, 25.46, 18.61, 26.07, 24.96, -1.22, 26.45, 26.5, 20.57, 3.08, 35.9 , 32.47, 20.84, 13.37, 42.44, 27.23, 35.65, 29.51, 31.28, 41.34, 32.19, 33.67, 25.64, 9.3, 14.63, 25.1, 4.69, 14.42, 47.53, 33.82, 32.2 , 24.81, 32.64, 45.11, 26.76, 68.01, 23.39, 43.49, 37.88, 36.01, 16.32, 19.77, 16.34, 19.57, 29.28, 16.62, 24.39, 43.77, 50.46, 47.09]) ############################# parameter optimization ################################## n_samples, n_features = X.shape n_subsamples = 2 fit_intercept = True # \"n choose k\" -> n Combination k number of samples. indices = np.array(list(combinations(range(n_samples), n_subsamples))) parameters = [] for subset in indices: X_subset = X[subset] y_subset = y[subset] model = LinearRegression(fit_intercept=fit_intercept) model.fit(X_subset, y_subset) parameters.append([model.intercept_, model.coef_[0]]) parameters = np.vstack(parameters) intercepts = parameters[:, 0] slopes = parameters[:, 1] ###################################### plotting ###################################### fig, axes = plt.subplots(1, 2, figsize=(10, 4)) items = [slopes, intercepts] nbins = [400, 400] for ax, item, nbin in zip(axes, items, nbins): mean = np.mean(item) median = np.median(item) ax.hist(item, bins=nbin, histtype='stepfilled', edgecolor='k', alpha=0.4, color='grey',) ax.axvline(x=median, color='r', alpha=0.7, label='Median=%.1f' % median) ax.axvline(x=mean, color='k', alpha=0.7, label='Mean=%.1f' % mean) ax.legend(loc='upper left', ncol=1) ax.grid(axis='y', linestyle='--', color='#acacac', alpha=0.5) ax.set_ylim(0, 340) ax.spines['top'].set_visible(False) ax.spines['right'].set_visible(False) ax.set_ylabel(\"Occurrences\", fontsize=12) ax.text(0.99, 0.1, 'aegis4048.github.io', fontsize=10, ha='right', va='center', transform=ax.transAxes, color='grey', alpha=0.5) axes[0].set_xlim(-275, 300) axes[1].set_xlim(-15, 65) axes[0].set_xlabel(\"Slope range\", fontsize=12) axes[1].set_xlabel(\"Intercept range\", fontsize=12) def setbold(txt): return ' '.join([r\"$\\bf{\" + item + \"}$\" for item in txt.split(' ')]) bold_txt = setbold('Distribution of Slopes & Intercepts, ') plain_txt = r'comparison of median vs mean, 1225 samples' fig.suptitle(bold_txt + plain_txt, verticalalignment='top', x=0, horizontalalignment='left', fontsize=14, y=0.98) yloc = 0.88 ax.annotate('', xy=(0.01, yloc + 0.01), xycoords='figure fraction', xytext=(1.02, yloc + 0.01), arrowprops=dict(arrowstyle=\"-\", color='k', lw=0.7)) fig.tight_layout() Figure 19 compares the performance of mean-based and median-based parameter estimators. The median-based estimator $y=74.6x + 29.5$ is observed to more closely approximate the true population model $y=92x + 30$ than other estimators. However, it's noteworthy that the spatial median TheilSen regressor does not align perfectly with the true population parameters as RANSAC does, as shown above . This difference arises because RANSAC identifies and excludes outliers before optimizing parameters, whereas TheilSen attempts to lessen the impact of outliers by taking the square root of the squared sum of residuals. Therefore, if outlier exclusion is preferable to mitigation, RANSAC tends to outperform TheilSen. Figure 19: From the original 50 data points, 1225 samples of size $k=2$ are generated, and 1225 lines (light grey) are fitted. The median slope and intercept model (orange line) demonstrates greater robustness to outliers compared to the mean-based (blue) and OLS models (green). Source Code For Figure (19) from itertools import combinations import random import numpy as np import matplotlib.pyplot as plt from sklearn.linear_model import LinearRegression, TheilSenRegressor from scipy.optimize import minimize ###################################### data ###################################### X = np.array([ 0.15, -0.34, 0.32, 0.43, -0.4, -0.04, -0.51, 0.3, 0.47, 0.12, 0.08, 0.04, -0.08, -0.23, 0.08, -0.03, 0.03, 0.04, 0.01, 0.06, 0.03, 0., -0.04, -0.18, -0.19, -0.06, -0.26, -0.16, 0.13, 0.09, 0.03, -0.03, 0.04, 0.14, -0.01, 0.4, -0.06, 0.15, 0.08, 0.05, -0.15, -0.09, -0.15, -0.11, -0.07, -0.19, -0.06, 0.17, 0.23, 0.18]).reshape(-1, 1) y = np.array([17.44, 25.46, 18.61, 26.07, 24.96, -1.22, 26.45, 26.5, 20.57, 3.08, 35.9 , 32.47, 20.84, 13.37, 42.44, 27.23, 35.65, 29.51, 31.28, 41.34, 32.19, 33.67, 25.64, 9.3, 14.63, 25.1, 4.69, 14.42, 47.53, 33.82, 32.2 , 24.81, 32.64, 45.11, 26.76, 68.01, 23.39, 43.49, 37.88, 36.01, 16.32, 19.77, 16.34, 19.57, 29.28, 16.62, 24.39, 43.77, 50.46, 47.09]) ############################# parameter optimization ################################## n_samples, n_features = X.shape n_subsamples = 2 fit_intercept = True # \"n choose k\" -> n Combination k number of samples. indices = np.array(list(combinations(range(n_samples), n_subsamples))) parameters = [] for subset in indices: X_subset = X[subset] y_subset = y[subset] model = LinearRegression(fit_intercept=fit_intercept) model.fit(X_subset, y_subset) parameters.append([model.intercept_, model.coef_[0]]) parameters = np.vstack(parameters) intercepts = parameters[:, 0] slopes = parameters[:, 1] ########################## spatial median approximation ############################### # L2 loss - euclidean distance def L2_objective_func(point, _x, _y): return np.sum(np.sqrt((point[0] - _x)**2 + (point[1] - _y)**2)) init_guess = [np.mean(intercepts), np.mean(slopes)] # starting with mean is a good guess to reduce computational cost result_L2 = minimize(L2_objective_func, init_guess, args=(intercepts, slopes), method='Nelder-Mead') slope_spatial_median = result_L2.x[1] intercept_spatial_median = result_L2.x[0] y_pred_spatial_median = X * slope_spatial_median + intercept_spatial_median ################################### result validation ################################# # Checks that the implemented codes here agree with the sklearn implementation TS = TheilSenRegressor().fit(X, y) y_pred_TS = TS.predict(X) np.testing.assert_almost_equal(slope_spatial_median, TS.coef_[0], decimal=2) np.testing.assert_almost_equal(intercept_spatial_median, TS.intercept_, decimal=2) ########################################## OLS ####################################### ols = LinearRegression().fit(X, y) y_pred_ols = ols.predict(X) ##################################### means ########################################## slope_mean = np.mean(slopes) intercept_mean = np.mean(intercepts) y_pred_mean = slope_mean * X + intercept_mean ###################################### plotting ###################################### fig, ax = plt.subplots(figsize=(8, 4.5)) ax.scatter(X, y) ax.plot(X, y_pred_mean, label='TS Mean : y = %.1fx + %.1f' % (slope_mean, intercept_mean)) ax.plot(X, y_pred_spatial_median, label='TS Median : y = %.1fx + %.1f' % (slope_spatial_median, intercept_spatial_median)) ax.plot(X, y_pred_ols, label='OLS : y = %.1fx + %.1f' % (ols.coef_[0], ols.intercept_), alpha=0.5) for slope, intercept in zip(slopes, intercepts): y_pred_sample = slope * X + intercept ax.plot(X, y_pred_sample, alpha=0.03, zorder=-99, color='silver') ax.plot(X, y_pred_sample, alpha=0.01, zorder=-99, color='lightgrey', label='Random sample fit, $k=2$') lg = ax.legend(loc='upper left', ncol=1) for i, lh in enumerate(lg.legendHandles): lh.set_alpha(1) ax.grid(axis='y', linestyle='--', color='#acacac', alpha=0.5) ax.spines['top'].set_visible(False) ax.spines['right'].set_visible(False) ax.set_ylim(-12, 72) ax.set_xlabel('X', fontsize=13) ax.set_ylabel('y', fontsize=13) ax.text(0.99, 0.1, 'aegis4048.github.io', fontsize=10, ha='right', va='center', transform=ax.transAxes, color='grey', alpha=0.5) ax.text(0.02, 0.62, 'True Solution: y = 92x + 30', fontsize=12, ha='left', va='center', transform=ax.transAxes, alpha=1, color='r') def setbold(txt): return ' '.join([r\"$\\bf{\" + item + \"}$\" for item in txt.split(' ')]) bold_txt = setbold('TheilSen Regression') plain_txt = r', robustness of median to the outliers' fig.suptitle(bold_txt + plain_txt, verticalalignment='top', x=0, horizontalalignment='left', fontsize=13, y=0.96) yloc = 0.88 ax.annotate('', xy=(0.01, yloc + 0.01), xycoords='figure fraction', xytext=(1.02, yloc + 0.01), arrowprops=dict(arrowstyle=\"-\", color='k', lw=0.7)) fig.tight_layout() 2.3.4. Theil-Sen code snippets For quick copy-paste, replace X and y with your own data. Make sure to reshape your X so that it is a 2D numpy.ndarray object with shape like (13, 1) . In [9]: import numpy as np from sklearn.linear_model import TheilSenRegressor from sklearn import linear_model import matplotlib.pyplot as plt # sample data X = np . array ([ 0.15 , - 0.34 , 0.32 , 0.43 , - 0.4 , - 0.04 , - 0.51 , 0.3 , 0.47 , 0.12 , 0.08 , 0.04 , - 0.08 , - 0.23 , 0.08 , - 0.03 , 0.03 , 0.04 , 0.01 , 0.06 , 0.03 , 0. , - 0.04 , - 0.18 , - 0.19 , - 0.06 , - 0.26 , - 0.16 , 0.13 , 0.09 , 0.03 , - 0.03 , 0.04 , 0.14 , - 0.01 , 0.4 , - 0.06 , 0.15 , 0.08 , 0.05 , - 0.15 , - 0.09 , - 0.15 , - 0.11 , - 0.07 , - 0.19 , - 0.06 , 0.17 , 0.23 , 0.18 ]) . reshape ( - 1 , 1 ) y = np . array ([ 17.44 , 25.46 , 18.61 , 26.07 , 24.96 , - 1.22 , 26.45 , 26.5 , 20.57 , 3.08 , 35.9 , 32.47 , 20.84 , 13.37 , 42.44 , 27.23 , 35.65 , 29.51 , 31.28 , 41.34 , 32.19 , 33.67 , 25.64 , 9.3 , 14.63 , 25.1 , 4.69 , 14.42 , 47.53 , 33.82 , 32.2 , 24.81 , 32.64 , 45.11 , 26.76 , 68.01 , 23.39 , 43.49 , 37.88 , 36.01 , 16.32 , 19.77 , 16.34 , 19.57 , 29.28 , 16.62 , 24.39 , 43.77 , 50.46 , 47.09 ]) # fit and predict: Huber TS = TheilSenRegressor ( n_subsamples = 2 ) . fit ( X , y ) # 2 by default for 2D linear regression. # Increasing it will decrease robustness (not recommended) y_pred_TS = TS . predict ( X ) # retrieve the fitted parameters coefs = TS . coef_ intercept = TS . intercept_ # TheilSen does not explicitly identify outliers from inliers. print ( \"TheilSen ----------------------------------- \\n \" ) print ( \"Coefficients :\" , coefs ) print ( \"Intercept :\" , intercept ) print ( \" \\n ------------------------------------------\" ) # fit and predict: Ordinary Least Squares ols = linear_model . LinearRegression () . fit ( X , y ) y_pred_ols = ols . predict ( X ) # plot plt . scatter ( X , y , s = 100 , c = 'green' , label = 'Data' ) plt . title ( 'TheilSen Regressor' , fontsize = 14 ) plt . plot ( X , y_pred_ols , label = 'OLS' ) plt . plot ( X , y_pred_TS , label = 'TheilSen' ) plt . legend (); TheilSen ----------------------------------- Coefficients : [73.65998569] Intercept : 29.378865864952733 ------------------------------------------ 2.4. Summary 1. RANSAC regressor Quick Scroll: Identifies and excludes outliers prior to fitting the final model. The outliers are defined as the points with residuals that exceed the Median Absolute Deviation (MAD). It repeats random sampling and fitting a test model until enough inliers are detected. 2. Huber regressor Quick Scroll: Detects outliers that exceed a certain threshold, and selectively apply different loss functions for outliers vs. inliers. 3. Theil-Sen regressor Quick Scroll: Fits OLS on all possible combinations of points (each of size 2 by default) and returns their spatial median slope and intercept as the final parameters. In the order of robustness: RANSAC > Theil-Sen > Huber. RANSAC shows the best robustness because it identifies and excludes outliers prior to fitting the final model, whereas the other two attempts to dampen the effect of outliers instead of excluding them. When in doubt, use RANSAC. However, when reporting a result obtained with RANSAC, make sure to fix the random seed RANSACRegressor(random_state=3) , as explained above . 3. Extension to 3D+ multivariate linear regressions All three of the robust models are applicable to 3D or more n-dimensional regressions. Below is the simple code snippet for multivariate linear regression with 2 features and an intercept. In [2]: from sklearn import datasets from sklearn.linear_model import LinearRegression , RANSACRegressor , HuberRegressor , TheilSenRegressor # generate sample data X , y , coef = datasets . make_regression ( n_samples = 50 , n_features = 2 , n_informative = 1 , noise = 10 , coef = True , random_state = 0 , ) # Ordinary Least Squares ols = linear_model . LinearRegression () . fit ( X , y ) y_pred_ols = ols . predict ( X ) # RANSAC ransac = RANSACRegressor ( random_state = 1 ) . fit ( X , y ) y_pred_ransac = ransac . predict ( X ) # Huber huber = HuberRegressor () . fit ( X , y ) y_pred_huber = huber . predict ( X ) # TheilSen TS = TheilSenRegressor () . fit ( X , y ) y_pred_TS = TS . predict ( X ) print ( \"OLS --------------------------------------\" ) print ( \"Coefficients :\" , np . round ( ols . coef_ , 2 )) print ( \"Intercept :\" , np . round ( ols . intercept_ , 2 )) print () print ( \"RANSAC -----------------------------------\" ) print ( \"Coefficients :\" , np . round ( ransac . estimator_ . coef_ , 2 )) print ( \"Intercept :\" , np . round ( ransac . estimator_ . intercept_ , 2 )) print () print ( \"Huber ------------------------------------\" ) print ( \"Coefficients :\" , np . round ( huber . coef_ , 2 )) print ( \"Intercept :\" , np . round ( huber . intercept_ , 2 )) print () print ( \"TheilSen ---------------------------------\" ) print ( \"Coefficients :\" , np . round ( TS . coef_ , 2 )) print ( \"Intercept :\" , np . round ( TS . intercept_ , 2 )) print ( \"------------------------------------------\" ) print () OLS -------------------------------------- Coefficients : [43.07 1.73] Intercept : 1.41 RANSAC ----------------------------------- Coefficients : [42.62 1.11] Intercept : 2.08 Huber ------------------------------------ Coefficients : [43.13 2.07] Intercept : 1.23 TheilSen --------------------------------- Coefficients : [42.15 2.95] Intercept : 0.71 ------------------------------------------ 3.1. Visual demonstrations ( Note that for models with more than 3D features, including intercept, they can't be visualized but the idea of robust regression still extends beyond 3D Cartesian space. ) For demonstrations, a 3D sample data set of 200 points is generated from the true model: $y = 200 x_{1} + 28x_{2} + 300$. Random Gaussian noise is added to the $y$ values to emulate the randomness and variability present in real-life data. Currently, the dataset is free of outliers. Fitting an OLS regression results in Figure 20 : Figure 20: This illustration presents the 3D linear model alongside the dataset within a 3D Cartesian space. When an OLS model is applied to this dataset, it closely approximates the true model parameters, particularly in scenarios devoid of outliers. The middle plot highlights how the data points are closely aligned with the 3D model fit (illustrated as a blue plane). Source Code For Figure (20) import pandas as pd import numpy as np import matplotlib.pyplot as plt from sklearn import linear_model from mpl_toolkits.mplot3d import Axes3D from sklearn.linear_model import (LinearRegression, HuberRegressor, RANSACRegressor, TheilSenRegressor) ###################################### Sample Data Generation ################################### np.random.seed(0) # for reproducibility num_samples = 200 # Porosity x1_min, x1_max = 5, 15 mean_x1 = (x1_min + x1_max) / 2 std_dev_x1 = (x1_max - x1_min) / 4 x1 = np.random.normal(mean_x1, std_dev_x1, num_samples) # Brittleness x2_min, x2_max = 20, 85 mean_x2 = (x2_min + x2_max) / 2 std_dev_x2 = (x2_max - x2_min) / 4 x2 = np.random.normal(mean_x2, std_dev_x2, num_samples) # Reshape X for comptibility with regression models X = np.vstack((x1, x2)).T # True model Y = 200* x1 + 28 * x2 + 300 # Add Gaussian noise to Y noise = np.random.normal(0, 200, num_samples) # Mean = 0, Standard deviation = 50 Y = Y + noise ######################## Prepare model data point for visualization ############################### x = X[:, 0] y = X[:, 1] z = Y x_pred = np.linspace(0, 25, 30) # range of porosity values y_pred = np.linspace(0, 100, 30) # range of brittleness values xx_pred, yy_pred = np.meshgrid(x_pred, y_pred) model_viz = np.array([xx_pred.flatten(), yy_pred.flatten()]).T ################################################ Train ############################################# model = LinearRegression() model.fit(X, Y) predicted = model.predict(model_viz) ############################################## Plot ################################################ plt.style.use('default') fig = plt.figure(figsize=(12, 4)) ax1 = fig.add_subplot(131, projection='3d') ax2 = fig.add_subplot(132, projection='3d') ax3 = fig.add_subplot(133, projection='3d') axes = [ax1, ax2, ax3] for ax in axes: ax.plot(x, y, z, color='k', zorder=15, linestyle='none', marker='o', alpha=0.5) ax.scatter(xx_pred.flatten(), yy_pred.flatten(), predicted, facecolor=(0,0,0,0), s=20, edgecolor='#70b3f0') ax.set_xlabel(r'$X_{1}$', fontsize=12) ax.set_ylabel(r'$X_{2}$', fontsize=12) ax.set_zlabel('Y', fontsize=12) ax.locator_params(nbins=4, axis='x') ax.locator_params(nbins=5, axis='x') ax.set_xlim(0, 25) ax.set_ylim(100, 0) ax.set_zlim(0, 8000) ax1.text2D(0.2, 0.32, 'aegis4048.github.io', fontsize=13, ha='center', va='center', transform=ax1.transAxes, color='grey', alpha=0.5) ax2.text2D(0.5, 0.32, 'aegis4048.github.io', fontsize=13, ha='center', va='center', transform=ax2.transAxes, color='grey', alpha=0.5) ax3.text2D(0.85, 0.85, 'aegis4048.github.io', fontsize=13, ha='center', va='center', transform=ax3.transAxes, color='grey', alpha=0.5) ax1.view_init(elev=30, azim=50) ax2.view_init(elev=2, azim=60) ax3.view_init(elev=60, azim=165) fig.tight_layout() def setbold(txt): return ' '.join([r\"$\\bf{\" + item + \"}$\" for item in txt.split(' ')]) bold_txt = setbold(' OLS 3D Linear Regression (without outliers), ') plain_txt = r'fitted model: $y = $ %.1f$x_{1}$ + %.1f$x_{2}$ + %.1f' % (model.coef_[0], model.coef_[1], model.intercept_) fig.suptitle(bold_txt + plain_txt, verticalalignment='top', x=0, horizontalalignment='left', fontsize=16, y=1.03) yloc = 0.95 ax.annotate('', xy=(0.01, yloc + 0.01), xycoords='figure fraction', xytext=(1.02, yloc + 0.01), arrowprops=dict(arrowstyle=\"-\", color='k', lw=0.7)) fig.text(0.48, 0.8, 'true model: $y = $ 200$x_{1}$ + 28$x_{2}$ + 300', ha='center', va='center', fontsize=16, color='grey'); Now we will introduce 20 outlier points, constituting 10% of the original dataset, and reapply the OLS regression, as depicted in Figure 21 . It is noticeable that the OLS parameters fitted in this scenario exhibit a more significant deviation from the population parameters compared to the initial fit that lacked outliers. Figure 21: This figure highlights outlier points in red. The introduction of these outliers has caused a noticeable shift in the fitted blue plane, which is now skewed towards these outliers, affecting the accuracy of the model fit. The left plot distinctly shows the misalignment of the model plane with the main dataset. For a comparative analysis, refer to the visual and numerical parameters in Figure 20 . Source Code For Figure (21) import pandas as pd import numpy as np import matplotlib.pyplot as plt from sklearn import linear_model from mpl_toolkits.mplot3d import Axes3D from sklearn.linear_model import LinearRegression ###################################### Sample Data Generation ################################### np.random.seed(0) # for reproducibility num_samples = 200 # Porosity x1_min, x1_max = 5, 15 mean_x1 = (x1_min + x1_max) / 2 std_dev_x1 = (x1_max - x1_min) / 4 x1 = np.random.normal(mean_x1, std_dev_x1, num_samples) # Brittleness x2_min, x2_max = 20, 85 mean_x2 = (x2_min + x2_max) / 2 std_dev_x2 = (x2_max - x2_min) / 4 x2 = np.random.normal(mean_x2, std_dev_x2, num_samples) # Reshape X for comptibility with regression models X = np.vstack((x1, x2)).T # True model Y = 200* x1 + 28 * x2 + 300 # Add Gaussian noise to Y noise = np.random.normal(0, 200, num_samples) # Mean = 0, Standard deviation = 50 Y_noisy = Y + noise # Add outliers num_outliers = int(0.1 * num_samples) # define 5% of data to be outliers outlier_x1 = np.random.uniform(10, 12, num_outliers) # outlier between range 10 ~ 12 outlier_x2 = np.random.uniform(0, 5, num_outliers) # outlier between range 0 ~ 5 outlier_Y = np.random.uniform(6500, 7000, num_outliers) # outlier between range 6500 ~ 7000 X_outliers = np.vstack((outlier_x1, outlier_x2)).T # Append outliers to the original data X = np.vstack((X, X_outliers)) Y = np.append(Y_noisy, outlier_Y) ######################## Prepare model data point for visualization ############################### x = X[:, 0] y = X[:, 1] z = Y x_pred = np.linspace(0, 25, 30) # range of porosity values y_pred = np.linspace(0, 100, 30) # range of brittleness values xx_pred, yy_pred = np.meshgrid(x_pred, y_pred) model_viz = np.array([xx_pred.flatten(), yy_pred.flatten()]).T ################################################ Train ############################################# model = LinearRegression() model.fit(X, Y) predicted = model.predict(model_viz) ############################################## Plot ################################################ plt.style.use('default') fig = plt.figure(figsize=(12, 4)) ax1 = fig.add_subplot(131, projection='3d') ax2 = fig.add_subplot(132, projection='3d') ax3 = fig.add_subplot(133, projection='3d') axes = [ax1, ax2, ax3] for ax in axes: ax.plot(x, y, z, color='k', zorder=15, linestyle='none', marker='o', alpha=0.5) ax.plot(X_outliers[:, 0], X_outliers[:, 1], outlier_Y, color='r', zorder=15, linestyle='none', marker='x', alpha=1, markersize=10) ax.scatter(xx_pred.flatten(), yy_pred.flatten(), predicted, facecolor=(0,0,0,0), s=20, edgecolor='#70b3f0') ax.set_xlabel(r'$X_{1}$', fontsize=12) ax.set_ylabel(r'$X_{2}$', fontsize=12) ax.set_zlabel('Y', fontsize=12) ax.locator_params(nbins=4, axis='x') ax.locator_params(nbins=5, axis='x') ax.set_xlim(0, 25) ax.set_ylim(100, 0) ax.set_zlim(0, 8000) ax1.text2D(0.2, 0.32, 'aegis4048.github.io', fontsize=13, ha='center', va='center', transform=ax1.transAxes, color='grey', alpha=0.5) ax2.text2D(0.5, 0.32, 'aegis4048.github.io', fontsize=13, ha='center', va='center', transform=ax2.transAxes, color='grey', alpha=0.5) ax3.text2D(0.85, 0.85, 'aegis4048.github.io', fontsize=13, ha='center', va='center', transform=ax3.transAxes, color='grey', alpha=0.5) ax1.view_init(elev=30, azim=50) ax2.view_init(elev=2, azim=60) ax3.view_init(elev=60, azim=165) fig.tight_layout() def setbold(txt): return ' '.join([r\"$\\bf{\" + item + \"}$\" for item in txt.split(' ')]) bold_txt = setbold(' OLS 3D Linear Regression (with outliers), ') plain_txt = r'fitted model: $y = $ %.1f$x_{1}$ + %.1f$x_{2}$ + %.1f' % (model.coef_[0], model.coef_[1], model.intercept_) fig.suptitle(bold_txt + plain_txt, verticalalignment='top', x=0, horizontalalignment='left', fontsize=16, y=1.03) yloc = 0.95 ax.annotate('', xy=(0.01, yloc + 0.01), xycoords='figure fraction', xytext=(1.02, yloc + 0.01), arrowprops=dict(arrowstyle=\"-\", color='k', lw=0.7)) fig.text(0.48, 0.8, 'true model: $y = $ 200$x_{1}$ + 28$x_{2}$ + 300', ha='center', va='center', fontsize=16, color='grey'); The below three figures ( Figure 22 , Figure 23 and Figure 24 ) presents 3D model fits and visualizations for the three robust models: RANSAC, Huber, and TheilSen. Figure 22: RANSAC robust linear regression on 3D dataset with outliers. Source Code For Figure (22) import pandas as pd import numpy as np import matplotlib.pyplot as plt from sklearn import linear_model from mpl_toolkits.mplot3d import Axes3D from sklearn.linear_model import RANSACRegressor ###################################### Sample Data Generation ################################### np.random.seed(0) # for reproducibility num_samples = 200 # Porosity x1_min, x1_max = 5, 15 mean_x1 = (x1_min + x1_max) / 2 std_dev_x1 = (x1_max - x1_min) / 4 x1 = np.random.normal(mean_x1, std_dev_x1, num_samples) # Brittleness x2_min, x2_max = 20, 85 mean_x2 = (x2_min + x2_max) / 2 std_dev_x2 = (x2_max - x2_min) / 4 x2 = np.random.normal(mean_x2, std_dev_x2, num_samples) # Reshape X for comptibility with regression models X = np.vstack((x1, x2)).T # True model Y = 200* x1 + 28 * x2 + 300 # Add Gaussian noise to Y noise = np.random.normal(0, 200, num_samples) # Mean = 0, Standard deviation = 50 Y_noisy = Y + noise # Add outliers num_outliers = int(0.1 * num_samples) # define 5% of data to be outliers outlier_x1 = np.random.uniform(10, 12, num_outliers) # outlier between range 10 ~ 12 outlier_x2 = np.random.uniform(0, 5, num_outliers) # outlier between range 0 ~ 5 outlier_Y = np.random.uniform(6500, 7000, num_outliers) # outlier between range 6500 ~ 7000 X_outliers = np.vstack((outlier_x1, outlier_x2)).T # Append outliers to the original data X = np.vstack((X, X_outliers)) Y = np.append(Y_noisy, outlier_Y) ######################## Prepare model data point for visualization ############################### x = X[:, 0] y = X[:, 1] z = Y x_pred = np.linspace(0, 25, 30) # range of porosity values y_pred = np.linspace(0, 100, 30) # range of brittleness values xx_pred, yy_pred = np.meshgrid(x_pred, y_pred) model_viz = np.array([xx_pred.flatten(), yy_pred.flatten()]).T ################################################ Train ############################################# model = RANSACRegressor() model.fit(X, Y) predicted = model.predict(model_viz) ############################################## Plot ################################################ plt.style.use('default') fig = plt.figure(figsize=(12, 4)) ax1 = fig.add_subplot(131, projection='3d') ax2 = fig.add_subplot(132, projection='3d') ax3 = fig.add_subplot(133, projection='3d') axes = [ax1, ax2, ax3] for ax in axes: ax.plot(x, y, z, color='k', zorder=15, linestyle='none', marker='o', alpha=0.5) ax.plot(X_outliers[:, 0], X_outliers[:, 1], outlier_Y, color='r', zorder=15, linestyle='none', marker='x', alpha=1, markersize=10) ax.scatter(xx_pred.flatten(), yy_pred.flatten(), predicted, facecolor=(0,0,0,0), s=20, edgecolor='#70b3f0') ax.set_xlabel(r'$X_{1}$', fontsize=12) ax.set_ylabel(r'$X_{2}$', fontsize=12) ax.set_zlabel('Y', fontsize=12) ax.locator_params(nbins=4, axis='x') ax.locator_params(nbins=5, axis='x') ax.set_xlim(0, 25) ax.set_ylim(100, 0) ax.set_zlim(0, 8000) ax1.text2D(0.2, 0.32, 'aegis4048.github.io', fontsize=13, ha='center', va='center', transform=ax1.transAxes, color='grey', alpha=0.5) ax2.text2D(0.5, 0.32, 'aegis4048.github.io', fontsize=13, ha='center', va='center', transform=ax2.transAxes, color='grey', alpha=0.5) ax3.text2D(0.85, 0.85, 'aegis4048.github.io', fontsize=13, ha='center', va='center', transform=ax3.transAxes, color='grey', alpha=0.5) ax1.view_init(elev=30, azim=50) ax2.view_init(elev=2, azim=60) ax3.view_init(elev=60, azim=165) fig.tight_layout() def setbold(txt): return ' '.join([r\"$\\bf{\" + item + \"}$\" for item in txt.split(' ')]) bold_txt = setbold(' RANSAC 3D Linear Regression, ') plain_txt = r'fitted model: $y = $ %.1f$x_{1}$ + %.1f$x_{2}$ + %.1f' % ( model.estimator_.coef_[0], model.estimator_.coef_[1], model.estimator_.intercept_ ) fig.suptitle(bold_txt + plain_txt, verticalalignment='top', x=0, horizontalalignment='left', fontsize=16, y=1.03) yloc = 0.95 ax.annotate('', xy=(0.01, yloc + 0.01), xycoords='figure fraction', xytext=(1.02, yloc + 0.01), arrowprops=dict(arrowstyle=\"-\", color='k', lw=0.7)) fig.text(0.48, 0.8, 'true model: $y = $ 200$x_{1}$ + 28$x_{2}$ + 300', ha='center', va='center', fontsize=16, color='grey'); Figure 23: Huber robust linear regression on 3D dataset with outliers. Source Code For Figure (23) import pandas as pd import numpy as np import matplotlib.pyplot as plt from sklearn import linear_model from mpl_toolkits.mplot3d import Axes3D from sklearn.linear_model import HuberRegressor ###################################### Sample Data Generation ################################### np.random.seed(0) # for reproducibility num_samples = 200 # Porosity x1_min, x1_max = 5, 15 mean_x1 = (x1_min + x1_max) / 2 std_dev_x1 = (x1_max - x1_min) / 4 x1 = np.random.normal(mean_x1, std_dev_x1, num_samples) # Brittleness x2_min, x2_max = 20, 85 mean_x2 = (x2_min + x2_max) / 2 std_dev_x2 = (x2_max - x2_min) / 4 x2 = np.random.normal(mean_x2, std_dev_x2, num_samples) # Reshape X for comptibility with regression models X = np.vstack((x1, x2)).T # True model Y = 200* x1 + 28 * x2 + 300 # Add Gaussian noise to Y noise = np.random.normal(0, 200, num_samples) # Mean = 0, Standard deviation = 50 Y_noisy = Y + noise # Add outliers num_outliers = int(0.1 * num_samples) # define 5% of data to be outliers outlier_x1 = np.random.uniform(10, 12, num_outliers) # outlier between range 10 ~ 12 outlier_x2 = np.random.uniform(0, 5, num_outliers) # outlier between range 0 ~ 5 outlier_Y = np.random.uniform(6500, 7000, num_outliers) # outlier between range 6500 ~ 7000 X_outliers = np.vstack((outlier_x1, outlier_x2)).T # Append outliers to the original data X = np.vstack((X, X_outliers)) Y = np.append(Y_noisy, outlier_Y) ######################## Prepare model data point for visualization ############################### x = X[:, 0] y = X[:, 1] z = Y x_pred = np.linspace(0, 25, 30) # range of porosity values y_pred = np.linspace(0, 100, 30) # range of brittleness values xx_pred, yy_pred = np.meshgrid(x_pred, y_pred) model_viz = np.array([xx_pred.flatten(), yy_pred.flatten()]).T ################################################ Train ############################################# model = HuberRegressor() model.fit(X, Y) predicted = model.predict(model_viz) ############################################## Plot ################################################ plt.style.use('default') fig = plt.figure(figsize=(12, 4)) ax1 = fig.add_subplot(131, projection='3d') ax2 = fig.add_subplot(132, projection='3d') ax3 = fig.add_subplot(133, projection='3d') axes = [ax1, ax2, ax3] for ax in axes: ax.plot(x, y, z, color='k', zorder=15, linestyle='none', marker='o', alpha=0.5) ax.plot(X_outliers[:, 0], X_outliers[:, 1], outlier_Y, color='r', zorder=15, linestyle='none', marker='x', alpha=1, markersize=10) ax.scatter(xx_pred.flatten(), yy_pred.flatten(), predicted, facecolor=(0,0,0,0), s=20, edgecolor='#70b3f0') ax.set_xlabel(r'$X_{1}$', fontsize=12) ax.set_ylabel(r'$X_{2}$', fontsize=12) ax.set_zlabel('Y', fontsize=12) ax.locator_params(nbins=4, axis='x') ax.locator_params(nbins=5, axis='x') ax.set_xlim(0, 25) ax.set_ylim(100, 0) ax.set_zlim(0, 8000) ax1.text2D(0.2, 0.32, 'aegis4048.github.io', fontsize=13, ha='center', va='center', transform=ax1.transAxes, color='grey', alpha=0.5) ax2.text2D(0.5, 0.32, 'aegis4048.github.io', fontsize=13, ha='center', va='center', transform=ax2.transAxes, color='grey', alpha=0.5) ax3.text2D(0.85, 0.85, 'aegis4048.github.io', fontsize=13, ha='center', va='center', transform=ax3.transAxes, color='grey', alpha=0.5) ax1.view_init(elev=30, azim=50) ax2.view_init(elev=2, azim=60) ax3.view_init(elev=60, azim=165) fig.tight_layout() def setbold(txt): return ' '.join([r\"$\\bf{\" + item + \"}$\" for item in txt.split(' ')]) bold_txt = setbold(' Huber 3D Linear Regression, ') plain_txt = r'fitted model: $y = $ %.1f$x_{1}$ + %.1f$x_{2}$ + %.1f' % (model.coef_[0], model.coef_[1], model.intercept_) fig.suptitle(bold_txt + plain_txt, verticalalignment='top', x=0, horizontalalignment='left', fontsize=16, y=1.03) yloc = 0.95 ax.annotate('', xy=(0.01, yloc + 0.01), xycoords='figure fraction', xytext=(1.02, yloc + 0.01), arrowprops=dict(arrowstyle=\"-\", color='k', lw=0.7)) fig.text(0.48, 0.8, 'true model: $y = $ 200$x_{1}$ + 28$x_{2}$ + 300', ha='center', va='center', fontsize=16, color='grey'); Figure 24: TheilSen robust linear regression on 3D dataset with outliers. Source Code For Figure (24) import pandas as pd import numpy as np import matplotlib.pyplot as plt from sklearn import linear_model from mpl_toolkits.mplot3d import Axes3D from sklearn.linear_model import TheilSenRegressor ###################################### Sample Data Generation ################################### np.random.seed(0) # for reproducibility num_samples = 200 # Porosity x1_min, x1_max = 5, 15 mean_x1 = (x1_min + x1_max) / 2 std_dev_x1 = (x1_max - x1_min) / 4 x1 = np.random.normal(mean_x1, std_dev_x1, num_samples) # Brittleness x2_min, x2_max = 20, 85 mean_x2 = (x2_min + x2_max) / 2 std_dev_x2 = (x2_max - x2_min) / 4 x2 = np.random.normal(mean_x2, std_dev_x2, num_samples) # Reshape X for comptibility with regression models X = np.vstack((x1, x2)).T # True model Y = 200* x1 + 28 * x2 + 300 # Add Gaussian noise to Y noise = np.random.normal(0, 200, num_samples) # Mean = 0, Standard deviation = 50 Y_noisy = Y + noise # Add outliers num_outliers = int(0.1 * num_samples) # define 5% of data to be outliers outlier_x1 = np.random.uniform(10, 12, num_outliers) # outlier between range 10 ~ 12 outlier_x2 = np.random.uniform(0, 5, num_outliers) # outlier between range 0 ~ 5 outlier_Y = np.random.uniform(6500, 7000, num_outliers) # outlier between range 6500 ~ 7000 X_outliers = np.vstack((outlier_x1, outlier_x2)).T # Append outliers to the original data X = np.vstack((X, X_outliers)) Y = np.append(Y_noisy, outlier_Y) ######################## Prepare model data point for visualization ############################### x = X[:, 0] y = X[:, 1] z = Y x_pred = np.linspace(0, 25, 30) # range of porosity values y_pred = np.linspace(0, 100, 30) # range of brittleness values xx_pred, yy_pred = np.meshgrid(x_pred, y_pred) model_viz = np.array([xx_pred.flatten(), yy_pred.flatten()]).T ################################################ Train ############################################# model = TheilSenRegressor() model.fit(X, Y) predicted = model.predict(model_viz) ############################################## Plot ################################################ plt.style.use('default') fig = plt.figure(figsize=(12, 4)) ax1 = fig.add_subplot(131, projection='3d') ax2 = fig.add_subplot(132, projection='3d') ax3 = fig.add_subplot(133, projection='3d') axes = [ax1, ax2, ax3] for ax in axes: ax.plot(x, y, z, color='k', zorder=15, linestyle='none', marker='o', alpha=0.5) ax.plot(X_outliers[:, 0], X_outliers[:, 1], outlier_Y, color='r', zorder=15, linestyle='none', marker='x', alpha=1, markersize=10) ax.scatter(xx_pred.flatten(), yy_pred.flatten(), predicted, facecolor=(0,0,0,0), s=20, edgecolor='#70b3f0') ax.set_xlabel(r'$X_{1}$', fontsize=12) ax.set_ylabel(r'$X_{2}$', fontsize=12) ax.set_zlabel('Y', fontsize=12) ax.locator_params(nbins=4, axis='x') ax.locator_params(nbins=5, axis='x') ax.set_xlim(0, 25) ax.set_ylim(100, 0) ax.set_zlim(0, 8000) ax1.text2D(0.2, 0.32, 'aegis4048.github.io', fontsize=13, ha='center', va='center', transform=ax1.transAxes, color='grey', alpha=0.5) ax2.text2D(0.5, 0.32, 'aegis4048.github.io', fontsize=13, ha='center', va='center', transform=ax2.transAxes, color='grey', alpha=0.5) ax3.text2D(0.85, 0.85, 'aegis4048.github.io', fontsize=13, ha='center', va='center', transform=ax3.transAxes, color='grey', alpha=0.5) ax1.view_init(elev=30, azim=50) ax2.view_init(elev=2, azim=60) ax3.view_init(elev=60, azim=165) fig.tight_layout() def setbold(txt): return ' '.join([r\"$\\bf{\" + item + \"}$\" for item in txt.split(' ')]) bold_txt = setbold(' TheilSen 3D Linear Regression, ') plain_txt = r'fitted model: $y = $ %.1f$x_{1}$ + %.1f$x_{2}$ + %.1f' % (model.coef_[0], model.coef_[1], model.intercept_) fig.suptitle(bold_txt + plain_txt, verticalalignment='top', x=0, horizontalalignment='left', fontsize=16, y=1.03) yloc = 0.95 ax.annotate('', xy=(0.01, yloc + 0.01), xycoords='figure fraction', xytext=(1.02, yloc + 0.01), arrowprops=dict(arrowstyle=\"-\", color='k', lw=0.7)) fig.text(0.48, 0.8, 'true model: $y = $ 200$x_{1}$ + 28$x_{2}$ + 300', ha='center', va='center', fontsize=16, color='grey');","tags":"Machine Learning","url":"https://aegis4048.github.io/robust_linear_regressions_in_python","loc":"https://aegis4048.github.io/robust_linear_regressions_in_python"},{"title":"Quantifying The Impact Of Completion Delay Since Drilled On Production","text":"Once a well is drilled, it must be completed to produce oil and gas. However, operators sometimes delay completing a well for various reasons, such as waiting for fracking crews or facing budget constraints. These delayed wells, known as Drilled but Uncompleted (DUC) wells, are considered to be a working oil field capital because the wells can be turned on line for production for cheap. However, the empirical evidences suggests that the DUC wells may no longer function as a working capital if completion is delayed for too long. The probability of completion falls drastically after two years of delay in completion, and DUC wells that still remain un-completed beyond this time frame are likely to be never completed. The study conducted in this article is built on top of the previous scientific findings which claim that the nearby parent wells can have a negative impact on the production of child wells, which worsens the longer the parent wells have been producing. This is particularly critical for DUC wells, as the wells awaiting completion are already exposed to these pressure variations from existing wells that continue to produce and deplete. This study aims to quantify the impact of increasing delays in completion on production by observing changes in estimated ultimate recovery (EUR) normalized by lateral length and completion concentration per perforated foot as a function of DUC time. Contents 0. Sample data description 1. Key takeaways 2. Quick overview on DUC wells 3. Reservoir depletion due to existing offset wells 4. Quantifying the impact of DUC time on EUR 4.1. Preliminary data cleaning 4.2. Raw EUR vs. DUC time 4.3. Factors that affect EUR: Completion size and lateral length 4.4. Normalized EUR vs. DUC time 5. \"Dead\" DUC wells 6. Conclusion 0. Sample data description The main data source used for this article is the Enverus Drilling Info application. Within Drilling Info's data base, I selected wells in the seven major basins in the US, defined by the US Energy Information Adminitration's (EIA) Drilling Productivity Report (DPR): Anadarko, Appalachia, Bakken, Eagle Ford, Haynesville, Niobrara, and Permian. Figure 1: 7 major basins in the US, Source: EIA The DPR provides which county and state they used to define each major basins. For your convenience, I compiled a text file ( Basins_Boundaries.txt ) that has the list of state and counties that can be easily pasted into Drilling Info's filter tab. Unfortunately Enverus Drilling Info is a commerical software and not everyone will be able to use it. Figure 2: Pasting the Eagle Ford basin's state and county data into Drilling Info for query Pulling all active wells in the specified counties and states gives this Excel file (EUR_vs_Lag.xlsx). The file has 7 tabs for each basin. The below code output shows small portion of the Anadarko wells data. Each row represents a record of one well. In [3]: import pandas as pd pd . read_excel ( 'https://aegis4048.github.io/downloads/notebooks/sample_data/EUR_vs_Lag.xlsx' , sheet_name = 'Anadarko' ) . head ( 5 ) Out[3]: Spud Date Completion Date Drill Type Lag EUR Oil (Full) EUR Gas (Full) EUR (Full) BOE First 12 BOE Production Type DI Lateral Length Total Proppant (First Treatment Job) Total Fluid (First Treatment Job) Proppant per Perforated Foot (First Treatment Job) Fluid per Perforated Foot (First Treatment Job) Proppant Concentration (lbs/gal) (First Treatment Job) 0 2020-01-09 2021-04-09 H 456.0 672317.75 0.00 672317.750000 NaN OIL 10776.0 NaN NaN NaN NaN NaN 1 2019-05-03 2019-07-26 H 84.0 129760.25 2348886.00 521241.250000 141195.17 GAS 5739.0 6137496.0 170614.00 1069.4365 29.73 0.86 2 2018-06-20 2018-07-28 H 38.0 89633.48 472189.47 168331.725000 61155.83 GAS 4338.0 4304900.0 100874.00 992.3698 23.25 1.02 3 2018-05-21 2018-07-07 H 47.0 158308.72 2718710.75 611427.178333 222754.17 GAS 4859.0 5627071.0 194148.98 1158.0718 39.96 0.69 4 2018-04-09 2018-07-06 H 88.0 153994.27 3887145.75 801851.895000 268463.00 GAS 4845.0 8990772.0 292805.38 1855.6805 60.43 0.73 1. Key takeaways 1. Characteristics of DUC wells DUC wells can be turned on line for cheap since the cost of drilling have been paid. This allows them to function as a secondary form of hydrocarbon storage under the ground, instead of traditional above-ground facilities. 2. Delaying completion during times of low oil prices in the hope of a future recovery may not be a financially wise decision This is because nearby wells can induce reservoir depletion, which can significantly reduce the potential yield. Operators should carefully consider the long-term financial implications of delaying completion and weigh them against the potential benefits of waiting for better market conditions. 3. Scientific wisdom states that the greater difference in completion timing among adjacent wells worsens child well's production The prolonged production from nearby parent wells can lead to reservoir depletion and pressure sinks, which can affect the productivity of the child well. This issue is particularly important for DUC wells because as existing wells continue to produce and deplete, the wells awaiting completion are already exposed to these pressure variations, which can worsen if completion is further delayed (Srinivasan et al. 2018) . 4. Contrary to the scientific wisdom, wells completed shortly after drilling tend to have smaller EUR compared to wells with longer DUC time This is due to the fact that younger DUC wells tend to have shorter lateral length and treated with smaller completion concentration for each lateral foot. This means that the operators tend to undersize completion design for their younger DUC wells, resulting in sub-optimal EUR. 5. To discern the genuine influence of DUC time on production, EUR is normalized by lateral length, proppant per perforated foot, and fluid per perforated foot This decision is made after visually confirming positive correlation between EUR vs. lateral length and completion size in Figure 7 . 6. Anadarko and Permian basins showed the worst reduction in normalized EUR in the shortest time span between drilling and completion Normalized EUR dropped by 81% and 74%, respectively, for Anadarko and Permian basins over a span of 150 days of additional delay in completion ( Figure 8 ), followed by the Haynesville basin which showed 46% decrease in normalized EUR over a span of additional 100 days of delay in completion. 7. Appalachia, Eagle Ford and Niobrara basins showed minimal loss in normalized EUR with increasing DUC time Appalachia experienced only 19% reduction in normalized EUR with 350 days of delay in completion, and Eagle Ford and Niobrara basins showed negligible change in normalized EUR with increasing DUC time. This finding shows that wells in these basins are the least likely to suffer from well-to-well interference problems with increasing DUC time compared to the other basins. 8. DUC wells have time limit till completion. The probability of completion falls drastically after a certain time limit due to the degrading economics associated with delays in completion. For example in Figure 16 , the completion time limits for Anadarko and Hanyesville are 308 days and 409 days after drilled, respectively. Beyond these limits, the probability of completion in these basins drops below 5%. This study demonstrates a positive correlation between the extent of these time limits and changes in both normalized EUR and absolute EUR as DUC time increases. 2. Quick overview on DUC wells Once a well is drilled, it must be completed to produce oil and gas. However, operators sometimes delay completing a well for various reasons, such as waiting for fracking crews or facing budget constraints. These delayed wells, known as Drilled but Uncompleted (DUC) wells, accumulate when drilling outpaces completion and decrease when the opposite occurs. Figure 3 illustrates the historical trend of DUC inventories in the seven major US basins, as defined by the EIA's DPR page. Figure 4 provides more detailed information on each basin, including hydrocarbon production volumes and drilling activities. DUC wells have unique features that make them an attractive option for low-cost production. The wells can be turned on line for production for cheap; the costs of drilling have been paid, and completion won't be as costly as new drilling. They can function as a secondary form of hydrocarbon storage underground, unlike traditional above-ground facilities. In times of low oil prices, operators may choose to complete them later, hoping that the oil price would recover in a near future. Throughout the rest of the article, however, I show why this may not be a smart idea for some basins (especially Anadarko and Permian) due to reservoir depletion induced by existing producing offset wells. Figure 3: DUCs accumulate when drilling activity exceeds completion, and vice versa. The line plots depict three distinct periods of significant fluctuations in DUC well inventories, with [1] and [3] representing instances of rapid depletion due to high completion rates, and [2] reflecting a period of accumulation resulting from high drilling activities. WTI Crude oil prices is appended at the bottom to show the effect of commodity prices on DUCs. Source Code For Figure (3) import pandas as pd import matplotlib.pyplot as plt import numpy as np import matplotlib.ticker as ticker ############################################# Basins ############################################## basins = sorted(['Anadarko', 'Appalachia', 'Eagle Ford', 'Haynesville', 'Niobrara', 'Permian', 'Bakken']) ################################### DUC Wells Count per Basin ##################################### # compile a DataFrame to store count of DUC wells per basin # source: EIA - https://www.eia.gov/petroleum/drilling/ dfs = [] for basin in basins: df_cur = pd.read_excel('https://aegis4048.github.io/downloads/notebooks/sample_data/Duc-EIA.xlsx', sheet_name=basin) df_cur['Basin'] = basin dfs.append(df_cur) df = pd.concat(dfs) df = df.sort_values('Date') df.index = pd.to_datetime(df['Date']) df_DUCs = df.groupby([pd.Grouper(freq='M'), 'Basin'])['DUC'].mean() df_DUCs = df_DUCs.unstack().fillna(0) df_DUCs = round(df_DUCs, 0) df_DUCs['Total DUCs'] = df_DUCs.T.apply(lambda x: sum(x)) df_drilled = df.groupby([pd.Grouper(freq='M'), 'Basin'])['Drilled'].sum() df_drilled = df_drilled.unstack().fillna(0) df_drilled['Total Drilled'] = df_drilled.T.apply(lambda x: sum(x)) df_completed = df.groupby([pd.Grouper(freq='M'), 'Basin'])['Completed'].sum() df_completed = df_completed.unstack().fillna(0) df_completed['Total Completed'] = df_completed.T.apply(lambda x: sum(x)) ############################################# Plot ############################################## fig, ax = plt.subplots(figsize=(8, 4)) ax2 = ax.twinx() ax.plot(df_drilled.index, df_drilled['Total Drilled'], color='k', label='Newly Drilled') ax.plot(df_completed.index, df_completed['Total Completed'], label='Completed') ax2.plot(df_DUCs.index, df_DUCs['Total DUCs'], color='purple', label='DUC') ax.fill_between(df_drilled.index, df_drilled['Total Drilled'], df_completed['Total Completed'], where=(df_drilled.index >= '2016-03-1') & (df_drilled.index <= '2017-1-1'), color='red', alpha=0.4) ax.fill_between(df_drilled.index, df_drilled['Total Drilled'], df_completed['Total Completed'], where=(df_drilled.index >= '2018-08-01') & (df_drilled.index <= '2019-3-1'), color='green', alpha=0.4) ax.fill_between(df_drilled.index, df_drilled['Total Drilled'], df_completed['Total Completed'], where=(df_drilled.index >= '2020-07-1') & (df_drilled.index <= '2022-8-1'), color='red', alpha=0.4) ax.axvline(x=df_drilled.index[26], color='k', linestyle='--', linewidth=1, alpha=0.7) ax.axvline(x=df_drilled.index[35], color='k', linestyle='--', linewidth=1, alpha=0.7) ax.axvspan(df_drilled.index[26], df_drilled.index[35], facecolor='lightgrey', alpha=0.3) ax.axvline(x=df_drilled.index[55], color='k', linestyle='--', linewidth=1, alpha=0.7) ax.axvline(x=df_drilled.index[61], color='k', linestyle='--', linewidth=1, alpha=0.7) ax.axvspan(df_drilled.index[55], df_drilled.index[61], facecolor='lightgrey', alpha=0.3) ax.axvline(x=df_drilled.index[78], color='k', linestyle='--', linewidth=1, alpha=0.7) ax.axvline(x=df_drilled.index[102], color='k', linestyle='--', linewidth=1, alpha=0.7) ax.axvspan(df_drilled.index[78], df_drilled.index[102], facecolor='lightgrey', alpha=0.3) ax.set_ylim(0, 2500) ax2.set_ylim(0, 10000) ax.grid(axis='y', alpha=0.5) ax.yaxis.get_major_ticks()[5].gridline.set_visible(False) ax.spines.top.set_visible(False) ax2.spines.top.set_visible(False) h1, l1 = ax.get_legend_handles_labels() h2, l2 = ax2.get_legend_handles_labels() ax.legend(h1 + h2, l1 + l2, fontsize=10, ncol=3, loc='upper left', framealpha=1) ax.set_ylabel('Drilled & Completed Wells', fontsize=11) ax2.set_ylabel('DUC Wells', fontsize=11) ax.arrow(df_drilled.index[71], 1500, 60, -270, head_width=40, head_length=60, fc='k', ec='k') ax.text(0.58, 0.62, 'Covid Crash', fontsize=9, transform=ax.transAxes, color='k') ax.text(0.295, 0.05, '[1]', fontsize=9, transform=ax.transAxes, color='k') ax.text(0.53, 0.05, '[2]', fontsize=9, transform=ax.transAxes, color='k') ax.text(0.805, 0.05, '[3]', fontsize=9, transform=ax.transAxes, color='k') def setbold(txt): return ' '.join([r\"$\\bf{\" + item + \"}$\" for item in txt.split(' ')]) ax.set_title(setbold('US Major Basins Completion & Drilling Activities') + \", Jan 2014 - Dec 2022\", fontsize=12, pad=10, x=0.4, y=1.06) ax.annotate('', xy=(-0.11, 1.07), xycoords='axes fraction', xytext=(1.11, 1.07), arrowprops=dict(arrowstyle=\"-\", color='k')) ax.text(0.145, 0.1, 'aegis4048.github.io', fontsize=10, ha='center', va='center', transform=ax.transAxes, color='grey', alpha=0.5) fig.set_facecolor(\"white\") fig.tight_layout() ############################################# Plot 2 ############################################## df = pd.read_excel('https://aegis4048.github.io/downloads/notebooks/sample_data/EIA-commodity.xls', sheet_name='Data 1') df = df.iloc[338:-1, :-1] # between Jan 2014 to Dec 2022 df.columns = ['date', 'value'] df.reset_index(inplace=True, drop=True) df['date'] = df['date'].apply(lambda x: x.strftime('%Y-%m')) df['date'] = pd.to_datetime(df['date']) fig, ax = plt.subplots(figsize=(7, 1)) ax.plot(df['date'], df['value'], color='green') ax.set_ylabel('oil ($)', fontsize=10) ax.set_ylim(0, 120) ax.set_yticks(np.linspace(0, 120, 4)) ax.tick_params(axis='both', which='major', labelsize=9) ax.axvspan(df_drilled.index[26], df_drilled.index[35], facecolor='lightgrey', alpha=0.3) ax.axvspan(df_drilled.index[55], df_drilled.index[61], facecolor='lightgrey', alpha=0.3) ax.axvspan(df_drilled.index[78], df_drilled.index[103], facecolor='lightgrey', alpha=0.3) ax.axvline(x=df_drilled.index[26], color='k', linestyle='--', linewidth=1, alpha=0.7) ax.axvline(x=df_drilled.index[35], color='k', linestyle='--', linewidth=1, alpha=0.7) ax.axvline(x=df_drilled.index[55], color='k', linestyle='--', linewidth=1, alpha=0.7) ax.axvline(x=df_drilled.index[61], color='k', linestyle='--', linewidth=1, alpha=0.7) ax.axvline(x=df_drilled.index[78], color='k', linestyle='--', linewidth=1, alpha=0.7) ax.axvline(x=df_drilled.index[103], color='k', linestyle='--', linewidth=1, alpha=0.7) ax.grid(axis='y', alpha=0.5) ax.spines['right'].set_visible(False) ax.spines['top'].set_visible(False) ax.yaxis.get_major_ticks()[-1].gridline.set_visible(False) ax.annotate('Source: Pythonic Excursions, EIA', xy=(-0.11, -.5), xycoords='axes fraction', fontsize=9) Figure 4: Quick glance at the EIA DPR data for each basin. Note that the Permian basin has different y-axis scale for DUC & newly drilled wells count. Source Code For Figure (4) import pandas as pd import numpy as np import matplotlib.pyplot as plt import seaborn as sns from matplotlib.colors import LinearSegmentedColormap import warnings import matplotlib.gridspec as gridspec import matplotlib.ticker as ticker ############################################# Basins ############################################## basins = sorted(['Anadarko', 'Appalachia', 'Eagle Ford', 'Haynesville', 'Niobrara', 'Permian', 'Bakken']) ################################### DUC Wells Count per Basin ##################################### # compile a DataFrame to store count of DUC wells per basin # source: EIA - https://www.eia.gov/petroleum/drilling/ dfs = [] for basin in basins: df_cur = pd.read_excel('https://aegis4048.github.io/downloads/notebooks/sample_data/Duc-EIA.xlsx', sheet_name=basin) df_cur['Basin'] = basin dfs.append(df_cur) df = pd.concat(dfs) df = df.sort_values('Date') df.index = pd.to_datetime(df['Date']) df_DUCs_year = df.groupby([df.index.year, 'Basin'])['DUC'].mean() df_DUCs_year = df_DUCs_year.unstack().fillna(0) df_DUCs_year = round(df_DUCs_year, 0) df_drilled_year = df.groupby([df.index.year, 'Basin'])['Drilled'].sum() df_drilled_year = df_drilled_year.unstack().fillna(0) df_drilled_year['Total Drilled'] = df_drilled_year.T.apply(lambda x: sum(x)) #################### Import All Data - Rig Count, Oil & Gas Prod per Basins ####################### dfs2 = [] for basin in basins: df_cur = pd.read_excel('https://aegis4048.github.io/downloads/notebooks/sample_data/dpr-data.xlsx', sheet_name=basin + ' Region', skiprows=1) df_cur['Basin'] = basin dfs2.append(df_cur) year_cutoff = '2013' # counting from year_cutoff + 1 date_header = 'Month' df2 = pd.concat(dfs2) df2 = df2.sort_values(date_header) df2.index = pd.to_datetime(df2[date_header]) df2.drop(df2.columns[[2, 3, 5, 6]], axis=1, inplace=True) df2.columns = ['Date', 'Rig Count', 'Total Oil Prod (BBLD)', 'Total Gas Prod (MCFD)', 'Basin'] df2 = df2[df2.index > year_cutoff + '-12-31'] ################################ Cross Table for Gas Production ################################### # Total Gas Production df2_total_gas_prod_year = df2.groupby([df2.index.year, 'Basin'])['Total Gas Prod (MCFD)'].sum() df2_total_gas_prod_year = df2_total_gas_prod_year.unstack().fillna(0) df2_total_gas_prod_year['Total Gas (BCFD)'] = df2_total_gas_prod_year.T.apply(lambda x: sum(x)) df2_total_gas_prod_year = df2_total_gas_prod_year / 1000000 ################################ Cross Table for Oil Production ################################### df2_total_oil_prod_year = df2.groupby([df2.index.year, 'Basin'])['Total Oil Prod (BBLD)'].sum() df2_total_oil_prod_year = df2_total_oil_prod_year.unstack().fillna(0) df2_total_oil_prod_year['Total (MMBBLD)'] = df2_total_oil_prod_year.T.apply(lambda x: sum(x)) df2_total_oil_prod_year = df2_total_oil_prod_year / 1000000 ############################################# Plot ############################################## # changing datetime index to str is necessary to overlay lineplot on top of barplot df_drilled_year.index = [str(item) for item in df_drilled_year.index] df2_total_gas_prod_year.index = [str(item) for item in df2_total_gas_prod_year.index] df2_total_oil_prod_year.index = [str(item) for item in df2_total_oil_prod_year.index] cmap_name = 'cubehelix' colors = sns.color_palette(cmap_name, n_colors=len(basins)) np.random.seed(34) np.random.shuffle(colors) colors = colors.as_hex() colors[0] = 'grey' colors[-2] = '#914810' cmap = LinearSegmentedColormap.from_list(\"my_colormap\", colors) barcolor_dict = {label: color for label, color in zip(basins, colors)} axis_fontsize = 15 axis_tick_fontsize = 11 title_fontsize = 18 figure_title_fontsize = 20 label_fontsize = 13 legend_fontsize = 16 markersize = 9 y_label_fontsize = 15 gas_prod_color = 'red' oil_prod_color = 'green' drilled_color = 'k' fig = plt.figure(figsize=(16, 17)) gs = gridspec.GridSpec(4, 4) ax1 = plt.subplot(gs[0, 0:2]) ax2 = plt.subplot(gs[0,2:]) ax3 = plt.subplot(gs[1,0:2]) ax4 = plt.subplot(gs[1,2:]) ax5 = plt.subplot(gs[2,0:2]) ax6 = plt.subplot(gs[2,2:]) ax7 = plt.subplot(gs[3,2:4]) axes = [ax1,ax2,ax3,ax4,ax5,ax6,ax7] for i, (ax, basin) in enumerate(zip(axes, basins)): df_DUCs_year.plot.bar(alpha=1, y=basin, ax=ax, legend=None, width=0.9, edgecolor='k', linewidth=0.1, label='DUC Count', color=barcolor_dict[basin]) ax1 = ax.twinx() df_drilled_year.plot(y=basin, ax=ax1, linestyle='-', legend=None, marker='o', color=drilled_color, markersize=markersize, label='New Drilled Wells') ax2 = ax.twinx() df2_total_oil_prod_year.plot(y=basin, ax=ax2, linestyle='-', marker='o', color='green', markersize=markersize, label='Oil Prod Total', legend=None) ax2.set_ylim(0, 70) ax3 = ax.twinx() df2_total_gas_prod_year.plot(y=basin, ax=ax3, linestyle='-', marker='o', color='red', markersize=markersize, label='Gas Prod Total', legend=None) ax3.set_ylim(0, 500) ax.set_facecolor('#eeeeee') ax.set_axisbelow(True) ax.grid(axis='y') ax.set_xticklabels([str(dt).split('-')[0] for dt in df_DUCs_year.index]) ax.tick_params(axis='x', labelrotation=45, labelsize=axis_tick_fontsize) ax.tick_params(axis='y', labelsize=axis_tick_fontsize) ax1.tick_params(axis='y', labelsize=axis_tick_fontsize) ax.set_title(basin, fontsize=title_fontsize) if i % 2 == 0 and i != 6: ax.set_ylabel('DUC Wells Count (k)', fontsize=axis_fontsize) ax2.set_yticks([]) ax3.set_yticks([]) else: ax1.set_ylabel('Drilled Wells Count (k)', fontsize=axis_fontsize) ax2.tick_params(axis='y', colors='green') ax2.spines['right'].set_position(('outward', 50)) ax2.spines['right'].set_color('green') ax2.set_ylabel('Total Oil Prod. (MMBBLD)', color='green', fontsize= axis_fontsize) ax3.tick_params(axis='y', colors='red') ax3.spines['right'].set_position(('outward', 100)) ax3.spines['right'].set_color('red') ax3.set_ylabel('Total Gas Prod. (BCFD)', color='red', fontsize= axis_fontsize) if i == 6: ax.set_ylabel('DUC Wells Count (k)', fontsize=axis_fontsize) #else: #ax1.set_yticks([]) #ax2.set_yticks([]) #ax3.set_yticks([]) if i == 4 or i == 6: ax.set_xlabel('Spud Date', fontsize=y_label_fontsize) else: ax.set_xlabel('') if basin == 'Permian': pass ax.set_ylim(0, 5000) ax1.set_ylim(0, 15000) else: ax.set_ylim(0, 1400) ax1.set_ylim(0, 7000) ax.yaxis.set_major_formatter(ticker.EngFormatter()) ax1.yaxis.set_major_formatter(ticker.EngFormatter()) ax.spines.top.set_visible(False) ax1.spines.top.set_visible(False) ax2.spines.top.set_visible(False) ax3.spines.top.set_visible(False) #ax.yaxis.get_major_ticks()[-1].gridline.set_visible(False) #ax1.yaxis.get_major_ticks()[-1].gridline.set_visible(False) h0, l0 = ax.get_legend_handles_labels() h1, l1 = ax1.get_legend_handles_labels() h2, l2 = ax2.get_legend_handles_labels() h3, l3 = ax3.get_legend_handles_labels() ax.legend(h0, l0, loc='upper left', ncol=2, fontsize=13, framealpha=0.5) fig.legend(h3 + h2 + h1, l3 + l2 + l1, loc='lower left', ncol=2, bbox_to_anchor=(0.046, 0.165), bbox_transform=fig.transFigure, fontsize=legend_fontsize) for c in ax.containers: ax.bar_label(c, label_type='center', color='white', weight='bold', fontsize=label_fontsize) ax.text(0.16, 0.80, 'aegis4048.github.io', fontsize=12, ha='center', va='center', transform=ax.transAxes, color='grey', alpha=0.5) fig.tight_layout() def setbold(txt): return ' '.join([r\"$\\bf{\" + item + \"}$\" for item in txt.split(' ')]) y_pos = 0.998 fig.suptitle(setbold('DUC & Newly Drilled Wells Count and Production') + \", Jan 2014 - June 2022\", fontsize=19, x=0.328, y=1.025) ax.annotate('', xy=(0.006, y_pos), xycoords='figure fraction', xytext=(1, y_pos), arrowprops=dict(arrowstyle=\"-\", color='k', lw=1.3)) ax.annotate('Source: Pythonic Excursions, EIA', xy=(0.05, 0.145), xycoords='figure fraction', fontsize=16) fig.set_facecolor(\"white\") 3. Reservoir depletion due to existing offset wells Rarely the operators get a chance to drill a well in a virgin reservoir condition, especially in the Permian basin. Most times it's infill drilling among pre-existing wells that have been on production for some time, potentially inducing reservoir depletion problems when chained with unoptimized fracture design and well spacing. Well-to-well interference is known to have negative impact on production for both parent (old) and child (new) wells. The prolonged production from a parent well induces reservoir depletion and creates pressure sinks (area of lower pressure). A child well's hydraulic fractures are more prone to grow towards the pressure sink, causing fracture hits. Ajisafe et al. (2016) studied 300 horizontal wells in the Avalon Shale of the Delaware basin and found that child wells were almost 30% less productive than parent wells. To mitigate the effects of frac hits, they simulated 1,320-ft and 660-ft lateral spacing and found that child wells with 1,320-ft spacing saw only a 5% reduction in cumulative production over five years compared to parent wells, while 660-ft spacing saw a 24% reduction. Srinivasan et al. (2018) conducted study on the Williston basin, and discovered that \"the longer the difference in timing between completions in areas of high reservoir quality, the more pronounced the impacts of pressure depletion will be on new adjacent completions.\" It has particular significance for DUC wells because, as existing wells continue to produce and deplete, wells waiting on completion are already experiencing these variations around them, which will worsen as completion is delayed further. While the impact of a parent well on a child well is a convoluted function of fracture design, well spacing, DUC time, and many other factors, this article focuses on the effect of DUC time (gap between drilling and completion) on EUR to show why it may not be wise to delay completion for wells in certain basins (especailly Anadarko and Permian). 4. Quantifying the impact of DUC time on EUR DUC time: Gap in time between drilling and completion of a well (days). PPF: Proppant per Perforated Foot (lbs/ft). FPF: Fluid per Perforated Foot (gal/ft). Completion size: Total volume of proppant and fracturing fluid. Data Description The primary data source used in this study is Enverus' Drilling Info application (refer to above for quick peek at the sample data). The data was filtered to include only horizontal wells (column: Drill Type ) and producing wells (column: Production Type ), while excluding injection wells, dry holes, cores, etc. DUC time (column: Lag ) is calculated by the difference between spud date and completion date. Since Drilling Info does not provide finished drilling date, spud date was assumed to be equivalent to drilled date. Modern day drilling typically takes less than 20 days. The EUR (column: EUR (Full) BOE ) is expressed in Barrels of Oil Equivalent (BOE). The oil and gas EURs are regularly calculated and updated by Enverus. Although these auto-fitted EURs may not be completely accurate when examined on a well-by-well basis, they provide valuable insights for basin-scale analysis. The gas EUR is converted from MCFD to BOE using a conversion factor of 6 before being combined with the oil EUR. Methodology Literature review suggests that the volume of hydrocarbons lost to parent wells from child wells increases with longer DUC times. However, visualizing the raw EUR vs. DUC time contradicts this proved scientific wisdom, as shown in Figure 6 , due to the factors that have inconsistent effects on production at different DUC times. The presented study below attempts to separate the sole effects of DUC time on reservoir depletion by normalizing EUR by lateral length and completion size. 4.1. Preliminary data cleaning It is of common phenomenon that real life data has unwanted extreme data points or outliers that skew the derived statistics. This study filters those unwanted data based on percentile method; data points bigger or smaller than 97.5% or 2.5% are removed for lateral length, PPF, and FPF. In case of EUR, data points bigger than 95% percentile are filtered out. Figure 5 illustrates the result of outlier removal process for Anadarko and Haynesville for EUR and PPF. Though not shown in the below figure, the outlier removal process is applied for all data types of interest (EUR, lateral, PPF, and FPF) for all seven DPR basins. Figure 5: Each dot represents one well. The wells are binned by 50 lag days. Each band of dots represents wells completed within the 50-day binned interval. For EUR, upper 5% are removed. For PPF (proppant per perforated foot), upper and lower 2.5% are removed. Source Code For Figure (5) import pandas as pd import numpy as np import matplotlib.pyplot as plt import matplotlib.gridspec as gridspec import matplotlib.ticker as ticker from scipy import stats ############################################# Basins ############################################## basins = ['Anadarko', 'Haynesville', 'Anadarko', 'Haynesville'] ################################### DUC Wells Count per Basin ##################################### EURs = ['EUR (Full) BOE', 'EUR Oil (Full)', 'EUR Gas (Full)'] dfs = [] for basin in basins: df_cur = pd.read_excel('https://aegis4048.github.io/downloads/notebooks/sample_data/EUR_vs_Lag.xlsx', sheet_name=basin) df_cur = df_cur.dropna(subset=['Completion Date']) # Drop wells that are not completed df_cur = df_cur[df_cur['Drill Type'] == 'H'] # Horizontal wells only # Drops non-hydrocarbon wells like: Dry hole, injection, core, etc. df_cur = df_cur[df_cur['Production Type'].isin(['OIL', 'GAS', 'OIL & GAS'])] df_cur = df_cur.dropna(subset=EURs) df_cur[EURs] = df_cur[EURs].div(1000).round(0) dfs.append(df_cur) ############################################# Plot ############################################## fig = plt.figure(figsize=(16, 9)) gs = gridspec.GridSpec(4, 4) ax1 = plt.subplot(gs[0:2, 0:2]) ax2 = plt.subplot(gs[0:2, 2:]) ax3 = plt.subplot(gs[2:, 0:2]) ax4 = plt.subplot(gs[2:, 2:]) axes = [ax1,ax2,ax3,ax4] axis_tick_fontsize = 15 title_fontsize = 19 markersize = 20 y_label_fontsize = 18 legend_annot_fontsize = 18 colors = ['grey', '#c6b4ee', 'grey', '#c6b4ee'] columns = ['EUR (Full) BOE', 'EUR (Full) BOE', 'Proppant per Perforated Foot (First Treatment Job)', 'Proppant per Perforated Foot (First Treatment Job)'] for i, (ax, basin, column) in enumerate (zip(axes, basins, columns)): df = dfs[i] stepsize= 50 bins = np.arange(0, 1000 + stepsize, stepsize) binned = [] for b in bins[: -1]: _ = df[df['Lag'].between(b, b + stepsize)][column] binned.append(_) binned_outlier = [] binned_cleaned = [] ################################# For EURs ###################################### if column == 'EUR (Full) BOE': for item in binned: threshold = item.quantile(0.975) binned_outlier.append(item[item >= threshold]) binned_cleaned.append(item[item < threshold]) if basin == 'Anadarko': y_max = 18000 n_ticks = 7 else: y_max = 25000 n_ticks = 6 ax.set_ylabel('EUR (MBOE)', fontsize=y_label_fontsize) ax.set_title(basin, fontsize=title_fontsize) ax.text(0.98, 0.65, 'EURs', fontsize=20, ha='right', transform=ax.transAxes, color='k', alpha=0.5) ################################# For Proppants ###################################### if column == 'Proppant per Perforated Foot (First Treatment Job)': for item in binned: threshold_upper = item.quantile(0.975) threshold_lower = item.quantile(0.025) binned_outlier.append(item[(item < threshold_lower) | (item > threshold_upper)]) binned_cleaned.append(item[(item >= threshold_lower) & (item <= threshold_upper)]) if basin == 'Anadarko': y_max = 7000 n_ticks = 8 else: y_max = 10000 n_ticks = 6 ax.set_ylabel('PPF (lbs/ft)', fontsize=y_label_fontsize) ax.set_xlabel('DUC Time (Days)', fontsize=y_label_fontsize + 1) ax.text(0.98, 0.65, 'Proppants/Ft', fontsize=20, ha='right', transform=ax.transAxes, color='k', alpha=0.5) ###################################################################################### # scatter plot of cleaned data for j, item in enumerate(binned_cleaned): x = np.random.normal(j + 1, 0.06, size=len(item)) ax.scatter(x, item, color=colors[i], alpha=0.5, s=10, edgecolors='k', linewidth=0.1, label='Original wells') # scatter plot of outliers for k, item_outlier in enumerate(binned_outlier): x_outlier = np.random.normal(k + 1, 0.06, size=len(item_outlier)) ax.scatter(x_outlier, item_outlier, color='r', alpha=0.5, s=50, marker='x', label='Outliers') # axis range ymin = 0 - 0.05 * y_max ax.set_ylim(ymin, y_max) # tick settings ax.yaxis.set_major_locator(plt.MaxNLocator(n_ticks)) ax.tick_params(axis='x', labelsize=axis_tick_fontsize) ax.tick_params(axis='y', labelsize=axis_tick_fontsize) ax.yaxis.set_major_formatter(ticker.EngFormatter()) ax.xaxis.set_major_locator(plt.MaxNLocator(11)) ax.yaxis.get_major_ticks()[-1].gridline.set_visible(False) ax.set_xticklabels([int(item) for item in ax.get_xticks() * stepsize], rotation=30, fontsize=axis_tick_fontsize) # spines and grid ax.spines.top.set_visible(False) ax.spines.right.set_visible(False) ax.grid(axis='y', alpha=0.5) ax.set_axisbelow(True) ax.set_facecolor('#eeeeee') # legend h0, l0 = ax.get_legend_handles_labels() leg = ax.legend([h0[0], h0[-1]], [l0[0], l0[-1]], fontsize=16, ncol=1, loc='upper right') for lh in leg.legendHandles: lh.set_alpha(1) lh.set_sizes([50]) ax.text(0.19, 0.88, 'aegis4048.github.io', fontsize=16, ha='center', va='center', transform=ax.transAxes, color='grey', alpha=0.5) fig.tight_layout() def setbold(txt): return ' '.join([r\"$\\bf{\" + item + \"}$\" for item in txt.split(' ')]) y_pos = 0.993 fig.suptitle(setbold('Extreme Data Removal for EURs and PPF') + \", Jan 2014 - June 2022\", fontsize=19, x=0.29, y=1.04) ax.annotate('', xy=(0.005, y_pos), xycoords='figure fraction', xytext=(1, y_pos), arrowprops=dict(arrowstyle=\"-\", color='k', lw=1.3)) ax.annotate('Source: Pythonic Excursions, DrillingInfo', xy=(0.01, 0.02), xycoords='figure fraction', fontsize=legend_annot_fontsize) fig.set_facecolor(\"white\") plt.subplots_adjust(bottom=.15) 4.2. Raw EUR vs. DUC time According to prevailing scientific knowledge, the duration of DUC time is expected to negatively impact production. Nevertheless, the findings displayed in Figure 6 appear to contradict this notion. The plot depicts the raw EUR on the y-axis and the DUC time on the x-axis. The mean EURs for wells completed soon after drilling (within 50 days, first bands of dots in each basin plot) appear to be lower than those completed between 250 and 350 days after drilling. Notably, this difference is most pronounced for the Haynesville wells, as those completed within 300-350 days exhibit an EUR that is 328% higher than wells completed within 50 days after drilling . This observation is illogical and suggests that further data analysis and investigation are required to discern the genuine influence of DUC time on EUR. Figure 6: Each dot represents one well. The wells are binned by 50 DUC days. Each band of dots represents wells completed within the 50-day binned interval. The blue line plot represents the average of wells in each 50-day bin. Note that the credibility of this average line plot falls with the increasing DUC time due to lack of data near the end of the x-axis. Index of peak is chosen by visual inspection; the point which \"looks like\" an inflection point is chosen, and is prone to subjectivity. Source Code For Figure (6) import pandas as pd import numpy as np import matplotlib.pyplot as plt import matplotlib.gridspec as gridspec import matplotlib.ticker as ticker ############################################# Basins ############################################## basins = sorted(['Anadarko', 'Appalachia', 'Eagle Ford', 'Haynesville', 'Niobrara', 'Permian', 'Bakken']) ################################### DUC Wells Count per Basin ##################################### no_NA = ['EUR (Full) BOE'] dfs = [] for basin in basins: df_cur = pd.read_excel('https://aegis4048.github.io/downloads/notebooks/sample_data/EUR_vs_Lag.xlsx', sheet_name=basin) df_cur = df_cur.dropna(subset=no_NA) # Drop wells that are not completed df_cur = df_cur[df_cur['Drill Type'] == 'H'] # Horizontal wells only # Drops non-hydrocarbon wells like: Dry hole, injection, core, etc. df_cur = df_cur[df_cur['Production Type'].isin(['OIL', 'GAS', 'OIL & GAS'])] # BOE to MBOE unit conversion df_cur[no_NA] = df_cur[no_NA].div(1000).round(0) dfs.append(df_cur) ############################################# Plot ################################################ axis_tick_fontsize = 14 title_fontsize = 19 markersize = 20 y_label_fontsize = 16 colors = ['grey', '#a1794a', '#d484a9', '#cbe8f0', '#c6b4ee', '#914810', '#16534c'] fig = plt.figure(figsize=(16, 17)) gs = gridspec.GridSpec(4, 4) ax1 = plt.subplot(gs[0, 0:2]) ax2 = plt.subplot(gs[0,2:]) ax3 = plt.subplot(gs[1,0:2]) ax4 = plt.subplot(gs[1,2:]) ax5 = plt.subplot(gs[2,0:2]) ax6 = plt.subplot(gs[2,2:]) ax7 = plt.subplot(gs[3,2:]) axes = [ax1,ax2,ax3,ax4,ax5,ax6,ax7] def annotate_arrows(data_y, num, tick_spacing, ymax, x_start=1, ox=0.2, fontsize=14, flip_y=False): ''' data_y = y coordinate of the datapoint of interest num = index of the datapoint of interest tick_spacing = spacing between two major ticks ox = offset of x ''' head_padding = ymax * 0.04 # extra space between the datapoint of interest and arrowhead oy = ymax * 0.15 # offset of y sx = x_start + (tick_spacing / 2) * num sy = data_y + head_padding if flip_y: oy = -ymax * 0.15 sy = data_y - head_padding ax.arrow(sx + ox, sy + oy, -ox, -oy, head_width=0.3, head_length=ymax * 0.0333, fc='k', ec='k', lw=2) t1 = ax.text(sx + ox, sy + oy, int(data_y), fontsize=fontsize, color='k', ha='center') t1.set_bbox(dict(facecolor='white', alpha=1, edgecolor='k', pad=5, lw=2)) xytext = (sx + ox, sy + oy) return xytext for i, (ax, basin) in enumerate (zip(axes, basins)): # select current basin data df = dfs[i] # cut off high lag bins with too few wells df = df[df['Lag'] <= 1000] # binning by 50 days stepsize = 50 bins = np.arange(0, 1000 + stepsize, stepsize) # scatter plot of binned wells eur_original = [] eur_cleaned = [] eur_outlier = [] #cleaned_indices = [] for b in bins[: -1]: eur_original_ = df[df['Lag'].between(b, b + stepsize)]['EUR (Full) BOE'] cutoff = 0.95 eur_cleaned_ = eur_original_[eur_original_ <= eur_original_.quantile(cutoff)] eur_outlier_ = eur_original_[eur_original_ > eur_original_.quantile(cutoff)] eur_cleaned.append(eur_cleaned_) eur_outlier.append(eur_outlier_) for j, item in enumerate(eur_cleaned): x = np.random.normal(j + 1, 0.06, size=len(item)) ax.scatter(x, item, color=colors[i], alpha=0.5, s=10, edgecolors='k', linewidth=0.1, label='One well data') for k, item_outlier in enumerate(eur_outlier): x_outlier = np.random.normal(k + 1, 0.06, size=len(item_outlier)) ax.scatter(x_outlier, item_outlier, color='r', alpha=0.3, s=30, marker='x', label='Outliers') # line plot of average of binned wells means = [np.mean(item) for item in eur_cleaned] ax.plot([i + 1 for i in range(len(means))], means, linestyle='-', marker='o', markersize=8, alpha=0.7, label='AVG. EUR, binned by 50 lag days') # axis range #ymax = y_max[basin] ymax = 5000 ymin = 0 - 0.05 * ymax ax.set_ylim(ymin, ymax) #ax.set_ylim(2500, 12000) # tick settings ax.minorticks_on() ax.xaxis.set_tick_params(which='minor', bottom=False) ax.xaxis.set_major_locator(plt.MaxNLocator(12)) ax.yaxis.set_major_locator(plt.MaxNLocator(7)) ax.yaxis.set_major_locator(plt.MaxNLocator(6)) ax.yaxis.set_minor_locator(ticker.AutoMinorLocator(2)) ax.set_xticklabels([int(item) for item in ax.get_xticks() * stepsize], fontsize=axis_tick_fontsize) ax.tick_params(axis='x', labelsize=axis_tick_fontsize, labelrotation=30) ax.tick_params(axis='y', labelsize=axis_tick_fontsize) ax.yaxis.get_major_ticks()[-1].gridline.set_visible(False) ax.yaxis.set_major_formatter(ticker.EngFormatter()) # spines and grid ax.spines.top.set_visible(False) ax.spines.right.set_visible(False) ax.grid(axis='y', alpha=0.5) ax.grid(axis='y', which='minor', color='grey', linestyle='--', alpha=0.2) ax.set_axisbelow(True) # texts ax.set_title(basin + ': ' + str(df.shape[0]) + ' wells', fontsize=title_fontsize) ax.set_ylabel('EUR (MBOE)', fontsize=y_label_fontsize) if i == 4 or i == 6: ax.set_xlabel('DUC time (Days)', fontsize=y_label_fontsize) ax.set_facecolor('#eeeeee') ax.text(0.15, 0.88, 'aegis4048.github.io', fontsize=12, ha='center', va='center', transform=ax.transAxes, color='grey', alpha=0.5) # legends h0, l0 = ax.get_legend_handles_labels() leg = ax.legend([h0[0]], [l0[0]], fontsize=14, ncol=2, loc='upper right') for lh in leg.legendHandles: lh.set_alpha(1) lh.set_sizes([50]) h1, l1 = ax.get_legend_handles_labels() fig.legend([h1[-1]], [l1[-1]], loc='lower left', ncol=2, bbox_to_anchor=(0.046, 0.195), bbox_transform=fig.transFigure, fontsize=18) fs = 15 lw = 2 _1 = 0 spacing = abs(ax.get_xticks()[1] - ax.get_xticks()[2]) if basin == 'Anadarko': _2 = 4 yloc = 3000 textxy1 = annotate_arrows(means[_1], _1, spacing, ymax, fontsize=fs) textxy2 = annotate_arrows(means[_2], _2, spacing, ymax, fontsize=fs) ax.plot([textxy1[0], textxy1[0]], [textxy1[1], yloc], lw=lw, color='k', ls='--') ax.plot([textxy2[0], textxy2[0]], [textxy2[1], yloc], lw=lw, color='k', ls='--') ax.plot([textxy1[0], textxy2[0]], [yloc, yloc], lw=lw, color='k', ls='--') diff_percentage = '+' + str(int((means[_2] - means[_1])/means[_1] * 100)) + '%' t1 = ax.text((textxy1[0] + textxy2[0])/2, yloc, diff_percentage, fontsize=fs, color='green', ha='center') t1.set_bbox(dict(facecolor='white', alpha=1, edgecolor='k', pad=5, lw=2)) if basin == 'Appalachia': _2 = 2 yloc = 1000 textxy1 = annotate_arrows(means[_1], _1, spacing, ymax, fontsize=fs) textxy2 = annotate_arrows(means[_2], _2, spacing, ymax, fontsize=fs) ax.plot([textxy1[0], textxy1[0]], [textxy1[1], yloc], lw=lw, color='k', ls='--') ax.plot([textxy2[0], textxy2[0]], [textxy2[1], yloc], lw=lw, color='k', ls='--') ax.plot([textxy1[0], textxy2[0]], [yloc, yloc], lw=lw, color='k', ls='--') diff_percentage = '+' + str(int((means[_2] - means[_1])/means[_1] * 100)) + '%' t1 = ax.text((textxy1[0] + textxy2[0])/2, yloc, diff_percentage, fontsize=fs, color='green', ha='center') t1.set_bbox(dict(facecolor='white', alpha=1, edgecolor='k', pad=5, lw=2)) if basin == 'Bakken': _2 = 4 yloc = 3000 textxy1 = annotate_arrows(means[_1], _1, spacing, ymax, fontsize=fs) textxy2 = annotate_arrows(means[_2], _2, spacing, ymax, fontsize=fs) ax.plot([textxy1[0], textxy1[0]], [textxy1[1], yloc], lw=lw, color='k', ls='--') ax.plot([textxy2[0], textxy2[0]], [textxy2[1], yloc], lw=lw, color='k', ls='--') ax.plot([textxy1[0], textxy2[0]], [yloc, yloc], lw=lw, color='k', ls='--') diff_percentage = '+' + str(int((means[_2] - means[_1])/means[_1] * 100)) + '%' t1 = ax.text((textxy1[0] + textxy2[0])/2, yloc, diff_percentage, fontsize=fs, color='green', ha='center') t1.set_bbox(dict(facecolor='white', alpha=1, edgecolor='k', pad=5, lw=2)) if basin == 'Eagle Ford': _2 = 4 yloc = 3000 textxy1 = annotate_arrows(means[_1], _1, spacing, ymax, fontsize=fs) textxy2 = annotate_arrows(means[_2], _2, spacing, ymax, fontsize=fs) ax.plot([textxy1[0], textxy1[0]], [textxy1[1], yloc], lw=lw, color='k', ls='--') ax.plot([textxy2[0], textxy2[0]], [textxy2[1], yloc], lw=lw, color='k', ls='--') ax.plot([textxy1[0], textxy2[0]], [yloc, yloc], lw=lw, color='k', ls='--') diff_percentage = '+' + str(int((means[_2] - means[_1])/means[_1] * 100)) + '%' t1 = ax.text((textxy1[0] + textxy2[0])/2, yloc, diff_percentage, fontsize=fs, color='green', ha='center') t1.set_bbox(dict(facecolor='white', alpha=1, edgecolor='k', pad=5, lw=2)) if basin == 'Haynesville': _2 = 6 yloc = 4200 textxy1 = annotate_arrows(means[_1], _1, spacing, ymax, fontsize=fs, flip_y=False) textxy2 = annotate_arrows(means[_2], _2, spacing, ymax, fontsize=fs, flip_y=True) ax.plot([textxy1[0], textxy1[0]], [textxy1[1], yloc], lw=lw, color='k', ls='--') ax.plot([textxy2[0], textxy2[0]], [textxy2[1], yloc], lw=lw, color='k', ls='--') ax.plot([textxy1[0], textxy2[0]], [yloc, yloc], lw=lw, color='k', ls='--') diff_percentage = '+' + str(int((means[_2] - means[_1])/means[_1] * 100)) + '%' t1 = ax.text((textxy1[0] + textxy2[0])/2, yloc, diff_percentage, fontsize=fs, color='green', ha='center') t1.set_bbox(dict(facecolor='white', alpha=1, edgecolor='k', pad=5, lw=2)) if basin == 'Niobrara': _2 = 4 yloc = 3000 textxy1 = annotate_arrows(means[_1], _1, spacing, ymax, fontsize=fs) textxy2 = annotate_arrows(means[_2], _2, spacing, ymax, fontsize=fs) ax.plot([textxy1[0], textxy1[0]], [textxy1[1], yloc], lw=lw, color='k', ls='--') ax.plot([textxy2[0], textxy2[0]], [textxy2[1], yloc], lw=lw, color='k', ls='--') ax.plot([textxy1[0], textxy2[0]], [yloc, yloc], lw=lw, color='k', ls='--') diff_percentage = '+' + str(int((means[_2] - means[_1])/means[_1] * 100)) + '%' t1 = ax.text((textxy1[0] + textxy2[0])/2, yloc, diff_percentage, fontsize=fs, color='green', ha='center') t1.set_bbox(dict(facecolor='white', alpha=1, edgecolor='k', pad=5, lw=2)) if basin == 'Permian': _2 = 6 yloc = 3500 textxy1 = annotate_arrows(means[_1], _1, spacing, ymax, fontsize=fs) textxy2 = annotate_arrows(means[_2], _2, spacing, ymax, fontsize=fs) ax.plot([textxy1[0], textxy1[0]], [textxy1[1], yloc], lw=lw, color='k', ls='--') ax.plot([textxy2[0], textxy2[0]], [textxy2[1], yloc], lw=lw, color='k', ls='--') ax.plot([textxy1[0], textxy2[0]], [yloc, yloc], lw=lw, color='k', ls='--') diff_percentage = '+' + str(int((means[_2] - means[_1])/means[_1] * 100)) + '%' t1 = ax.text((textxy1[0] + textxy2[0])/2, yloc, diff_percentage, fontsize=fs, color='green', ha='center') t1.set_bbox(dict(facecolor='white', alpha=1, edgecolor='k', pad=5, lw=2)) fig.tight_layout() def setbold(txt): return ' '.join([r\"$\\bf{\" + item + \"}$\" for item in txt.split(' ')]) y_pos = 0.998 fig.suptitle(setbold('EUR vs. Days Till Completion') + \", Jan 2014 - June 2022\", fontsize=20, x=0.24, y=1.025) ax.annotate('', xy=(0.006, y_pos), xycoords='figure fraction', xytext=(1, y_pos), arrowprops=dict(arrowstyle=\"-\", color='k', lw=1.3)) ax.annotate('Source: Pythonic Excursions, DrillingInfo', xy=(0.055, 0.173), xycoords='figure fraction', fontsize=16) fig.set_facecolor(\"white\") Table 1: Summary table of Figure 6. 4.3. Factors that affect EUR: Completion size and lateral length The trend observed in Figure 6 above, where EURs increase with increasing DUC time, contradicts the established scientific knowledge. However, we have found that this illogical trend is due to the use of smaller completion sizes and shorter lateral lengths for wells with shorter DUC times. Interestingly, the basins showing the greatest difference in EUR between the peak and trough in Table 1 (Anandarko and Haynesville) are also those with the largest increase in completion size and lateral length as DUC time increases. Figure 7 illustrates the variation in EUR due to completion size and lateral length with increasing DUC time for different basins. The largest increase in EUR between two time periods ([1] and [2]) was observed in the Haynesville basin, with a difference of 2,223 MBOE (a 209% increase)( Table 2 ) ( grey bars) . A correlation analysis showed that this increase was positively associated with a 46% increase (2,111 ft) in lateral length ( Table 3 ) ( black line) , 121% boost in PPF ( Table 4 ) ( purple lines) and a 95% boost in FPF ( Table 5 ) ( blue lines) for wells completed between 200-250 days after drilling, compared to wells completed within 50 days after drilling. Similar trends were observed for the Anadarko, Permian, and Bakken basins, although the EUR increase in the Bakken basin occurred at a much slower pace over a 500-day period. Recall that PPF and FPF represent completion size normalized by lateral length (per foot). An increase in PPF and FPF implies that operators are not only increasing the total volume of proppant and fluid to fracture additional lateral length, but also increasing the completion concentration for each foot of the lateral for older DUC wells. This increase in completion size may be an effort to compensate for potential reservoir depletion issues that older DUC wells may encounter due to nearby parent wells have have already been producing for a while. Such attempt to compensate for potential reservoir depletion induced by parent wells can be best observed in the Anadarko and Hanyesville wells. An examination of Table 2 reveals a significant 217% (Anadarko) and 209% (Haynesville) increase in EUR ( grey bars) when completion concentration is boosted between [1] and [2]. Anadarko observed 104% and 60% additional PPF and FPF, and Hanyesville 121% and 95% bigger PPF and FPF, respectively ( Table 4 and Table 5 ). These findings from Figure 7 prove the positive correlation between bigger EUR and bigger completion size. Conversely, however, the figure also suggests that the operators tend to undersize their completion design for wells completed shortly after drilling (within 250 days), resulting in sub-optimal EUR . Put another way, wells completed shortly after drilling often receive smaller completion concentrations and yield smaller EUR, despite the fact that they have superior reservoir condition, compared to older DUC wells. As such, operators could potentially profit more from larger completion investments for wells completed shortly after drilling. In contrast, wells in the Appalachia, Eagle Ford, and Niobrara basins showed only a slight increase in completion size and lateral length with increasing DUC time, with the exception of the Niobrara basin which showed a decrease in PPF ( Table 4 ) ( purple line) despite the longer lateral length. The magnitude of the difference between [1] and [2] was not as drastic as in the other basins. This finding explains why wells in these basins showed minimal difference in EUR with increasing DUC time in contrast to the Anadarko, Bakken, Haynesville, and Permian basins; because the volume of proppants and fluids used for completion and lateral length stayed relatively constant with increasing DUC time. Additionally, it is worth noting that the wells with longer lateral length tended to have longer DUC time. While this may sound obvious, taking a closer look at it reveals that it is not so. Between [1] and [2], the average difference in lateral length of wells in the Haynesville basin was 2,111 ft and the difference in time was 200 days. An experienced drilling and completion engineer would recognize that it does not take an additional 200 days to drill an extra 2,111 ft and hydraulically fracture that additional length. Modern day drilling takes under 15~25 days, and completion 3~5 days. One possible explanation for operators' tendency to delay completion for longer lateral wells is that the costs and time associated with bigger proppant and fluid volumes required to adequately fracture longer lateral sections are higher. Nevertheless, further research is needed to fully understand the economic trade-offs involved in completion timing for longer lateral wells. Figure 7: Each data point represents an average data (lateral length, PPF and FPF) of wells completed within 50-day binned interval. Each bar represents raw EUR. Note that the credibility of the average line plots decrease with the increasing DUC time due to lack of data near the end of the x-axis (refer to Figure 6 to visually understand how each average is calculated). [1] and [2] represent local trough and peak used for comparison in Table 2, 3, 4 and 5. Indices of [2]s are chosen by visual inspection; the point which \"looks like\" an inflection point is chosen, and is prone to subjectivity. Source Code For Figure (7) import pandas as pd import numpy as np import matplotlib.pyplot as plt import matplotlib.gridspec as gridspec import matplotlib.ticker as ticker ############################################# Basins ############################################## basins = sorted(['Anadarko', 'Appalachia', 'Eagle Ford', 'Haynesville', 'Niobrara', 'Permian', 'Bakken']) ################################### DUC Wells Count per Basin ##################################### no_NA = ['EUR (Full) BOE', 'Fluid per Perforated Foot (First Treatment Job)', 'DI Lateral Length', 'Proppant per Perforated Foot (First Treatment Job)'] dfs = [] for basin in basins: df_cur = pd.read_excel('https://aegis4048.github.io/downloads/notebooks/sample_data/EUR_vs_Lag.xlsx', sheet_name=basin) df_cur = df_cur.dropna(subset=no_NA) # Drop wells that are not completed df_cur = df_cur[df_cur['Drill Type'] == 'H'] # Horizontal wells only # Drops non-hydrocarbon wells like: Dry hole, injection, core, etc. df_cur = df_cur[df_cur['Production Type'].isin(['OIL', 'GAS', 'OIL & GAS'])] dfs.append(df_cur) ############################################# Plot ############################################## colors = ['grey', '#a1794a', '#d484a9', '#cbe8f0', '#c6b4ee', '#914810', '#16534c'] axis_tick_fontsize = 14 title_fontsize = 19 markersize = 20 y_label_fontsize = 16 gas_prod_color = 'red' oil_prod_color = 'green' drilled_color = 'k' fig = plt.figure(figsize=(16, 17)) gs = gridspec.GridSpec(4, 4) ax1 = plt.subplot(gs[0, 0:2]) ax2 = plt.subplot(gs[0,2:]) ax3 = plt.subplot(gs[1,0:2]) ax4 = plt.subplot(gs[1,2:]) ax5 = plt.subplot(gs[2,0:2]) ax6 = plt.subplot(gs[2,2:]) ax7 = plt.subplot(gs[3,2:4]) axes = [ax1,ax2,ax3,ax4,ax5,ax6,ax7] for i, (ax, basin) in enumerate(zip(axes, basins)): # select current basin data df = dfs[i] # cut off high lag bins with too few wells df = df[df['Lag'] <= 1000] # binning by 50 days stepsize = 50 bins = np.arange(0, 1000 + stepsize, stepsize) # extreme data removal - 5% binned_cleaned_lateral = [] binned_cleaned_ppf = [] binned_cleaned_fpf = [] binned_cleaned_eur = [] for b in bins[: -1]: binned_lateral = df[df['Lag'].between(b, b + stepsize)]['DI Lateral Length'] binned_ppf = df[df['Lag'].between(b, b + stepsize)]['Proppant per Perforated Foot (First Treatment Job)'] binned_fpf = df[df['Lag'].between(b, b + stepsize)]['Fluid per Perforated Foot (First Treatment Job)'] binned_eur = df[df['Lag'].between(b, b + stepsize)]['EUR (Full) BOE'] # cutoff between 2.5% and 97.5% binned_cleaned_lateral_ = np.clip(binned_lateral, binned_lateral.quantile(0.025), binned_lateral.quantile(0.975)) binned_cleaned_ppf_ = np.clip(binned_ppf, binned_ppf.quantile(0.025), binned_ppf.quantile(0.975)) binned_cleaned_fpf_ = np.clip(binned_fpf, binned_fpf.quantile(0.025), binned_fpf.quantile(0.975)) binned_cleaned_eur_ = np.clip(binned_eur, binned_eur.quantile(0.025), binned_eur.quantile(0.975)) binned_cleaned_lateral.append(binned_cleaned_lateral_) binned_cleaned_ppf.append(binned_cleaned_ppf_) binned_cleaned_fpf.append(binned_cleaned_fpf_) binned_cleaned_eur.append(binned_cleaned_eur_) means_lateral = [np.mean(item) for item in binned_cleaned_lateral] means_ppf = [np.mean(item) for item in binned_cleaned_ppf] means_fpf = [np.mean(item) for item in binned_cleaned_fpf] means_eur = [np.mean(item) / 1000 for item in binned_cleaned_eur] # BOE to MBOE # colors color_lat = 'k' color_ppf = 'darkviolet' color_fpf = '#1f77b4' # EUR (MBOE) - bar plot ax.bar([i + 1 for i in range(len(means_eur))], [item for item in means_eur], label='Avg. EUR', alpha=0.3, color='k') ymax = 5000 ax.set_ylim(0 - 0.025 * ymax, ymax) # Lateral length (ft) - line plot ax1 = ax.twinx() ax1.plot([i + 1 for i in range(len(means_lateral))], means_lateral, linestyle='--', marker='o', markersize=6, alpha=0.7, label='Avg. Lateral length', color=color_lat) ymax = 15000 ax1.set_ylim(0 - 0.025 * ymax, ymax) # PPF (lbs/ft) - line plot ax2 = ax.twinx() ax2.plot([i + 1 for i in range(len(means_ppf))], means_ppf, linestyle='--', marker='o', markersize=6, alpha=0.7, label='Avg. PPF', color=color_ppf) ymax = 5000 ax2.set_ylim(0 - 0.025 * ymax, ymax) # FPF (gal/ft) - line plot ax3 = ax.twinx() ax3.plot([i + 1 for i in range(len(means_fpf))], means_fpf, linestyle='--', marker='o', markersize=6, alpha=0.7, label='Avg. FPF', color=color_fpf) ymax = 100 ax3.set_ylim(0 - 0.025 * ymax, ymax) # ax.yaxis.set_major_locator(plt.MaxNLocator(6)) ax1.yaxis.set_major_locator(plt.MaxNLocator(6)) ax2.yaxis.set_major_locator(plt.MaxNLocator(6)) ax3.yaxis.set_major_locator(plt.MaxNLocator(6)) ax.minorticks_on() ax.xaxis.set_tick_params(which='minor', bottom=False) ax.yaxis.set_minor_locator(ticker.AutoMinorLocator(3)) ax.set_title(basin + ': ' + str(df.shape[0]) + ' wells', fontsize=title_fontsize) ax.set_facecolor('#eeeeee') ax.set_axisbelow(True) ax.grid(axis='y') ax.tick_params(axis='x', labelrotation=45, labelsize=axis_tick_fontsize) ax.tick_params(axis='y', labelsize=axis_tick_fontsize) ax1.tick_params(axis='y', labelsize=axis_tick_fontsize) ax2.tick_params(axis='y', labelsize=axis_tick_fontsize) ax3.tick_params(axis='y', labelsize=axis_tick_fontsize) if i % 2 == 0 and i != 6: ax.set_ylabel('EUR (MBOE)', fontsize=y_label_fontsize) ax1.set_yticks([]) ax2.set_yticks([]) ax3.set_yticks([]) else: ax1.tick_params(axis='y', colors=color_lat) #ax1.spines['right'].set_color(color_lat) ax1.set_ylabel('Lateral Length (ft)', color=color_lat, fontsize=y_label_fontsize) ax2.tick_params(axis='y', colors=color_ppf) ax2.spines['right'].set_position(('outward', 65)) ax2.spines['right'].set_color(color_ppf) ax2.set_ylabel('PPF (lbs/ft)', color=color_ppf, fontsize=y_label_fontsize) ax3.tick_params(axis='y', colors=color_fpf) ax3.spines['right'].set_position(('outward', 120)) ax3.spines['right'].set_color(color_fpf) ax3.set_ylabel('FPF (gal/ft)', color=color_fpf, fontsize=y_label_fontsize) if i == 6: ax.set_ylabel('EUR (MBOE)', fontsize=y_label_fontsize) if i == 4 or i == 6: ax.set_xlabel('DUC Time (Days)', fontsize=y_label_fontsize) else: ax.set_xlabel('') ax.grid(axis='y', which='minor', color='grey', linestyle='--', alpha=0.2) ax.xaxis.set_major_locator(plt.MaxNLocator(12)) ax.set_xticklabels([int(item) for item in ax.get_xticks() * stepsize], fontsize=axis_tick_fontsize) ax.tick_params(axis='x', labelsize=axis_tick_fontsize, labelrotation=30) ax.yaxis.set_major_formatter(ticker.EngFormatter()) ax1.yaxis.set_major_formatter(ticker.EngFormatter()) ax2.yaxis.set_major_formatter(ticker.EngFormatter()) ax1.yaxis.set_major_formatter(ticker.EngFormatter()) ax.spines.top.set_visible(False) ax1.spines.top.set_visible(False) ax2.spines.top.set_visible(False) h0, l0 = ax.get_legend_handles_labels() h1, l1 = ax1.get_legend_handles_labels() h2, l2 = ax2.get_legend_handles_labels() h3, l3 = ax3.get_legend_handles_labels() fig.legend(h0 + h1 + h2 + h3, l0 + l1 + l2 + l3, loc='lower left', ncol=2, bbox_to_anchor=(0.046, 0.18), bbox_transform=fig.transFigure, fontsize=18) ax.text(0.86, 0.85, 'aegis4048.github.io', fontsize=12, ha='center', va='center', transform=ax.transAxes, color='grey', alpha=0.5) _1 = 1 if basin == 'Anadarko': _2 = 5 elif basin == 'Appalachia': _2 = 4 elif basin == 'Bakken': _2 = 10 elif basin == 'Eagle Ford': _2 = 5 elif basin == 'Haynesville': _2 = 5 elif basin == 'Niobrara': _2 = 6 else: _2 = 7 ax.axvline(x=_1, color='k', linestyle='--', linewidth=1, alpha=0.7) t1 = ax.text(_1, 4000, '[1]', fontsize=14, color='k', ha='center', va='center') t1.set_bbox(dict(facecolor='#eeeeee', alpha=1, edgecolor='#eeeeee', pad=1)) ax.axvline(x=_2, color='k', linestyle='--', linewidth=1, alpha=0.7) t2 = ax.text(_2, 4000, '[2]', fontsize=14, color='k', ha='center', va='center') t2.set_bbox(dict(facecolor='#eeeeee', alpha=1, edgecolor='#eeeeee', pad=1)) fig.tight_layout() def setbold(txt): return ' '.join([r\"$\\bf{\" + item + \"}$\" for item in txt.split(' ')]) y_pos = 0.998 fig.suptitle(setbold('Factors Affecting EUR: Lateral Length, PPF, and FPF') + \", Jan 2014 - June 2022\", fontsize=19, x=0.345, y=1.025) ax.annotate('', xy=(0.006, y_pos), xycoords='figure fraction', xytext=(1, y_pos), arrowprops=dict(arrowstyle=\"-\", color='k', lw=1.3)) ax.annotate('Source: Pythonic Excursions, EIA', xy=(0.055, 0.16), xycoords='figure fraction', fontsize=16) fig.set_facecolor(\"white\") >Table 2: Summary table for Figure 7 - Estimated Ultimate Recovery Table 3: Summary table for Figure 7 - Lateral length Table 4: Summary table for Figure 7 - Proppant per Perforated Feet Table 5: Summary table for Figure 7 - Fluid per Perforated Feet 4.4. Normalized EUR vs. DUC time Figure 7 above revealed positive correlation between completion size and lateral length with EUR; increased lateral length requires a larger completion size to fracture the additional length, leading to a greater EUR. To isolate the effect of DUC time on EUR and eliminate the influence of other confounding variables, EURs (BOE) are normalized (divided) by PPF (lbs/ft), FPF (gal/ft), and lateral length (ft). The resulting statistic has an unit of (BOE ft)/(lbs gal). Its absolute value is meaningless, as it is a statistic created solely to distinguish the effects of other variables on EUR. Only the relative percentage change in the statistic is relevant. Figure 8 displays the impact of DUC time on normalized EUR for each basin. The local troughs and peaks in each basin represent points of interest used to quantify the relative change in EUR for wells completed shortly after drilling compared to old DUC wells. A detailed analysis and commentary for each basin are presented below . Moreover, Figure 8 has the potential to be a useful tool in evaluating completion investment efficiency. For instance, the Bakken basin, which has the highest mean normalized EUR along the x-axis, can be argued to have superior rate of return for each dollars spent for hydraulic fracturing compared to the other basins. Conversely, the Eagle Ford and Permian basins have lower mean normalized EUR values at different DUC times, which is expected due to their status as some of the oldest oil and gas plays in the US and the significant depletion of their \"sweet spots.\" It is important to note that the calculated statistic in Figure 8 , the average normalized EUR ( blue lines) , is heavily influenced by extreme data points due to two reasons. Firstly, the statistic is a combination of four variables, each of which may not contain outliers individually but may do so when combined. Therefore, if multiple variables have extreme data points, the calculated statistic will have greater skewness due to a larger standard deviation. Secondly, the derivation of the statistic necessitates the presence of four variables (EUR, PPF, FPF, and lateral length) simultaneously, causing the source data file ( EUR_vs_Lag.xlsx ) to drop rows with any of the four variables missing. Consequently, the Haynesville basin, which had 2,858 wells in Figure 6 , had only 1,724 wells in Figure 8 . Figure 8: Each dot represents one well's normalized EUR. The wells are binned by 50 DUC days. Each band of dots represents wells completed within the 50-day binned interval. The blue line plot represetns the average of wells in each 50-day bin. Note that the credibility of this average line plot is affected by the number of data points in each bin. Bakken and Haynesville have suffers from lack of data for their first bin. Indices of the local peaks and troughs are chosen by visual inspection; the points which \"look like\" local troughs and peaks are picked, and are prone to subjectivity. Source Code For Figure (8) import pandas as pd import numpy as np import matplotlib.pyplot as plt import matplotlib.gridspec as gridspec import matplotlib.ticker as ticker ############################################# Basins ############################################## basins = sorted(['Anadarko', 'Appalachia', 'Eagle Ford', 'Haynesville', 'Niobrara', 'Permian', 'Bakken']) ################################### DUC Wells Count per Basin ##################################### no_NA = ['EUR (Full) BOE', 'Fluid per Perforated Foot (First Treatment Job)', 'DI Lateral Length', 'Proppant per Perforated Foot (First Treatment Job)'] dfs = [] for basin in basins: df_cur = pd.read_excel('https://aegis4048.github.io/downloads/notebooks/sample_data/EUR_vs_Lag.xlsx', sheet_name=basin) df_cur = df_cur.dropna(subset=no_NA) # Drop wells that are not completed df_cur = df_cur[df_cur['Drill Type'] == 'H'] # Horizontal wells only # Drops non-hydrocarbon wells like: Dry hole, injection, core, etc. df_cur = df_cur[df_cur['Production Type'].isin(['OIL', 'GAS', 'OIL & GAS'])] #df_cur[EURs] = df_cur[EURs].div(1000).round(0) dfs.append(df_cur) ############################################# Plot ################################################ axis_tick_fontsize = 14 title_fontsize = 19 markersize = 20 y_label_fontsize = 16 colors = ['grey', '#a1794a', '#d484a9', '#cbe8f0', '#c6b4ee', '#914810', '#16534c'] fig = plt.figure(figsize=(16, 17)) gs = gridspec.GridSpec(4, 4) ax1 = plt.subplot(gs[0, 0:2]) ax2 = plt.subplot(gs[0,2:]) ax3 = plt.subplot(gs[1,0:2]) ax4 = plt.subplot(gs[1,2:]) ax5 = plt.subplot(gs[2,0:2]) ax6 = plt.subplot(gs[2,2:]) ax7 = plt.subplot(gs[3,2:]) axes = [ax1,ax2,ax3,ax4,ax5,ax6,ax7] def annotate_arrows(data_y, num, tick_spacing, ymax, x_start=1, ox=0.2, fontsize=14, flip_y=False): ''' data_y = y coordinate of the datapoint of interest num = index of the datapoint of interest tick_spacing = spacing between two major ticks ox = offset of x ''' head_padding = ymax * 0.04 # extra space between the datapoint of interest and arrowhead oy = ymax * 0.15 # offset of y sx = x_start + (tick_spacing / 2) * num sy = data_y + head_padding if flip_y: oy = -ymax * 0.15 sy = data_y - head_padding #------------------------------------------------------------------------------------------------------------------ ax.arrow(sx + ox, sy + oy, -ox, -oy, head_width=0.0, fc='k', ec='k', lw=2) t1 = ax.text(sx + ox, sy + oy, round(data_y, 5), fontsize=fontsize, color='k', ha='center') t1.set_bbox(dict(facecolor='white', alpha=1, edgecolor='k', pad=5, lw=2)) #------------------------------------------------------------------------------------------------------------------ xytext = (sx + ox, sy + oy) return xytext for i, (ax, basin) in enumerate (zip(axes, basins)): # select current basin data df = dfs[i] # cut off high lag bins with too few wells df = df[df['Lag'] <= 1000] # binning by 50 days stepsize = 50 bins = np.arange(0, 1000 + stepsize, stepsize) # scatter plot of binned wells #norm_eur_original = [] norm_eur_cleaned = [] norm_eur_outlier = [] for b in bins[: -1]: eur_bin = df[df['Lag'].between(b, b + stepsize)]['EUR (Full) BOE'] lateral_bin = df[df['Lag'].between(b, b + stepsize)]['DI Lateral Length'] fpf_bin = df[df['Lag'].between(b, b + stepsize)]['Fluid per Perforated Foot (First Treatment Job)'] ppf_bin = df[df['Lag'].between(b, b + stepsize)]['Proppant per Perforated Foot (First Treatment Job)'] cutoff = 0.95 norm_eur_original_ = eur_bin/lateral_bin/fpf_bin/ppf_bin norm_eur_cleaned_ = norm_eur_original_[norm_eur_original_ <= norm_eur_original_.quantile(cutoff)] norm_eur_outlier_ = norm_eur_original_[norm_eur_original_ > norm_eur_original_.quantile(cutoff)] norm_eur_cleaned.append(norm_eur_cleaned_) norm_eur_outlier.append(norm_eur_outlier_) for j, item in enumerate(norm_eur_cleaned): x = np.random.normal(j + 1, 0.06, size=len(item)) ax.scatter(x, item, color=colors[i], alpha=0.5, s=10, edgecolors='k', linewidth=0.0, label='One well data') for k, item_outlier in enumerate(norm_eur_outlier): x_outlier = np.random.normal(k + 1, 0.06, size=len(item_outlier)) ax.scatter(x_outlier, item_outlier, color='r', alpha=0.3, s=30, marker='x', label='Outliers') # line plot of average of binned wells means = [np.mean(item) for item in norm_eur_cleaned] ax.plot([i + 1 for i in range(len(means))], means, linestyle='-', marker='o', markersize=8, alpha=0.7, label='AVG. normalized EUR, binned by 50 lag days') # axis range ymax = 0.02 ymin = 0 - 0.05 * ymax ax.set_ylim(ymin, ymax) # tick settings ax.xaxis.set_major_locator(plt.MaxNLocator(12)) ax.yaxis.set_major_locator(plt.MaxNLocator(6)) ax.set_xticklabels([int(item) for item in ax.get_xticks() * stepsize], fontsize=axis_tick_fontsize) ax.tick_params(axis='x', labelsize=axis_tick_fontsize, labelrotation=30) ax.tick_params(axis='y', labelsize=axis_tick_fontsize) ax.yaxis.get_major_ticks()[-1].gridline.set_visible(False) #ax.yaxis.set_major_formatter(ticker.EngFormatter()) ax.minorticks_on() ax.xaxis.set_tick_params(which='minor', bottom=False) ax.yaxis.set_minor_locator(ticker.AutoMinorLocator(2)) # spines and grid ax.spines.top.set_visible(False) ax.spines.right.set_visible(False) ax.grid(axis='y', alpha=0.5) ax.grid(axis='y', which='minor', color='grey', linestyle='--', alpha=0.2) ax.set_axisbelow(True) # texts ax.set_title(basin + ': ' + str(np.concatenate(norm_eur_cleaned).shape[0]) + ' wells', fontsize=title_fontsize) ax.set_ylabel('EUR/PPF/FPF/Lateral', fontsize=y_label_fontsize) if i == 4 or i == 6: ax.set_xlabel('DUC time (Days)', fontsize=y_label_fontsize) ax.set_facecolor('#eeeeee') ax.text(0.15, 0.88, 'aegis4048.github.io', fontsize=12, ha='center', va='center', transform=ax.transAxes, color='grey', alpha=0.5) # legends h0, l0 = ax.get_legend_handles_labels() leg = ax.legend([h0[0]], [l0[0]], fontsize=14, ncol=2, loc='upper right') for lh in leg.legendHandles: lh.set_alpha(1) lh.set_sizes([50]) h1, l1 = ax.get_legend_handles_labels() fig.legend([h1[-1]], [l1[-1]], loc='lower left', ncol=2, bbox_to_anchor=(0.046, 0.195), bbox_transform=fig.transFigure, fontsize=18) fs = 15 lw = 2 _1 = 0 spacing = abs(ax.get_xticks()[1] - ax.get_xticks()[2]) if basin == 'Anadarko': _2 = 3 yloc = 0.011 textxy1 = annotate_arrows(means[_1], _1, spacing, ymax, fontsize=fs) textxy2 = annotate_arrows(means[_2], _2, spacing, ymax, fontsize=fs) ax.plot([textxy1[0], textxy1[0]], [textxy1[1], yloc], lw=lw, color='k', ls='--') ax.plot([textxy2[0], textxy2[0]], [textxy2[1], yloc], lw=lw, color='k', ls='--') ax.plot([textxy1[0], textxy2[0]], [yloc, yloc], lw=lw, color='k', ls='--') diff_percentage = '' + str(int((means[_2] - means[_1])/means[_1] * 100)) + '%' t1 = ax.text((textxy1[0] + textxy2[0])/2, yloc, diff_percentage, fontsize=fs, color='red', ha='center') t1.set_bbox(dict(facecolor='white', alpha=1, edgecolor='k', pad=5, lw=2)) if basin == 'Appalachia': _2 = 6 yloc = 0.015 textxy1 = annotate_arrows(means[_1], _1, spacing, ymax, fontsize=fs, flip_y=False) textxy2 = annotate_arrows(means[_2], _2, spacing, ymax, fontsize=fs, flip_y=False) ax.plot([textxy1[0], textxy1[0]], [textxy1[1], yloc], lw=lw, color='k', ls='--') ax.plot([textxy2[0], textxy2[0]], [textxy2[1], yloc], lw=lw, color='k', ls='--') ax.plot([textxy1[0], textxy2[0]], [yloc, yloc], lw=lw, color='k', ls='--') diff_percentage = '' + str(int((means[_2] - means[_1])/means[_1] * 100)) + '%' t1 = ax.text((textxy1[0] + textxy2[0])/2, yloc, diff_percentage, fontsize=fs, color='red', ha='center') t1.set_bbox(dict(facecolor='white', alpha=1, edgecolor='k', pad=5, lw=2)) if basin == 'Bakken': _1 = 1 _2 = 14 yloc = 0.012 textxy1 = annotate_arrows(means[_1], _1, spacing, ymax, fontsize=fs) textxy2 = annotate_arrows(means[_2], _2, spacing, ymax, fontsize=fs) ax.plot([textxy1[0], textxy1[0]], [textxy1[1], yloc], lw=lw, color='k', ls='--') ax.plot([textxy2[0], textxy2[0]], [textxy2[1], yloc], lw=lw, color='k', ls='--') ax.plot([textxy1[0], textxy2[0]], [yloc, yloc], lw=lw, color='k', ls='--') diff_percentage = '' + str(int((means[_2] - means[_1])/means[_1] * 100)) + '%' t1 = ax.text((textxy1[0] + textxy2[0])/2, yloc, diff_percentage, fontsize=fs, color='red', ha='center') t1.set_bbox(dict(facecolor='white', alpha=1, edgecolor='k', pad=5, lw=2)) if basin == 'Eagle Ford': _2 = 5 yloc = 0.014 textxy1 = annotate_arrows(means[_1], _1, spacing, ymax, fontsize=fs) textxy2 = annotate_arrows(means[_2], _2, spacing, ymax, fontsize=fs) ax.plot([textxy1[0], textxy1[0]], [textxy1[1], yloc], lw=lw, color='k', ls='--') ax.plot([textxy2[0], textxy2[0]], [textxy2[1], yloc], lw=lw, color='k', ls='--') ax.plot([textxy1[0], textxy2[0]], [yloc, yloc], lw=lw, color='k', ls='--') diff_percentage = '' + str(int((means[_2] - means[_1])/means[_1] * 100)) + '%' t1 = ax.text((textxy1[0] + textxy2[0])/2, yloc, diff_percentage, fontsize=fs, color='red', ha='center') t1.set_bbox(dict(facecolor='white', alpha=1, edgecolor='k', pad=5, lw=2)) if basin == 'Haynesville': _1 = 1 _2 = 3 yloc = 0.015 textxy1 = annotate_arrows(means[_1], _1, spacing, ymax, fontsize=fs, flip_y=False) textxy2 = annotate_arrows(means[_2], _2, spacing, ymax, fontsize=fs, flip_y=False) ax.plot([textxy1[0], textxy1[0]], [textxy1[1], yloc], lw=lw, color='k', ls='--') ax.plot([textxy2[0], textxy2[0]], [textxy2[1], yloc], lw=lw, color='k', ls='--') ax.plot([textxy1[0], textxy2[0]], [yloc, yloc], lw=lw, color='k', ls='--') diff_percentage = '' + str(int((means[_2] - means[_1])/means[_1] * 100)) + '%' t1 = ax.text((textxy1[0] + textxy2[0])/2, yloc, diff_percentage, fontsize=fs, color='red', ha='center') t1.set_bbox(dict(facecolor='white', alpha=1, edgecolor='k', pad=5, lw=2)) if basin == 'Niobrara': _2 = 5 yloc = 0.015 textxy1 = annotate_arrows(means[_1], _1, spacing, ymax, fontsize=fs) textxy2 = annotate_arrows(means[_2], _2, spacing, ymax, fontsize=fs) ax.plot([textxy1[0], textxy1[0]], [textxy1[1], yloc], lw=lw, color='k', ls='--') ax.plot([textxy2[0], textxy2[0]], [textxy2[1], yloc], lw=lw, color='k', ls='--') ax.plot([textxy1[0], textxy2[0]], [yloc, yloc], lw=lw, color='k', ls='--') diff_percentage = '+' + str(int((means[_2] - means[_1])/means[_1] * 100)) + '%' t1 = ax.text((textxy1[0] + textxy2[0])/2, yloc, diff_percentage, fontsize=fs, color='green', ha='center') t1.set_bbox(dict(facecolor='white', alpha=1, edgecolor='k', pad=5, lw=2)) if basin == 'Permian': _2 = 3 yloc = 0.014 textxy1 = annotate_arrows(means[_1], _1, spacing, ymax, fontsize=fs) textxy2 = annotate_arrows(means[_2], _2, spacing, ymax, fontsize=fs) ax.plot([textxy1[0], textxy1[0]], [textxy1[1], yloc], lw=lw, color='k', ls='--') ax.plot([textxy2[0], textxy2[0]], [textxy2[1], yloc], lw=lw, color='k', ls='--') ax.plot([textxy1[0], textxy2[0]], [yloc, yloc], lw=lw, color='k', ls='--') diff_percentage = '' + str(int((means[_2] - means[_1])/means[_1] * 100)) + '%' t1 = ax.text((textxy1[0] + textxy2[0])/2, yloc, diff_percentage, fontsize=fs, color='red', ha='center') t1.set_bbox(dict(facecolor='white', alpha=1, edgecolor='k', pad=5, lw=2)) #break fig.tight_layout() def setbold(txt): return ' '.join([r\"$\\bf{\" + item + \"}$\" for item in txt.split(' ')]) y_pos = 0.998 fig.suptitle(setbold('EUR Normalized by PPF, FPF and Lateral Length') + \", Jan 2014 - June 2022\", fontsize=20, x=0.34, y=1.025) ax.annotate('', xy=(0.006, y_pos), xycoords='figure fraction', xytext=(1, y_pos), arrowprops=dict(arrowstyle=\"-\", color='k', lw=1.3)) ax.annotate('Source: Pythonic Excursions, DrillingInfo', xy=(0.055, 0.173), xycoords='figure fraction', fontsize=16) fig.set_facecolor(\"white\") Anadarko The normalized EUR for Anadarko wells completed beyond 50 days since drilled dropped by 81% over a 150-day span. The two outlier points in the average normalized EUR at 400-day and 450-day bins were identified as a result of extreme data points that could not be filtered out using the percentile method due to a lack of data. The observed decrease in normalized EUR with increasing DUC time follows the pattern observed in typical logarithmic production decline curve with a high b-factor, indicating a fast initial decline in production due to reservoir depletion. This could be an indication of severe well-to-well interference problems in the Anadarko basin. Figure 9: Anadarko basin from Figure 8 Appalachia The normalized EUR dropped by 19% over a span of 300 DUC days beyond the initial 50-day bin average. It seems to decrease linearly with DUC time at a very slow pace compared to Anadarko or Permian. Assuming linear regression, one could argue that the Appalachia DUC wells lose 2.7% EUR for every 50 days of delay in completion. Figure 10: Appalachia basin from Figure 8 Bakken The normalized EUR of Bakken wells exhibits an exponential decline trend, with the initial decline rate smaller than Anadarko but greater than Appalachia. Note that the first bin of data (50-day bin) for Bakken wells is characterized by a limited number of data points, including extreme outliers that heavily skew the calculated normalized EUR values. Therefore, the second bin (100-day bin) is considered as the starting point, where Bakken wells experienced a 70% decrease in production over 700 DUC days. It is important to note that this decline would likely be greater if more data points were available in the first bin, thereby yielding a more reliable average normalized EUR value. Based on the observation that Anadarko and Permian wells exhibit logarithmic decline in normalized EUR and experience rapid decline during early DUC time, it is expected that the initial average normalized EUR value for Bakken wells would be higher in the first day bin compared to the second day bin. Assuming an initial average normalized EUR value of 0.016 (although this is subjective), it is projected that the Bakken basin would experience a 78% decrease in production over a span of 750 DUC days. Figure 11: Bakken basin from Figure 8 Eagle Ford The Eagle Ford wells showed very minimal or no degradation in EUR with increasing DUC time. One can deduce that the Eagle Ford wells are more resilient to the well-to-well interference problems than the other basins. Note that this shows the trend of the entire basin; limiting the data pool to a small group of wells in a small area of interest may reveal different trend. Figure 12: Eagle Ford basin from Figure 8 Haynesville The statistical analysis of the Haynesville basin is limited due to the dearth of available data. Specifically, only 1,792 wells in the basin have complete information on EUR, proppant, fluid volume, and lateral length, which was further reduced to 1,724 after removing outliers. Notably, the average normalized EUR for the 50-day and 450-day bins extends beyond the y-axis range, while only the wells within the 100- to 350-day bins exhibit reliable results with sufficient number of records. Treating the 100-day bin as the first index of a local peak and the 200-day bin as the index of a local trough, the Haynesville wells demonstrated a 46% decrease in normalized EUR during 100 days of DUC time. However, given the logarithmic decline in average normalized EUR observed in Anadarko and Permian wells, an interpolation can be performed to estimate a plausible value of the first bin, around 0.008, although subjectivity is a possible limitation. This estimation leads to an anticipated 69% production decline in the Haynesville basin during the subsequent 150 DUC days after the initial 50 DUC days, followed by a relatively constant level of production afterwards. Figure 13: Haynesville basin from Figure 8 Niobrara Notably, the Niobrara basin demonstrated a substantial increasing trend in normalized EUR with increasing DUC time up to the 300-day bin, followed by a relatively constant or slowly declining trend in normalized EUR. The observation of increasing normalized EUR with increasing DUC time in Niobrara wells is intriguing and contradicts the prevailing scientific literature, which suggests that child wells tend to underperform relative to parent wells. However, this premise assumes the presence of a parent-child relationship between wells. In the absence of well-to-well interference factors, DUC wells waiting to be completed may be free from such negative impacts on production. One potential explanation for this counter-intuitive trend in normalized EUR could be the presence of unique geologic characteristics specific to the Niobrara basin, or the existence of hidden factors that were not accounted for during the normalization process. For example, Niobrara may be relatively free from vertical fracture interference problems associated with stacked laterals compared to other basins, or may have been subjected to substantially different completion or production techniques. It is important to note that Figure 7 and Table 4 above revealed that Niobrara was the only basin to exhibit a decrease in PPF with increasing DUC time, which suggests the presence of hidden factors not captured in the data set used for this study. Therefore, further analysis and study is required to understand this unexpected behavior. Figure 14: Niobrara basin from Figure 8 Permian The normalized EUR for Permian wells completed beyond 50 days since drilled dropped by 74% over a span of 150 days. The normalized EUR does not substantially drop further after the 200-day bin. This trend is very similar to that of the Anadarko basin's. Figure 15: Permian basin from Figure 8 5. \"Dead\" DUC wells DUC wells are treated as a form of oilfield working capital. They are essentially a long-term, partially funded storage of hydrocarbons \"under\" the ground instead of \"on\" the ground. However, if a well remains in DUC stage for too long, it no longer functions as a working oilfield capital. Analyst John Freeman from Raymond James & Associates Inc. said that that 95% of the drilled wells have been completed within 2 years, and that DUCs older than 2 years have low chance of ever being completed and considered out-of-the-system (a.k.a. Dead DUCs). Comparing with the Enverus data of wells in the seven DPR regions supports this claim to be reasonably true. Figure 16 presents a 95% box plot of the DUC time for wells in each basin, providing insight into the time limit till completion for oil and gas wells in the US. The plot indicates that 95% of the wells in the Anadarko basin were completed within 309 days after drilling, while wells in the Appalachia basin were completed within 956 days. The weighted average for all DPR regions reveals that 95% of wells in the US are completed within 567 days after drilling. The implications of these results suggest that any well not completed within this timeframe has a less than 5% chance of being completed, thereby rendering the expensive drilling cost that the operator incurred a waste. The completion time limit for DUC wells depends on their ultimate rate of return, which is a complex function of various factors, including EUR, well-to-well interference problems, royalty rate, and costs associated with site-clearing, drilling, lateral length, completion size and concentration per foot, daily operating costs, and lease purchase and renewal. However, comparing the 95% upper limit for DUC time across basins in Figure 16 reveals an interesting correlation with the other findings of this study. Notably, the three basins with the shortest DUC time limit, namely Anadarko (309 days), Haynesville (409 days), and Permian (470 days), also had the worst depletion rates in normalized EUR. Recall that in Figure 8 , Anadarko experienced an 81% drop in normalized EUR in 150 DUC days, while Haynesville and Permian experienced a 46% and 74% drop, respectively. Conversely, the Appalachia, Eagle Ford, and Niobrara basins demonstrated greater resilience to reductions in normalized EUR with increasing DUC time, and showed longer time limit till completion in Figure 16 . When considering the economic factors that impact the time limit till completion, it is essential to consider EUR as it has a direct impact on the final economic rate of return of a well. In Figure 16 , Appalachia has the longest time limit till completion, which can be attributed to its highest EUR along the x-axis compared to other basins. The high EUR renders operators to be more flexible with the costs associated with maintaining their DUC wells' lease. This understanding explains why Niobrara has a close DUC time limit with that of Permian despite Niobrara's strong resilience to the decrease in normalized EUR with increasing DUC time. In Figure 7 's Table 2 , Niobrara shows 541 MBOE of EUR at its local peak [2], while Permian shows 1,061 MBOE of EUR, which is almost double. In summary, Niobrara and Permian have opposing pros and cons that balance their time limit till completion to be similar. Figure 16: 95% Box plot of gap time between drilling and completion. The plot shows the time limit until completion for each basin. Each dot (very small, 99.5% transparency) represent one well of a basin. Upper whisker and the numeric annotation above it represent 95% upper limit. This means that wells with DUC time longer than the annotated number have less then 95% chance of ever being completed. Box plot for \"All Regions\" represent weighted average of all basins. Source Code For Figure (16) import pandas as pd import numpy as np import matplotlib.pyplot as plt ############################################# Basins ############################################## basins = sorted(['Anadarko', 'Appalachia', 'Eagle Ford', 'Haynesville', 'Niobrara', 'Permian', 'Bakken']) ################################### DUC Wells Count per Basin ##################################### # compile a DataFrame to store the completion & drilling date difference data of 7 basins # data restricted to wells that has \"Completion Date\" column populated # source: DrillingInfo App of Enverus lags = [] for basin in basins: df_cur = pd.read_excel('https://aegis4048.github.io/downloads/notebooks/sample_data/EUR_vs_Lag.xlsx', sheet_name=basin) df_cur = df_cur.dropna(subset=['Completion Date']) # Drop wells that are not completed df_cur = df_cur[df_cur['Drill Type'] == 'H'] lags.append(df_cur['Lag'].values) flattened = [] temp = [list(lag) for lag in lags] for item in temp: flattened += item lags.append(flattened) basins.append('All Regions') ############################################# Plot ############################################## fig, ax = plt.subplots(figsize=(8, 4.5)) colors = ['grey', '#a1794a', '#d484a9', '#cbe8f0', '#c6b4ee', '#914810', '#16534c', 'lightgrey'] cutoff = 95 for i, basin_lag in enumerate(lags): i = i + 1 x = np.random.normal(i, 0.06, size=len(basin_lag)) ax.scatter(x, basin_lag, color=colors[i - 1], alpha=0.005) _cutoff = int(np.percentile(basin_lag, cutoff)) ax.text(i, _cutoff + 150, str(_cutoff) + ' days', fontsize=10, ha='center', va='center', color='k') lw = 1 caps = ax.boxplot(lags, sym='', whis=[0, cutoff], showfliers=True, boxprops=dict(linewidth=lw, color='#4e98c3'), whiskerprops=dict(linewidth=lw, color='#4e98c3', linestyle='--'), vert=True, capprops=dict(linewidth=lw, color='k'), medianprops=dict(linewidth=lw, color='#ad203e')); n = 0.12 for cap in caps['caps']: cap.set(xdata=cap.get_xdata() + (-n, +n)) ax.set_xticklabels(basins, rotation=20) ax.set_ylim(-70, 2000) ax.set_ylabel('Lag (Days)') ax.yaxis.set_major_locator(plt.MaxNLocator(6)) ax.grid(axis='y', alpha=0.5) ax.yaxis.get_major_ticks()[-1].gridline.set_visible(False) ax.spines.top.set_visible(False) ax.spines.right.set_visible(False) def setbold(txt): return ' '.join([r\"$\\bf{\" + item + \"}$\" for item in txt.split(' ')]) ax.set_title(setbold('Time Limit Until Completion: 95\\% Boxplot') + \", Jan 2014 - June 2022\", fontsize=12, pad=10, x=0.31, y=1.06) ax.annotate('', xy=(-0.11, 1.07), xycoords='axes fraction', xytext=(1.02, 1.07), arrowprops=dict(arrowstyle=\"-\", color='k')) ax.annotate('Data Source: Pythonic Excursions, DrillingInfo', xy=(-0.11, -0.3), xycoords='axes fraction', fontsize=10) ax.text(0.16, 0.23, 'aegis4048.github.io', fontsize=10, ha='center', va='center', transform=ax.transAxes, color='white', alpha=0.5) ax.axvline(x=7.5, color='k', linestyle='--', linewidth=1, alpha=0.7) fig.set_facecolor(\"white\") fig.tight_layout() 6. Conclusion The well-established scientific wisdom suggests that parent (old) wells perform better than their nearby child (new) wells, due to the parent wells having already drained a significant volume of hydrocarbons and left the child well with lower reservoir pressure and smaller volume of fluids left to produce. Wells completed shortly after drilling (within 50 days) tend to have substantially smaller EUR compared to wells with longer DUC time ( Figure 6 ). This is because wells with longer DUC time tend to have longer lateral length and are treated with bigger completion. Conversely, it means that operators tend to undersize their completion concentration per lateral foot for wells completed shortly after drilling ( Figure 7 ). Thus, investing in larger completion size for wells completed shortly after drilling may yield greater profit for operators. To assess the genuine impact of DUC time on production, EUR was normalized by lateral length, proppant per perforated foot (PPF), and fluid per perforated foot (FPF). It has been found that increasing DUC time has a significant negative impact on the EUR of Anadarko, Bakken, Hanyesville, and Permian wells. Notably, Anadarko and Permian showed 81% and 74% drop in noramlized EUR, respectively, over a span of 150 days of DUC time. However, Appalachia, Eagle Ford and Niobrara wells have been found to be resilient from the EUR loss due to increasing DUC time ( Figure 8 ), showing negligible decrease in normalized EUR along the x-axis. In some basins, delaying completion due to unfavorable commodity prices may not be a financially sound strategy. Figure 8 provides valuable insight to evaluate completion investment efficiency as it shows EUR normalized by completion size and lateral length. DUC wells have time limit till completion. For example, Anadarko wells have less than 5% chance of completion for DUC wells older than 308 days since drilled ( Figure 16 ). Chance of completion drastically falls afterwards, which may render the drilling investment made for the well to be a waste. Anadarko and Haynesville wells have the shortest time limit till completion ( Figure 16 ). This can be attributed to the fact that they are the ones with the worst decrease in normalized EUR with increasing DUC time, as demonstrated in Figure 8 . On the other hand, Appalachia wells have the longest time limit till completion because they yield the greatest EUR at differnt DUC times, and their normalized EUR decreases minimally with increasing DUC time. WARNING! The presented analysis provides a basin-wide overview at a high level, and the conclusions drawn assume homogeneity within each basin. However, this assumption may not hold true for many regions. By restricting the analysis to a county-level, different trends may emerge compared to those observed at the basin level. In Figure 8 , it was observed that delaying completion by 150 days resulted in an 81% decline in normalized EUR for Anadarko wells. In contrast, no drop in EUR was observed for Eagle Ford wells. These conclusions should be interpreted as follows: \"If a new well is drilled in a typical area where most other operators drill, and typical well spacing, completion, and production techniques are applied across the basin, an 81% loss of EUR can be expected for Anadarko wells if completion is delayed by 150 days. It would be advisable to complete Anadarko wells as soon as possible to avoid drainage from neighboring competitor wells. However, for Eagle Ford wells, completion can be delayed for a prolonged period if the oil price is not favorable.\"","tags":"Oil and Gas","url":"https://aegis4048.github.io/quantifying_the_impact_of_completion_delay_since_drilled_on_production","loc":"https://aegis4048.github.io/quantifying_the_impact_of_completion_delay_since_drilled_on_production"},{"title":"DUC Wells and Their Economic Complications","text":"Once a well is drilled, it must be completed to produce. However, operators may choose not to immediately complete a well for various reasons: Availability of fracking crews Drilling contracts that are too expensive to revoke Constraints on capital spending Commodity prices Many others Whatever the reasons may be, this in-transit time between drilling and completion stages creates an inventory of DUC wells. The term, DUC wells , have a literal definition - Drilled but Uncompleted wells . They essentially serve as a secodary form of hydrocarbon storage underground, instead of surface storage facilities. The wells can be turned on line for production for cheap; the costs of drilling have been paid, and completion won't be as costly as new drilling. This unique characteristic of DUC wells allows the operators to make strategic decisions during the time of financial crisis (ex: 2020 COVID crash); they complete the existing DUC wells and take profits instead of drilling new wells. However, this profit taking process inevitably leads to the reduction in size of DUC well inventories, which gave rise to the concerns regarding potential energy supply shortage due to the depletion of DUC wells around early 2022. In this article I attempt to help the readers to understand what it means to have DUC well inventories, why they exist, and the economic impact they have on the US energy industry. Contents 0. Sample data description 1. Key takeaways 2. How are DUC wells created? 3. DUC wells and Economic Complications 3.1. Potential depletion of DUC wells and enegy supply shortage 3.2. Recent trend: more production with less drilling 4. Continued discussion: \"Dead DUC Wells\" 0. Sample data description The main data source I use for this article is the Drilling Productivity Report (DPR) from the US Energy Information Administration (EIA), and some supplemental data from the Enverus Drilling Info application since 2014. While the DPR data is readily available to the public as its provided by the US government, Enverus' Drilling Info data is not because its a commerical software. The DPR data covers the 7 major oil and gas basins in the US: Anadarko, Appalachia, Bakken, Eagle Ford, Haynesville, Niobrara, and Permian. Figure 1: 7 major basins in the US, Source: EIA I downloaded the Report data (aggregated by region) (dpr-data.xlsx) and the DUC data (aggregated by region) (duc-data.xlsx) , which can be found in the \"Contents\" tab in the right side of this page . Visualizing the DPR data for each basin generates the below figure: Figure 2: Quick glance at the EIA DPR data for each basin. Note that the Permian basin has different y-axis scale for DUC & newly drilled wells count. Source Code For Figure (2) import pandas as pd import numpy as np import matplotlib.pyplot as plt import seaborn as sns from matplotlib.colors import LinearSegmentedColormap import warnings import matplotlib.gridspec as gridspec import matplotlib.ticker as ticker ############################################# Basins ############################################## basins = sorted(['Anadarko', 'Appalachia', 'Eagle Ford', 'Haynesville', 'Niobrara', 'Permian', 'Bakken']) ################################### DUC Wells Count per Basin ##################################### # compile a DataFrame to store count of DUC wells per basin # source: EIA - https://www.eia.gov/petroleum/drilling/ dfs = [] for basin in basins: df_cur = pd.read_excel('https://aegis4048.github.io/downloads/notebooks/sample_data/Duc-EIA.xlsx', sheet_name=basin) df_cur['Basin'] = basin dfs.append(df_cur) df = pd.concat(dfs) df = df.sort_values('Date') df.index = pd.to_datetime(df['Date']) df_DUCs_year = df.groupby([df.index.year, 'Basin'])['DUC'].mean() df_DUCs_year = df_DUCs_year.unstack().fillna(0) df_DUCs_year = round(df_DUCs_year, 0) df_drilled_year = df.groupby([df.index.year, 'Basin'])['Drilled'].sum() df_drilled_year = df_drilled_year.unstack().fillna(0) df_drilled_year['Total Drilled'] = df_drilled_year.T.apply(lambda x: sum(x)) #################### Import All Data - Rig Count, Oil & Gas Prod per Basins ####################### dfs2 = [] for basin in basins: df_cur = pd.read_excel('https://aegis4048.github.io/downloads/notebooks/sample_data/dpr-data.xlsx', sheet_name=basin + ' Region', skiprows=1) df_cur['Basin'] = basin dfs2.append(df_cur) year_cutoff = '2013' # counting from year_cutoff + 1 date_header = 'Month' df2 = pd.concat(dfs2) df2 = df2.sort_values(date_header) df2.index = pd.to_datetime(df2[date_header]) df2.drop(df2.columns[[2, 3, 5, 6]], axis=1, inplace=True) df2.columns = ['Date', 'Rig Count', 'Total Oil Prod (BBLD)', 'Total Gas Prod (MCFD)', 'Basin'] df2 = df2[df2.index > year_cutoff + '-12-31'] ################################ Cross Table for Gas Production ################################### # Total Gas Production df2_total_gas_prod_year = df2.groupby([df2.index.year, 'Basin'])['Total Gas Prod (MCFD)'].sum() df2_total_gas_prod_year = df2_total_gas_prod_year.unstack().fillna(0) df2_total_gas_prod_year['Total Gas (BCFD)'] = df2_total_gas_prod_year.T.apply(lambda x: sum(x)) df2_total_gas_prod_year = df2_total_gas_prod_year / 1000000 ################################ Cross Table for Oil Production ################################### df2_total_oil_prod_year = df2.groupby([df2.index.year, 'Basin'])['Total Oil Prod (BBLD)'].sum() df2_total_oil_prod_year = df2_total_oil_prod_year.unstack().fillna(0) df2_total_oil_prod_year['Total (MMBBLD)'] = df2_total_oil_prod_year.T.apply(lambda x: sum(x)) df2_total_oil_prod_year = df2_total_oil_prod_year / 1000000 ############################################# Plot ############################################## # changing datetime index to str is necessary to overlay lineplot on top of barplot df_drilled_year.index = [str(item) for item in df_drilled_year.index] df2_total_gas_prod_year.index = [str(item) for item in df2_total_gas_prod_year.index] df2_total_oil_prod_year.index = [str(item) for item in df2_total_oil_prod_year.index] cmap_name = 'cubehelix' colors = sns.color_palette(cmap_name, n_colors=len(basins)) np.random.seed(34) np.random.shuffle(colors) colors = colors.as_hex() colors[0] = 'grey' colors[-2] = '#914810' cmap = LinearSegmentedColormap.from_list(\"my_colormap\", colors) barcolor_dict = {label: color for label, color in zip(basins, colors)} axis_fontsize = 15 axis_tick_fontsize = 11 title_fontsize = 18 figure_title_fontsize = 20 label_fontsize = 13 legend_fontsize = 16 markersize = 9 y_label_fontsize = 15 gas_prod_color = 'red' oil_prod_color = 'green' drilled_color = 'k' fig = plt.figure(figsize=(16, 17)) gs = gridspec.GridSpec(4, 4) ax1 = plt.subplot(gs[0, 0:2]) ax2 = plt.subplot(gs[0,2:]) ax3 = plt.subplot(gs[1,0:2]) ax4 = plt.subplot(gs[1,2:]) ax5 = plt.subplot(gs[2,0:2]) ax6 = plt.subplot(gs[2,2:]) ax7 = plt.subplot(gs[3,2:4]) axes = [ax1,ax2,ax3,ax4,ax5,ax6,ax7] for i, (ax, basin) in enumerate(zip(axes, basins)): df_DUCs_year.plot.bar(alpha=1, y=basin, ax=ax, legend=None, width=0.9, edgecolor='k', linewidth=0.1, label='DUC Count', color=barcolor_dict[basin]) ax1 = ax.twinx() df_drilled_year.plot(y=basin, ax=ax1, linestyle='-', legend=None, marker='o', color=drilled_color, markersize=markersize, label='New Drilled Wells') ax2 = ax.twinx() df2_total_oil_prod_year.plot(y=basin, ax=ax2, linestyle='-', marker='o', color='green', markersize=markersize, label='Oil Prod Total', legend=None) ax2.set_ylim(0, 70) ax3 = ax.twinx() df2_total_gas_prod_year.plot(y=basin, ax=ax3, linestyle='-', marker='o', color='red', markersize=markersize, label='Gas Prod Total', legend=None) ax3.set_ylim(0, 500) ax.set_facecolor('#eeeeee') ax.set_axisbelow(True) ax.grid(axis='y') ax.set_xticklabels([str(dt).split('-')[0] for dt in df_DUCs_year.index]) ax.tick_params(axis='x', labelrotation=45, labelsize=axis_tick_fontsize) ax.tick_params(axis='y', labelsize=axis_tick_fontsize) ax1.tick_params(axis='y', labelsize=axis_tick_fontsize) ax.set_title(basin, fontsize=title_fontsize) if i % 2 == 0 and i != 6: ax.set_ylabel('DUC Wells Count (k)', fontsize=axis_fontsize) ax2.set_yticks([]) ax3.set_yticks([]) else: ax1.set_ylabel('Drilled Wells Count (k)', fontsize=axis_fontsize) ax2.tick_params(axis='y', colors='green') ax2.spines['right'].set_position(('outward', 50)) ax2.spines['right'].set_color('green') ax2.set_ylabel('Total Oil Prod. (MMBBLD)', color='green', fontsize= axis_fontsize) ax3.tick_params(axis='y', colors='red') ax3.spines['right'].set_position(('outward', 100)) ax3.spines['right'].set_color('red') ax3.set_ylabel('Total Gas Prod. (BCFD)', color='red', fontsize= axis_fontsize) if i == 6: ax.set_ylabel('DUC Wells Count (k)', fontsize=axis_fontsize) #else: #ax1.set_yticks([]) #ax2.set_yticks([]) #ax3.set_yticks([]) if i == 4 or i == 6: ax.set_xlabel('Spud Date', fontsize=y_label_fontsize) else: ax.set_xlabel('') if basin == 'Permian': pass ax.set_ylim(0, 5000) ax1.set_ylim(0, 15000) else: ax.set_ylim(0, 1400) ax1.set_ylim(0, 7000) ax.yaxis.set_major_formatter(ticker.EngFormatter()) ax1.yaxis.set_major_formatter(ticker.EngFormatter()) ax.spines.top.set_visible(False) ax1.spines.top.set_visible(False) ax2.spines.top.set_visible(False) ax3.spines.top.set_visible(False) #ax.yaxis.get_major_ticks()[-1].gridline.set_visible(False) #ax1.yaxis.get_major_ticks()[-1].gridline.set_visible(False) h0, l0 = ax.get_legend_handles_labels() h1, l1 = ax1.get_legend_handles_labels() h2, l2 = ax2.get_legend_handles_labels() h3, l3 = ax3.get_legend_handles_labels() ax.legend(h0, l0, loc='upper left', ncol=2, fontsize=13, framealpha=0.5) fig.legend(h3 + h2 + h1, l3 + l2 + l1, loc='lower left', ncol=2, bbox_to_anchor=(0.046, 0.165), bbox_transform=fig.transFigure, fontsize=legend_fontsize) for c in ax.containers: ax.bar_label(c, label_type='center', color='white', weight='bold', fontsize=label_fontsize) ax.text(0.16, 0.80, 'aegis4048.github.io', fontsize=12, ha='center', va='center', transform=ax.transAxes, color='grey', alpha=0.5) fig.tight_layout() def setbold(txt): return ' '.join([r\"$\\bf{\" + item + \"}$\" for item in txt.split(' ')]) y_pos = 0.998 fig.suptitle(setbold('DUC & Newly Drilled Wells Count and Production') + \", Jan 2014 - June 2022\", fontsize=19, x=0.328, y=1.025) ax.annotate('', xy=(0.006, y_pos), xycoords='figure fraction', xytext=(1, y_pos), arrowprops=dict(arrowstyle=\"-\", color='k', lw=1.3)) ax.annotate('Source: Pythonic Excursions, EIA', xy=(0.05, 0.145), xycoords='figure fraction', fontsize=16) fig.set_facecolor(\"white\") For the Enverus data, I synchronized the wells from the EIA database with the Drilling Info database by matching their states and counties. The \"RegionCounties\" tab in the dpr-data.xlsx shows how EIA decides which states and counties belong to which basins. If you have Driling Info subscription and would like to follow the steps I've taken, you can download this text file I created, and paste its state and county data into Drilling Info's \"Paste Filter Selections\" as shown below. You will need to do this 7 times for each basins. Figure 3: Pasting the Eagle Ford basin's state and county data into Drilling Info for query 1. Key takeaways 1. DUCs are accumulated when drilling activity exceeds completion, and vice versa 2. DUCs are created due to the following reasons: (1) Cost efficiency with batch-fracking or batch-drilling (2) Drilling contracts that are too expensive to revoke (3) Constraitns on capital spending and (4) Lack of infrastructure. 3. DUC wells function as a working oilfield capital DUCs can quickly be turned on line for production for cheap because the cost of drilling has already been paid. Completing DUC wells is essentially a profit-taking process, so long as there are DUCs left to complete. 4. Depletion of DUC wells may cause energy shortage problems The recent COVID crash left operators with scarce capitals to spend. They chose to complete the existing DUC wells for profit instead of drilling new wells, which resulted in fast depletion of DUCs in the US. Prolonged profit-taking process from DUCs without new drilling may result in energy shortage problems when the operators run out of DUC wells to complete. 5. Modern day industry is producing more hydrocarbons with less drilling activities This is due to the fact horizontal wells produce 2.5 to 7 times more than verticals, and that modern day drilling is composed of 91% horizontals and 9% verticals. 6. Gap time between drilling and completion should not be a contributing factor to the birth of \"Dead\" DUCs The impact of gap time is not significant enough to render operators to shy away from completing their old DUCs. This is discussed in more detail in my next article. 2. How are DUC wells created? The formation of DUC wells in the oil and gas industry is caused by a variety of factors. The most straightforward reason is that they occur when operators drill more wells than they complete, and vice versa, whether by choice or not. If they drill more wells than they complete, DUC wells accumulate. If they complete more wells than they drill, DUC wells deplete. This trend can be observed in Figure 4 , which showcases three distinct periods where the US DUC well counts in the seven DPR regions experienced rapid increases and decreases as a result of imbalanced drilling and completion activities. Figure 4: DUCs accumulate when drilling activity exceeds completion, and vice versa. The line plots depict three distinct periods of significant fluctuations in DUC well inventories, with [1] and [3] representing instances of rapid depletion due to high completion rates, and [2] reflecting a period of accumulation resulting from high drilling activities. WTI Crude oil prices is appended at the bottom to show the effect of commodity prices on DUCs. Source Code For Figure (4) import pandas as pd import matplotlib.pyplot as plt import numpy as np import matplotlib.ticker as ticker ############################################# Basins ############################################## basins = sorted(['Anadarko', 'Appalachia', 'Eagle Ford', 'Haynesville', 'Niobrara', 'Permian', 'Bakken']) ################################### DUC Wells Count per Basin ##################################### # compile a DataFrame to store count of DUC wells per basin # source: EIA - https://www.eia.gov/petroleum/drilling/ dfs = [] for basin in basins: df_cur = pd.read_excel('https://aegis4048.github.io/downloads/notebooks/sample_data/Duc-EIA.xlsx', sheet_name=basin) df_cur['Basin'] = basin dfs.append(df_cur) df = pd.concat(dfs) df = df.sort_values('Date') df.index = pd.to_datetime(df['Date']) df_DUCs = df.groupby([pd.Grouper(freq='M'), 'Basin'])['DUC'].mean() df_DUCs = df_DUCs.unstack().fillna(0) df_DUCs = round(df_DUCs, 0) df_DUCs['Total DUCs'] = df_DUCs.T.apply(lambda x: sum(x)) df_drilled = df.groupby([pd.Grouper(freq='M'), 'Basin'])['Drilled'].sum() df_drilled = df_drilled.unstack().fillna(0) df_drilled['Total Drilled'] = df_drilled.T.apply(lambda x: sum(x)) df_completed = df.groupby([pd.Grouper(freq='M'), 'Basin'])['Completed'].sum() df_completed = df_completed.unstack().fillna(0) df_completed['Total Completed'] = df_completed.T.apply(lambda x: sum(x)) ############################################# Plot ############################################## fig, ax = plt.subplots(figsize=(8, 4)) ax2 = ax.twinx() ax.plot(df_drilled.index, df_drilled['Total Drilled'], color='k', label='Newly Drilled') ax.plot(df_completed.index, df_completed['Total Completed'], label='Completed') ax2.plot(df_DUCs.index, df_DUCs['Total DUCs'], color='purple', label='DUC') ax.fill_between(df_drilled.index, df_drilled['Total Drilled'], df_completed['Total Completed'], where=(df_drilled.index >= '2016-03-1') & (df_drilled.index <= '2017-1-1'), color='red', alpha=0.4) ax.fill_between(df_drilled.index, df_drilled['Total Drilled'], df_completed['Total Completed'], where=(df_drilled.index >= '2018-08-01') & (df_drilled.index <= '2019-3-1'), color='green', alpha=0.4) ax.fill_between(df_drilled.index, df_drilled['Total Drilled'], df_completed['Total Completed'], where=(df_drilled.index >= '2020-07-1') & (df_drilled.index <= '2022-8-1'), color='red', alpha=0.4) ax.axvline(x=df_drilled.index[26], color='k', linestyle='--', linewidth=1, alpha=0.7) ax.axvline(x=df_drilled.index[35], color='k', linestyle='--', linewidth=1, alpha=0.7) ax.axvspan(df_drilled.index[26], df_drilled.index[35], facecolor='lightgrey', alpha=0.3) ax.axvline(x=df_drilled.index[55], color='k', linestyle='--', linewidth=1, alpha=0.7) ax.axvline(x=df_drilled.index[61], color='k', linestyle='--', linewidth=1, alpha=0.7) ax.axvspan(df_drilled.index[55], df_drilled.index[61], facecolor='lightgrey', alpha=0.3) ax.axvline(x=df_drilled.index[78], color='k', linestyle='--', linewidth=1, alpha=0.7) ax.axvline(x=df_drilled.index[102], color='k', linestyle='--', linewidth=1, alpha=0.7) ax.axvspan(df_drilled.index[78], df_drilled.index[102], facecolor='lightgrey', alpha=0.3) ax.set_ylim(0, 2500) ax2.set_ylim(0, 10000) ax.grid(axis='y', alpha=0.5) ax.yaxis.get_major_ticks()[5].gridline.set_visible(False) ax.spines.top.set_visible(False) ax2.spines.top.set_visible(False) h1, l1 = ax.get_legend_handles_labels() h2, l2 = ax2.get_legend_handles_labels() ax.legend(h1 + h2, l1 + l2, fontsize=10, ncol=3, loc='upper left', framealpha=1) ax.set_ylabel('Drilled & Completed Wells', fontsize=11) ax2.set_ylabel('DUC Wells', fontsize=11) ax.arrow(df_drilled.index[71], 1500, 60, -270, head_width=40, head_length=60, fc='k', ec='k') ax.text(0.58, 0.62, 'Covid Crash', fontsize=9, transform=ax.transAxes, color='k') ax.text(0.295, 0.05, '[1]', fontsize=9, transform=ax.transAxes, color='k') ax.text(0.53, 0.05, '[2]', fontsize=9, transform=ax.transAxes, color='k') ax.text(0.805, 0.05, '[3]', fontsize=9, transform=ax.transAxes, color='k') def setbold(txt): return ' '.join([r\"$\\bf{\" + item + \"}$\" for item in txt.split(' ')]) ax.set_title(setbold('US Major Basins Completion & Drilling Activities') + \", Jan 2014 - Dec 2022\", fontsize=12, pad=10, x=0.4, y=1.06) ax.annotate('', xy=(-0.11, 1.07), xycoords='axes fraction', xytext=(1.11, 1.07), arrowprops=dict(arrowstyle=\"-\", color='k')) ax.text(0.145, 0.1, 'aegis4048.github.io', fontsize=10, ha='center', va='center', transform=ax.transAxes, color='grey', alpha=0.5) fig.set_facecolor(\"white\") fig.tight_layout() ############################################# Plot 2 ############################################## df = pd.read_excel('https://aegis4048.github.io/downloads/notebooks/sample_data/EIA-commodity.xls', sheet_name='Data 1') df = df.iloc[338:-1, :-1] # between Jan 2014 to Dec 2022 df.columns = ['date', 'value'] df.reset_index(inplace=True, drop=True) df['date'] = df['date'].apply(lambda x: x.strftime('%Y-%m')) df['date'] = pd.to_datetime(df['date']) fig, ax = plt.subplots(figsize=(7, 1)) ax.plot(df['date'], df['value'], color='green') ax.set_ylabel('oil ($)', fontsize=10) ax.set_ylim(0, 120) ax.set_yticks(np.linspace(0, 120, 4)) ax.tick_params(axis='both', which='major', labelsize=9) ax.axvspan(df_drilled.index[26], df_drilled.index[35], facecolor='lightgrey', alpha=0.3) ax.axvspan(df_drilled.index[55], df_drilled.index[61], facecolor='lightgrey', alpha=0.3) ax.axvspan(df_drilled.index[78], df_drilled.index[103], facecolor='lightgrey', alpha=0.3) ax.axvline(x=df_drilled.index[26], color='k', linestyle='--', linewidth=1, alpha=0.7) ax.axvline(x=df_drilled.index[35], color='k', linestyle='--', linewidth=1, alpha=0.7) ax.axvline(x=df_drilled.index[55], color='k', linestyle='--', linewidth=1, alpha=0.7) ax.axvline(x=df_drilled.index[61], color='k', linestyle='--', linewidth=1, alpha=0.7) ax.axvline(x=df_drilled.index[78], color='k', linestyle='--', linewidth=1, alpha=0.7) ax.axvline(x=df_drilled.index[103], color='k', linestyle='--', linewidth=1, alpha=0.7) ax.grid(axis='y', alpha=0.5) ax.spines['right'].set_visible(False) ax.spines['top'].set_visible(False) ax.yaxis.get_major_ticks()[-1].gridline.set_visible(False) ax.annotate('Source: Pythonic Excursions, EIA', xy=(-0.11, -.5), xycoords='axes fraction', fontsize=9) Here I list more detailed reasons as to why operators may choose to not complete their well. Cost efficiency with batch-fracking or batch-drilling As an efficient operator, it is ideal to avoid engaging a fracturing fleet until all logistical requirements have been thoroughly understood and procured and a substantial number of wells have been collected for prolonged, uninterrupted operations. This can be achieved either by maintaining a consistent inventory of wells waiting to be completed or by alternating the company's focus in the region between drilling a series of wells and then completing them all. If there are 10 wells in one location and it takes 10 days to drill each well, the first drilled well will stay idle for at least 90 days before it can be completed. This lag can result in the formation of additional DUC wells in the event of an unforeseen circumstance that renders fracturing unfeasible or undesirable. Drilling contracts that are too expensive to revoke Often times the drilling contracts between an operator and a service company entails drilling a batch of wells and expensive contract revokation fee. If the oil price suddenly drops while drilling the contracted batch of wells, the operator may choose not to complete the drilled wells while letting the rig crews keep drilling new wells due to the existing contract. This was the most apparent in the recent COVID crash, which took place around March of 2020. Taking a close look at Figure 4 around March of 2020 (slightly to the left of [3]), one can observe that there higher drilling than completion activities, which temporarily boosted the number of DUCs until July of 2020 (start of [3]). This happened because there were rig contracts that were still in effect few months after the COVID crash. However, this phenomena did not last long once those contracts expired or have been fulfilled, thus rapidly depleting the number of DUCs until late 2022. Constraints on capital spending The operator may choose not to finish their wells due to restrictions on the company's capital expenditures or adverse market circumstances, opting instead to wait for better economics. However, this approach may not always be advisable, as DUC wells older than two years have a low probability, less than 5%, of being completed due to issues such as lost leases. Lack of infrastructure An operator may leave a well uncompleted if there is no nearby pipeline to transport the produced gas after a pad site is cleared and a well is drilled. However, unexpected issues during pipeline construction may result in the well remaining as a DUC well for an extended period until resolved. 3. DUC wells and economic complications DUC wells are a form of hydrocarbon storage under the ground instead of on the surface storage facilities. The operators invest on drilling first and take profits later when its desirable. This unique characteristics of DUCs create economic complications for the oil and gas industry due to the following reasons: Timing: DUC wells are partially completed sources of supply that can be brought to market more quickly and at a lower cost than a newly drilled well. However, completing DUC wells at the right time to balance the market demand for energy can be challenging. Market fluctuations: DUC wells are often used to keep up with demand during times of high energy demand. However, during periods of declining oil prices, completing DUC wells may not be economically viable, leading to a buildup of uncompleted wells. This has been the most apparent between late 2018 to early 2019; WTI crude dropped from $ 86 to $ 54. This accumulation of DUCs due to oil price drop can be observed in Figure 4 's highlighted area [2]. Rig contracts: In times of declining oil prices, operators may continue to drill wells due to rig contracts, but not complete these wells and bring them into production. This can result in a buildup of DUC wells that may not be economically viable to complete in the future (due to reasons like lost leases). Profit-taking: Completing DUC wells is essentially a profit-taking process done on wells with substantial investment already made. However, if the timing of completion is not aligned with market demand or if capital is not available, the profit-taking process may not yield the expected results. Capital management: The inventory levels of DUC wells are closely linked to the availability of industry capital. When capital is abundant, producers tend to increase drilling relative to completions, leading to a rise in DUC inventory. On the other hand, when capital is scarce, producers prioritize well completion over new drilling, which can lead to a depletion of DUC wells. 3.1. Potential depletion of DUC wells and enegy supply shortage The COVID-19 pandemic resulted in a reduction of available capital for the oil and gas industry. To conserve cash, many operators prioritized well completion over new drilling, enabling them to maintain production with less capital. However, this strategy of profit-taking cannot be sustained in the long-term and resulted in a rapid decrease of DUC wells in the US, raising concerns about the potential depletion of DUCs and energy supply shortages. Figure 5 demonstrates the swift depletion rate of DUCs since July 2020. If this rate continued, the US oil and gas industry would have exhausted its DUC wells by the end of 2023, potentially leading to an energy shortage. Fortunately, drilling activity caught up with completion activities by mid-2022, resulting in a stable level of DUC wells now. Figure 5: Rapid depletion of US DUC wells since July 2020. Prolonged depletion of DUC wells may result in potential shortage with energy supply if the operators don't drill more wells. Source Code For Figure (5) import pandas as pd import matplotlib.pyplot as plt import numpy as np import matplotlib.ticker as ticker from sklearn import linear_model import datetime ############################################# Basins ############################################## basins = sorted(['Anadarko', 'Appalachia', 'Eagle Ford', 'Haynesville', 'Niobrara', 'Permian', 'Bakken']) ################################### DUC Wells Count per Basin ##################################### # compile a DataFrame to store count of DUC wells per basin # source: EIA - https://www.eia.gov/petroleum/drilling/ dfs = [] for basin in basins: df_cur = pd.read_excel('https://aegis4048.github.io/downloads/notebooks/sample_data/Duc-EIA-old.xlsx', sheet_name=basin) df_cur['Basin'] = basin dfs.append(df_cur) df = pd.concat(dfs) df = df.sort_values('Date') df.index = pd.to_datetime(df['Date']) df_DUCs = df.groupby([pd.Grouper(freq='M'), 'Basin'])['DUC'].mean() df_DUCs = df_DUCs.unstack().fillna(0) df_DUCs = round(df_DUCs, 0) df_DUCs['Total DUCs'] = df_DUCs.T.apply(lambda x: sum(x)) df_drilled = df.groupby([pd.Grouper(freq='M'), 'Basin'])['Drilled'].sum() df_drilled = df_drilled.unstack().fillna(0) df_drilled['Total Drilled'] = df_drilled.T.apply(lambda x: sum(x)) df_completed = df.groupby([pd.Grouper(freq='M'), 'Basin'])['Completed'].sum() df_completed = df_completed.unstack().fillna(0) df_completed['Total Completed'] = df_completed.T.apply(lambda x: sum(x)) #################### Import All Data - Rig Count, Oil & Gas Prod per Basins ####################### dfs2 = [] for basin in basins: df_cur = pd.read_excel('https://aegis4048.github.io/downloads/notebooks/sample_data/dpr-data-old.xlsx', sheet_name=basin + ' Region', skiprows=1) df_cur['Basin'] = basin dfs2.append(df_cur) year_cutoff = '2013' # counting from year_cutoff + 1 date_header = 'Month' df2 = pd.concat(dfs2) df2 = df2.sort_values(date_header) df2.index = pd.to_datetime(df2[date_header]) df2.drop(df2.columns[[2, 3, 5, 6]], axis=1, inplace=True) df2.columns = ['Date', 'Rig Count', 'Total Oil Prod (BBLD)', 'Total Gas Prod (MCFD)', 'Basin'] df2 = df2[df2.index > year_cutoff + '-12-31'] # current year (2022) prediction parameter num_month_2022 = 6 # number of months in data set for current year pred_factor = 12 / num_month_2022 ################################ Cross Table for Gas Production ################################### # Total Gas Production df2_total_gas_prod = df2.groupby([pd.Grouper(freq='M'), 'Basin'])['Total Gas Prod (MCFD)'].sum() df2_total_gas_prod = df2_total_gas_prod.unstack().fillna(0) df2_total_gas_prod['Total Gas (BCFD)'] = df2_total_gas_prod.T.apply(lambda x: sum(x)) df2_total_gas_prod = df2_total_gas_prod / 1000000 ################################ Cross Table for Oil Production ################################### df2_total_oil_prod = df2.groupby([pd.Grouper(freq='M'), 'Basin'])['Total Oil Prod (BBLD)'].sum() df2_total_oil_prod = df2_total_oil_prod.unstack().fillna(0) df2_total_oil_prod['Total Oil (MBBLD)'] = df2_total_oil_prod.T.apply(lambda x: sum(x)) df2_total_oil_prod = df2_total_oil_prod / 1000 ############################################# Plot ############################################## axis_label_fontsize = 12 title_fontsize = 13 legend_fontsize = 11 fig, axes = plt.subplots(2, 1, figsize=(9, 6.5)) ax1 = axes[0] ax2 = axes[1] ax3 = ax2.twinx() n = 16 # extends the right-end of the x-axis ext = 38 x_extender = df_DUCs.index.union(df_DUCs.index.shift(n + ext)[-(n + ext):]) x_extender_y = [1 for item in x_extender] ax1.plot(x_extender, x_extender_y, alpha=0) ax2.plot(x_extender, x_extender_y, alpha=0) ax1.axvspan(x_extender[-ext-1], x_extender[-1], facecolor='lightgrey', alpha=0.5) ax2.axvspan(x_extender[-ext-1], x_extender[-1], facecolor='lightgrey', alpha=0.5) ################################### DUC Wells Linear Regression ################################### X_pred_plot = df_DUCs.index.union(df_DUCs.index.shift(n)[-n:]) X_pred = X_pred_plot.to_julian_date().values.reshape(-1, 1) X = df_DUCs.index.to_julian_date().values.reshape(-1, 1) y = df_DUCs['Total DUCs'].values fit_begin = 79 fit_end = -3 plot_begin = 90 ols = linear_model.LinearRegression() model = ols.fit(X[fit_begin: fit_end], y[fit_begin: fit_end]) response = model.predict(X_pred[plot_begin:]) ax1.plot(df_DUCs.index, df_DUCs['Total DUCs'], color='purple', label='DUC') ax1.plot(X_pred_plot[plot_begin:], response, color='b', label='DUC Prediction', linestyle='--', linewidth=2) ax1.vlines(x=df_DUCs.index[fit_begin - 1], ymin=0, ymax=max(y) - 100, color='purple', alpha=0.7, linestyle='--') ax1.scatter(X_pred_plot[-1], 0, marker='o', color='b', label='point', clip_on=False, s=80) ############################# Oil and Gas Production Linear Regression ############################ n = n - 2 X_pred_plot = df2_total_oil_prod.index.union(df2_total_oil_prod.index.shift(n)[-n:]) X_pred = X_pred_plot.to_julian_date().values.reshape(-1, 1) X = df2_total_oil_prod.index.to_julian_date().values.reshape(-1, 1) y_oil = df2_total_oil_prod['Total Oil (MBBLD)'].values y_gas = df2_total_gas_prod['Total Gas (BCFD)'].values ols_oil = linear_model.LinearRegression() ols_gas = linear_model.LinearRegression() model_oil = ols_oil.fit(X, y_oil) model_gas = ols_gas.fit(X, y_gas) response_oil = model_oil.predict(X_pred) response_gas = model_gas.predict(X_pred) ax2.plot(X_pred_plot[plot_begin:], response_oil[plot_begin:], label='Oil Prediction', color='green', linestyle='--') ax3.plot(X_pred_plot[plot_begin:], response_gas[plot_begin:], label='Gas Prediction', color='red', linestyle='--') ax2.scatter(X_pred_plot[-1], response_oil[-1], marker='o', color='green', clip_on=False, s=80) ax3.scatter(X_pred_plot[-1], response_gas[-1], marker='o', color='red', clip_on=False, s=80) ax2.scatter(df2_total_oil_prod.index, df2_total_oil_prod['Total Oil (MBBLD)'], label='Oil', marker='o', color='green', s=10, alpha=0.3) ax3.scatter(df2_total_gas_prod.index, df2_total_gas_prod['Total Gas (BCFD)'], label='Gas', marker='o', color='red', s=10, alpha=0.3) ax1.set_ylim(0, 10000) ax2.set_ylim(0, 15000) ax3.set_ylim(0, 150) ax1.set_ylabel('Drilled & DUC Well Counts', fontsize=axis_label_fontsize) ax2.set_ylabel('Total Oil Prod. (MBBLD)', fontsize=axis_label_fontsize) ax2.set_yticks(np.arange(0, 15001, 3000)) ax3.set_ylabel('Total Gas Prod. (BCFD)', fontsize=axis_label_fontsize) ax3.set_yticks(np.arange(0, 151, 30)) ax1.tick_params(axis='both', which='major', labelsize=11) ax2.tick_params(axis='both', which='major', labelsize=11) ax1.grid(axis='y', alpha=0.5) ax1.yaxis.get_major_ticks()[5].gridline.set_visible(False) ax2.grid(axis='y', alpha=0.5) ax2.yaxis.get_major_ticks()[5].gridline.set_visible(False) ax1.spines.top.set_visible(False) ax1.spines.right.set_visible(False) ax2.spines.top.set_visible(False) ax3.spines.top.set_visible(False) h1, l1 = ax1.get_legend_handles_labels() ax1.legend(h1[:2], l1[:2], loc='upper left', fontsize=legend_fontsize) h2, l2 = ax2.get_legend_handles_labels() h3, l3 = ax3.get_legend_handles_labels() ax2.legend(h2 + h3, l2 + l3, fontsize=legend_fontsize, ncol=2, loc='upper left') ax1.yaxis.set_major_formatter(ticker.EngFormatter()) ax2.yaxis.set_major_formatter(ticker.EngFormatter()) ax3.yaxis.set_major_formatter(ticker.EngFormatter()) def setbold(txt): return ' '.join([r\"$\\bf{\" + item + \"}$\" for item in txt.split(' ')]) ax1.set_title(setbold('Depletion of DUC Wells and Energy Supply') + \", Jan 2014 - June 2022\", fontsize=title_fontsize, pad=10, x=0.335, y=1.06) ax1.annotate('', xy=(-0.1, 1.07), xycoords='axes fraction', xytext=(1.07, 1.07), arrowprops=dict(arrowstyle=\"-\", color='k')) ax1.text(0.535, 0.4, 'July 2020', fontsize=10, ha='center', va='center', transform=ax1.transAxes, color='purple', alpha=0.7, rotation=270) ax2.annotate('Source: Pythonic Excursions, EIA', xy=(-0.11, -0.22), xycoords='axes fraction', fontsize=12) ax1.text(0.145, 0.1, 'aegis4048.github.io', fontsize=10, ha='center', va='center', transform=ax1.transAxes, color='grey', alpha=0.5) ax2.text(0.145, 0.1, 'aegis4048.github.io', fontsize=10, ha='center', va='center', transform=ax2.transAxes, color='grey', alpha=0.5) ax1.text(0.765, 0.43, 'Depleted DUCs', fontsize=11, transform=ax1.transAxes, color='k') ax2.text(0.75, 0.43, 'Potential Shortage', fontsize=11, transform=ax2.transAxes, color='k') ax2.text(0.74, 0.33, 'with Energy Supply', fontsize=11, transform=ax2.transAxes, color='k') fig.set_facecolor(\"white\") fig.tight_layout() 3.2. Recent trend: more production with less drilling Some readers may recall President Biden's request for the US oil and gas industry to ramp up drilling in order to curb the recent surge in gasoline prices. The industry reportedly hesitated to comply due to pressure from Wall Street for immediate financial returns. However, this is not the sole reason for the industry's reluctance to return to their previous drilling levels from 2014. Advances in technology (specifically horizontal drilling) have enabled the industry to produce more oil and gas with fewer wells, meaning they no longer need to drill as many wells as in the past to meet production demands. Figure 6 highlights three key points that demonstrate a clear trend. The decline in the number of drilling operations can be observed at [1], [2], and [3], while production steadily increases. A comparison to July 2018 [2] shows a 31% reduction in the number of newly drilled wells, but a 26% increase in oil production and a 35% increase in gas production. The contrast becomes even more striking when compared to September 2014 [1], where drilling activity decreased by 50% while production of both oil and gas nearly doubled. Figure 6: Recent trend in production and drilling activities and count of DUC wells. We've been producing more oil and gas with less drilling activities over time. Source Code For Figure (6) import pandas as pd import matplotlib.pyplot as plt import numpy as np import matplotlib.ticker as ticker ############################################# Basins ############################################## basins = sorted(['Anadarko', 'Appalachia', 'Eagle Ford', 'Haynesville', 'Niobrara', 'Permian', 'Bakken']) ################################### DUC Wells Count per Basin ##################################### # compile a DataFrame to store count of DUC wells per basin # source: EIA - https://www.eia.gov/petroleum/drilling/ dfs = [] for basin in basins: df_cur = pd.read_excel('https://aegis4048.github.io/downloads/notebooks/sample_data/Duc-EIA.xlsx', sheet_name=basin) df_cur['Basin'] = basin dfs.append(df_cur) df = pd.concat(dfs) df = df.sort_values('Date') df.index = pd.to_datetime(df['Date']) df_DUCs = df.groupby([pd.Grouper(freq='M'), 'Basin'])['DUC'].mean() df_DUCs = df_DUCs.unstack().fillna(0) df_DUCs = round(df_DUCs, 0) df_DUCs['Total DUCs'] = df_DUCs.T.apply(lambda x: sum(x)) df_drilled = df.groupby([pd.Grouper(freq='M'), 'Basin'])['Drilled'].sum() df_drilled = df_drilled.unstack().fillna(0) df_drilled['Total Drilled'] = df_drilled.T.apply(lambda x: sum(x)) df_completed = df.groupby([pd.Grouper(freq='M'), 'Basin'])['Completed'].sum() df_completed = df_completed.unstack().fillna(0) df_completed['Total Completed'] = df_completed.T.apply(lambda x: sum(x)) #################### Import All Data - Rig Count, Oil & Gas Prod per Basins ####################### dfs2 = [] for basin in basins: df_cur = pd.read_excel('https://aegis4048.github.io/downloads/notebooks/sample_data/dpr-data.xlsx', sheet_name=basin + ' Region', skiprows=1) df_cur['Basin'] = basin dfs2.append(df_cur) year_cutoff = '2013' # counting from year_cutoff + 1 date_header = 'Month' df2 = pd.concat(dfs2) df2 = df2.sort_values(date_header) df2.index = pd.to_datetime(df2[date_header]) df2.drop(df2.columns[[2, 3, 5, 6]], axis=1, inplace=True) df2.columns = ['Date', 'Rig Count', 'Total Oil Prod (BBLD)', 'Total Gas Prod (MCFD)', 'Basin'] df2 = df2[df2.index > year_cutoff + '-12-31'] # current year (2022) prediction parameter num_month_2022 = 6 # number of months in data set for current year pred_factor = 12 / num_month_2022 ################################ Cross Table for Gas Production ################################### # Total Gas Production df2_total_gas_prod = df2.groupby([pd.Grouper(freq='M'), 'Basin'])['Total Gas Prod (MCFD)'].sum() df2_total_gas_prod = df2_total_gas_prod.unstack().fillna(0) df2_total_gas_prod['Total Gas (BCFD)'] = df2_total_gas_prod.T.apply(lambda x: sum(x)) df2_total_gas_prod = df2_total_gas_prod / 1000000 ################################ Cross Table for Oil Production ################################### df2_total_oil_prod = df2.groupby([pd.Grouper(freq='M'), 'Basin'])['Total Oil Prod (BBLD)'].sum() df2_total_oil_prod = df2_total_oil_prod.unstack().fillna(0) df2_total_oil_prod['Total Oil (MBBLD)'] = df2_total_oil_prod.T.apply(lambda x: sum(x)) df2_total_oil_prod = df2_total_oil_prod / 1000 ############################################# Plot ############################################## axis_label_fontsize = 13 title_fontsize = 13 legend_fontsize = 11 fig, ax = plt.subplots(figsize=(9, 4.5)) ax3 = ax.twinx() ax4 = ax.twinx() ax.plot(df_drilled.index, df_drilled['Total Drilled'], color='k', label='Drilled') ax.plot(df_DUCs.index, df_DUCs['Total DUCs'], color='purple', label='DUC') ax3.plot(df2_total_oil_prod.index, df2_total_oil_prod['Total Oil (MBBLD)'], label='Oil', color='green') ax4.plot(df2_total_gas_prod.index, df2_total_gas_prod['Total Gas (BCFD)'], label='Gas', color='red') ax.set_ylim(0, 10000) ax3.set_ylim(0, 15000) ax4.set_ylim(0, 150) ax.set_ylabel('Drilled & DUC Well Counts', fontsize=axis_label_fontsize) ax3.tick_params(axis='y', colors='green') #ax3.spines['right'].set_position(('outward', 50)) ax3.spines['right'].set_color('green') ax3.set_ylabel('Total Oil Prod. (MBBLD)', color='green', fontsize=axis_label_fontsize) ax3.set_yticks(np.arange(0, 15001, 3000)) ax4.tick_params(axis='y', colors='red') ax4.spines['right'].set_position(('outward', 50)) ax4.spines['right'].set_color('red') ax4.set_ylabel('Total Gas Prod. (BCFD)', color='red', fontsize=axis_label_fontsize) ax4.set_yticks(np.arange(0, 151, 30)) ax.tick_params(axis='both', which='major', labelsize=11) ax.grid(axis='y', alpha=0.5) ax.yaxis.get_major_ticks()[5].gridline.set_visible(False) ax.spines.top.set_visible(False) ax3.spines.top.set_visible(False) ax4.spines.top.set_visible(False) h1, l1 = ax.get_legend_handles_labels() h3, l3 = ax3.get_legend_handles_labels() h4, l4 = ax4.get_legend_handles_labels() ax.legend(h1 + h3 + h4, l1 + l3 + l4, fontsize=legend_fontsize, ncol=4, loc='upper left', framealpha=1) ax.yaxis.set_major_formatter(ticker.EngFormatter()) ax3.yaxis.set_major_formatter(ticker.EngFormatter()) ax4.yaxis.set_major_formatter(ticker.EngFormatter()) def setbold(txt): return ' '.join([r\"$\\bf{\" + item + \"}$\" for item in txt.split(' ')]) ax.set_title(setbold('US Major Basins Well Counts & Production') + \", Jan 2014 - June 2022\", fontsize=title_fontsize, pad=10, x=0.37, y=1.06) ax.annotate('', xy=(-0.115, 1.07), xycoords='axes fraction', xytext=(1.2, 1.07), arrowprops=dict(arrowstyle=\"-\", color='k')) ax.annotate('Source: Pythonic Excursions, EIA', xy=(-0.11, -0.18), xycoords='axes fraction', fontsize=12) ax.text(0.145, 0.7, 'aegis4048.github.io', fontsize=10, ha='center', va='center', transform=ax.transAxes, color='grey', alpha=0.5) ax.axvline(x=df_drilled.index[8], color='k', linestyle='--', linewidth=1, alpha=0.7) ax.axvline(x=df_drilled.index[54], color='k', linestyle='--', linewidth=1, alpha=0.7) ax.axvline(x=df_drilled.index[-1], color='k', linestyle='--', linewidth=1, alpha=0.7) t1 = ax.text(0.1, 0.25, '[1]', fontsize=10, transform=ax.transAxes, color='k', ) t2 = ax.text(0.49, 0.25, '[2]', fontsize=10, transform=ax.transAxes, color='k') t3 = ax.text(0.94, 0.25, '[3]', fontsize=10, transform=ax.transAxes, color='k') t1.set_bbox(dict(facecolor='white', alpha=1, edgecolor='white', pad=1)) t2.set_bbox(dict(facecolor='white', alpha=1, edgecolor='white', pad=1)) t3.set_bbox(dict(facecolor='white', alpha=1, edgecolor='white', pad=1)) oil1 = str(int(df2_total_oil_prod['Total Oil (MBBLD)'][8])) + ' MBBLD' gas1 = str(int(df2_total_gas_prod['Total Gas (BCFD)'][8])) + ' BCFD' drill1 = str(int(df_drilled['Total Drilled'][8])) + ' drilled' ax.text(df_drilled.index[8 + 4], 2500, oil1, fontsize=9, color='green', bbox=dict(facecolor='none', edgecolor='none')) ax.text(df_drilled.index[8 + 4], 3000, gas1, fontsize=9, color='red', bbox=dict(facecolor='none', edgecolor='none')) ax.text(df_drilled.index[8 + 4], 2000, drill1, fontsize=9, color='k', bbox=dict(facecolor='none', edgecolor='none')) oil2 = str(int(df2_total_oil_prod['Total Oil (MBBLD)'][54])) + ' MBBLD' gas2 = str(int(df2_total_gas_prod['Total Gas (BCFD)'][54])) + ' BCFD' drill2 = str(int(df_drilled['Total Drilled'][54])) + ' drilled' ax.text(df_drilled.index[54 + 4], 2500, oil2, fontsize=9, color='green', bbox=dict(facecolor='none', edgecolor='none')) ax.text(df_drilled.index[54 + 4], 3000, gas2, fontsize=9, color='red', bbox=dict(facecolor='none', edgecolor='none')) ax.text(df_drilled.index[54 + 4], 2000, drill2, fontsize=9, color='k', bbox=dict(facecolor='none', edgecolor='none')) ha = 'right' oil3 = str(int(df2_total_oil_prod['Total Oil (MBBLD)'][-1])) + ' MBBLD' gas3 = str(int(df2_total_gas_prod['Total Gas (BCFD)'][-1])) + ' BCFD' drill3 = str(int(df_drilled['Total Drilled'][-1])) + ' drilled' ax.text(df_drilled.index[-1 - 4], 2500, oil3, fontsize=9, color='green', ha=ha, bbox=dict(facecolor='none', edgecolor='none')) ax.text(df_drilled.index[-1 - 4], 3000, gas3, fontsize=9, color='red', ha=ha, bbox=dict(facecolor='none', edgecolor='none')) ax.text(df_drilled.index[-1 - 4], 2000, drill3, fontsize=9, color='k', ha=ha, bbox=dict(facecolor='none', edgecolor='none')) fig.set_facecolor(\"white\") fig.tight_layout() The improvement in production efficiency despite the reduction in drilling can be attributed to the increasing use of horizontal wells. As compared to vertical wells, horizontal wells produce 2.5 to 7 times more, allowing the industry to meet demand with fewer drilling operations. As illustrated in Figure 7 , during economic downturns, the industry has favored horizontal wells by significantly reducing the number of vertical wells. It demonstrates a steady decrease in vertical wells, indicating that the majority of modern day drilling operations now consist of 91% horizontal wells and only 9% vertical wells. Figure 7: Historically the US oil and gas industry has turned into horizontal drilling during the times of economic crisis. Modern day drilling is mostly horizontal wells, which explains the higher production rates despite low drilling activities compared to 2014. Source Code For Figure (7) import pandas as pd import numpy as np import matplotlib.pyplot as plt import seaborn as sns from matplotlib.colors import LinearSegmentedColormap import matplotlib.ticker as ticker import warnings ############################################# Basins ############################################## basins = sorted(['Anadarko', 'Appalachia', 'Eagle Ford', 'Haynesville', 'Niobrara', 'Permian', 'Bakken']) ################################# Drilled Wells Count per Basin ################################### # import multiple csv files of basins and compile them into yearly count sum # source: DrillingInfo filename = 'https://aegis4048.github.io/downloads/notebooks/sample_data/Drill_Type_2004.xlsx' # suppress warnings warnings.filterwarnings('ignore') warnings.simplefilter('ignore') dfs = [] for basin in basins: df = pd.read_excel(filename, sheet_name=basin) df = df[df['Spud Date'].notna()] df['Basin'] = basin df['Year'] = df['Spud Date'].apply(lambda x: str(x).split('-')[0]) dfs.append(df) df_drilltype = pd.concat(dfs) df_drilltype = df_drilltype.sort_values('Spud Date') df_drilltype.index = pd.to_datetime(df_drilltype['Spud Date']) df_drilltype = df_drilltype.groupby(pd.Grouper(freq='M'))['Drill Type'].value_counts(sort=True) df_drilltype = df_drilltype.unstack().fillna(0).drop(['D', 'U'], axis=1) df_drilltype_percent = df_drilltype.apply(lambda x: round(x / x.sum() * 100, 2), axis=1) df_drilltype_percent = round(df_drilltype_percent, 1) ############################################# Plot ############################################## fig, ax = plt.subplots(figsize=(8, 4)) x = df_drilltype.index y = np.array([df_drilltype['H'].values, df_drilltype['V'].values]) ax.stackplot(x, *y, labels=['Horizontal', 'Vertical'], colors=['lightgrey', 'grey']) ax.set_ylabel('Drilled Wells', fontsize=13) ax.axvline(x=df_drilltype.index[-93], color='k', linestyle='--') ax.axvline(x=df_drilltype.index[-28], color='k', linestyle='--') ax.axvline(x=df_drilltype.index[-166], color='k', linestyle='--') ax.grid(axis='y', alpha=0.5) ax.yaxis.get_major_ticks()[6].gridline.set_visible(False) ax.spines.top.set_visible(False) ax.spines.right.set_visible(False) ax.legend(fontsize=11, ncol=3, loc='upper right') ax.tick_params(axis='both', which='major', labelsize=11) def setbold(txt): return ' '.join([r\"$\\bf{\" + item + \"}$\" for item in txt.split(' ')]) ax.set_title(setbold('Newly Drilled Wells Horizontal vs. Vertical Counts Trend') + \", Jan 2004 - June 2022\", fontsize=12, pad=10, x=0.41, y=1.06) ax.annotate('', xy=(-0.11, 1.07), xycoords='axes fraction', xytext=(1.02, 1.07), arrowprops=dict(arrowstyle=\"-\", color='k')) ax.annotate('Data Source: Pythonic Excursions, DrillingInfo', xy=(-0.11, -0.18), xycoords='axes fraction', fontsize=12) ax.text(0.16, 0.23, 'aegis4048.github.io', fontsize=10, ha='center', va='center', transform=ax.transAxes, color='white', alpha=0.5) ax.text(0.705, 0.63, '2014 Oil Price Crash', fontsize=11, ha='center', va='center', transform=ax.transAxes, color='k') ax.text(0.91, 0.55, 'Covid-19', fontsize=11, ha='center', va='center', transform=ax.transAxes, color='k') ax.text(0.375, 0.75, '2008 Recession', fontsize=11, ha='center', va='center', transform=ax.transAxes, color='k') fig.set_facecolor(\"white\") fig.tight_layout() ################################################################################################# fig, ax = plt.subplots(figsize=(8, 4)) ax.plot(df_drilltype_percent.index, df_drilltype_percent['H'], color='grey', label='Horizontal') ax.plot(df_drilltype_percent.index, df_drilltype_percent['V'], color='k', label='Vertical') ax.set_ylabel('Percent of Total', fontsize=13) ax.tick_params(axis='both', which='major', labelsize=11) ax.set_yticks(np.arange(0, 101, 10)) ax.grid(axis='y', alpha=0.5) ax.yaxis.get_major_ticks()[10].gridline.set_visible(False) ax.spines.top.set_visible(False) ax.spines.right.set_visible(False) ax.legend(fontsize=11, loc='center left') def setbold(txt): return ' '.join([r\"$\\bf{\" + item + \"}$\" for item in txt.split(' ')]) ax.set_title(setbold('Newly Drilled Wells Horizontal vs. Vertical Ratio Trend') + \", Jan 2004 - June 2022\", fontsize=12, pad=10, x=0.41, y=1.06) ax.annotate('', xy=(-0.11, 1.07), xycoords='axes fraction', xytext=(1.02, 1.07), arrowprops=dict(arrowstyle=\"-\", color='k')) ax.annotate('Data Source: Pythonic Excursions, DrillingInfo', xy=(-0.11, -0.18), xycoords='axes fraction', fontsize=12) ax.text(0.16, 0.23, 'aegis4048.github.io', fontsize=10, ha='center', va='center', transform=ax.transAxes, color='grey', alpha=0.5) ax.text(0.8, 0.43, '91% Horizontal', fontsize=12, ha='left', va='center', transform=ax.transAxes, color='k') ax.text(0.8, 0.34, '9% Vertical', fontsize=12, ha='left', va='center', transform=ax.transAxes, color='k') ax.text(0.8, 0.53, 'June 2022:', fontsize=12, ha='left', va='center', transform=ax.transAxes, color='k') fig.set_facecolor(\"white\") fig.tight_layout() 4. Continued discussion: \"Dead\" DUC wells DUC wells are considered to be a working oil field capital because the wells can be turned on line for production for cheap. However, the empirical evidences suggests that the DUC wells may no longer function as a working capital if completion is delayed for too long. The empirical evidence presented in Figure 8 reveals that 95% of the wells in Anadarko are completed within 309 days afterd drilled, and Appalachia wells within 956 days. Failing to complete within this time frame may give birth to \"dead\" DUCs - wells that will never be completed. The expensive drilling cost the operator paid would be wasted. Two main causes for the birth of dead DUCs include costs associated with extending oil and gas lease and degrading economics with increasing DUC time due to reservoir depletion. In another study conducted in my next article, I demonstrate and quantify the impact of delay in completion since drilled on production by normalizing EURs with lateral length and completion size. Check out my next article Quantifying The Impact Of Completion Delay Since Drilled On Production for more information. Figure 8: 95% Box plot of gap time between drilling and completion. The plot shows the time limit until completion for each basin. Each dot (very small, 99.5% transparency) represent one well of a basin. Upper whisker and the numeric annotation above it represent 95% upper limit. This means that wells with DUC time longer than the annotated number have less then 95% chance of ever being completed. Box plot for \"All Regions\" represent weighted average of all basins.","tags":"Oil and Gas","url":"https://aegis4048.github.io/duc_wells_and_their_economic_complications","loc":"https://aegis4048.github.io/duc_wells_and_their_economic_complications"},{"title":"Multiple Linear Regression and Visualization in Python","text":"import pandas as pd import numpy as np import matplotlib.pyplot as plt from sklearn import linear_model from mpl_toolkits.mplot3d import Axes3D ######################################## Data preparation ######################################### file = 'https://aegis4048.github.io/downloads/notebooks/sample_data/unconv_MV_v5.csv' df = pd.read_csv(file) X = df[['Por', 'Brittle']].values.reshape(-1,2) Y = df['Prod'] ######################## Prepare model data point for visualization ############################### x = X[:, 0] y = X[:, 1] z = Y x_pred = np.linspace(6, 24, 30) # range of porosity values y_pred = np.linspace(0, 100, 30) # range of brittleness values xx_pred, yy_pred = np.meshgrid(x_pred, y_pred) model_viz = np.array([xx_pred.flatten(), yy_pred.flatten()]).T ################################################ Train ############################################# ols = linear_model.LinearRegression() model = ols.fit(X, Y) predicted = model.predict(model_viz) ############################################## Evaluate ############################################ r2 = model.score(X, Y) ############################################## Plot ################################################ plt.style.use('default') fig = plt.figure(figsize=(9, 4)) ax1 = fig.add_subplot(121, projection='3d') ax2 = fig.add_subplot(122, projection='3d') axes = [ax1, ax2] for ax in axes: ax.plot(x, y, z, color='k', zorder=15, linestyle='none', marker='o', alpha=0.5) ax.scatter(xx_pred.flatten(), yy_pred.flatten(), predicted, facecolor=(0,0,0,0), s=20, edgecolor='#70b3f0') ax.set_xlabel('Porosity (%)', fontsize=12) ax.set_ylabel('Brittleness', fontsize=12) ax.set_zlabel('Gas Prod. (Mcf/day)', fontsize=12) ax.locator_params(nbins=4, axis='x') ax.locator_params(nbins=5, axis='x') ax1.view_init(elev=28, azim=120) ax2.view_init(elev=4, azim=114) fig.suptitle('3D multiple linear regression model', fontsize=20) fig.tight_layout() There are many advanced machine learning methods with robust prediction accuracy. While complex models may outperform simple models in predicting a response variable, simple models are better for understanding the impact & importance of each feature on a response variable. When the task at hand can be described by a linear model, linear regression triumphs over all other machine learning methods in feature interpretation due to its simplicity. This post attempts to help your understanding of linear regression in multi-dimensional feature space, model accuracy assessment, and provide code snippets for multiple linear regression in Python. Contents 0. Sample data description 1. Multiple linear regression Notes: Data encoding - regression with categorical variables Pythonic Tip: 2D regression with scikit-learn Pythonic Tip: Forcing zero y-intercept Pythonic Tip: 3D+ regression with scikit-learn 2. Introduction to multicollinearity 0. Sample data description We will use one sample data throughout this post. The sample data is relevant to the oil & gas industry. It is originally from Dr. Michael Pyrcz , petroleum engineering professor at the University of Texas at Austin. The original data can be found from his github repo . You can also use direct download , or directly access it using pandas url like below: In [1]: import pandas as pd file = 'https://aegis4048.github.io/downloads/notebooks/sample_data/unconv_MV_v5.csv' df = pd . read_csv ( file ) In [2]: df . head ( 10 ) Out[2]: Well Por Perm AI Brittle TOC VR Prod 0 1 12.08 2.92 2.80 81.40 1.16 2.31 4165.196191 1 2 12.38 3.53 3.22 46.17 0.89 1.88 3561.146205 2 3 14.02 2.59 4.01 72.80 0.89 2.72 4284.348574 3 4 17.67 6.75 2.63 39.81 1.08 1.88 5098.680869 4 5 17.52 4.57 3.18 10.94 1.51 1.90 3406.132832 5 6 14.53 4.81 2.69 53.60 0.94 1.67 4395.763259 6 7 13.49 3.60 2.93 63.71 0.80 1.85 4104.400989 7 8 11.58 3.03 3.25 53.00 0.69 1.93 3496.742701 8 9 12.52 2.72 2.43 65.77 0.95 1.98 4025.851153 9 10 13.25 3.94 3.71 66.20 1.14 2.65 4285.026122 Description of headers Well : well index Por : well average porosity (%) Perm : permeability (mD) AI : accoustic impedance (kg/m2s*10&#94;6) Brittle : brittleness ratio (%) TOC : total organic carbon (%) VR : vitrinite reflectance (%) Prod : gas production per day (MCFD) - Response Variable We have six features ( Por, Perm, AI, Brittle, TOC, VR ) to predict the response variable ( Prod ). Based on the permutation feature importances shown in figure (1) , Por is the most important feature, and Brittle is the second most important feature. Permutation feature ranking is out of the scope of this post, and will not be discussed in detail. Feature importances are obtained with rfpimp python library. For more information about permutation feature ranking, refer to this article: Beware Default Random Forest Importances Figure 1: Permutation feature ranking Source Code For Figure (1) import rfpimp import pandas as pd import numpy as np from sklearn.ensemble import RandomForestRegressor from sklearn.model_selection import train_test_split ######################################## Data preparation ######################################### file = 'https://aegis4048.github.io/downloads/notebooks/sample_data/unconv_MV_v5.csv' df = pd.read_csv(file) features = ['Por', 'Perm', 'AI', 'Brittle', 'TOC', 'VR', 'Prod'] ######################################## Train/test split ######################################### df_train, df_test = train_test_split(df, test_size=0.20) df_train = df_train[features] df_test = df_test[features] X_train, y_train = df_train.drop('Prod',axis=1), df_train['Prod'] X_test, y_test = df_test.drop('Prod',axis=1), df_test['Prod'] ################################################ Train ############################################# rf = RandomForestRegressor(n_estimators=100, n_jobs=-1) rf.fit(X_train, y_train) ############################### Permutation feature importance ##################################### imp = rfpimp.importances(rf, X_test, y_test) ############################################## Plot ################################################ fig, ax = plt.subplots(figsize=(6, 3)) ax.barh(imp.index, imp['Importance'], height=0.8, facecolor='grey', alpha=0.8, edgecolor='k') ax.set_xlabel('Importance score') ax.set_title('Permutation feature importance') ax.text(0.8, 0.15, 'aegis4048.github.io', fontsize=12, ha='center', va='center', transform=ax.transAxes, color='grey', alpha=0.5) plt.gca().invert_yaxis() fig.tight_layout() 1. Multiple linear regression Multiple linear regression model has the following structure: $$ y = \\beta_1 x_1 + \\beta_2 x_2 + \\cdots + \\beta_n x_n + \\beta_0\\tag{1}$$ where $y$ : response variable $n$ : number of features $x_n$ : $n$ -th feature $\\beta_n$ : regression coefficient (weight) of the $n$ -th feature $\\beta_0$ : y -intercept Bivarate linear regression model (that can be visualized in 2D space) is a simplification of eq (1) . Bivariate model has the following structure: $$ y = \\beta_1 x_1 + \\beta_0 \\tag{2}$$ A picture is worth a thousand words. Let's try to understand the properties of multiple linear regression models with visualizations. First, 2D bivariate linear regression model is visualized in figure (2) , using Por as a single feature. Although porosity is the most important feature regarding gas production, porosity alone captured only 74% of variance of the data. Figure 2: 2D Linear regression model Source Code For Figure (2) import pandas as pd import matplotlib.pyplot as plt from sklearn import linear_model ######################################## Data preparation ######################################### file = 'https://aegis4048.github.io/downloads/notebooks/sample_data/unconv_MV_v5.csv' df = pd.read_csv(file) X = df['Por'].values.reshape(-1,1) y = df['Prod'].values ################################################ Train ############################################# ols = linear_model.LinearRegression() model = ols.fit(X, y) response = model.predict(X) ############################################## Evaluate ############################################ r2 = model.score(X, y) ############################################## Plot ################################################ plt.style.use('default') plt.style.use('ggplot') fig, ax = plt.subplots(figsize=(8, 4)) ax.plot(X, response, color='k', label='Regression model') ax.scatter(X, y, edgecolor='k', facecolor='grey', alpha=0.7, label='Sample data') ax.set_ylabel('Gas production (Mcf/day)', fontsize=14) ax.set_xlabel('Porosity (%)', fontsize=14) ax.text(0.8, 0.1, 'aegis4048.github.io', fontsize=13, ha='center', va='center', transform=ax.transAxes, color='grey', alpha=0.5) ax.legend(facecolor='white', fontsize=11) ax.set_title('$R&#94;2= %.2f$' % r2, fontsize=18) fig.tight_layout() How would the model look like in 3D space? Let's take a look at figure (3) . Due to the 3D nature of the plot, multiple plots were generated from different angles. Two features ( Por and Brittle ) were used to predict the response variable Prod . With the help of the additional feature Brittle , the linear model experience significant gain in accuracy, now capturing 93% variability of data. Figure 3: 3D Linear regression model with strong features Source Code For Figure (3) import pandas as pd import numpy as np import matplotlib.pyplot as plt from sklearn import linear_model from mpl_toolkits.mplot3d import Axes3D ######################################## Data preparation ######################################### file = 'https://aegis4048.github.io/downloads/notebooks/sample_data/unconv_MV_v5.csv' df = pd.read_csv(file) X = df[['Por', 'Brittle']].values.reshape(-1,2) Y = df['Prod'] ######################## Prepare model data point for visualization ############################### x = X[:, 0] y = X[:, 1] z = Y x_pred = np.linspace(6, 24, 30) # range of porosity values y_pred = np.linspace(0, 100, 30) # range of brittleness values xx_pred, yy_pred = np.meshgrid(x_pred, y_pred) model_viz = np.array([xx_pred.flatten(), yy_pred.flatten()]).T ################################################ Train ############################################# ols = linear_model.LinearRegression() model = ols.fit(X, Y) predicted = model.predict(model_viz) ############################################## Evaluate ############################################ r2 = model.score(X, Y) ############################################## Plot ################################################ plt.style.use('default') fig = plt.figure(figsize=(12, 4)) ax1 = fig.add_subplot(131, projection='3d') ax2 = fig.add_subplot(132, projection='3d') ax3 = fig.add_subplot(133, projection='3d') axes = [ax1, ax2, ax3] for ax in axes: ax.plot(x, y, z, color='k', zorder=15, linestyle='none', marker='o', alpha=0.5) ax.scatter(xx_pred.flatten(), yy_pred.flatten(), predicted, facecolor=(0,0,0,0), s=20, edgecolor='#70b3f0') ax.set_xlabel('Porosity (%)', fontsize=12) ax.set_ylabel('Brittleness', fontsize=12) ax.set_zlabel('Gas Prod. (Mcf/day)', fontsize=12) ax.locator_params(nbins=4, axis='x') ax.locator_params(nbins=5, axis='x') ax1.text2D(0.2, 0.32, 'aegis4048.github.io', fontsize=13, ha='center', va='center', transform=ax1.transAxes, color='grey', alpha=0.5) ax2.text2D(0.3, 0.42, 'aegis4048.github.io', fontsize=13, ha='center', va='center', transform=ax2.transAxes, color='grey', alpha=0.5) ax3.text2D(0.85, 0.85, 'aegis4048.github.io', fontsize=13, ha='center', va='center', transform=ax3.transAxes, color='grey', alpha=0.5) ax1.view_init(elev=28, azim=120) ax2.view_init(elev=4, azim=114) ax3.view_init(elev=60, azim=165) fig.suptitle('$R&#94;2 = %.2f$' % r2, fontsize=20) fig.tight_layout() How would the 3D linear model look like if less powerful features are selected? Let's choose Por and VR as our new features and fit a linear model. In figure (4) below, we see that R-squared decreased compared to figure (3) above. The effect of decreased model performance can be visually observed by comparing their middle plots; the scatter plots in figure (3) are more densely populated around the 2D model plane than the scatter plots in figure (4) . Figure 4: 3D Linear regression model with weak features Source Code For Figure (4) import pandas as pd import numpy as np import matplotlib.pyplot as plt from sklearn import linear_model from mpl_toolkits.mplot3d import Axes3D ######################################## Data preparation ######################################### file = 'https://aegis4048.github.io/downloads/notebooks/sample_data/unconv_MV_v5.csv' df = pd.read_csv(file) X = df[['Por', 'VR']].values.reshape(-1,2) Y = df['Prod'] ######################## Prepare model data point for visualization ############################### x = X[:, 0] y = X[:, 1] z = Y x_pred = np.linspace(6, 24, 30) # range of porosity values y_pred = np.linspace(0.93, 2.9, 30) # range of VR values xx_pred, yy_pred = np.meshgrid(x_pred, y_pred) model_viz = np.array([xx_pred.flatten(), yy_pred.flatten()]).T ################################################ Train ############################################# ols = linear_model.LinearRegression() model = ols.fit(X, Y) predicted = model.predict(model_viz) ############################################## Evaluate ############################################ r2 = model.score(X, Y) ############################################## Plot ################################################ plt.style.use('default') fig = plt.figure(figsize=(12, 4)) ax1 = fig.add_subplot(131, projection='3d') ax2 = fig.add_subplot(132, projection='3d') ax3 = fig.add_subplot(133, projection='3d') axes = [ax1, ax2, ax3] for ax in axes: ax.plot(x, y, z, color='k', zorder=15, linestyle='none', marker='o', alpha=0.5) ax.scatter(xx_pred.flatten(), yy_pred.flatten(), predicted, facecolor=(0,0,0,0), s=20, edgecolor='#70b3f0') ax.set_xlabel('Porosity (%)', fontsize=12) ax.set_ylabel('VR', fontsize=12) ax.set_zlabel('Gas Prod. (Mcf/day)', fontsize=12) ax.locator_params(nbins=4, axis='x') ax.locator_params(nbins=5, axis='x') ax1.text2D(0.2, 0.32, 'aegis4048.github.io', fontsize=13, ha='center', va='center', transform=ax1.transAxes, color='grey', alpha=0.5) ax2.text2D(0.3, 0.42, 'aegis4048.github.io', fontsize=13, ha='center', va='center', transform=ax2.transAxes, color='grey', alpha=0.5) ax3.text2D(0.85, 0.85, 'aegis4048.github.io', fontsize=13, ha='center', va='center', transform=ax3.transAxes, color='grey', alpha=0.5) ax1.view_init(elev=27, azim=112) ax2.view_init(elev=16, azim=-51) ax3.view_init(elev=60, azim=165) fig.suptitle('$R&#94;2 = %.2f$' % r2, fontsize=20) fig.tight_layout() The full-rotation view of linear models are constructed below in a form of gif. Notice that the blue plane is always projected linearly, no matter of the angle. This is the reason that we call this a multiple \"LINEAR\" regression model. The model will always be linear, no matter of the dimensionality of your features. When you have more than 3 features, the model will be very difficult to be visualized, but you can expect that high dimensional linear models will also exhibit linear trend within their feature space. Figure 5: Porosity and Brittleness Linear model GIF Figure 6: Porosity and VR Linear model GIF The gif was generated by creating 360 different plots viewed from different angles with the following code snippet, and combined into a single gif from imgflip . In [ ]: for ii in np . arange ( 0 , 360 , 1 ): ax . view_init ( elev = 32 , azim = ii ) fig . savefig ( 'gif_image %d .png' % ii ) Notes: Data encoding - regression with categorical variables Regression requires features to be continuous. What happens if you have categorical features that are important? Do you have to ignore categorical variables, and run regression only with continuous variables? We can encode categorical variables into numerical variables to avoid this issue. Take a look at the below figure. The feature level was originally a categorial variable with three categories of ordinality. This means that there are hierarchy among the categories (ex: low < medium < high), and that their encoding needs to capture their ordinality. It is achieved by converting them in to 1, 2, and 3. This kind of encoding is called integer encoding What if there are no ordinality among the categories of a feature? Then we use a technique called one-hot encoding to prevent a model from assuming natural ordering among categories that may suffer from model bias. Instead of using integer variables, we use binary variables. 1 indicates that the sample data falls into the specified category, while 0 indicates the otherwise. One-hot encoding is used in almost all natural languages problems, because vocabularies do not have ordinal relationships among themselves. Pythonic Tip: 2D linear regression with scikit-learn Linear regression is implemented in scikit-learn with sklearn.linear_model (check the documentation ). For code demonstration, we will use the same oil & gas data set described in Section 0: Sample data description above . Data prepration First, import modules and data. We will use a single feature: Por . Response variable is Prod . In [8]: import pandas as pd from sklearn import linear_model import numpy as np file = 'https://aegis4048.github.io/downloads/notebooks/sample_data/unconv_MV_v5.csv' df = pd . read_csv ( file ) Preprocess If you get error messages like ValueError: Expected 2D array, got 1D array instead: ... , its the issue of preprocessing. Most scikit-learn training functions require reshape of features, such as reshape(-1, len(features)) . In case you import data from Pandas dataframe, the first argument is always -1 , and the second argument is the number of features, in a form of an integer. This preprocessing will also be required when you make predictions based on the fitted model later. Check the shape of your features and response variable if you are experiencing errors. In [9]: import numpy as np features = [ 'Por' ] target = 'Prod' X = df [ features ] . values . reshape ( - 1 , len ( features )) y = df [ target ] . values In [12]: print ( X . shape ) print ( y . shape ) (200, 1) (200,) Fit linear model Since we have only one feature, the linear model we want to fit has the following structure: $$ \\text{Gas Prod.} = \\beta_1 \\cdot \\text{Por} + \\beta_0 \\tag{3}$$ Let's find out the values of $\\beta_1$ (regression coefficient) and $\\beta_2$ (y-intercept). Just like many other scikit-learn libraries, you instantiate the training model object with linear_model.LinearRegression() , and than fit the model with the feature X and the response variable y . Note that ols stands for Ordinary Least Squares. In [42]: from sklearn import linear_model ols = linear_model . LinearRegression () model = ols . fit ( X , y ) The linear regression coefficient can be accessed in a form of class attribute with model.coef_ In [43]: model . coef_ Out[43]: array([287.78074285]) The y-intercept can be accessed in a form of class attribute with model.intercept_ In [44]: model . intercept_ Out[44]: -2.9444310537137426 Based on the result of the fit, we conclude that the gas production can be predicted from porosity, with the following linear model: $$ \\text{Gas Prod.} = 287.78 \\cdot \\text{Por} - 2.94 \\tag{4}$$ Accuracy assessment: $R&#94;2$ How good was your model? You can evaluate your model performance in a form of R-squared, with model.score(X, y) . X is the features, and y is the response variable used to fit the model. In [45]: model . score ( X , y ) Out[45]: 0.7428880535051594 Make future prediction Scikit-learn supports making predictions based on the fitted model with model.predict(X) method. X is a feature that requires preprocessing explained above . Recall that we used only one feature, and that len(features) = 1 . Let's say that you want to predict gas production when porosity is 15%. Then: In [46]: x_pred = np . array ([ 15 ]) x_pred = x_pred . reshape ( - 1 , len ( features )) # preprocessing required by scikit-learn functions In [47]: model . predict ( x_pred ) Out[47]: array([4313.76671169]) According to the model, gas production = 4313 Mcf/day when porosity = 15%. What if we want predict the response variable from multiple instances of a feature? Let's try porosity 14% and 18%. Then: In [48]: x_pred = np . array ([ 14 , 18 ]) x_pred = x_pred . reshape ( - 1 , len ( features )) # preprocessing required by scikit-learn functions In [49]: model . predict ( x_pred ) Out[49]: array([4025.98596884, 5177.10894024]) We can extend on this, and draw a prediction line for all possible values of the feature. Reasonable real-life values of rock porosity ranges between $[0, 40]$ . In [50]: x_pred = np . linspace ( 0 , 40 , 200 ) # 200 data points between 0 ~ 40 x_pred = x_pred . reshape ( - 1 , len ( features )) # preprocessing required by scikit-learn functions y_pred = model . predict ( x_pred ) In [52]: import matplotlib.pyplot as plt plt . style . use ( 'default' ) plt . style . use ( 'ggplot' ) fig , ax = plt . subplots ( figsize = ( 7 , 3.5 )) ax . plot ( x_pred , y_pred , color = 'k' , label = 'Regression model' ) ax . scatter ( X , y , edgecolor = 'k' , facecolor = 'grey' , alpha = 0.7 , label = 'Sample data' ) ax . set_ylabel ( 'Gas production (Mcf/day)' , fontsize = 14 ) ax . set_xlabel ( 'Porosity (%)' , fontsize = 14 ) ax . legend ( facecolor = 'white' , fontsize = 11 ) ax . text ( 0.55 , 0.15 , '$y = %.2f x_1 - %.2f $' % ( model . coef_ [ 0 ], abs ( model . intercept_ )), fontsize = 17 , transform = ax . transAxes ) fig . tight_layout () WARNING! Be careful when predicting a point outside the observed range of feautures. The relationship among variables may change as you move outside the observed range, but you never know because you don't have the data. The observed relationship may be locally linear, but there may be unobserved curves on the outside range of your data. Pythonic Tip: Forcing zero y-intercept Sometimes you want to force y-intercept = 0. This can be done by setting fit_intercept=False when instantiating the linear regression model class. Printing the model y-intercept will output 0.0 . But be aware, \"generally it is essential to include the constant in a regression model\", because \"the constant (y-intercept) absorbs the bias for the regression model\", as Jim Frost says in his post . In [53]: ols = linear_model . LinearRegression ( fit_intercept = False ) model = ols . fit ( X , y ) In [54]: model . intercept_ Out[54]: 0.0 In figure (7) , I generated some synthetic data below to illustrate the effect of forcing zero y-intercept. Forcing a zero y-intercept can be both desirable or undesirable. If you have a reason to believe that y-intercept must be zero, set fit_intercept=False . Figure 7: Effect of forcing zero y-intercept Source Code For Figure (7) import pandas as pd import numpy as np import matplotlib.pyplot as plt from sklearn import linear_model df = pd.read_csv('https://aegis4048.github.io/downloads/notebooks/sample_data/linear_data.csv') features = ['X'] target = 'Y' X = df[features].values.reshape(-1, len(features)) y = df[target].values x_pred = np.linspace(0, 40, 200).reshape(-1, len(features)) # prediction line ##################################### Fit linear model #################################### ols_1 = linear_model.LinearRegression() model_1 = ols_1.fit(X, y) response_1 = model_1.predict(x_pred) ################################# Force zero y-intercept ################################## ols_2 = linear_model.LinearRegression(fit_intercept=False) model_2 = ols_2.fit(X, y) response_2 = model_2.predict(x_pred) ########################################## Plot ########################################### plt.style.use('default') plt.style.use('ggplot') fig, axes = plt.subplots(1, 2, figsize=(10, 4)) fig.suptitle('Effect of enforcing zero y-intercept', fontsize=15) axes[0].plot(x_pred, response_1, color='k', label='Regression model') axes[0].scatter(X, y, edgecolor='k', facecolor='grey', alpha=0.7, label='Sample data') axes[0].set_ylabel('Y', fontsize=14) axes[0].set_xlabel('X', fontsize=14) axes[0].legend(facecolor='white', fontsize=11, loc='best') axes[0].set_ylim(0, 1600) axes[0].set_xlim(0, 0.05) axes[0].text(0.47, 0.15, '$y = %.1f x_1 + %.1f $' % (model_1.coef_[0], model_1.intercept_), fontsize=12, transform=axes[0].transAxes) axes[0].text(0.77, 0.3, 'aegis4048.github.io', fontsize=10, ha='center', va='center', transform=axes[0].transAxes, color='grey', alpha=0.5) axes[1].plot(x_pred, response_2, color='k', label='Regression model') axes[1].scatter(X, y, edgecolor='k', facecolor='grey', alpha=0.7, label='Sample data') axes[1].set_ylabel('Y', fontsize=14) axes[1].set_xlabel('X', fontsize=14) axes[1].legend(facecolor='white', fontsize=11, loc='best') axes[1].set_ylim(0, 1600) axes[1].set_xlim(0, 0.05) axes[1].text(0.55, 0.15, '$y = %.1f x_1 + %.1f $' % (model_2.coef_[0], model_2.intercept_), fontsize=12, transform=axes[1].transAxes) axes[1].text(0.77, 0.3, 'aegis4048.github.io', fontsize=10, ha='center', va='center', transform=axes[1].transAxes, color='grey', alpha=0.5) fig.tight_layout(rect=[0, 0, 1, 0.94]) Pythonic Tip: 3D+ linear regression with scikit-learn Fit multi-linear model Let's fit the model with four features: Por , Brittle , Perm , and TOC . Then the model of our interest has the following structure: $$ \\text{Gas Prod.} = \\beta_1 \\cdot \\text{Por} + \\beta_2 \\cdot \\text{Brittle} + \\beta_3 \\cdot \\text{Perm} + \\beta_4 \\cdot \\text{TOC} + \\beta_0 \\tag{5}$$ With scikit-learn, fitting 3D+ linear regression is no different from 2D linear regression, other than declaring multiple features in the beginning. The rest is exactly the same. We will declare four features: features = ['Por', 'Brittle', 'Perm', 'TOC']. In [16]: import pandas as pd import numpy as np from sklearn import linear_model file = 'https://aegis4048.github.io/downloads/notebooks/sample_data/unconv_MV_v5.csv' df = pd . read_csv ( file ) features = [ 'Por' , 'Brittle' , 'Perm' , 'TOC' ] target = 'Prod' X = df [ features ] . values . reshape ( - 1 , len ( features )) y = df [ target ] . values ols = linear_model . LinearRegression () model = ols . fit ( X , y ) In [17]: model . coef_ Out[17]: array([244.60011793, 31.58801063, 86.87367291, 325.19354135]) In [15]: model . intercept_ Out[15]: -1616.4561900851832 Based on the result of the fit, we obtain the following linear regression model: $$ \\text{Gas Prod.} = 244.6 \\cdot \\text{Por} + 31.6 \\cdot \\text{Brittle} + 86.9 \\cdot \\text{Perm} + 325.2 \\cdot \\text{TOC} - 1616.5 \\tag{6}$$ Accuracy assessment: $R&#94;2$ In the same we evaluated model performance with 2D linear model above , we can evaluate the 3D+ model performance with R-squared with model.score(X, y) . X is the features, and y is the response variable used to fit the model. In [106]: model . score ( X , y ) Out[106]: 0.9452003827311295 Make future prediction Let's make one prediction of gas production rate when: Por = 12 (%) Brittle = 81 (%) VR = 2.31 (%) AI = 2.8 (kg/m2s*10&#94;6) In [107]: x_pred = np . array ([ 12 , 81 , 2.31 , 2.8 ]) x_pred = x_pred . reshape ( - 1 , len ( features )) In [108]: model . predict ( x_pred ) Out[108]: array([4555.02177976]) This time, let's make two predictions of gas production rate when: Por = 12 (%) Brittle = 81 (%) VR = 2.31 (%) AI = 2.8 (kg/m2s*10&#94;6) Por = 15 (%) Brittle = 60 (%) VR = 2.5 (%) AI = 1 (kg/m2s*10&#94;6) In [109]: x_pred = np . array ([[ 12 , 81 , 2.31 , 2.8 ], [ 15 , 60 , 2.5 , 1 ]]) x_pred = x_pred . reshape ( - 1 , len ( features )) In [100]: model . predict ( x_pred ) Out[100]: array([4555.02177976, 5312.03205244]) 2. Introduction to multicollinearity While an accuracy of a multi-linear model in predicting a response variable may be reliable, the value of individual regression coefficient may not be reliable under multicollinearity. Note that the value of regression coefficient for porosity in eq (4) is 287.7, while it is 244.6 in eq (6) . In figure (8) , I simulated multiple model fits with different combinations of features to show the fluctuating regression coefficient values, even when the R-squared value is high. Figure 8: Unstable regression coefficients due to multicollinearity Source Code For Figure (8) import pandas as pd import numpy as np from sklearn import linear_model import matplotlib.pyplot as plt file = 'https://aegis4048.github.io/downloads/notebooks/sample_data/unconv_MV_v5.csv' df = pd.read_csv(file) ######################################################################################## features = ['Por', 'Brittle', 'Perm', 'TOC', 'AI', 'VR'] target = 'Prod' X = df[features].values.reshape(-1, len(features)) y = df[target].values ols = linear_model.LinearRegression() model = ols.fit(X, y) print('Features : %s' % features) print('Regression Coefficients : ', [round(item, 2) for item in model.coef_]) print('R-squared : %.2f' % model.score(X, y)) print('Y-intercept : %.2f' % model.intercept_) print('') ######################################################################################## features = ['Por', 'Brittle', 'Perm', 'TOC', 'VR'] target = 'Prod' X = df[features].values.reshape(-1, len(features)) y = df[target].values ols = linear_model.LinearRegression() model = ols.fit(X, y) print('Features : %s' % features) print('Regression Coefficients : ', [round(item, 2) for item in model.coef_]) print('R-squared : %.2f' % model.score(X, y)) print('Y-intercept : %.2f' % model.intercept_) print('') ######################################################################################## features = ['Por', 'Brittle', 'Perm', 'TOC', 'AI'] target = 'Prod' X = df[features].values.reshape(-1, len(features)) y = df[target].values ols = linear_model.LinearRegression() model = ols.fit(X, y) print('Features : %s' % features) print('Regression Coefficients : ', [round(item, 2) for item in model.coef_]) print('R-squared : %.2f' % model.score(X, y)) print('Y-intercept : %.2f' % model.intercept_) print('') ######################################################################################## features = ['Por', 'Brittle', 'Perm', 'TOC'] target = 'Prod' X = df[features].values.reshape(-1, len(features)) y = df[target].values ols = linear_model.LinearRegression() model = ols.fit(X, y) print('Features : %s' % features) print('Regression Coefficients : ', [round(item, 2) for item in model.coef_]) print('R-squared : %.2f' % model.score(X, y)) print('Y-intercept : %.2f' % model.intercept_) print('') ######################################################################################## features = ['Por', 'Brittle', 'Perm', 'AI'] target = 'Prod' X = df[features].values.reshape(-1, len(features)) y = df[target].values ols = linear_model.LinearRegression() model = ols.fit(X, y) print('Features : %s' % features) print('Regression Coefficients : ', [round(item, 2) for item in model.coef_]) print('R-squared : %.2f' % model.score(X, y)) print('Y-intercept : %.2f' % model.intercept_) print('') ######################################################################################## features = ['Por', 'Brittle', 'Perm', 'VR'] target = 'Prod' X = df[features].values.reshape(-1, len(features)) y = df[target].values ols = linear_model.LinearRegression() model = ols.fit(X, y) print('Features : %s' % features) print('Regression Coefficients : ', [round(item, 2) for item in model.coef_]) print('R-squared : %.2f' % model.score(X, y)) print('Y-intercept : %.2f' % model.intercept_) print('') ######################################################################################## features = ['Por', 'Brittle', 'TOC', 'VR'] target = 'Prod' X = df[features].values.reshape(-1, len(features)) y = df[target].values ols = linear_model.LinearRegression() model = ols.fit(X, y) print('Features : %s' % features) print('Regression Coefficients : ', [round(item, 2) for item in model.coef_]) print('R-squared : %.2f' % model.score(X, y)) print('Y-intercept : %.2f' % model.intercept_) print('') ######################################################################################## features = ['Por', 'Brittle', 'TOC'] target = 'Prod' X = df[features].values.reshape(-1, len(features)) y = df[target].values ols = linear_model.LinearRegression() model = ols.fit(X, y) print('Features : %s' % features) print('Regression Coefficients : ', [round(item, 2) for item in model.coef_]) print('R-squared : %.2f' % model.score(X, y)) print('Y-intercept : %.2f' % model.intercept_) print('') ######################################################################################## features = ['Por', 'Brittle', 'VR'] target = 'Prod' X = df[features].values.reshape(-1, len(features)) y = df[target].values ols = linear_model.LinearRegression() model = ols.fit(X, y) print('Features : %s' % features) print('Regression Coefficients : ', [round(item, 2) for item in model.coef_]) print('R-squared : %.2f' % model.score(X, y)) print('Y-intercept : %.2f' % model.intercept_) print('') ######################################################################################## features = ['Por', 'Brittle', 'AI'] target = 'Prod' X = df[features].values.reshape(-1, len(features)) y = df[target].values ols = linear_model.LinearRegression() model = ols.fit(X, y) print('Features : %s' % features) print('Regression Coefficients : ', [round(item, 2) for item in model.coef_]) print('R-squared : %.2f' % model.score(X, y)) print('Y-intercept : %.2f' % model.intercept_) print('') ######################################################################################## features = ['Por', 'Brittle'] target = 'Prod' X = df[features].values.reshape(-1, len(features)) y = df[target].values ols = linear_model.LinearRegression() model = ols.fit(X, y) print('Features : %s' % features) print('Regression Coefficients : ', [round(item, 2) for item in model.coef_]) print('R-squared : %.2f' % model.score(X, y)) print('Y-intercept : %.2f' % model.intercept_) The simulation result tells us that even if the model is good at predicting the response variable given features (high R-squared), linear model is not robust enough to fully understand the effect of individual features on the response variable. In such circumstance, we can't trust the values of regression coefficients. Where is this instability coming from? This is because the Por , TOC , and Perm shows strong linear correlation with one another, as shown in the below spearnman's correlation matrix in figure (9) . Figure 9: Correlation matrix of features Source Code For Figure (9) import numpy as np import pandas as pd import seaborn as sns import matplotlib.pyplot as plt file = 'https://aegis4048.github.io/downloads/notebooks/sample_data/unconv_MV_v5.csv' df = pd.read_csv(file) df = df.iloc[:, 1:-1] corr = df.corr(method='spearman') # Generate a mask for the upper triangle mask = np.zeros_like(corr, dtype=np.bool) mask[np.triu_indices_from(mask)] = True # Set up the matplotlib figure fig, ax = plt.subplots(figsize=(6, 5)) # Generate a custom diverging colormap cmap = sns.diverging_palette(220, 10, as_cmap=True, sep=100) # Draw the heatmap with the mask and correct aspect ratio sns.heatmap(corr, mask=mask, cmap=cmap, vmin=-1, vmax=1, center=0, linewidths=.5) fig.suptitle('Correlation matrix of features', fontsize=15) ax.text(0.77, 0.2, 'aegis4048.github.io', fontsize=13, ha='center', va='center', transform=ax.transAxes, color='grey', alpha=0.5) fig.tight_layout() When more than two features are used for prediction, you must consider the possibility of each features interacting with one another. For thought experiment, think of two features $x_1$ and $x_2$, and a response variable $y$. Assume that $x_1$ is positively related to $y$. In the other words, increasing $x_1$ increases $y$, and decreasing $x_1$ also decreases $y$. $x_2$ is negatively related to $y$. There is a positive correlation between $x_1$ and $x_2$. Under this sitution, when you increase $x_1$, you expect to increase the value of $y$ because of the positive relationship between $x_1$ and $y$, but this is not always true because increasing $x_1$ also increases $x_2$, which in turn decreases $y$ . The situation in which features are correlated with one another is called muticollinearity . Under multicollinearity, the values of individual regression coefficients are unreliable, and the impact of individual features on a response variable is obfuscated. However, prediction on a response variable is still reliable. When using linear regression coefficients to make business decisions, you must remove the effect of multicollinearity to obtain reliable regression coefficients. Let's say that you are doing a medical research on cervical cancer. You trained a linear regression model with patients' survival rate with respect to many features, in which water consumption being one of them. Your linear regression coefficient for water consumption reports that if a patient increases water consumption by 1.5 L everyday, his survival rate will increase by 2%. Can you trust this analysis? The answer is yes, if there is no sign of multicollinearity. You can actually tell the patient, with confidence, that he must drink more water to increase his chance of survival. However, if there is a sign of multicollinearity, this analysis is not valid. Note that multicollinearity is not restricted on 1 vs 1 relationship. Even if there is minimum 1 vs 1 correlation among features, three or more features together may show multicollinearity. Also note that multicollinearity does not affect prediction accuracy . While the values of individual coefficients may be unreliable, it does not undermine the prediction power of the model. Multicollinearity is an issue only when you want to study the impact of individual features on a response variable. The details of detection & remedies of mutlicollinearity is not discussed here (though I plan to write about it very soon). While the focus of this post is only on multiple linear regression itself, I still wanted to grab your attention as to why you should not always trust your regression coefficients.","tags":"Machine Learning","url":"https://aegis4048.github.io/mutiple_linear_regression_and_visualization_in_python","loc":"https://aegis4048.github.io/mutiple_linear_regression_and_visualization_in_python"},{"title":"Comprehensive Confidence Intervals for Python Developers","text":"Confidence interval is uncertainty in summary statistic represented as a range. In the other words, it is a range of values we are fairly sure our true value lies in. For example: I am 95% confident that the population mean falls between 8.76 and 15.88 $\\rightarrow$ (12.32 $\\pm$ 3.56) Confidence interval tells you how confident you can be that the results from a poll or survey reflect what you would expect to find if it were possible to survey the entire population. It is difficult to obtain measurement data of an entire data set ( population ) due to limited resource & time. Your best shot is to survey a small fraction ( samples ) of the entire data set, and pray that your sample data represents the population reasonably well. Sample data may not be a good representation of a population by numerous factors (Ex: bias), and as a result, uncertainty is always introduced in any estimations derived from sample data. Due to the uncertainty involved with sample data, any statistical estimation needs to be delivered in a range, not in a point estimate . How well a sample statistic estimates an underlying population parameter is always an issue ( Population vs. Samples ). A confidence interval addresses this issue by providing a range of values, which is likely to contain the population parameter of interest within the range of uncertainty. Contents 1. Understanding confidence interval with analogy Example 1: Uncertainty in rock porosity Example 2: Purity of methamphetamine (crystal) in Breaking Bad Example 3: Uncertainty in oil production forecast 2. Key takeaways 3. Population vs Samples Notes: Population variance $\\sigma&#94;2$ vs. Sample variance $s&#94;2$ Pythonic Tip: Difference between Numpy variance and Pandas variance 4. Confidence interval of normal distribution 4.1. Confidence interval of mean Notes: Distribution of various statistics Notes: z-score vs t-score Pythonic Tip: Computing confidence interval of mean with SciPy 4.2. Confidence interval of difference in mean Notes: Comparing means of more than two samples with ANOVA 4.2.1. Independent (unpaired) samples, equal variance - Student's t-interval Pythonic Tip: Computing student's t-interval 4.2.2. Independent (unpaired) samples, unequal variance - Welch's t-interval Pythonic Tip: Computing Welch's t-interval 4.2.3. Dependent (paired) samples - Paired t-interval Pythonic Tip: Computing paired t-interval Notes: Deciding which t-test to use 4.3. Confidence interval of variance Notes: Chi-square $\\chi&#94;2$ distribution Notes: One-tail vs two-tail Pythonic Tip: Computing confidence interval of variance with SciPy 4.4. Confidence interval of other statistics: Bootstrap Notes: Monte-Carlo method Pythonic Tip: Bootstrapping in Python 5. Confidence interval of non-normal distribution 5.1. Problems of non-normal distributions and central tendency Notes: Cauchy distribution 5.2. Robustness of confidence intervals to non-normality Notes: Be cautious with hypothesis testing for normality Pythonic Tip: Q-Q plots with SciPy 5.3. Transform to normal distribution: Box-Cox Pythonic Tip: Box-Cox transform with SciPy and Scikit-Learn 5.4. Non-parametric alternative: Bootstrap 1. Understanding confidence interval with analogy If you've taken a science class with lab reports in your highschool or college, you probably had to include measurement error in your lab reports. For example, if you were asked to measure the length of a paper clip with a ruler, you have to include $\\pm0.5 \\,\\text{cm}$ or $\\pm0.05\\,\\text{cm}$ (depending on the spacing of tick marks) to account for the measurement error that shows the precision of your measuring tool. Based on figure (1) , the paper clip seems to be about 2.7 cm long, but we don't know for sure because the tickmarks in the ruler is not precise enough to measure decimal length. However, I can tell with 100% confidence that the paper clip has a length between 2 ~ 3 cm, because the clip is between the 2 cm and 3 cm tickmarks. You record the length of the paper clip in a range , instead of a point estimate , to account for the uncertainty introduced by the limitation of the measuring tool. Figure 1: Measurement error in ruler Similar idea can be applied to a confidence interval of mean . You want to obtain a mean of a whole data set ( population ), but you can measure values of only a small fraction ( samples ) of the whole data set. This boils down to the traditional issue of Population vs Samples , due to the cost of obtaining measurement data of a large data set. Uncertainty is introduced in your samples, because you don't know if your samples are 100% representative of the population, free of bias. Therefore, you deliver your conclusion in a range, not in a point estimate, to account for the uncertainty. Example 1: Uncertainty in rock porosity (Borrowed from Dr. Michael Pyrcz's Geostatistics class) A reservoir engineer in the oil & gas industry wants to know the rock porosity of a formation to estimate the total oil reserve 9,500 ft underground. Due to the high cost of obtaining rock core samples from the deep formations, he could acquire only 12 rock core samples. Since the uncertainty of a point estimation scales inversely with a sample size, his estimation is subject to non-negligible uncertainty. He obtains 14.5% average rock porosity with 4.3% standard deviation. Executives in the company wants to know the worst-case scenario (P10) and the best-case scenario (P90) to make business decisions. You can convey your estimation of average porosity with uncertainty by constructing the confidence interval of mean . Assuming that you have a reason to believe that the rock porosity follows normal distribution, you can construct its 80% confidence interval, with the procedure described below : In [133]: stats . t . interval ( 1 - 0.2 , 12 - 1 , loc = 14.5 , scale = 4.3 / np . sqrt ( 12 )) Out[133]: (12.807569748569543, 16.19243025143046) The above range of uncertainty was acquired from the 12 rock core samples. In the worst-case scenario, the rock formation at 9,500 ft underground has 12.8% porosity. In the best-case scenario, the oil reservoir has 16.2% porosity. The same procedures can be applied for the core samples collected at different depths, which give us the confidence interval plot of rock porosities shown in figure (2) . Figure 2: Confidence interval of core samples porosities along depths Source Code For Figure (2) import numpy as np from scipy import stats import matplotlib.pyplot as plt np.random.seed(39) depth = [i * 10 + 8000 for i in range(100)] l = len(depth) avg_por = [] p10_por = [] p90_por = [] for i, item in enumerate(depth): # You collect 12 rock core samples for each depth # Assume that sample porosity follows a normal distribution sample_size = 12 por_samples = np.random.normal(loc=0.15 - i/2000, scale=0.022, size=sample_size) avg_por.append(np.mean(por_samples)) # 80% confidence interval of mean p10, p90 = stats.t.interval(1 - 0.2, sample_size - 1, loc=np.mean(por_samples), scale=stats.sem(por_samples)) p10_por.append(p10) p90_por.append(p90) # plotting plt.style.use('seaborn-whitegrid') fig, ax = plt.subplots(1, 2, figsize=(8, 4)) ax[0].plot(avg_por[:l//2], depth[:l//2], 'k', label='P50', alpha=0.8) ax[0].plot(p10_por[:l//2], depth[:l//2], 'grey', linewidth=0.7, label='P10', linestyle='--') ax[0].plot(p90_por[:l//2], depth[:l//2], 'grey', linewidth=0.7, label='P90') ax[0].set_xlim(0.08, 0.17) ax[0].set_ylabel('Depth (ft)', fontsize=15) ax[0].set_xlabel('Porosity', fontsize=15) ax[0].fill_betweenx(depth[:l//2], p10_por[:l//2], p90_por[:l//2], facecolor='lightgrey', alpha=0.3) ax[0].invert_yaxis() ax[1].plot(avg_por[l//2:], depth[l//2:], 'k', label='P50', alpha=0.8) ax[1].plot(p10_por[l//2:], depth[l//2:], 'grey', linewidth=0.7, label='P10', linestyle='--') ax[1].plot(p90_por[l//2:], depth[l//2:], 'grey', linewidth=0.7, label='P90') ax[1].set_xlim(0.08, 0.17) ax[1].set_xlabel('Porosity', fontsize=15) ax[1].legend(loc='best', fontsize=14, framealpha=1, frameon=True) ax[1].fill_betweenx(depth[l//2:], p10_por[l//2:], p90_por[l//2:], facecolor='lightgrey', alpha=0.3) ax[1].invert_yaxis() Example 2: Purity of methamphetamine (crystal) in Breaking Bad 21 batches of crystal cooked by Mr. White shows 99.1% average purity with 3% standard deviation. 18 batches of crystal cooked by Mr. Pinkman shows 96.2% average purity with 4% standard deviation. Does Mr. White always cook better crystal than Mr. Pinkman, or is it possible for Mr. Pinkman to beat Mr. White in purity of cooked crystals, by luck? We can construct 95% confidence interval assuming normal distribution, with the procedure described below : In [78]: # Mr. White's stats . t . interval ( 1 - 0.05 , 21 - 1 , loc = 99.1 , scale = 3 / np . sqrt ( 21 )) Out[78]: (97.73441637228476, 100.46558362771523) In [79]: # Mr. Pinkman's stats . t . interval ( 1 - 0.05 , 18 - 1 , loc = 96.2 , scale = 4 / np . sqrt ( 18 )) Out[79]: (94.21084679714819, 98.18915320285181) There's a small overlap between the confidence intervals of Mr. White's and Mr. Pinkman's. Although it is true that Mr. White is a better cooker, Mr. Pinkman can cook a purer batch of crystals by a small chance, if he has the luck. Comparing the means of two sample data sets is closely related to constructing confidence interval of difference in mean . Figure 3: Overlap in the 95% confidence interval of two samples Source Code For Figure (3) import matplotlib.pyplot as plt from scipy import stats import numpy as np conf_pinkman = stats.t.interval(1 - 0.05, 18 - 1, loc=96.2, scale= 4 / np.sqrt(18)) conf_white = stats.t.interval(1 - 0.05, 21 - 1, loc=99.1, scale= 3 / np.sqrt(21)) plt.style.use('seaborn-whitegrid') fig, ax = plt.subplots(figsize=(5, 2)) ax.errorbar(99.1, 1, xerr=(conf_white[1] - conf_white[0]) / 2, fmt='o', markersize=8, capsize=5, label='Mr. White\\'s', color='grey') ax.errorbar(96.2, 0, xerr=(conf_pinkman[1] - conf_pinkman[0]) / 2, fmt='o', markersize=8, capsize=5, label='Mr. Pinkman\\'s', color='k') ax.set_ylim(-0.6, 1.6) ax.fill_betweenx([1, 0], conf_white[0], conf_pinkman[1], facecolor='lightgrey', alpha=0.3) ax.legend(loc='best', fontsize=11, framealpha=1, frameon=True) ax.set_xlabel('Purity (%)', fontsize=12) ax.yaxis.set_major_formatter(plt.NullFormatter()) fig.tight_layout(); Example 3: Uncertainty in oil production forecast A production engineer in the oil & gas industry wants to know the worst-case scenario (P10) and the best-case scenario (P90) of hydrocarbon production forecast. In petroleum engineering, we use a technique called Decline Curve Analysis (DCA) to project future hydrocarbon production. It is important to quantify the uncertanties of your DCA model, as the uncertainty in oil production can be as large as millions of dollars worth; executives in a company make business decisions based on P10 and P90 values. There are mainly three models for DCA: exponential, hyperbolic, and harmonic. For demonstration purpose, hyperbolic model will be used here. Hyperbolic decline curve can be defined as: $$ q = \\frac{q_i}{(1+bD_it)&#94;{1/b}}$$ It is a non-linear regression problem with three parameters to optimize: $Di$, $q_i$, and $b$. The independent variable is time $t$. We can construct the confidence interval of regression model to get P10 and P90 values. Since the analytical solution for the confidence interval of non-linear model is complicated, and the data is not normally distributed , we use non-parametric numerical alternative, bootstrap , to obtain the following uncertainty forecast model: Figure 4: Uncertainty in decline curve analysis forecast Source Code For Figure (4) import numpy as np import pandas as pd import matplotlib.pyplot as plt from scipy.optimize import curve_fit ##################################### Prepare Data ######################################### file = 'https://aegis4048.github.io/downloads/notebooks/sample_data/decline_curve.xlsx' df = pd.read_excel(file, sheet_name='sheet_1') df = df[df['Oil Prod. (bopd)'] > 200] # remove bad data points t = df['Time'][1:].values[: -40] y = df['Oil Prod. (bopd)'][1:].values[:-40] x = np.array([i for i in range(len(t))]) #################################### Define function ####################################### def hyperbolic(x, qi, b, Di): return qi / (1 + b * Di * x) ** (1 / b) ################################# Bootstrap regression ##################################### np.random.seed(42) y_boot_reg = [] for i in range(1000): # Bootstrapping boot_index = np.random.choice(range(0, len(y)), len(y)) x_boot = x[boot_index] y_boot = y[boot_index] # Curve fit data popt, pcov = curve_fit(hyperbolic, x_boot, y_boot, maxfev=100000, p0=[max(y), 0.1, 0.1]) # Define predicted region pred_x = [i for i in range(x[-1], x[-1] + 20)][1:] x_new = np.append(x, np.array([pred_x])) # Predict y_boot_reg = y_boot_reg + [hyperbolic(x_new, *popt)] y_boot_reg = np.array(y_boot_reg) p10 = np.percentile(y_boot_reg, 10, axis=0) p50 = np.percentile(y_boot_reg, 50, axis=0) p90 = np.percentile(y_boot_reg, 90, axis=0) # Basic curve fit popt, pcov = curve_fit(hyperbolic, x, y, maxfev=100000, p0=[max(y), 0.1, 0.1]) ###################################### Plotting ########################################## fig, ax = plt.subplots(figsize=(8, 4)) for reg_sample in y_boot_reg: ax.plot(x_new, reg_sample, color='grey', alpha=0.3) ax.plot(x, y, '--o', color='k', alpha=1) ax.plot(x_new, p10, ':',color='#1f77b4', alpha=1, label='P10') ax.plot(x_new, hyperbolic(x_new, *popt), color='r', label='P50') ax.plot(x_new, p90, '-.',color='#1f77b4', alpha=1, label='P90') ax.set_yscale('log') ax.set_ylim(80, 30000) ax.set_xlim(-2, 57) ax.set_xlabel('Months', fontsize=15) ax.set_ylabel('Oil Production (bopd)', fontsize=15) ax.set_title('Uncertainty in decline curve analysis with Bootstrap', fontsize=18) ax.grid(True, linestyle='--', color='#acacac') ax.axvspan(-2, x[-1], facecolor='#efefef', alpha=0.5) ax.axvspan(x[-1], 57, facecolor='lightgrey', alpha=0.5) ax.text(0.38, 0.1, 'Fitted region', fontsize=15, transform=ax.transAxes, color='k') ax.text(0.7, 0.1, 'Predicted region', fontsize=15, transform=ax.transAxes, color='k') ax.text(0.27, 0.91, 'aegis4048.github.io', fontsize=13, ha='center', va='center', transform=ax.transAxes, color='grey', alpha=0.5) ax.legend() fig.tight_layout() 2. Key takeaways 1. Confidence interval quantifies uncertainty of statistical estimation Confidence interval qunatifies the uncertainty related to a statistical estimation to mitigate the issue of Population vs. Samples . It is always expressed in a range like — $\\text{C.I.}: \\quad \\bar{x} \\pm 3.43$ or $-51.4 < \\bar{x} < -43.2$ 2. Confidence interval is the basis of parametric hypothesis tests Confidence interval is the basis of parametric hypothesis tests. For example, t-test computes its p-value using the confidence interval of difference in mean . When samples follow a normal distribution, and therefore their centeral tendency can be described by their means, t-test can be used to conclude if two distributions are significantly different from each other. 3. Formula for confidence interval varies with statistics For confidence interval of mean $$ \\text{C.I.}_{\\text{mean}}: \\quad \\mu \\pm (t_{\\frac{\\alpha}{2},df} \\times \\frac{s}{\\sqrt{n}})$$ For confidence interval of difference in mean $$ \\text{C.I.}_{\\Delta \\text{mean}}: \\quad (\\mu_{1}- \\mu_{2}) \\pm (t_{1-\\frac{\\alpha}{2},df} \\times \\sqrt{\\frac{s_1&#94;2}{n_1}+\\frac{s_2&#94;2}{n_2}})$$ For confidence interval of proportion $$ \\text{C.I.}_{\\text{proportion}}: \\quad \\hat{p} \\pm (t_{\\frac{\\alpha}{2},df} \\times \\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n}} )$$ For confidence interval of variance $$ \\text{C.I.}_{\\text{variance}}: \\frac{(n-1)s&#94;{2}}{\\chi&#94;{2}_{\\frac{\\alpha}{2}}} \\leq \\sigma&#94;2 \\leq \\frac{(n-1)s&#94;{2}}{\\chi&#94;{2}_{1-\\frac{\\alpha}{2}}}$$ For confidence interval of standard deviation $$ \\text{C.I.}_{\\text{standard deviation}}: \\sqrt{\\frac{(n-1)s&#94;{2}}{\\chi&#94;{2}_{\\frac{\\alpha}{2}}}} \\leq \\sigma \\leq \\sqrt{\\frac{(n-1)s&#94;{2}}{\\chi&#94;{2}_{1-\\frac{\\alpha}{2}}}}$$ Different analytical solutions exist for different statistics. However, confidence interval for many other statistics cannot be analytically solved, simply because there are no formulas for them. If the statistic of your interest does not have an analytical solution for its confidence interval, or you simply don't know it, numerical methods like boostrapping can be a good alternative (and its powerful). 4. Parametric methods suffer loss in accuracy under non-normality The equations listed above are not valid if sample data set is not normally distributed . In case of non-normally distributed data, its confidence interval can be obatined with non-parametric methods like boostrapping , or instead use credible interval, which is a Baysian equivalent of confidence interval. Or you can transform your data into normal distribution using Box-Cox transformation . Make sure that you understand robustness of parametric vs. non-parametric methods for non-normal distributions, which is discussed in detail below . 5. 95% C.I. does not mean 95% of the sample data lie within the interval. It means that there's 95% chance that the estimated statistic falls within the interval. 95% confidence interval relates to the reliability of the estimation procedure. Ex: How reliable is your estimation of population variance? 6. Always use t-score instead of z-score When constructing confidence interval of mean, or running t-test, always use t-score instead of z-score. This is because t-distribution accounts for bigger uncertainty in samples than normal distribution when sample size is samll, but converges to normal distribution when sample size is bigger than 30. The concept is described in detail below . 7. Bigger sample size gives narrower confidence intervals Intuitively, this is because the more samples we have, the less uncertainty we have with our statistical estimation. Mathematically, this is because the the confidence interval is inversely related to the sample size $n$, as shown in eq (1) . 8. Means are not always equivalent to central tendency When samples are not normally distributed, their means are not a good measure of their centeral tendencies . For example, if you are comparing the means of two non-normal data sets with t-test to conclude if they came from the same population, your approach is wrong. The more viable alternative would be to use non-parametric alternatives that uses median, or other statistics that capture the central tendency of non-normal distributions. 3. Population vs. samples Confidence interval describes the amount of uncertainty associated with a sample estimate of a population parameter. One needs to have a good understanding of the difference between samples and population to understand the necessity of delivering statistical estimations in a range, a.k.a. confidence interval. Figure 5: Population vs samples Population : data set that contains all members of a specified group. Ex: ALL people living in the US. Samples : data set that contains a part, or a subset, of a population Ex: SOME people living in the US. Let's say that you are conducting a phone-call survey to investigate the society's perception of The Affordable Care Act (\"Obamacare\"). Since you can't call all 327.2 million people ( population ) in the US, you call about 1,000 people ( samples ). Your poll showed that 59% of the registered voters support Obamacare. This does not agree with the actual survey conducted in 2018; 53% favorable, 42% unfavorable ( source ). What could be the source of error? Since (formal) president Obama is a member of the Democratic Party, the voters' response can be affected by their political preference. How could you tell that the 1,000 people you called happened to be mostly Democrats, who's more likely to support Obama's policy, because they share similar political view? The samples you collected could have been biased , but you don't that know for sure. Of course, the voters' response could be affected by many other factors like race, age, place of residence, or financial status. The idea is that, there will always be uncertainty involved with your estimation, because you don't have an access to the entire population. Confidence interval is a technique that quantifies the uncertainty when estimating a population parameter from samples. Notes: Population variance $\\sigma&#94;2$ vs. Sample variance $s&#94;2$ Distinction between population parameter and sample parameter is important. In statistics, it is a common practice to denote population variance as $\\sigma&#94;2$, and sample variance as $s&#94;2$. The distinction is important because different equations are used for each. For population: $$ \\text{variance} = \\sigma&#94;2 = \\frac{\\sum(x - \\bar{x})&#94;2}{n} $$ For samples: $$ \\text{variance} = s&#94;2 = \\frac{\\sum(x - \\bar{x})&#94;2}{n-1} $$ The divisor $n-1$ is a correction factor for bias. Note that the correction has a larger proportional effect when $n$ is small than when $n$ is large, which is what we want because the more samples we have, the better the estimation. This idea is well explained on this StackExchange thread . Pythonic Tip: Difference between Numpy variance and Pandas variance Different libraries make different assumption about an input array. The default value of ddof is different for Pandas and Numpy, resulting in different variance. ddof represent degrees of freedom, and setting ddof=True or ddof=1 tells the variance function to calculate sample variance by accounting for the bias factor $n-1$ (recall that in Python, True==1 .) Remember that there is a distinction between Population variance ($\\sigma&#94;2$) vs. Sample variance ($s&#94;2$). If you are confused which library is computing which variance (sample or population), just remember this: whatever library you are using, use ddof=True or ddof=1 to compute sample variance, and use ddof=False or ddof=0 to compute population variance. In [1]: import numpy as np import pandas as pd arr = pd . DataFrame ([ 5 , 3 , 1 , 6 ]) In [7]: # numpy, population arr . values . var () Out[7]: 3.6875 In [8]: # numpy, sample arr . values . var ( ddof = 1 ) Out[8]: 4.916666666666667 In [10]: # pandas, population arr . var ( ddof = 0 ) Out[10]: 0 3.6875 dtype: float64 In [99]: # pandas, sample arr . var () Out[99]: 0 4.916667 dtype: float64 4. Confidence interval of normal distribution Computing confidence interval of a statistic depends on two factors: type of statistic, and type of sample distribution. As explained above , different formulas exist for different type of statistics (Ex: mean, std, variance), and different methods (Ex: boostrapping , credible interval, Box-Cox transformation ) are used for non-normal data set. We will cover confidence interval of mean, difference in mean and variance. 4.1. Confidence interval of mean Confidence interval of mean is used to estimate the population mean from sample data and quantify the related uncertainty. Consider the following figure: Figure 6: Distribution of population and C.I. of mean In figure (6) , assume that the population is normally distributed. Since we don't have an access to the entire population, we have to guess the population mean (unknown) to the best of our ability using sample data set. We do this by computing the sample mean and constructing its 95% confidence interval . Note that the popular choices of confidence level are: 90%, 95%, and 99% Assuming normality of population, its sample means are also normally distributed. Let's say that you have a population, and you draw small fractions of it $N$ times. Then, the computed means of $N$ sample sets $\\boldsymbol{\\mu}=(\\mu_1, \\mu_2,..., \\mu_{N-1}, \\mu_N)$ is normally distributed as shown in figure (7) . Their confidence intervals are represented as the black horizontal arrows:. Figure 7: Distribution of sample mean and its C.I. You can see that the confidence interval of $\\mu_5$ does NOT include the green vertical dashed line , 12.31. Let's assume that 12.31 is the true population mean (we never know if this is the actual population mean or not, but let's assume). If we get $\\mu_5$ and its confidence interval as our estimation of the population mean, then our estimation is wrong. There is a 5% chance of this happening, because we set our confidence level as 95%. Note that the width of the confidence intervals (black horizontal arrows) depend on the sample size, as shown in eq (1) The grey area of figure (6) is essentially equivalent to the grey area of figure (7) . $\\mu_1$ = 12.32 is the sample mean, and $\\pm$ 3.56 is the uncertainty related to the sample mean with 95% confidence. The uncertainty is a product of distribution score and standard error of mean. Distribution score essentially tells how many standard error are the limits (8.76 and 15.88) away from the center (12.32). Choosing larger confidence level results in larger confidence interval. This increases the grey area in figure (6) and figure (7) . We convey 95% confidence interval of mean like this: I am 95% confident that the population mean falls between 8.76 and 15.88. If I sample data 20 times, 19 times the sample mean will fall between 8.76 ~ 15.88, but expect that I will be wrong 1 time. Notes: Distribution of various statistics Different statistics exhibit different distributions. Normality of samples does not guarantee normality of its statistics. When the samples are normally distributed, their means are normally distributed, but their variances are chi-square $\\chi&#94;2$ distributed. More discussion about the distribution of variance and $\\chi&#94;2$ distribution is covered below . Note that these assumptions are invalid when samples are non-normal. Source Code For The Figure from scipy import stats import matplotlib.pyplot as plt import numpy as np df_values = [1, 2, 6, 9] linestyles = ['-', '--', ':', '-.'] normal_params = [(10, 1), (11, 1), (10, 2), (10, 3)] x = np.linspace(-1, 20, 1000) plt.style.use('seaborn-whitegrid') fig, ax = plt.subplots(1, 2, figsize=(13.3, 5)) fig.tight_layout() plt.subplots_adjust(left=0.09, right=0.96, bottom=0.12, top=0.93) for df, norm_p, ls in zip(df_values, normal_params, linestyles): ax[1].plot(x, stats.chi2.pdf(x, df, loc=0, scale=1), ls=ls, c='black', label=r'Degrees of freedom$=%i$' % df) ax[0].plot(x, stats.norm.pdf(x, loc=norm_p[0], scale=norm_p[1]), ls=ls, c='black', label='Mean = %d, ' % norm_p[0] + 'Std = %s' % norm_p[1]) ax[0].set_xlim(4, 16) ax[0].set_ylim(-0.025, 0.525) ax[0].set_xlabel('$x$', fontsize=20) ax[0].set_ylabel(r'Probability', fontsize=20) ax[0].set_title(r'Distribution of means: normal distribution', fontsize=20) ax[0].legend(loc='upper left', fontsize=16, framealpha=1, frameon=True) ax[1].set_xlim(0, 10) ax[1].set_ylim(-0.025, 0.525) ax[1].set_xlabel('$\\chi&#94;2$', fontsize=20) ax[1].set_title(r'Distribution of variances: $\\chi&#94;2$ distribution', fontsize=20) ax[1].legend(loc='best', fontsize=16, framealpha=1, frameon=True) If sample data is normal or normal-like distributed, we almost always assume t-distribution to compute confidence interval, as explained below . Then, the confidence interval of mean has the following analytical solution: $$ \\text{C.I.}_{\\text{mean}}: \\quad \\mu \\pm (t_{1-\\frac{\\alpha}{2},df} \\times \\frac{s}{\\sqrt{n}}) \\tag{1}$$ where $\\mu$ : sample mean $\\alpha$ : significance level $n$ : number of samples $df$ : degrees of freedom. In this example, df = $n$ - 1 $s$ : sample standard deviation $t$ : t-score. depends on $\\alpha$ and $df$ Recall that when computing $s$, correction factor ($n-1$) is applied to account for sample bias, as explained above . Pay close attention to the standard error $\\frac{s}{\\sqrt{(n)}}$. As the sample size $n$ increases, the standard error decreases, reducing the range of confidence interval. This is intuitive in a sense that, the more samples we have, the less uncertainty we have with our statistical estimation. The length of the black horizontal arrows in figure (7) depends on the sample size. The larger the sample size, the narrower the width of arrows, and vice versa. Notes: z-score vs t-score You've probably seen mixed use of z-score and t-score for confidence interval during your studies. Long story short, it is safe and almost always better to use t-score than z-score. Z-score ($z_{\\frac{\\alpha}{2}}$) is used for normal distribution, and t-score ($t_{\\frac{\\alpha}{2},df}$) is used for t-distribution. You use z-score if you know the population variance $\\sigma&#94;2$. If not, you use t-score. Since the population variance $\\sigma&#94;2$ is almost never known, you almost always use t-score for confidence interval. After all, the purpose of using confidence interval is to mitigate the issue of Population vs. Samples when estimating population parameter ($\\sigma&#94;2$) from samples. If you know the population parameters, you probably don't need confidence interval in the first place. A natural question is, \"how is it safe to use t-score instead of z-score? Shouldn't I be using z-score since I know that the population is normally distributed, from previous knowledge?\" It is safe to do so because t-distribution converges to normal distribution according to the Centeral Limit Theorem. Recall that t-distribution behaves more and more like a normal distribution as the sample size increases. Google \"95% confidence z-score\" and you will see $z$ = 1.96 at 95% confidence level. On the other hand, t-score approaches 1.96 as its degrees of freedom increases: $\\lim_{df \\to \\infty}t$ = 1.96. For 95% confidence level, $t$ = 2.228 when $n$ - 1 = 10 and $t$ = 2.086 when $n$ - 1 = 20. This is why it is safe to always replace z-score with t-score when computing confidence interval. Pythonic Tip: Computing confidence interval of mean with SciPy We can compute confidence interval of mean directly from using eq (1) . Recall to pass ddof=1 to make sure to compute sample standard deviation $s$, not population standard deviation $\\sigma$, as explained above . We will draw random samples from normal distribution using np.random.normal(). Note that loc is for population mean, and scale is for population standard deviation, and size is for number of samples to draw. In [15]: from scipy import stats import numpy as np np . random . seed ( 42 ) arr = np . random . normal ( loc = 74 , scale = 4.3 , size = 20 ) alpha = 0.05 # significance level = 5% df = len ( arr ) - 1 # degress of freedom = 20 t = stats . t . ppf ( 1 - alpha / 2 , df ) # t-critical value for 95% CI = 2.093 s = np . std ( arr , ddof = 1 ) # sample standard deviation = 2.502 n = len ( arr ) lower = np . mean ( arr ) - ( t * s / np . sqrt ( n )) upper = np . mean ( arr ) + ( t * s / np . sqrt ( n )) In [2]: ( lower , upper ) Out[2]: (71.33139551903422, 75.19543685256606) Or we can compute with scipy.stats.t.interval(). Note that you don't divide alpha by 2, because the function does that for you. Also note that the standard error of mean $\\frac{s}{\\sqrt{n}}$ can be computed with scipy.stats.sem() In [130]: stats . t . interval ( 1 - alpha , len ( arr ) - 1 , loc = np . mean ( arr ), scale = stats . sem ( arr )) Out[130]: (71.33139551903422, 75.19543685256606) Note the default value of loc=0 and scale=1 . This will assume sample mean $\\mu$ to be 0, and standard error $\\frac{s}{\\sqrt{n}}$ to be 1, which assumes standard normal distribution of mean = 0 and standard deviation = 1. This is NOT what we want. In [8]: stats . t . interval ( 1 - alpha , len ( arr ) - 1 ) Out[8]: (-2.093024054408263, 2.093024054408263) 4.2. Confidence interval of difference in mean Confidence interval of difference in mean is not very useful by itself. But it is important to understand how it works, because it forms the basis of one of the most widely used hypothesis test: t-test . Often we are interested in knowing if two distributions are significantly different. In the other words, we want to know if two sample data sets came from the same population by comparing central tendency of populations . A standard approach is to check if the sample means are different. However, this is a misleading approach in a sense that the means of samples are almost always different, even if the difference is microscopic. More useful would be to estimate the difference in a range to account for uncertainty, and compute probability that it is big enough to be of practical importance. T-test checks if the difference is \"close enough\" to zero by computing the confidence interval of difference in means. T-test hypothesis $$ H_0: \\mu_1 - \\mu_2 = 0 \\tag{2}$$ $$ H_1: \\mu_1 - \\mu_2 \\neq 0 \\tag{3}$$ where $\\mu$ : sample mean $H_0$ : null hypothesis — sample means are the same \"enough\" $H_1$ : alternate hypothesis — sample means are \"significantly\" different Note that the above hypothesis tests whether the mean of one group is significantly DIFFERENT from the mean of the other group; we are using two-tailed test. This does not check if the mean of one group is significantly GREATER than the mean of the other group, which uses one-tailed test. Notes: Comparing means of more than two samples with ANOVA Analysis of variance (ANOVA) checks if the means of two or more samples are significantly different from each other. Using t-test is not reliable in cases where there are more than 2 samples. If we conduct multiple t-tests for comparing more than two samples, it will have a compounded effect on the error rate of the result. ANOVA has the following hypothesis: $$ \\begin{align} H_0: &\\mu_1 = \\mu_2 = \\, \\cdots \\, =\\mu_L \\\\[5pt] H_1: &\\mu_a \\neq \\mu_b \\end{align} $$ where $L$ is the number of groups, and $\\mu_a$ and $\\mu_b$ belong to any two sample means of any groups. This article illustrates the concept of ANOVA very well. Figure 8: Distributions of samples In figure (8) , $\\mu$ represents the sample mean. If two sample data sets are from the same population, the distribution of means will be similar \"enough\". If not, they will be \"significantly\" different. It can be visually inspected by the area of overlap. The larger the overlap, the bigger the chance of the two distributions originating from the same population. The more robust way to compare sample means would be to construct the confidence interval of difference in means. If the two samples came from the same population, they should have the similar \"enough\" means. Their difference should be close to zero and satisfy (or fail to reject) the null hypothesis $H_0: \\mu_1 - \\mu_2 = 0$ within a range of uncertainty. Consider the following figure: Figure 9: Distribution of difference in means In figure (9) , the calculated difference in sample means is $\\mu_1 - \\mu_2 = 1.00$ . We deliver the uncertainty related to our estimation of difference in sample means by constructing its 95% confidence interval $[$-1.31 ~ 3.31$]$ . Since the null hypothesis $H_0: \\mu_1 - \\mu_2 = 0$ is within the 95% confidence interval ( grey shaded area ), we accept the null hypothesis; we conclude that the samples have the same means within the uncertainty. However, if the null hypothesis is not within the confidence interval and falls in the 2.5% outliers zone, we reject the null hypothesis and accept the alternate hypothesis $H_1: \\mu_1 - \\mu_2 \\neq 0$ . In the other words, we conclude that the sample means are significantly different. Three variations of confidence interval of difference in means There are three variations of t-test, and therefore there are three variations of confidence interval of difference in means. The difference & application of the three variations are really well-explained in Wikipedia (one of the few that are actually easy to understand, with minimum jargons.) Independent (unpaired) samples, equal variance - Student's t-interval Independent (unpaired) samples, unequal variance - Welch's t-interval Dependent (paired) samples Recall that all t-tests assume normality of data. However, they are pretty robust to non-normality as long as the deviation from normality isn't large. Visualize your distributions to test this. Robustness of t-test to non-normality is discussed in detail below . 4.2.1. Independent (unpaired) samples, equal variance - student's t-interval When you have a reason to believe that samples have nearly equal variances, you can use student's t-test to check if difference in means are significantly different. Note that student's t-test works pretty well even with unequal variances as long as sample sample sizes are equal or nearly equal, and sample sizes are not tiny. However, it is recommended to always use Welch's t-test by assuming unequal variances, as explained below . Use student's t-test if you are ABSOLUTELY sure that the population variances are nearly equal. Confidence interval of difference in mean assuming equal variance (student's t-interval) can be calculated as follows: $$ \\text{C.I.}_{\\Delta \\text{mean}}: \\quad (\\mu_{1}- \\mu_{2}) \\pm (t_{1-\\frac{\\alpha}{2},df} \\times s_p\\sqrt{\\frac{1}{n_1}+\\frac{1}{n_2}})\\,, \\quad s_p = \\sqrt{\\frac{(n_1-1)s_{1}&#94;2 + (n_2-1)s_{2}&#94;2}{n_1+n_2-2}} \\tag{4}$$ where $\\mu$ : sample mean $\\alpha$ : significance level $n$ : number of samples $df$ : degrees of freedom $s_p$ : pooled standard deviation $s$ : sample standard deviation $t$ : t-score. depends on $\\alpha$ and degrees of freedom $n-1$ The formula for the pooled standard deviation $s_p$ looks a bit overwhelming, but its just an weighted average standard deviation of two samples, with bias correction factor $n_i-1$ for each sample. Recall that student's t-test assumes equal variances of two samples. You calculate what is assumed to be the common variance (=pooled variance, $s_p&#94;2$) by computing the weighted average from each sample's variance. In eq (4) , $t$-score depends on significance level $\\alpha$ and degrees of freedom $df$. In student's t-test, which assumes equal variance: $$ df = n_1 + n_2 -2 \\tag{5}$$ Pythonic Tip: Computing student's t-interval Unfortunately, SciPy doesn't support computing confidence intereval of difference in mean separately. It is incorporated into computing t-statistic and p-value of t-test, but users can't access its underlying confidence interval. Note that in R, users have access to the CI of difference in means. We can compute CI of difference in means assuming equal variance with eq (4) . Don't forget to compute sample variance, instead of population variance by setting ddof=1 as explained above . In [4]: from scipy import stats import numpy as np In [16]: x1 = [ 12.9 , 10.2 , 7.4 , 7.0 , 10.5 , 11.9 , 7.1 , 9.9 , 14.4 , 11.3 ] x2 = [ 10.2 , 6.9 , 10.9 , 11.0 , 10.1 , 5.3 , 7.5 , 10.3 , 9.2 , 8.8 ] alpha = 0.05 # significance level = 5% n1 , n2 = len ( x1 ), len ( x2 ) # sample sizes s1 , s2 = np . var ( x1 , ddof = 1 ), np . var ( x2 , ddof = 1 ) # sample variances s = np . sqrt ((( n1 - 1 ) * s1 + ( n2 - 1 ) * s2 ) / ( n1 + n2 - 2 )) # pooled standard deviation df = n1 + n2 - 2 # degrees of freedom t = stats . t . ppf ( 1 - alpha / 2 , df ) # t-critical value for 95% CI lower = ( np . mean ( x1 ) - np . mean ( x2 )) - t * np . sqrt ( 1 / len ( x1 ) + 1 / len ( x2 )) * s upper = ( np . mean ( x1 ) - np . mean ( x2 )) + t * np . sqrt ( 1 / len ( x1 ) + 1 / len ( x2 )) * s In [6]: ( lower , upper ) Out[6]: (-0.8520326742900641, 3.332032674290068) The 95% confidence interval of difference in means has 0 within its interval. This means that the null hypothesis, $H_0: \\mu_1 - \\mu_2 = 0$ in figure (9) , falls within the interval and we fail to reject the null hypothesis. We conclude that the sample means are not significantly different. We can confirm this by running a formal hypothesis testing with scipy.stats.ttest_ind() , and setting equal_var=True . Note that this assumes independent t-test with pooled variance, which is equivalent to student's t-test. In [185]: stats . ttest_ind ( x1 , x2 , equal_var = True ) Out[185]: Ttest_indResult(statistic=1.2452689491491107, pvalue=0.22900078577218805) The computed pvalue=0.229 is bigger than the significance level of alpha = 0.05 , and therefore we fail to reject the null hypothesis, which is consistent with the conclusion drawn from the confidence interval of difference in mean. Checking results with R : a <- c(12.9, 10.2, 7.4, 7.0, 10.5, 11.9, 7.1, 9.9, 14.4, 11.3) b <- c(10.2, 6.9, 10.9, 11.0, 10.1, 5.3, 7.5, 10.3, 9.2, 8.8) t.test(a, b, var.equal = TRUE) # Two Sample t-test # data: a and b # t = 1.2453, df = 18, p-value = 0.229 # 95 percent confidence interval: # -0.8520327 3.3320327 # sample estimates: # mean of x mean of y # 10.26 9.02 4.2.2. Independent (unpaired) samples, unequal variance - Welch's t-interval When comparing central tendency of normal distributions, it is safer, and therefore recommended to always use Welch's t-test, which assumes unequal variances of samples, as explained below . Equal variance t-test is not robust when population variances are different, but unequal variances are robust even when population variances are equal. Confidence interval of difference in mean assuming unequal variance (Welch's t-interval) can be calculated as follows: $$ \\text{C.I.}_{\\Delta \\text{mean}}: \\quad (\\mu_{1}- \\mu_{2}) \\pm (t_{1-\\frac{\\alpha}{2},df} \\times \\sqrt{\\frac{s_1&#94;2}{n_1}+\\frac{s_2&#94;2}{n_2}}) \\tag{6}$$ where $\\mu$ : sample mean $\\alpha$ : significance level $n$ : number of samples $df$ : degrees of freedom $s$ : sample standard deviation $t$ : t-score. depends on $\\alpha$ and degrees of freedom $n-1$ The formula is very similar to student's t-interval. There are two main differences: 1. We use each sample's own variance $s_1&#94;2$ and $s_2&#94;2$, instead of pooled (weighted average) variance $s_p&#94;2$. 2. Degrees of freedom $df$ is computed with eq (7). $$ df = \\frac{(\\frac{s&#94;2_1}{n_1} + \\frac{s&#94;2_2}{n_2})&#94;2}{\\frac{(s&#94;2_1/n_1)&#94;2}{n_1-1} + \\frac{(s&#94;2_2/n_2)&#94;2}{n_2-1}} \\tag{7}$$ Pythonic Tip: Computing Welch's t-interval The procedure is very similar to Computing student's t-interval . We will compute confidence interval of difference in mean assuming unequal variance, with eq (6). Although Scipy supports computing t-statistic for Welch's t-test, it doesn't support a function that allows us to compute Welch's t-interval. We will have to write our own codes to compute it. Don't forget to compute sample variance, instead of population variance by setting ddof=1 as explained above . In [186]: from scipy import stats import numpy as np In [18]: x1 = [ 12.9 , 10.2 , 7.4 , 7.0 , 10.5 , 11.9 , 7.1 , 9.9 , 14.4 , 11.3 ] x2 = [ 10.2 , 6.9 , 10.9 , 11.0 , 10.1 , 5.3 , 7.5 , 10.3 , 9.2 , 8.8 ] alpha = 0.05 # significance level = 5% n1 , n2 = len ( x1 ), len ( x2 ) # sample sizes s1 , s2 = np . var ( x1 , ddof = 1 ), np . var ( x2 , ddof = 1 ) # sample variances df = ( s1 / n1 + s2 / n2 ) ** 2 / (( s1 / n1 ) ** 2 / ( n1 - 1 ) + ( s2 / n2 ) ** 2 / ( n2 - 1 )) # degrees of freedom t = stats . t . ppf ( 1 - alpha / 2 , df ) # t-critical value for 95% CI lower = ( np . mean ( x1 ) - np . mean ( x2 )) - t * np . sqrt ( 1 / len ( x1 ) + 1 / len ( x2 )) * s upper = ( np . mean ( x1 ) - np . mean ( x2 )) + t * np . sqrt ( 1 / len ( x1 ) + 1 / len ( x2 )) * s In [188]: ( lower , upper ) Out[188]: (-0.8633815129922358, 3.3433815129922397) The 95% confidence interval of difference in means has 0 within its interval. This means that the null hypothesis, $H_0: \\mu_1 - \\mu_2 = 0$ in figure (9) , falls within the interval and we fail to reject the null hypothesis. We conclude that the sample means are not significantly different. We can confirm this by running a formal hypothesis testing with scipy.stats.ttest_ind() , and setting equal_var=False . Note that this assumes independent t-test with pooled variance, which is equivalent to student's t-test. In [189]: stats . ttest_ind ( x1 , x2 , equal_var = False ) Out[189]: Ttest_indResult(statistic=1.245268949149111, pvalue=0.23018336828903668) The computed pvalue=0.230 is bigger than the significance level of alpha = 0.05 , and therefore we fail to reject the null hypothesis, which is consistent with the conclusion drawn from the confidence interval of difference in mean. Checking results with R : a <- c(12.9, 10.2, 7.4, 7.0, 10.5, 11.9, 7.1, 9.9, 14.4, 11.3) b <- c(10.2, 6.9, 10.9, 11.0, 10.1, 5.3, 7.5, 10.3, 9.2, 8.8) t.test(a, b, var.equal = FALSE) # Welch Two Sample t-test # data: a and b # t = 1.2453, df = 16.74, p-value = 0.2302 # alternative hypothesis: true difference in means is not equal to 0 # 95 percent confidence interval: # -0.8633815 3.3433815 # sample estimates: # mean of x mean of y # 10.26 9.02 4.2.3. Dependent (paired) samples - Paired t-interval This test is used when the samples are dependent; that is, when there is only one sample that has been tested twice (repeated measures) or when there are two samples that have been matched or \"paired\" (paired or unpaired? read below. ) Confidence interval of difference in means assuming paired samples can be calculated as follows: $$ \\text{C.I.}_{\\Delta \\text{mean}}: \\quad \\bar{d} \\pm (t_{1-\\frac{\\alpha}{2}, df} \\times \\frac{s_d}{\\sqrt{n}})\\tag{8}$$ where $\\bar{d}$ : average of sample differences $\\alpha$ : significance level $n$ : number of samples $df$ : degrees of freedom $s_d$ : standard deviation of sample differences $t$ : t-score. depends on $\\alpha$ and degrees of freedom $n-1$ The equation is very similar to eq (1) , except that we are computing mean and standard deviation of differences between before & after state of test subjects. Let's try to understand this with an example. A school develops a tutoring program to improve the SAT scores of high school students. A school requires students to take tests before & after tutoring, and checks if the tutoring had a significant impact on the SAT scores of students. Because the test subjects are compared to themselves, not anyone elses, the measurements taken before & after the training are not independent. To compute dependent t-interval, we compute differences of test scores before & after tutoring: Student # $X_1$ $X_2$ $X_1$ - $X_2$ 1 1480 1510 -30 2 1280 1460 -180 3 890 1320 -430 4 340 700 -360 5 1550 1550 0 6 1230 1420 -190 7 1010 1340 -330 8 1590 1570 20 9 1390 1500 -110 10 980 1300 -320 We find $\\bar{d}$ = -193.0, and $s_d$ = 161.7. These values are plugged into eq (8). Degrees of freedom $df$ for dependent t-interval can be computed with: $$ df = n - 1 \\tag{9}$$ Unlike independent t-test, in which two samples can have different sample sizes $n_1$ and $n_2$, depedent t-test has only one sample size, because the test subjects are compared to themselves. Also note that dependent t-test assumes difference of test scores to be normally distributed, not test scores of students themselves. But as long as the test scores are normally distributed, the difference of test scores will also be normally distributed due to the property of normal distributions. Pythonic Tip: Computing paired t-interval Although Scipy supports computing t-statistic for dependent t-test, it doesn't support a function that allows us to compute dependent t-interval. We will have to write our own codes to compute it. You can compute it with eq (8) Don't forget to compute sample standard devaition, instead of population standard deviation by setting ddof=1 as explained above . In [190]: from scipy import stats import numpy as np In [20]: x1 = np . array ([ 1480 , 1280 , 890 , 340 , 1550 , 1230 , 1010 , 1590 , 1390 , 980 ]) x2 = np . array ([ 1510 , 1460 , 1320 , 700 , 1550 , 1420 , 1340 , 1570 , 1500 , 1300 ]) alpha = 0.05 # significance level = 5% d_bar = np . mean ( x1 - x2 ) # average of sample differences s_d = np . std ( x1 - x2 , ddof = 1 ) # sample standard deviation of sample differences n = len ( x1 ) # sample size df = n - 1 # degrees of freedom t = stats . t . ppf ( 1 - alpha / 2 , df ) # t-critical value for 95% CI lower = d_bar - t * s_d / np . sqrt ( n ) upper = d_bar + t * s_d / np . sqrt ( n ) In [192]: ( lower , upper ) Out[192]: (-308.64567899681356, -77.35432100318641) The 95% confidence interval of difference in means for dependent samples does not have 0 within its interval. This means that the null hypothesis, $H_0: \\mu_1 - \\mu_2 = 0$ in figure (9) , does not fall within the interval. Instead, our estimation falls within the 2.5% outlier zone on the left, $H_1: \\mu_1 - \\mu_2 \\neq 0$ . We reject the null hypothesis $H_0$, and accept the alternate hypothesis $H_1$. We conclude that the sample means are significantly different. In the other words, the tutoring program developed by the school had significant impact on the SAT score of its students. We can confirm this by running a formal hypothesis testing with scipy.stats.ttest_rel(). Note that this assumes dependent t-test. In [193]: stats . ttest_rel ( x1 , x2 ) Out[193]: Ttest_relResult(statistic=-3.7752930865755987, pvalue=0.004380623368522125) The computed pvalue=0.004 is smaller than the significance level of alpha = 0.05 , and therefore we reject the null hypothesis and accept the alternate hypothesis, which is consistent with the conclusion drawn from the confidence interval of difference in mean. Notes : The above hypothesis testing answers the question of \"Did this tutoring program had a significant impact on the SAT scores of students?\". However, in cases like this, a more intuitive question is \"Did this tutoring program significantly improve the SAT scores of students?\" The former uses two-tailed test, and the latter uses one-tailed test, and the procedures for them are a little different. Checking results with R : x1 = c(1480, 1280, 890, 340, 1550, 1230, 1010, 1590, 1390, 980) x2 = c(1510, 1460, 1320, 700, 1550, 1420, 1340, 1570, 1500, 1300) t.test(x1, x2, paired=TRUE) # Paired t-test # data: x1 and x2 # t = -3.7753, df = 9, p-value = 0.004381 # alternative hypothesis: true difference in means is not equal to 0 # 95 percent confidence interval: # -308.64568 -77.35432 # sample estimates: # mean of the differences # -193 Notes: Deciding which t-test to use Equal or unequal variance? Long story short, always assume unequal variance of samples when using t-test or constructing confidence interval of difference in means. Student's t-test is used for samples of equal variance, and Welch's t-test is used for samples of unequal variance. A natural question is, how do you know which test to use? While there exist techniques to check homogeneity of variances (f-test, Barlett's test, Levene's test), it is dangerous to run hypothesis testing for equality of variances to decide which t-test to use (student's t-test or Welch's t-test), because it increases Type I error (asserting something that is absent, false positive). This is shown by Moser and Stevens (1992) and Hayes and Cai (2010). Kubinger, Rasch and Moder (2009) argue that when the assumptions of normality and homogeneity of variances are met, Welch's t-test performs equally well, but outperforms when the assumptions are not met. Ruxton (2006) argues that the \"unequal variance t-test should always be used in preference to the Student's t-test\" (Note: what he means by \"always\" is assuming normality of distribution) Also note that R uses Welch's t-test as the default for the t.test() function. Independent (unpaired) or dependent (paired) samples? Paired t-test compares the same subjects at 2 different times . Unpaired t-test compares two different subjects. Samples are independent (unpaired) if one measurement is taken on different groups. For example in medical treament, group A is a control group, and is given a placebo with no medical effect. Group B is a test group, and receives a prescribed treatment with expected medical effect. Health check is applied on two groups, and the measurements are recorded. We say that the measurement from group A is independent from that of group B. Samples are dependent (paired) when repeated measures are taken on the same or related subjects. For example, there may be instances of the same patients being tested repeatedly - before and after receiving a particular treatment. In such cases, each patient is being used as a control sample against themselves. This method also applies to cases where the samples are related in some manner or have matching characteristics, like a comparative analysis involving children, parents or siblings. If you have a reason to believe that samples are correlated in any ways, it is recommended to use dependent test to reduce the effect of confounding factors . 4.3. Confidence interval of variance Confidence interval of variance is used to estimate the population variance from sample data and quantify the related uncertainty. C.I. of variance is seldom used by itself, but rather used in conjunction with f-test , which tests equality of variances of different populations. Similar to how the confidence interval of difference in mean forms the foundation of t-test , C.I. of variance forms the foundation of f-test. In the field of statistics and machine learning, the equality of variance is an important assumption when choosing which technique to use. For example, when comparing the means of two samples, student's t-test should not be used when you have a reason to believe that the two samples have different variances. Personally, I found f-test to be useful for the purpose of reading and understanding scientific papers, as many of the papers I have read use f-test to test their hypothesis, or use a variation of f-test for more advanced techniques. It is a pre-requisite knowledge you need to know to understand the more advanced techniques. I mentioned that different statistics exhibit different distributions above . When a sample data set originates from a normal distribution, its sample means are normally distributed as shown in figure (7) . On the other hand, its sample variances are chi-square ( $\\chi&#94;2$ ) distributed as shown in figure (10) The curve is asymptotic, and never touches the x-axis. The cumulative probabilty, which is often referred to as \"p-value\" in hypothesis testing, propagates from the right (p-value=0) to the left (p-value=1). For example, $\\chi&#94;2_{.975}=2.70$ is in the lower/left-tail and $\\chi&#94;2_{.025} = 19.02$ is in the upper/right-tail. When the samples follow a normal distribution, the $\\chi&#94;2$ statistic values can be plugged into eq (10) to compute the confidence interval of variance. Figure 10: 95% confidence interval of variance. Source Code For Figure (10) from scipy import stats import matplotlib.pyplot as plt import numpy as np df = 9 x = np.linspace(-1, 28, 1000) y = stats.chi2.pdf(x, df, loc=0, scale=1) right_tail = stats.chi2.ppf(1 - 0.025, df) left_tail = stats.chi2.ppf(1 - 0.975, df) plt.style.use('seaborn-whitegrid') fig, ax = plt.subplots(figsize=(12, 5)) ax.plot(x, y, c='black', label='Degrees of freedom = %d' % df) ax.set_xlabel('$\\chi&#94;2$', fontsize=17) ax.set_ylabel(r'Probability', fontsize=17) ax.set_title(r'$\\chi&#94;2\\ \\mathrm{Distribution}$, df = %d' % df, fontsize=17) ax.fill_between(x, 0, y, where=(np.array(x) > min(x)) & (np.array(x) <= left_tail), facecolor='grey') ax.fill_between(x, 0, y, where=(np.array(x) > left_tail) & (np.array(x) < right_tail), facecolor='lightgrey') ax.fill_between(x, 0, y, where=(np.array(x) > right_tail) & (np.array(x) <= max(x)), facecolor='grey') ax.grid(False) ax.text(22, 0.008, '2.5% outlier', fontsize=13) ax.text(-2, 0.008, '2.5% outlier', fontsize=13) ax.text(0.5, 0.04, '$\\chi&#94;2_{.975} = %.2f$' % left_tail, fontsize=14, bbox=dict(boxstyle='round', facecolor='white')) ax.text(16.5, 0.015, '$\\chi&#94;2_{.025} = %.2f$' % right_tail, fontsize=14, bbox=dict(boxstyle='round', facecolor='white')) ax.text(20, 0.08, '$\\chi&#94;2_{.975} \\leq \\chi&#94;2 \\leq \\chi&#94;2_{.025}$', fontsize=16) ax.text(20, 0.06, '$2.70 \\leq \\chi&#94;2 \\leq 19.02$', fontsize=16) ax.text(6, 0.05, '95% confidence interval', fontsize=16) ax.text(6, 0.04, 'of variance', fontsize=16); $$ \\text{C.I.}_{\\text{variance}}: \\frac{(n-1)s&#94;{2}}{\\chi&#94;{2}_{\\frac{\\alpha}{2}, df}} \\leq \\sigma&#94;2 \\leq \\frac{(n-1)s&#94;{2}}{\\chi&#94;{2}_{1-\\frac{\\alpha}{2}, df}} \\tag{10}$$ where $\\sigma&#94;2$ : population variance $s&#94;2$ : sample variance $\\alpha$ : significance level $n$ : number of samples $df$ : degrees of freedom. $\\chi&#94;2$ : chi-squared statistic. Depends on $\\alpha$ and $df$ In confidence interval of variance, the degrees of freedom is: $$df = n - 1$$ Recall that the goal of any confidence interval is to estimate the population parameter from a fraction of its samples due to the high cost of obtaining measurement data of the entire data set, as explained in Population vs Samples. You attempt to estimate the population variance $\\sigma&#94;2$ within the range of uncertainty with the sample variance $s&#94;2$ obtained from a set of n samples that are \"hopefully\" representative of the true population. Confidence interval of variance assumes normality of samples, and is very sensitive to the sample distribution's deviation from normality. In case of non-normal sample distributions, you can either 1) transform the distribution to normal distribution with Box-Cox transformation , or 2) use non-parametric alternatives. For practitioners, I do not recommend 1) unless you really understand what you are doing, as the back transformation process of Box-Cox transformation can be tricky. Furthermore, it doesn't always result in successful transformation of non-normal to normal distribution, as discussed below . I recommend to use 2). If you have non-normal samples and your goal is to compute the C.I. of variance, use bootstrap . If your goal is to check the equality of variances of multiple sample data sets with hypothesis testing, use Levene's test. Both are the non-parametric alternatives that does not require normality of samples. Notes: Chi-square $\\chi&#94;2$ distribution Chi-square $\\chi&#94;2$ distribution is a function of degrees of freedom $df$ . It is a special case of the gamma distribution and is one of the most widely used probability distributions in inferential statistics, notably in hypothesis testing or in construction of confidence intervals. It is used in the common chi-square goodness of fit test of an observed data set to a theoretical one. Let's say that there's a company that prints baseball cards. The company claims that 30% of the cards are rookies, 60% veterans but not All-Stars, and 10% are veteran All-Stars. Suppose that you purchased a deck of 100 cards. You found out that the card deck has 50 rookies, 45 veterans, and 5 All-Stars. Is this consistent with the company's claim? An answer to this question is explained in detail here using the chi-squared goodness of fit test. Note that the chi-square goodness of fit test does NOT require normality of data, but the chi-square test that checks if a variance equals a specified value DOES require normality of data. When samples have a normal distribution, some of their statistics can be described by $\\chi&#94;2$ distributions. For example, the Mahalanobis distance follows $\\chi&#94;2$ distribution when samples are normally distributed, and can be used for multivariate outlier detection using $\\chi&#94;2$ hypothesis test. Variance of samples also follows $\\chi&#94;2$ distributions when samples are normally distributed, and can be used to construct the confidence interval of variances with eq (10) . By the central limit theorem, a $\\chi&#94;2$ distribution converges to a normal distribution for large sample size $n$ . For many practical purposes, for $n$ > 50 the distribution is sufficiently close to a normal distribution for the difference to be ignored. Note that the sampling distribution of $ln(\\chi&#94;2)$ converges to normality much faster than the sampling distribution of $\\chi&#94;2$ as the logarithm removes much of the asymmetry. Source Code For The Figure from scipy import stats import matplotlib.pyplot as plt import numpy as np df_values = [1, 2, 6, 9] linestyles = ['-', '--', ':', '-.'] x = np.linspace(-1, 20, 1000) plt.style.use('seaborn-whitegrid') fig, ax = plt.subplots(figsize=(6.6666666, 5)) fig.tight_layout() plt.subplots_adjust(left=0.09, right=0.96, bottom=0.12, top=0.93) for df, ls in zip(df_values, linestyles): ax.plot(x, stats.chi2.pdf(x, df, loc=0, scale=1), ls=ls, c='black', label=r'Degrees of freedom$=%i$' % df) ax.set_xlim(0, 10) ax.set_ylim(0, 0.5) ax.set_xlabel('$\\chi&#94;2$', fontsize=14) ax.set_ylabel(r'Probability', fontsize=14) ax.set_title(r'$\\chi&#94;2\\ \\mathrm{Distribution}$') ax.legend(loc='best', fontsize=11, framealpha=1, frameon=True) Notes: One-tail vs two-tail As you explore more about the field of statistics, you will encounter many scientific papers or articles using mostly upper/right-tailed f-test, instead of two-tailed or lower/left-tailed f-test. Why? That's because they don't have much practical use in real-life. This information is little beyond the scope of this article, but I still want to touch on it because the C.I. of variance forms the foundation of f-test. When it comes to the test of variances, we often want to maintain a low variance than high variance, because the high variance is often related to high risk or instability. We are usually interested in knowing if a target population variance $\\sigma&#94;2$ is lower than a specified value $\\sigma&#94;2_0$ , not the other way around. This can be doen by using the upper/right-tailed hypothesis test, which is shown in the middle plot below. If the calculated statistic for f-test falls within the dark grey area, you reject your null hypothesis $H_0$ , and accept the alternate hypothesis $H_a$ Source Code For The Figure from scipy import stats import matplotlib.pyplot as plt import numpy as np df = 9 x = np.linspace(-1, 28, 1000) y = stats.chi2.pdf(x, df, loc=0, scale=1) # two-tailed two_right_tail = stats.chi2.ppf(1 - 0.025, df) two_left_tail = stats.chi2.ppf(1 - 0.975, df) # one tailed one_right_tail = stats.chi2.ppf(1 - 0.05, df) one_left_tail = stats.chi2.ppf(1 - 0.95, df) plt.style.use('seaborn-whitegrid') fig, axes = plt.subplots(1, 3, figsize=(12, 3)) for ax in axes: ax.plot(x, y, c='black') ax.grid(False) ax.xaxis.set_major_formatter(plt.NullFormatter()) ax.yaxis.set_major_formatter(plt.NullFormatter()) axes[0].fill_between(x, 0, y, where=(np.array(x) > min(x)) & (np.array(x) <= two_left_tail), facecolor='grey') axes[0].fill_between(x, 0, y, where=(np.array(x) > two_left_tail) & (np.array(x) < two_right_tail), facecolor='lightgrey') axes[0].fill_between(x, 0, y, where=(np.array(x) > two_right_tail) & (np.array(x) <= max(x)), facecolor='grey') axes[0].set_title('Two-tailed', fontsize=20) axes[0].text(14, 0.08, r'$H_0: \\sigma&#94;2 = \\sigma_0&#94;2$', fontsize=20) axes[0].text(14, 0.057, r'$H_a: \\sigma&#94;2 \\neq \\sigma_0&#94;2$', fontsize=20) axes[1].fill_between(x, 0, y, where=(np.array(x) > min(x)) & (np.array(x) < one_right_tail), facecolor='lightgrey') axes[1].fill_between(x, 0, y, where=(np.array(x) > one_right_tail) & (np.array(x) <= max(x)), facecolor='grey') axes[1].set_title('Upper/right-tailed', fontsize=20) axes[1].text(14, 0.08, r'$H_0: \\sigma&#94;2 \\leq \\sigma_0&#94;2$', fontsize=20) axes[1].text(14, 0.057, r'$H_a: \\sigma&#94;2 > \\sigma_0&#94;2$', fontsize=20) axes[2].fill_between(x, 0, y, where=(np.array(x) > min(x)) & (np.array(x) <= one_left_tail), facecolor='grey') axes[2].fill_between(x, 0, y, where=(np.array(x) > one_left_tail) & (np.array(x) <= max(x)), facecolor='lightgrey') axes[2].set_title('Lower/left-tailed', fontsize=20) axes[2].text(14, 0.08, r'$H_0: \\sigma&#94;2 \\geq \\sigma_0&#94;2$', fontsize=20) axes[2].text(14, 0.057, r'$H_a: \\sigma&#94;2 < \\sigma_0&#94;2$', fontsize=20) fig.tight_layout() Pythonic Tip: Computing confidence interval of variance Unfortunately, there's no Python or R library that computes the confidence interval of variance. The fact that the pre-built function does not exist both in Python and R suggests that the C.I. of variance is seldom used. But as I mentioned before, the reason that I introduce the C.I. of variance is because it forms the foundation of f-test, a statistical hypothesis test that is widely used in scientific papers. The C.I. of variance can be manually computed with eq (10) . Don't forget to compute sample variance, instead of population variance by setting ddof=1 as explained above . In [5]: from scipy import stats import numpy as np In [6]: arr = [ 8.69 , 8.15 , 9.25 , 9.45 , 8.96 , 8.65 , 8.43 , 8.79 , 8.63 ] alpha = 0.05 # significance level = 5% n = len ( arr ) # sample sizes s2 = np . var ( arr , ddof = 1 ) # sample variance df = n - 1 # degrees of freedom upper = ( n - 1 ) * s2 / stats . chi2 . ppf ( alpha / 2 , df ) lower = ( n - 1 ) * s2 / stats . chi2 . ppf ( 1 - alpha / 2 , df ) In [7]: ( lower , upper ) Out[7]: (0.07238029119542731, 0.5822533618682987) The output suggests that the 95% confidence interval of variance is — $\\text{C.I.}_{variance}: \\,\\, 0.072 < \\sigma&#94;2 < 0.582$ 4.4. Confidence interval of other statistics: Bootstrap (Note: For those people who have web-development experience, this is not CSS Bootstrap .) I mentioned that different formulas are used to construct confidence intervals of different statistics above. There are three problems with computing the confidence interval of statistics with analytical solutions: Not all statistics have formulas for their confidence intervals Their formulas can be so convoluted, that it may be better to use numerical alternatives You have to memorize their formulas Bootstrapping is nice because it allows you to avoid these practical concerns. For example, there are no formulas to compute the confidence interval of covariance and median. On the other hand, regression coefficient has its own formula for its confidence interval, but the formulas get really messy in cases of multi-linear or non-linear regression. Wouldn't it be nice if there's a \"magic\" that saves you from all the math you have to worry about? Bootstrapping is a statistical method for estimating the sampling distribution of a statistic by sampling with replacement from the original sample, most often with the purpose of estimating confidence intervals of a population parameter like a mean, median, proportion, correlation coefficient or regression coefficient. Bootstrap can construct confidence intervals of any statistics when combined with Monte Carlo method . The process is visually shown in fig (?) . Initially you have 5 samples $[8, 5, 4, 6, 2]$ that you collected from an unknown population. You randomly draw $n=5$ samples from the original sample pool WITH REPLACEMENT, and they become your single bootstrap sample . You repeat this process $r=6$ times to collect multiple bootstrap samples . For each bootstrap sample, you run your functions to compute the statistic of your interest: in this case, np.mean(single_boot) . Now you have $r=6$ sample means obtained from $r$ bootstrap samples. You can construct 95% confidence interval of mean with percentile method: np.percentile(mutliple_boot_means, 97.5) , np.percentile(mutliple_boot_means, 2.5) Note that the bootstraped samples will contain duplicate elements a lot, due to random sampling WITH REPLACEMENT. This causes problems with bootstrapping regression models, as explained below . Figure 11: Bootstrap 95% confidence interval of mean. Source Code For Figure (11) import matplotlib.pyplot as plt import numpy as np np.random.seed(42) arr = [8, 5, 4, 6, 2] ####################### Bootstrap ####################### num_boot_samples = 1000 def estimator(l): # statistic of interest; Ex: mean, median, variance ... return np.mean(l) boot = [estimator(np.random.choice(arr, len(arr))) for _ in range(num_boot_samples)] ######################################################### plt.style.use('seaborn-whitegrid') fig, ax = plt.subplots(figsize=(10, 4)) returns = ax.boxplot(boot, widths=0.5, whis=[2.5, 97.5], showfliers=False, patch_artist=True, boxprops=dict(linewidth=3.0, color='grey'), whiskerprops=dict(linewidth=3.0, color='grey'), vert=False, capprops=dict(linewidth=2.0, color='grey'), medianprops=dict(linewidth=2.0, color='yellow')) ax.set_aspect(1) #ax.set_ylim(1.5, -.5) ax.set_xlabel(r'$\\bar{X}$', fontsize=20) ax.yaxis.set_major_formatter(plt.NullFormatter()) ax.set_title('Bootstrap 95% confidence interval of mean', fontsize=20) ax.scatter(arr, [1, 1, 1, 1, 1], facecolors='grey', edgecolors='k', zorder=10, label='Original Samples', s=100) ax.legend(fontsize=15, fancybox=True, framealpha=1, shadow=True, borderpad=0.5, frameon=True); Notes: Monte-Carlo method During your study of statistics, there's a good chance that you've heard of the word, \"Monte-Carlo\". It refers to the process that relies on repeated generation of random numbers to investigate some characteristic of a statistic which is hard to derive analytically. The process is composed of mainly two parts: random number generator, and for-loop. The random number generator can be parametric, or non-parametric. In case of parametric simulation, you must have some previous knowledge about the population of your interest, such as its shape. In the below code snippet, you assume that the sample is from a specific distribution: normal , lognormal , chisquare . Then, you repeatedly randomly draw samples from the pre-defined distribution with a for-loop: Parametric Monte-Carlo simulation # random number generator = normal distribution sim_1 = [np.random.normal(np.mean(sample), np.mean(sample)) for _ in range(iterations)] # random number generator = lognormal distribution sim_2 = [np.random.lognormal(np.mean(sample), np.mean(sample)) for _ in range(iterations)] # random number generator = chi-square distribution sim_3 = [np.random.chisquare(len(sample)) for _ in range(iterations)] In case of non-parametric simulation, the random number generator does not assume anything about the shape of the population. Non-parmetric bootstrap would be the choice of your random number generator in this case (Note: some variations of bootstrap are parametric): Non-Parametric Monte-Carlo simulation # random number generator = non-parametric bootstrap sim_4 = [np.random.choice(original_sample) for _ in range(iterations)] You can create multiple instances of sim_n objects above to experiment with your data set; you use Monte-Carlo simulations to produce hundreds or thousands of \"possible outcomes\" . The results are analyzed to get probabilities of different outcomes occuring. The application of Monte-Carlo method includes constructing confidence interval of statistics with Bootstrap shown in figure (11) , and profit modeling of casino dice roll games . Why does bootstrapping work? (Before you read this section, make sure you understand the difference between Population vs Samples. ) Practitioners wonder WHY bootstrapping works: why is it that resampling the same sample over and over gives good results? If we are resampling from our sample, how is it that we are learning something about the population rather than only about the sample? There seems to be a leap which is somewhat counter-intuitive. The idea comes from the assumption that the sample is a reasonable representation of its underlying population — the population is to the sample as the sample is to the bootstrap samples. You want to ask question of a population, but you can't because you lack the resources to get measurement data of all possible data points. So you take a fraction of the population, a.k.a the sample, and ask the question of it instead. Now, how confident you should be that the sample answer is close to the population answer depends on how well the sample represents the underlying population. One way you might learn about this is to take samples from different portions of the population again and again. You ask the same question to the multiple samples you collected, and see the variability of the different sample answers to quantify the related uncertainty of your estimation. When this is not possible due to practical limitations, you make some assumptions about the population (ex: population is normally distributed), or use the information in the collected sample to learn about the population. In bootstrapping, you treat the original sample (size=$n$) you randomly acquired from the population as if it's the population itself of size $n$. From the original sample that you treated to be the population, you randomly draw samples, each of size $n$, with replacement multiple times to simulate direct sampling from the original population. By doing so, you essentially imitate sampling different portions of the original population multiple times. This idea is shown in figure (12) . This is a reasonable thing to do for two reasons. First, the sample in your had is the best you've got, indeed the only informatoin you have about the population. Second, if the original sample is randomly chosen, it will look like the original population they came from. This means that the sample is a good representation of its underlying population. However, if its not a good representation of the population, bootstrap fails. In fact, there's not much you can do in the first place if your sample is biased. Figure 12: Intuitive idea behind Bootstrapping Assumptions and Limitations of Bootstrap Bootstrapping is great because it saves you from the normality assumption of distributions and all the math you have to know to construct confidence intervals. However, just like many other techniques, bootstrap has its own caveats. While bootstrap is distribution-free, it is not assumption-free. The assumptions are listed in this section. Please note that there is a humongous variety of the bootstrap procedures, each addressing the particular quirk in either the statistic, the sample size, the dependence, or whatever an issue with the bootstrap could be. I am not introducing all of them here as the in-depth technical discussion of bootstrap needs another devoted post, but I still want you to know some of the critical assumptions; I want you to know what you don't know, so that you can google later to learn in-depth. A sample is a good representation of its underlying population The fundamental principle of bootstrapping is that the original sample is a good representation of its underlying population. Bootstrapping resamples from the original samples. This means that if the original sample is biased, the resulting bootstrap samples will also be biased. However, this is a problem of not just bootstrapping, but all statistical techniques. There's not much you can do if the only piece of information you have about the population is corrupted, after all. Insufficient samples make the bootstrap C.I. to be narrower than the analytical C.I. There's a myth in the field of statistics that bootstrap is a \"cure\" for small sample size. NO, it's not. First, if you have too small sample, by a high chance it is not diverse enough to represent all (reasonably) possible aspects of its population. Therefore, it is not a good representation of its population. Second, small sample size makes its bootstrap C.I. to be narrower than the analytical C.I.. This means that bootstrap C.I. reports small uncertainty even when the sample size is small. Not only this is counter-intuitive, but also it is a violation of the mathematic property of C.I. described by eq (1) ; small sample size $n$ in the denominator of eq (1) should give wider C.I.. But this is not true with bootstrap C.I. as shown in the below simulation result in figure (13) . Three things to note in the figure. First, the upper & lower error bars of bootstrap C.I. of means are asymmetric. This is because bootstrap C.I. is not based on $\\pm$ standard error method. This is a very useful property to estimate the central tendency of asymmetric (skewed) populations. Second, both bootstrap and analytical C.I. become narrower with the increasing sample size. This intuitively and mathematically makes sense. Third, bootstap C.I. approximates the analytical C.I. very well with large sample size. This is perhaps the most important advantage of using bootstrap. If you have large sample size, you really don't have to worry anything else (except the indepence of samples), and just stick to bootstrap . All the disadvantages of bootstrap will be overcome by the large sample size. One might wonder what is \"large\" enough in practical applications. Unfortunately, the definition of \"large\" is different for every applications. In the simulation result of figure (13) , it seems that $n = 20$ falls in the category of \"large\" to approximate C.I. of the mean of a normally distributed population with bootstrap. On the other hand, $n=100$ seems to be \"large\" in case of C.I. of the variances of a normally distributed population. The definition of \"large $n$\" can vary with different applications (ex: non-normal data, C.I. of regression coefficient or covariance). Carefully investigate your samples to have a good definition of \"large\" . Figure 13: Comparison of Bootstrap vs Analytical C.I. for different sample sizes Source Code For Figure (13) import matplotlib.pyplot as plt import numpy as np from scipy import stats sample_sizes = [3, 5, 8, 10, 15, 20, 30, 50, 80, 100, 500, 1000] mean = 4 # sample mean std = 3 # sample standard deviation boot_iter = 10000 # bootstrap iterations boot_mean_lo = np.array([]) boot_mean_hi = np.array([]) analy_mean_lo = np.array([]) analy_mean_hi = np.array([]) boot_var_lo = np.array([]) boot_var_hi = np.array([]) analy_var_lo = np.array([]) analy_var_hi = np.array([]) means = np.array([]) variances = np.array([]) for size in sample_sizes: np.random.seed(size * 5) arr = np.random.normal(mean, std, size) # randomly draw from a normal distribution # analytical confidence interval of mean means = np.append(means, np.mean(arr)) analy_conf_mean = stats.t.interval(1 - 0.05, len(arr) - 1, loc=np.mean(arr), scale=stats.sem(arr)) analy_mean_lo = np.append(analy_mean_lo, analy_conf_mean[0]) analy_mean_hi = np.append(analy_mean_hi, analy_conf_mean[1]) # bootstrap confidence interval of mean boot_means = [np.mean(np.random.choice(arr, len(arr))) for _ in range(boot_iter)] boot_mean_lo = np.append(boot_mean_lo, np.percentile(boot_means, 2.5)) boot_mean_hi = np.append(boot_mean_hi, np.percentile(boot_means, 97.5)) # analytical confidence interval of variance variances = np.append(variances, np.var(arr, ddof=1)) analy_conf_var = ( (len(arr) - 1) * np.var(arr, ddof=1) / stats.chi2.ppf(1 - 0.05 / 2, len(arr) - 1), (len(arr) - 1) * np.var(arr, ddof=1) / stats.chi2.ppf(0.05 / 2, len(arr) - 1) ) analy_var_lo = np.append(analy_var_lo, analy_conf_var[0]) analy_var_hi = np.append(analy_var_hi, analy_conf_var[1]) # bootstrap confidence interval of variance boot_vars = [np.var(np.random.choice(arr, len(arr)), ddof=1) for _ in range(boot_iter)] boot_var_lo = np.append(boot_var_lo, np.percentile(boot_vars, 2.5)) boot_var_hi = np.append(boot_var_hi, np.percentile(boot_vars, 97.5)) # plotting def styling(ax, xticks, xticklables): ax.legend(fontsize=14, loc='lower right', framealpha=1, frameon=True) ax.set_xlabel('Sample sizes', fontsize=16) ax.set_facecolor('#eeeeee') ax.grid(True, linestyle='--', color='#acacac') ax.tick_params(color='grey') ax.set_xticks(xticks) ax.set_xticklabels([str(label) for label in xticklables]) _ = [spine.set_edgecolor('grey') for spine in ax.spines.values()] x = np.array([i for i in range(len(sample_sizes))]) fig, axes = plt.subplots(1, 2, figsize=(14, 5)) axes[0].errorbar(x - 0.15, means, yerr=[abs(boot_mean_lo - means), abs(boot_mean_hi - means)], fmt='o', label='95% bootstrap C.I.', color='k', markersize=8, capsize=5, linewidth=2) axes[0].errorbar(x + 0.15, means, yerr=np.array([abs(analy_mean_hi - means), abs(analy_mean_lo - means)]), fmt='o', label='95% analytical C.I.', color='grey', markersize=8, capsize=5, linewidth=2) styling(axes[0], x, sample_sizes) axes[0].set_ylabel('Sample mean', fontsize=16) axes[0].set_title('Confidence interval of means $\\mu$', fontsize=18) axes[0].text(0.75, 0.85, 'aegis4048.github.io', fontsize=15, ha='center', va='center', transform=axes[0].transAxes, color='grey', alpha=0.5); axes[1].errorbar(x - 0.15, means, yerr=[abs(boot_var_lo - variances), abs(boot_var_hi - variances)], fmt='o', label='95% bootstrap C.I.', color='k', markersize=8, capsize=5, linewidth=2) axes[1].errorbar(x + 0.15, means, yerr=np.array([abs(analy_var_hi - variances), abs(analy_var_lo - variances)]), fmt='o', label='95% analytical C.I.', color='grey', markersize=8, capsize=5, linewidth=2) styling(axes[1], x, sample_sizes) axes[1].set_ylabel('Sample variance', fontsize=16) axes[1].set_title('Confidence interval of variances $\\sigma&#94;2$', fontsize=18) axes[1].text(0.75, 0.35, 'aegis4048.github.io', fontsize=15, ha='center', va='center', transform=axes[1].transAxes, color='grey', alpha=0.5); fig.tight_layout() Bootstrap fails to estimate extreme quantiles Bootstrap fails to estimate some really weird statistics that depend on very small features of the data. For example, using bootstrapping to determine anything close to extreme values (ex: min, max) of a distribution can be unreliable. There are also problems with estimating extreme quantiles, like 1% or 99%. Note that bootstrapped 95% or 99% CI are themselves at tails of a distribution, and thus could suffer from such a problem, particularly with small sample sizes. Bootstrap works better in the middle of a distribution than at the tails, which makes bootstrapping the median to be robust, whereas bootstrapping the min or max to fail. Samples are independent and identically distributed (i.i.d.) Another central issue with bootrapping is, \"does the resampling procedure preserve the structure of the original sample?\" The greatest problem with bootstrapping dependent data is to create samples that have the dependence structures that are sufficiently close to those in the original data. Because it is impossible to preserve it with the naive bootstrap, a sample needs to be i.i.d. This assumption raises a few practical issues when dealing with time series. First, by randomly sampling without constraints, naive bootstrap destroys the time-dependence structure in time series. In time series, all data points are aligned with respect to time, but random resampling does not respect their orders. Second, if there's an upward or downward trend in the means or variances, the trend will be lost due to random resampling. Third, because time series is essentially continuous samples of size 1 for each point in time, resampling a sample is equivalent to the original samples; one learns nothing by resampling. Therefore, resampling of a time series requires new ideas, such as block bootstrapping. There are a few variations of bootstrap that attemtp to preserve the dependency structure of samples, which I will not introduce here due to their mathematical complexities. When using tehchniques based on random sampling, ensure that the samples are i.i.d., or use techniques that preserve (reasonably) the structure of the original data. Bootstrap iteration ( Monte-Carlo method ) should be sufficient to reproduce consistent C.I's. Because bootstrap relies on \"random\" resampling, the result of any statistical analysis performed with bootstrap can vary from time to time. The extent of variability depends on the number of bootstrap samples $r$, and $r$ should be large enough to guarantee convergence of bootstrap statistics to a stable value. Note that there's a distinction between the size of the original sample $n$ and the number of bootstrap samples $r$. We can't change $n$, but we can change $r$ because $r$ is equivalent to the number of Monte-Carlo iterations, which can be set by a statistician. So how do we determine what value of $r$ is \"large\" enough to guarantee convergence of bootstrap statistics? You can do it by obtaining multiple bootstrap analysis results for increasing number of simulations $r$, and see if the result converges to certain range of values, as shown in figure (15) . In the figure, it seems that $r=10,000$ is a good choice. But in practice, you want $r$ to be as large as possible, to an extent where the computational cost is not too huge. I've read a research paper where the authors used $r = 500,000$ to really ensure convergence. Figure 14: Convergence of Bootstrap C.I. Source Code For Figure (14) import matplotlib.pyplot as plt import numpy as np from scipy import stats import pandas as pd # r, or number of bootstrap samples, or number of Monte-Carlo iterations r_boots = [10, 20, 50, 70, 100, 150, 300, 500, 700, 1000, 2000, 10000, 20000, 50000, 100000] size = 50 # original sample size mean = 50 # sample mean std = 13 # sample standard deviation # randomly draw from a normal distribution arr = np.random.normal(mean, std, size) boot_mean_lo = np.array([]) boot_mean_hi = np.array([]) results = [] for r_boot in r_boots: np.random.seed(r_boot) boot_means = [np.mean(np.random.choice(arr, len(arr))) for _ in range(r_boot)] results.append(boot_means) # plotting fig, ax = plt.subplots(figsize=(8, 4)) ax.boxplot(results, sym='', whis=[2.5, 97.5], showfliers=False, boxprops=dict(linewidth=2.0, color='#4e98c3'), whiskerprops=dict(linewidth=2.0, color='#4e98c3', linestyle='--'), vert=True, capprops=dict(linewidth=2.0, color='k'), medianprops=dict(linewidth=2.0, color='#ad203e')) ax.set_title('Convergence of 95% Bootstrap C.I. with increasing Monte-Carlo iterations', fontsize=15) ax.set_ylabel('Sample mean', fontsize=15) ax.set_xlabel('# of bootstrap samples (Monte-Carlo iterations)', fontsize=15) ax.set_xticklabels([str(r_boot) for r_boot in r_boots], rotation=45) ax.set_ylim(41.7, 52.3) ax.set_facecolor('#eeeeee') ax.grid(True, linestyle='--', color='#acacac') ax.tick_params(color='grey') _ = [spine.set_edgecolor('grey') for spine in ax.spines.values()] ax.text(0.21, 0.1, 'aegis4048.github.io', fontsize=15, ha='center', va='center', transform=ax.transAxes, color='grey', alpha=0.5); Coverage of naive bootstrap is relatively weak compared to more robust bootstrap methods The term, coverage , means the chance at which the statistical estimation with uncertainty includes the population parameter. Ideally, this coverage rate should be close to the nominal value set by a statistician (ex: 90%, 95%, 99%), but this is not always the case. We call the difference between the inferencial sample statistic and the population statistic as bias . Certain statistics, in certain situations, are biased: no matter how many experiments we perform, the average of the statistics is systematically off, either above or below the population value. For example, the sample median is biased when the original sample size $n$ is small, and we sample from skewed distributions. Variations of bootstrapping, such as the Bias Corrected (BC), and Bias Corrected & Accelerated (BCa) attempt to minimize the sampling bias. Bootstrapping continuous data is a bit tricky The biggest motivation for bootstrapping continuous data would be to acquire uncertainty of a fitted regression model. In regression problems, we assume the data points to be continuous. Bootstrapping makes this a little weird when the dependent variable ($y$) is continuous, because the original populatoin does not have even one exact duplicate, while bootstrap samples are likely to have many exact duplicates. Moreover, the range of independent variables ($x$ in single-regession, $x_1, x_2, ... , x_n$ in multi-regression) changes for each bootstrap sample due to randomness. Since the range of the independent variables defines the amount information available, bootstrapping will lose some of the information in the data. In such cases, alternatives like residual bootstrap (assumes homoscedasticity, or stationary varianace) or wild bootstrap (works for heteroscedasticity, or non-stationary varianace) may be used. Unfortunately, the procedure for these alternatives are very complicated, and they are not implemented in Python . To my knowledge, they are implemented in R though. Bootstrap is not robust in heavy-tailed distributions (This section assumes that you understand the caveats of heavy-tailed distributions) Heavy-tailed distributions have a few extreme values (NOT outliers) that are very different from the most of the samples. These extreme values have non-negligible impact on statistical estimations from samples, because the samples are not likely to contain them due to their low chance of occurrence. This violates the first assumption of bootstrapping I explained above , because the sample is NOT a good representation of its underlying population. Since bootstrapping heavily depends on the quality of the original sample, it is not robust for distributions with heavy tails. It will require extremely large size of the original sample to overcome such problems. Investigate the distribution shape of the population of your interest, and decide if that particular distribution shape will cause problems with heavy-tailedness. For example, exponential distribution is heavier-tailed than normal distribution, but it is not heavy enough to cause problems. Pareto (infinite variance, infinite mean), t-distribution with df = 2 and Cauchy (infinite variance, finite mean) are highly problematic category of distributions. Log-normal distribution has finite variance, so it is theoretically OK, but it can sometimes be heavy tailed enough that the population mean will almost always exceed all of your sample means, which can make inference via a bootstrap tricky. Note that the population mean for lognnormal will not be lower than the sample mean, as the low-occurrence extreme values are on the right tail of the distribution. Pythonic Tip: Bootstrapping in Python Basic bootstrap Bootstrap by itself means resampling from a sample. In its simplest form, it can be implemented in just one-line code with np.random.choice . For demonstration, assume that the original sample of size n=500 was randomly drawn from a normal distribution. In [9]: import numpy as np import pandas as pd In [10]: # prepare original sample data np . random . seed ( 42 ) n = 500 arr = np . random . normal ( loc = 0.15 , scale = 0.022 , size = n ) In [11]: # single bootstrapping single_boot = np . random . choice ( arr , len ( arr )) Basic bootstrap with Monte-Carlo method Bootstrap is often combined with Monte-Carlo method to quantify uncertainty in statistics (ex: mean, median, variance, etc...). It just means that you generate multiple instances of the single_boot object above. In [12]: # 100 monte-carlo bootstrapping r = 1000 monte_boot = [ np . random . choice ( arr , len ( arr )) for _ in range ( r )] With the above simulation, we stored r=1000 instances of bootstrap samples in the object monte_boot . It is a two dimensional array of size $r$ x $n$ (1000 x 500). I will show only the first 10 bootstrap samples with the Pandas DataFrame, since it will be too lengthy if I output all 1000 rows. In [13]: pd . DataFrame ( monte_boot , index = [ 'boot % s ' % str ( i ) for i in range ( r )]) . head ( 10 ) . round ( 3 ) Out[13]: 0 1 2 3 4 5 6 7 8 9 ... 490 491 492 493 494 495 496 497 498 499 boot 0 0.109 0.121 0.174 0.135 0.165 0.150 0.120 0.155 0.159 0.142 ... 0.157 0.158 0.128 0.134 0.129 0.152 0.163 0.140 0.158 0.127 boot 1 0.115 0.155 0.134 0.156 0.151 0.151 0.128 0.132 0.165 0.146 ... 0.127 0.144 0.129 0.140 0.169 0.160 0.164 0.147 0.167 0.122 boot 2 0.168 0.198 0.129 0.133 0.153 0.197 0.145 0.121 0.165 0.161 ... 0.108 0.149 0.147 0.140 0.175 0.180 0.118 0.139 0.176 0.145 boot 3 0.129 0.160 0.145 0.139 0.123 0.138 0.127 0.152 0.197 0.146 ... 0.173 0.120 0.191 0.133 0.173 0.119 0.167 0.160 0.158 0.150 boot 4 0.124 0.148 0.171 0.150 0.150 0.157 0.141 0.163 0.164 0.158 ... 0.121 0.150 0.170 0.152 0.161 0.235 0.146 0.140 0.160 0.122 boot 5 0.139 0.127 0.105 0.152 0.146 0.133 0.134 0.107 0.179 0.145 ... 0.146 0.134 0.156 0.157 0.159 0.127 0.132 0.111 0.103 0.183 boot 6 0.182 0.164 0.168 0.157 0.196 0.131 0.133 0.171 0.158 0.142 ... 0.166 0.124 0.185 0.164 0.133 0.160 0.156 0.109 0.151 0.167 boot 7 0.195 0.160 0.130 0.175 0.115 0.135 0.150 0.173 0.149 0.185 ... 0.153 0.122 0.145 0.163 0.149 0.155 0.136 0.149 0.138 0.137 boot 8 0.124 0.164 0.132 0.126 0.158 0.168 0.150 0.156 0.156 0.126 ... 0.127 0.117 0.115 0.140 0.132 0.132 0.127 0.133 0.160 0.154 boot 9 0.157 0.143 0.157 0.143 0.156 0.178 0.161 0.198 0.197 0.134 ... 0.184 0.197 0.157 0.147 0.161 0.119 0.135 0.124 0.119 0.109 10 rows × 500 columns Let's visualize the results in Matplotlib to understand Monte-Carlo simulations applied with bootstrap more intuitively. Recall that the goal of Monte-Carlo method is to simulate hundreds or thousands of \"possible outcomes\". Each line in the right plot below represents a single possible outcome. Figure 15: 1000 Bootstrap simulations Source Code For Figure (15) import numpy as np import matplotlib.pyplot as plt np.random.seed(42) n = 500 arr = np.random.normal(loc=0.15, scale=0.022, size=n) r = 1000 monte_boot = [np.random.choice(arr, len(arr)) for _ in range(r)] fig, axes = plt.subplots(1, 2, figsize=(14, 4)) axes[0].hist(arr, bins='auto', range=(min(arr), max(arr)), histtype='step', density=True) for boot in monte_boot: axes[1].hist(boot, bins='auto', range=(min(boot), max(boot)), histtype='step', density=True) for ax in axes: ax.grid(True, linestyle='--', color='#acacac') ax.set_ylabel('Probability', fontsize=15) ax.set_xlabel('$X$', fontsize=15) axes[0].set_title('Original sample', fontsize=15) axes[1].set_title('% s bootstrap samples' % r, fontsize=15) fig.tight_layout() Basic bootstrap with Monte-Carlo method + constructing confidence intervals We then analyze the simulation result to do whatever statistical estimation we want to do. Since this article is about confidence intervals, I will show how to construct confidence intervals of various statistics with bootstrap percentile method. It's actually very simple to implement. Just wrap your single bootstrap sample with a function that calculates the statistic of your interest. The result is an array of statistics of 1000 sample sets. In the other words, thousand data points of the statistics. In [5]: from scipy import stats import numpy as np # 100 monte-carlo bootstrapping r = 1000 monte_boot_mean = [ np . mean ( np . random . choice ( arr , len ( arr ))) for _ in range ( r )] monte_boot_median = [ np . median ( np . random . choice ( arr , len ( arr ))) for _ in range ( r )] monte_boot_std = [ np . std ( np . random . choice ( arr , len ( arr )), ddof = 1 ) for _ in range ( r )] monte_boot_skew = [ stats . skew ( np . random . choice ( arr , len ( arr ))) for _ in range ( r )] monte_boot_kurtosis = [ stats . kurtosis ( np . random . choice ( arr , len ( arr ))) for _ in range ( r )] In [6]: len ( monte_boot_mean ) Out[6]: 1000 If the statistic of your interest does not have a library function, you can define your own function, and wrap the bootstrap sample with it. In [7]: def custom_stats ( arr ): return sum ( arr ) / len ( arr ) + 0.002 In [8]: monte_boot_custom = [ custom_stats ( np . random . choice ( arr , len ( arr ))) for _ in range ( r )] Let's visualize the 95% confidence interval of various statistics obtained from Monte-Carlo bootstrap. Note that statistics like median, skew, kurtosis do not have analytical solutions to construct C.I., and can only be constructed from numerical methods like Monte-Carlo bootstrap (which makes bootstrap very powerful). Figure 16: Uncertainty models obtained from Monte-Carlo bootstrap Source Code For Figure (16) from scipy import stats import matplotlib.pyplot as plt import numpy as np # generate original sample np.random.seed(42) n = 500 arr = np.random.normal(loc=0.15, scale=0.022, size=n) # 100 monte-carlo bootstrapping r = 1000 monte_boot_mean = [np.mean(np.random.choice(arr, len(arr))) for _ in range(r)] monte_boot_median = [np.median(np.random.choice(arr, len(arr))) for _ in range(r)] monte_boot_std = [np.std(np.random.choice(arr, len(arr)), ddof=1) for _ in range(r)] monte_boot_skew = [stats.skew(np.random.choice(arr, len(arr))) for _ in range(r)] monte_boot_kurtosis = [stats.kurtosis(np.random.choice(arr, len(arr))) for _ in range(r)] def custom_stats(arr): return sum(arr) / len(arr) + 0.002 monte_boot_custom = [custom_stats(np.random.choice(arr, len(arr))) for _ in range(r)] # plotting styling = {'sym': '', 'whis': [2.5, 97.5], 'showfliers': False, 'vert': True, 'boxprops': dict(linewidth=2.0, color='#4e98c3'), 'whiskerprops': dict(linewidth=2.0, color='#4e98c3', linestyle='--'), 'capprops': dict(linewidth=2.0, color='k'), 'medianprops': dict(linewidth=2.0, color='#ad203e') } fig, axes = plt.subplots(1, 2, figsize=(14, 4)) axes[0].boxplot([monte_boot_mean, monte_boot_median, monte_boot_custom], **styling) axes[1].boxplot([monte_boot_std, monte_boot_skew, monte_boot_kurtosis], **styling) axes[0].set_xticklabels(['Mean', 'Median', '\"Custom\"'], fontsize=20) axes[1].set_xticklabels(['Stdev', 'Skew', 'Kurtosis'], fontsize=20) for ax in axes: ax.set_facecolor('#eeeeee') ax.grid(True, linestyle='--', color='#acacac') ax.text(0.25, 0.85, 'aegis4048.github.io', fontsize=17, ha='center', va='center', transform=ax.transAxes, color='grey', alpha=0.5); fig.tight_layout(rect=[0, 0.03, 1, 0.92]) fig.suptitle('Monte-Carlo bootstrap uncertainty modeling', fontsize=25) 5. Confidence interval of non-normal distribution One of the biggest assumptions in the field of statistics is the assumption of normality . It is important because of the two key characteristics of normal distribution: symmetry (low skew) and light/short-tails (low kurtosis). Symmetry is important because many statistical techniques use $\\pm$ standard error methods that assume equal properties from both sides of a distribution. On the other hand, heavy-tailedness is detrimental in inferencing population parameters because heavy tailed distributions (ex: log-normal distribution, cauchy distribution) have non-negligible number of extreme data points that may introduce bias in your samples (understanding the impact of heavy-tailedness is very important in inferencial statistics. Make sure you understand the issues of it by reading below ). As I mentioned in the Key Takeaway 4 & 8 above, constructing confidence intervals are very different when data is not normally distributed. This section explains the reasons why they are different, and their non-parametric alternatives. 5.1 Problems of non-normal distributions and central tendency When reporting a summary of a sample data, statisticians tend to report the mean (average) of a sample. But have you wondered why they bother specifically about the means? Often times the ultimate goal is NOT to compute a mean of a distribution, but to compute a measure of central tendency of a distribution. A measure of central tendency is a single value that attempts to describe a set of data by identifying the central position within that set of data. The mean is the measure of central tendency that you are the most familiar with, but there are others, such as the median and the mode. However, mean is not a good measure of central tendency when there is a sign of deviation from normality, which can be characterized by skewness (asymmetry) and kutrosis (heavy-tails). Problems with Skewness (asymmetry) Figure (17) below shows the effectiveness of mean, median, and mode as a measure of central tendency for different skewness. For normal distribution (a), most samples are found at the middle, and they are symmetrically distributed around the mean. This makes the mean as a good measure of central tendency. However, in asymmetrical distributions like (b) and (c), the median (or arguably the mode) is a better choice of central tendency, as it is closer to the central location of the distribution than the mean does. Naively reporting the mean as a summary of data results in misinterpretation of the central location of population. Note that some parametric techniques that assume normality of data are robust to mild skewness. There are many parametric alternatives that account for skewness, such as skewness-adjusted t-test and Box-Cox transformation. More discussion of robustness of parametric techinques relative to skewness is done below . Figure 17: Central tendency of distributions Source Code For Figure (17) from scipy import stats import matplotlib.pyplot as plt import numpy as np np.random.seed(42) y_norm = stats.norm.rvs(size=1000) y_expon = stats.expon.rvs(size=1000) y_skewnorm = stats.skewnorm.rvs(a=10, size=1000) plt.style.use('seaborn-whitegrid') fig, axes = plt.subplots(1, 3, figsize=(13, 3)) axes[0].set_title('Normal distribution', fontsize=20) axes[1].set_title('Skewed distribution', fontsize=20) axes[2].set_title('Exponential distribution', fontsize=20) adjust = 0.12 for ax, y, label in zip(axes, [y_norm, y_skewnorm, y_expon], ['(a)', '(b)', '(c)']): ax.grid(True, linestyle='--', color='#acacac') n, bins, patches = ax.hist(y, bins='auto', histtype='stepfilled', edgecolor='k', alpha=0.4, color='grey', density=True) ax.axvline(np.mean(y) + adjust, color='k', label='Mean', linestyle='-', alpha=0.9) ax.axvline(np.median(y) + adjust, color='k', label='Median', linestyle='--', alpha=0.9) ax.axvline(bins[np.where(n == max(n))[0][0]] + adjust, color='k', label='Mode', linestyle=':', alpha=0.9) ax.xaxis.set_major_formatter(plt.NullFormatter()) ax.yaxis.set_major_formatter(plt.NullFormatter()) ax.text(0.9, 0.4, label, fontsize=20, ha='center', va='center', transform=ax.transAxes) ax.text(0.75, 0.15, 'aegis4048.github.io', fontsize=13, ha='center', va='center', transform=ax.transAxes, color='grey', alpha=0.5) ax.legend(loc='upper right', framealpha=1, frameon=True) fig.tight_layout() Problems with kurtosis (heavy-tails) (Note that heavy-, long-, fat-tails are all synonyms. Some literature distinguishes among their meanings, but in general, they are all equivalent terms.) Unlike skewness, which can be relatively easily adjusted, heavy-tails can be very painful in inferencial statistics. When a distribution has heavy-tails, it means that the sample has extreme data points that can introduce bias to statistical inferences (note that the term bias is defined as the difference between the statistical estimation vs. the actual population parameter). This is because the occurrence of extreme data points are so low that they are unlikely to be included in the sample you collected, and yet there are substantial number of them lurking in the uncollected portion of a population that impacts population parameters due to their extremity. Figure (18) below shows the comparison of heavy- vs light-tailed distributions in a form of probability density function (PDF). Let's talk about the left plot first. Observe that in the heavy-tailed distribution (black line), the chance of occurrence for extreme values does not decay to zero as fast as that of the light-tailed distribution (grey line). This means that you have non-negligible chance of observing extreme values, and they may not have upper/lower bounds. In finance, such property is translated into high risk & unpredictability, which is modeled by Cauchy distributions. The right plot in figure (18) compares normal vs Cauchy distributions. Note that the tails of Cauchy distribution is higher than that of the normal distributions. The heavy tails in Cauchy attempt to model financial/investment risks by representing randomness in which extremes are encountered. When data arise from an underlying heavy-tailed distribution, shoehorning in the \"normal distribution\" model of risk would severely understate the true degree of risk. The narrow peak of the Cauchy distribution signifies that less samples fall within the range of Six-Sigma ( $\\pm 3\\sigma$ ), increasing unpredictability as less samples have the typical values . Practitioners often neglect the distinction between light- vs heavy-tail because they visually don't look very different when plotted in PDF. This is a big misconception. Figure 18: Heavy-tailed distributions have more extreme data points that can be detrimental to statistical inferences Source Code For Figure (18) from scipy import stats import matplotlib.pyplot as plt import numpy as np plt.style.use('seaborn-whitegrid') x = np.linspace(0, 5, 1000) y1 = stats.expon.pdf(x, scale=1) + 0.1 y2 = stats.lognorm.pdf(x, 0.5, loc=-0.001, scale=1) y3 = stats.norm.pdf(x, loc=2.5) y4 = stats.cauchy.pdf(x, loc=2.5) fig, axes = plt.subplots(1, 2, figsize=(14, 4)) axes[0].plot(x, y1, label='Heavy-tail', color='k', linewidth=2) axes[0].plot(x, y2, label='Light-tail', color='grey', linewidth=2) axes[0].set_xlabel(r'Extreme values $\\longrightarrow$', fontsize=20) axes[1].plot(x, y3, label='Normal', color='k', linewidth=2) axes[1].plot(x, y4, label='Cauchy', color='grey', linewidth=2) axes[1].set_xlabel(r'$\\longleftarrow$ Extreme values $\\longrightarrow$', fontsize=20) for ax in axes: ax.set_ylabel(r'Chance of occurrence $\\longrightarrow$', fontsize=20) ax.yaxis.set_major_formatter(plt.NullFormatter()) ax.xaxis.set_major_formatter(plt.NullFormatter()) ax.legend(fontsize=17, loc='upper right', framealpha=1, frameon=True) ax.grid(True, linestyle='--', color='#acacac') _ = [spine.set_edgecolor('grey') for spine in ax.spines.values()] ax.text(0.78, 0.3, 'aegis4048.github.io', fontsize=20, ha='center', va='center', transform=ax.transAxes, color='grey', alpha=0.5) fig.tight_layout() Notes: Cauchy distribution Some distributions have undefined mean, or undefined variance, or both. Cauchy distributions have undefined mean, and as such, the Law of Large Numbers (LLM: the result of performing the same experiment a large number of times will converge to a single value) and the Central Limit Theorem (CLT: the distribution of sample means approximates a normal distribution with the increasing sample size) do not apply. In the below figure, I simulated 1000 Cauchy samples with the increasing sample size, and plotted their means. It seems that the sample means are mostly 50 (because I set loc=50 ), but then all of a sudden a huge outlier appears in the sample and adds so much weight to the average that it pulls the sample mean away from 50 by a considerable amount. The extreme outliers keep coming and hitting the sample mean like a hammer, and just kepps going on like this. The sample mean never converge to a single value, which is a violation of LLM and CLT. This casts a few practical problems with inferential statistics, because the collected sample is unlikely to contain the extreme values due to their low chance of occurrence, and yet they significantly impact the population parameters due to their extremity. It is translated as a greater risk in financial modeling. Source Code For The Figure from scipy import stats import matplotlib.pyplot as plt import numpy as np np.random.seed(18) cauchy_means = [] sample_sizes = [i for i in range(1000)] for size in sample_sizes: y_cauchy = stats.cauchy.rvs(loc=50, size=size) cauchy_means.append(np.mean(y_cauchy)) plt.style.use('seaborn-whitegrid') fig, ax = plt.subplots(figsize=(8, 4)) ax.plot(sample_sizes, means, 'k', alpha=0.7) ax.set_xlabel('Sample sizes', fontsize=18) ax.set_ylabel('Sample means', fontsize=18) ax.set_title('Undefined means of Cauchy distributions', fontsize=18) ax.text(0.23, 0.85, 'aegis4048.github.io', fontsize=16, ha='center', va='center', transform=ax.transAxes, color='grey', alpha=0.5) fig.tight_layout() Treatment method for heavy-tailed distribution depends on the situation. The first step would be to map the distribution to a normal distribution using transformation techniques discussed below . However, the heavy-tails may still be present even after the presentation. If you are running a regression model, you may simply remove the tails and see if there's any performance improvement. In case of inferencial statistics, naive removal maybe detrimental, as it completely ignores the extremity in your population. Sometimes the heavy-tails may suggest amalgamation of two distinct populations, in which case you may need to separate them apart before running your model. This can be seen easily by running a quick test. I generated two samples from two distinct normal distribution as pop_1 and pop_2 , and plotted their combined distributions. Even though the samples were both generated from normal distributions, it exhibit heavy tails due to their difference in central tendencies. Generating two distinct normal samples : pop_1 = np.random.normal(10, 3, 100) pop_2 = np.random.normal(15, 6, 100) Figure 19: Heavy-tails due to two distinct populations Source Code For Figure (19) %matplotlib inline import numpy as np import matplotlib.pyplot as plt import matplotlib.gridspec as gridspec np.random.seed(4) # generate samples from two distinct populations pop_1 = np.random.normal(10, 3, 100) pop_2 = np.random.normal(15, 6, 100) # combine data = np.append(pop_1, pop_2) fig = plt.figure(figsize=(10, 4)) plt.suptitle('Heavy-tails due to amalgamation of two distinct populations', fontsize=15) gs = gridspec.GridSpec(2, 2) axes = [plt.subplot(gs[0, 0]), plt.subplot(gs[1, 0]), plt.subplot(gs[:2, 1])] axes[0].hist(pop_1, histtype='stepfilled', edgecolor='k', alpha=0.4, color='grey', density=True) axes[1].hist(pop_2, histtype='stepfilled', edgecolor='k', alpha=0.4, color='grey', density=True) axes[2].hist(data, histtype='stepfilled', edgecolor='k', alpha=0.4, color='grey', density=True) for ax in axes: ax.text(0.73, 0.85, 'aegis4048.github.io', fontsize=11, ha='center', va='center', transform=ax.transAxes, color='grey', alpha=0.5) ax.grid(True, linestyle='--', color='#acacac') ax.set_xlim(0, 35) plt.subplots_adjust(wspace=0.5) axes[2].text(-0.415, 0.55, 'Combine', transform=axes[2].transAxes, fontsize=13) axes[2].text(-0.4, 0.43, r'$\\Longrightarrow$', transform=axes[2].transAxes, fontsize=30) Problems with multi-modal distributions In case of multi-modal distributions, reporting the mean, median, or mode can be uninformative at all, as none of them describes the central location of populations. While it is possible to compute the confidence interval of statistics using non-parameteric methods like bootstrap, it is useless . I hope that figure (20) below is descriptive enough to understand the problems of multi-modal distributions. You will need to devise your own method to describe the central location of data, such as separating the multi-modes into distinct unimodal distributions. Figure 20: Absence of central tendency in multi-modal distributions Source Code For Figure (20) import numpy as np import matplotlib.pyplot as plt np.random.seed(4) n = 1000 np.random.seed(1) x = np.concatenate((np.random.normal(0, 1, int(0.3 * n)), np.random.normal(14, 1, int(0.7 * n)), np.random.normal(8, 2, int(0.7 * n))))[:, np.newaxis] plt.rcParams.update(plt.rcParamsDefault) fig, ax = plt.subplots(figsize=(8, 4)) adjust = 0.12 n, bins, patches = ax.hist(x, bins=30, histtype='stepfilled', edgecolor='k', alpha=0.4, color='grey', density=True) ax.axvline(np.mean(x) + adjust, color='k', label='Mean', linestyle='-', alpha=0.9) ax.axvline(np.median(x) + adjust, color='k', label='Median', linestyle='--', alpha=0.9) ax.axvline(bins[np.where(n == max(n))[0][0]] + adjust, color='k', label='Mode', linestyle=':', alpha=0.9) ax.text(0.2, 0.9, 'aegis4048.github.io', fontsize=15, ha='center', va='center', transform=ax.transAxes, color='grey', alpha=0.5) ax.grid(True, linestyle='--', color='#acacac') ax.set_xlabel('X', fontsize=16) ax.set_ylabel('Probability', fontsize=16) ax.legend(loc='upper right', framealpha=1, frameon=True) fig.tight_layout() 5.2 Robustness of confidence intervals to non-normality Confidence interval of mean is robust to mild deviation from normality. This means that hypothesis tests that rely on the means (ex: t-test) are also robust to mild deviation from non-normality. In fact, there are a few variations of t-test that deals with mild non-normality (ex: skewness-adjusted t-statistic). Confidence interval of variance is VERY sensitive to even small deviation from non-normality. Its low popularity in real-life applications can be attributed to its high sensitivity. In real-life, people tend to not care much about the C.I. of variance, but techniques that check the equality of variances of multiple samples with hypothesis testing. When sample is normal, you can use f-test or Barlett's test to check equality of variances. When the sample is not normal, use non-parametric alternative like Levene's test. Many techniques that assume normality of data are still resistant to mild departure from its assumptions. Then the natural question is, how do I know the severity of deviation from normality? I found from here that the skewness and kurtosis of $-2 < 0 < +2$ is an acceptable deviation from normality, but I've seen some people arguing $-1 < 0 < +1$ is the limit. This all depends on how much you are willing to tolerate the deviation. Another way to assess departure from normality is visualizing the distribution in Q-Q plots, as shown in figure (21). You can also use this online interactive QQ-plot generator to study the effect of asymmetry and heavy-tailedness. Note that I introduced the code snippet for visualizing Q-Q plots in Python below . Figure 21: Assessing deviation from normality with Q-Q plots Notes: Be cautious with hypothesis testing for normality If you have very large sample size, hypothesis tests that check normality (ex: Shapiro-Wilk, D'Agostino's $K&#94;2$ , Anderson-Darling) are essentially useless. This is counter-intuitive. Isn't it always better to have larger sample size than the otherwise? The reason is that the confidence intervals for test statistics become too narrow with large sample size. For example, when using t-test, we compute the confidence interval of difference in means with eq (6) . If the C.I. of difference in mean includes 0, we accept the null hypothesis. However, the width of the C.I. decreases with increasing sample size, because of sample sizes $n_1$ and $n_2$ are in the denominator of eq (6) . While the C.I. becoming narrow signifies small uncertainty in your statistical estimation, it is not desirable from a hypothesis test's point of view, because the test becomes too exact to be practical. Consider the plots below. I generated multiple samples from a normal distribution with increasing sample sizes, and added small noises from a uniform distribution. Visual inspection tells us that the samples are normally distributed, but the p-values from the Shapiro-Wilk test tell the otherwise. You can see that the p-value is almost always smaller than 0.05 when sample size > 15,000 and the null hypothesis (null: sample is normally distributed) is rejected. Is there something wrong with the Shapiro-Wilk test? No, it's actually doing \"exactly\" what it's supposed to do; it's testing if a sample is \"perfectly\" normally distributed. In fact, all samples contain some white noise that are not from a normal distribution, and the test is precisely detecting them. The problem is, that the test is too \"exact\" to be \"practical\" . The test has so many samples available that it is able to detect even the smallest non-normal noises from the sample. If you try to run Shaprio-Wilk test with SciPy for sample size > 5,000, you will see this warning message: UserWarning: p-value may not be accurate for N > 5000 . I recommend you to almost always use Q-Q plots to assess normality. If that's not possible, make sure that you don't have too large samples for normality test. Source Code For The Figure import numpy as np from scipy import stats import matplotlib.pyplot as plt import matplotlib ########################### Simulation Parameters ################################ np.random.seed(5) mean = 10 std = 3 lo_noise = 6 hi_noise = 14 sizes = [100, 1500, 10000, 20000] dist_arr = [] p_vals = [] ####################### Hypothesis testing simulation ############################ for size in sizes: data = np.random.normal(mean, std, size) noise = np.random.uniform(low=lo_noise, high=hi_noise, size=size) stat, p = stats.shapiro(data + noise) dist_arr.append(data + noise) p_vals.append(p) ################################## Plotting ###################################### matplotlib.style.use('ggplot') fig, axes = plt.subplots(1, len(sizes), figsize=(15, 3)) for ax, dist, size, p in zip(axes, dist_arr, sizes, p_vals): ax.hist(dist, bins='auto', color='k', alpha=0.5) ax.set_ylabel('Count') ax.set_xlabel('X') ax.set_title('Sample Size = %d' % size) ax.text(0.45, 0.3, 'p-val = %.4f' % p, fontsize=14, bbox=dict(boxstyle='round', facecolor='white', edgecolor='k'), transform=ax.transAxes) ax.text(0.5, 0.1, 'aegis4048.github.io', fontsize=8, ha='center', va='center', transform=ax.transAxes, color='white', alpha=0.5) fig.tight_layout() Source Code For The Figure import numpy as np from scipy import stats import matplotlib.pyplot as plt np.random.seed(1) p_vals = [] sizes = [i for i in range(100, 25000, 100)] for size in sizes: data = np.random.normal(10, 3, size) noise = np.random.uniform(low=6, high=14, size=size) stat, p = stats.shapiro(data + noise) p_vals.append(p) plt.style.use('default') fig, ax = plt.subplots(figsize=(8, 4)) ax.plot(sizes, p_vals) ax.axhspan(0, 0.05, alpha=0.5, color='#ad203e') ax.set_title('Change in p-value with increasing sample size', fontsize=18) ax.set_ylim(-0.05, 1.05) ax.set_ylabel('P-value', fontsize=15) ax.set_xlabel('Sample size', fontsize=15) ax.set_facecolor('#eeeeee') ax.grid(True, linestyle='--', color='#acacac') _ = [spine.set_edgecolor('grey') for spine in ax.spines.values()] ax.axhline(0.05, color='#ad203e', linestyle='--') ax.text(16000, 0.1, 'Reject the null hypothesis', fontsize=14, color='#ad203e') ax.text(0.6, 0.8, '$H_0$: Sample is normal\\n$H_1$: Sample is non-normal', fontsize=14, bbox=dict(boxstyle='round', facecolor='white', edgecolor='k'), transform=ax.transAxes) ax.text(0.86, 0.3, 'aegis4048.github.io', fontsize=13, ha='center', va='center', transform=ax.transAxes, color='grey', alpha=0.5) fig.tight_layout() Pythonic Tip: Q-Q plots with SciPy Q-Q plots are useful when assessing deviation from a pre-defined distribution. It is implemented in SciPy pakcage as scipy.stats.probplot . Note that the pre-defined distribution can be anything of your choice by changing the argument dist ; it can be lognormal, Weibull, exponential, Cauchy, or anything. The red line in the Q-Q plot represents the theoretical distribution of your data in a form of Cumulative Density Function (CDF). In [1]: from scipy import stats import matplotlib.pyplot as plt In [2]: # sample data generation data_1 = stats . skewnorm . rvs ( 4 , size = 1000 , random_state = 1 ) data_2 = stats . norm . rvs ( size = 1000 , random_state = 1 ) In [17]: # plotting plt . style . use ( 'seaborn-whitegrid' ) fig , axes = plt . subplots ( 2 , 2 , figsize = ( 6 , 4 )) # Q-Q plot stats . probplot ( data_1 , dist = stats . norm , plot = axes [ 0 , 0 ]) stats . probplot ( data_2 , dist = stats . norm , plot = axes [ 0 , 1 ]) axes [ 0 , 0 ] . set_title ( 'Normal Q-Q plot' ) axes [ 0 , 1 ] . set_title ( 'Normal Q-Q plot' ) axes [ 1 , 0 ] . hist ( data_1 , density = True , bins = 'auto' ) axes [ 1 , 1 ] . hist ( data_2 , density = True , bins = 'auto' ) fig . tight_layout () 5.3 Transform to normal distribution: Box-Cox When you want to construct a confidence interval of a sample, but the sample is non-normal, you have two options: transform the sample into a normal distribution, or use non-parametric alternatives. This section discusses the transformation part. WARNING! Use transformation techniques only if you really know what you are doing! The assumptions & back-transform process related with non-normal to normal distributions are very tricky, and can lead to wrong inferences when implemented wrongly. Box-Cox transformations are most useful when the transformed variable has its own interpretation. For example, when the transformed variable is symmetric, taking an inverse of the transformed mean yields the median of the original variable. This is a useful property if your goal is to estimate the central tendecy of a distribution. But its useless if your goal is to inference the C.I. of the mean of the original variable, in which case you will use non-parametric alternatives like bootstrap . There are three techniques used to map data to a normal distribution: Box-Cox, Yeo-Johnson, and normal quantile transform (check the SciPy documentation to see the comparison plot of each tecnnique). I will discuss Box-Cox transformation here. Box-Cox transformation is a type of power transformation to convert non-normal data to normal data by raising the distribution to a power of lambda ($\\lambda$). The algorithm automatically solves for $\\lambda$ that best transforms a distribution into a normal distribution. Box-Cox transformation is a statistical technique known to have remedial effects on highly skewed data. If $\\lambda$ is determined to be 2, then the distribution will be raised to a power of 2 — $Y&#94;2$. The exception to this rule is when the $\\lambda$ is 0 - log will be taken to the distribution — log($Y$). Note that using a $\\lambda = 1$ does not do anything to the distribution. If the Box-Cox algorithm spits out $\\lambda = 1$, it probably means that your data is Gaussian-like or Gaussian enough to an extent that there is no need for transformation. The figure below shows the effect of raising the exponential distribution (left) with the power of $\\lambda = .316$ to acquire the transformed normal distribution (right). Figure 22: Box-cox transformation Source Code For Figure (22) from scipy import stats import matplotlib.plyplot as plt # generate sample data x = stats.expon.rvs(size=100, random_state=1) # Box-Cox transform xt, lmbda = stats.boxcox(x) # plot fig, axes = plt.subplots(2, 2, figsize=(10, 5)) axes[0, 0].hist(x, histtype='stepfilled', edgecolor='k', alpha=0.4, color='grey', density=True) axes[0, 1].hist(xt, histtype='stepfilled', edgecolor='k', alpha=0.4, color='grey', density=True) stats.probplot(x, dist=stats.norm, plot=axes[1, 0]) stats.probplot(xt, dist=stats.norm, plot=axes[1, 1]) axes[0, 0].set_title('Original data') axes[0, 1].set_title(r'Transformed data, $\\lambda$=%.3f' % lmbda) axes[1, 0].set_title('Normal Q-Q plot') axes[1, 1].set_title('Normal Q-Q plot') axes[1, 1].set_ylabel('') for ax in axes.flatten(): ax.grid(True, linestyle='--', color='#acacac') ax.text(0.05, 0.85, 'aegis4048.github.io', fontsize=12, transform=ax.transAxes, color='grey', alpha=0.5) for ax in axes.flatten()[2:]: ax.get_lines()[0].set_marker('o') ax.get_lines()[0].set_color('grey') ax.get_lines()[0].set_markeredgecolor('black') ax.get_lines()[0].set_markersize(5.0) ax.get_lines()[0].set_alpha(0.5) ax.get_lines()[1].set_linewidth(1.0) ax.get_lines()[1].set_color('k') plt.subplots_adjust(wspace=13) fig.text(0.47, 0.33, 'Box-Cox', size=14, ha='center', va='top') fig.text(0.465, 0.3, r'$\\Longleftrightarrow$', size=30, ha='center', va='top') fig.text(0.47, 0.8, 'Box-Cox', size=14, ha='center', va='top') fig.text(0.465, 0.77, r'$\\Longleftrightarrow$', size=30, ha='center', va='top') fig.tight_layout() Not all transformations work perfectly The example illustrated above is the most ideal case of Box-Cox transform. Not all transforms work perfectly, especially in case of heavy tails. Visualize your transformed histrogram, and the Q-Q plot to evaluate the performance of the transform. If the transform did not mitigate the skewness (this is rare), you will have to look for non-parametric alternatives, like bootstrap explained above. If the transform did not mitigate the heavy-tails, your solution depends on the situation, as discussed above . Figure 23: Box-cox transformation Source Code For Figure (23) from scipy import stats import matplotlib.pyplot as plt # generate sample data x = stats.skewnorm.rvs(100, size=1000) + 1 # Box-Cox transform xt, lmbda = stats.boxcox(x) # plot fig, axes = plt.subplots(2, 2, figsize=(10, 5)) axes[0, 0].hist(x, histtype='stepfilled', edgecolor='k', alpha=0.4, color='grey', density=True) axes[0, 1].hist(xt, histtype='stepfilled', edgecolor='k', alpha=0.4, color='grey', density=True) stats.probplot(x, dist=stats.norm, plot=axes[1, 0]) stats.probplot(xt, dist=stats.norm, plot=axes[1, 1]) axes[0, 0].set_title('Original data') axes[0, 1].set_title(r'Transformed data, $\\lambda$=%.3f' % lmbda) axes[1, 0].set_title('Normal Q-Q plot') axes[1, 1].set_title('Normal Q-Q plot') axes[1, 1].set_ylabel('') for ax in axes.flatten(): ax.grid(True, linestyle='--', color='#acacac') ax.text(0.05, 0.85, 'aegis4048.github.io', fontsize=12, transform=ax.transAxes, color='grey', alpha=0.5) for ax in axes.flatten()[2:]: ax.get_lines()[0].set_marker('o') ax.get_lines()[0].set_color('grey') ax.get_lines()[0].set_markeredgecolor('black') ax.get_lines()[0].set_markersize(5.0) ax.get_lines()[0].set_alpha(0.5) ax.get_lines()[1].set_linewidth(1.0) ax.get_lines()[1].set_color('k') plt.subplots_adjust(wspace=13) fig.text(0.47, 0.33, 'Box-Cox', size=14, ha='center', va='top') fig.text(0.465, 0.3, r'$\\Longleftrightarrow$', size=30, ha='center', va='top') fig.text(0.47, 0.8, 'Box-Cox', size=14, ha='center', va='top') fig.text(0.465, 0.77, r'$\\Longleftrightarrow$', size=30, ha='center', va='top') fig.tight_layout() All data to be positive and greater than 0 (Y > 0) Box-Cox transformation does not work if data is smaller than 0. This can easily be fixed by adding a constant ($C$) that will make all your data greater than zero. The transformation equation is then: $$Y&#94;{'} = (Y + C)&#94;{\\lambda}$$ Note that you do not have to worry about this if you are using Yeo-Johnson transform instead of Box-Cox transform. Back-transformation Since box-cox transform raises a sample to the power of $\\lambda$, the scale of your sample changes. While removing skewness in a sample is desirable, the change in scale is not. Once you construct the confidence interval of a statistic of your interest, you have to take the inverse of the $\\lambda$ power to your estimated statistics to put it back to its original scale. In the same way, if you've added a constant $C$ to satisfy the assumption above, if have to subtract your statistical estimation by $C$ at the end. Pythonic Tip: Box-Cox transform with SciPy and Scikit-Learn There are two ways to do Box-Cox transform, and its back transform: with scipy.special.boxcox and sklearn.preprocessing.PowerTransformer . Scipy's implementation is preferrable for one-dimensional data, and Scikit-Learn's for multi-dimensional data. Note that you also need to backtransform your data, or your calculated statistics to its original scale. Here, I will calculate the centeral tendency of non-normal distribution , which is described by a median. SciPy Implementation First, you need to transform the data with scipy.special.boxcox . Make sure to return & store lmbda , which raises the original distribution to the power of $\\lambda$. You need it later to back-transform the calculated statistic into its original scale. Let's first generate non-normal sample data we are going to use, and take a look at it. Assess the normality of your sample with Q-Q plots explained above . In [26]: from scipy import stats import matplotlib.pyplot as plt import pandas as pd import numpy as np # generate non-normal sample x = stats . lognorm . rvs ( s = 1 , loc = 0 , scale = 5 , size = 1000 , random_state = 4 ) # plot fig , axes = plt . subplots ( 1 , 2 , figsize = ( 10 , 3 )) axes [ 0 ] . hist ( x ) stats . probplot ( x , dist = stats . norm , plot = axes [ 1 ]) fig . tight_layout () # summary statistics df = pd . DataFrame ( x ) . describe () . T df [ 'Median' ] = np . median ( x ) df Out[26]: count mean std min 25% 50% 75% max Median 0 1000.0 8.300435 9.606277 0.279772 2.768348 5.256457 9.943978 81.145311 5.256457 The distribution seems to be non-normal. The mean is 8.30, and the median is 5.26. Let's transform this data into a normal distribution with Box-Cox transform, and see how it changes. In [27]: from scipy import stats import matplotlib.pyplot as plt import pandas as pd import numpy as np # box-cox transform xt , lmbda = stats . boxcox ( x ) # plot fig , axes = plt . subplots ( 1 , 2 , figsize = ( 10 , 3 )) axes [ 0 ] . hist ( xt ) stats . probplot ( xt , dist = stats . norm , plot = axes [ 1 ]) fig . tight_layout () # summary statistics df = pd . DataFrame ( xt ) . describe () . T df [ 'Median' ] = np . median ( xt ) df Out[27]: count mean std min 25% 50% 75% max Median 0 1000.0 1.628602 0.924383 -1.288433 1.009014 1.635019 2.250321 4.227487 1.635019 The transformation seemed to work well. Note the change in scale due to transform. The mean changed from 8.30 to 1.63, and the median changed from 5.26 to 1.64. This shows the need to apply back-transformation to the calculated statistic. We proceed to calculate the confidence interval of your statistic. In this case, we compute the 95% C.I. of mean, with the code snippet described above . If the transformation did not work well, use a non-parametric alternative explained below . In [5]: import numpy as np from scipy import stats # 95% confidence interval of mean in a transformed scale lo_xt , hi_xt = stats . t . interval ( 1 - 0.05 , len ( xt ) - 1 , loc = np . mean ( xt , ddof = 1 ), scale = stats . sem ( xt )) ( lo_xt , hi_xt ) Out[5]: (1.571239695096202, 1.6859642780066368) Note that the above C.I. of mean is in the transformed scale. We need to apply back-transform to put it back to its original scale. We use scipy.special.inv_boxcox . In [6]: from scipy.special import inv_boxcox # inverse transform lo_x , hi_x = inv_boxcox ( lo_xt , lmbda ), inv_boxcox ( hi_xt , lmbda ) ( lo_x , hi_x ) Out[6]: (4.922371850163526, 5.5398361421708175) Taking the inverse of the transformed C.I. of mean returned a range of 4.93 ~ 5.54. Recall that in the original sample, Mean = 8.30, and median = 5.26. The result is consistent with the statement in the WARNING above : \"when the transformed variable is symmetric, taking an inverse of the transformed mean yields the median of the original variable.\" Additionally, we can also try a non-parametric alternative, bootstrap , to obtain the C.I. of median of the non-normal distribution. Recall that the variable x generated above is a non-normal sample. One can see that the C.I. of median obtained from Box-Cox and bootstrap agree with each other. In [7]: import numpy as np iteration = 100000 boot_median = [ np . median ( np . random . choice ( x , len ( x ))) for _ in range ( iteration )] lo_x_boot = np . percentile ( boot_median , 2.5 ) hi_x_boot = np . percentile ( boot_median , 97.5 ) ( lo_x_boot , hi_x_boot ) Out[7]: (4.832966540955844, 5.616555703219367) Scikit-Learn implementation Box-cox transform can also be implemented with Scikiy Learn: sklearn.preprocessing.PowerTransformer . Scikit-Learn's implementation of box-cox transformation is specialized for multi-dimensional data, and therefore it accepts 2-D array as its input. It's ability to handle multi-dimensional data is quite useful in machine learning. This function is available in versions scikit-learn v0.20 or above. Most versions of Anaconda support scikit-learn v0.19.1 . If you are using Anaconda Python, you will need to update Scikit Learn with the following command in the console (note that if you are using Jupyter Notebook, you must restart your Jupyter after upgrading Scikit-Learn): In [40]: # pip install scikit-learn==0.20 For consistency, we will keep using the non-normal sample x I generated above. Before we start, recall that x is an 1-D array. Since Scikiy-Learn's Box-Cox expects 2-D array as input, you will have to change the data type into a numpy matrix with np.asmatrix : In [5]: import numpy as np # convert data type x_mat = np . asmatrix ( x ) . T (Note: You need to have some basic understanding of Objected Oriented Programming (OOP) to understand this explanation) Let's apply the transform. First, instantiate the transformer object pt . Second, fit the $\\lambda$ parameter using fit() , which will store the $\\lambda$ parameter as a class attribute inside the pt object. The $\\lambda$ parameter stored will be used later to apply inverse transform. Last, use transform() to apply Box-Cox transformation. Note that appling pt.fit() and pt.transform() can also be done with one-line with pt.fit_transform() . In [30]: from sklearn.preprocessing import PowerTransformer # box-cox transform pt = PowerTransformer ( method = 'box-cox' , standardize = False ) pt . fit ( x_mat ) xt = pt . transform ( x_mat ) # convert data type for plotting xt = xt . flatten () # plot fig , axes = plt . subplots ( 1 , 2 , figsize = ( 10 , 3 )) axes [ 0 ] . hist ( xt ) stats . probplot ( xt , dist = stats . norm , plot = axes [ 1 ]) fig . tight_layout () # summary statistics df = pd . DataFrame ( xt ) . describe () . T df [ 'Median' ] = np . median ( xt ) df Out[30]: count mean std min 25% 50% 75% max Median 0 1000.0 1.628602 0.924383 -1.288433 1.009014 1.635019 2.250321 4.227487 1.635019 From this point, the procedures for obtaining the C.I. of median of a non-normal distribution are the same as in SciPy's implementation, except that we use pt.inverse_transform (Scikit-Learn) instead of inv_boxcox (SciPy), and convert 1-D array to 2-D datatype array. Note that the output is the exact same is the one given by the SciPy implementation above. In [29]: import numpy as np from scipy import stats # 95% confidence interval of mean in a transformed scale lo_xt , hi_xt = stats . t . interval ( 1 - 0.05 , len ( xt ) - 1 , loc = np . mean ( xt ), scale = stats . sem ( xt )) # convert data type lo_xt_mat = np . asmatrix ( lo_xt ) hi_xt_mat = np . asmatrix ( hi_xt ) # inverse transform hi_x = pt . inverse_transform ( hi_xt_mat )[ 0 ][ 0 ] lo_x = pt . inverse_transform ( lo_xt_mat )[ 0 ][ 0 ] ( lo_x , hi_x ) Out[29]: (4.9223718501635245, 5.539836142170828) 5.4 Non-parametric alternative: Bootstrap (Note: This section is a continuation of Section 4.4. Confidence interval of other statistics: Bootstrap explained above . For bootstrap code, navigate to here .) Bootstrap is the most popular choice when making a point estimation of a population parameter of an unknown distribution. It is distribution-free. This means that it does not assume anything about the shape of the distribution (Ex: normal, lognormal, weibull, exponential). For example, computing the confidence interval of mean with eq (1) requires symmetry of a distribution, as it relies on $\\pm$ standard error method. Any parametric methods (Ex: t-test, f-test) that assume normality of data has less coverage than their non-parametric alternatives when their assumptions are violated. Let's take a deeper look at the robustness of parametric vs non-parametric methods for different distributions by running simulations. We compute the confidence interval of mean and variance. CI of mean is known to be resistent to deviation from normality, while CI of variance is very sensitive. The procedure is as follows: Randomly generate POPUPLATION_SIZE observations from a specified distribution. Assume this to be a population. We generate 4 populations: normal, exponential, skewed, and lognormal. Each population goes through step 2-5. Randomly draw samples NUM_SAMPLES times from a population. Each sample has a size = SAMPLE_SIZE . Each sample goes through step 3-4. Compute the parametric & non-parametric 95% confidence interval of variance and mean for a sample. For parametric CI of mean and variance, use eq (1) and eq (10) . For non-parametric CI, use bootstrap . Check if a population mean or variance falls within the computed confidence interval. Count the number of times a population mean and variance falls within the computed confidence interval. The above procedure describes a situation in which we attempt to inference the unknown population parameters from a small portion of the population, a.k.a. the samples (check Section 3. Population vs Samples above ). We want to know if the computed confidence interval of samples contain the true population parameter. Figure (24) summarizes the simulation result: Figure 24: Coverage of parametric vs non-parametric confidence intervals Source Code For Figure (24) from scipy import stats import matplotlib.pyplot as plt import numpy as np # simulation settings POPUPLATION_SIZE = 100000 NUM_SAMPLES = 10000 SAMPLE_SIZE = 100 BOOT_SAMPLE_SIZE = 100 # analytical confidence interval of mean def ci_mean_analy(arr, alpha): return stats.t.interval(1 - alpha, len(arr) - 1, loc=np.mean(arr), scale=stats.sem(arr)) # bootstrap confidence interval of mean def ci_mean_boot(arr, p_lo, p_hi, r=BOOT_SAMPLE_SIZE): boot_var = [np.mean(np.random.choice(arr, len(arr))) for _ in range(r)] lo = np.percentile(boot_var, p_lo) hi = np.percentile(boot_var, p_hi) return (lo, hi) # analytical confidence interval of variance def ci_var_analy(arr, alpha): lo, hi = ( (len(arr) - 1) * np.var(arr, ddof=1) / stats.chi2.ppf(1 - alpha / 2, len(arr) - 1), (len(arr) - 1) * np.var(arr, ddof=1) / stats.chi2.ppf(alpha / 2, len(arr) - 1) ) return (lo, hi) # bootstrap confidence interval of variance def ci_var_boot(arr, p_lo, p_hi, r=BOOT_SAMPLE_SIZE): boot_var = [np.var(np.random.choice(arr, len(arr)), ddof=1) for _ in range(r)] lo = np.percentile(boot_var, p_lo) hi = np.percentile(boot_var, p_hi) return (lo, hi) # check if the confidence interval includes the population parameter def coverage_test(lo, hi, target): if lo <= target <= hi: return 1 else: return 0 # define population norm_pop = stats.norm.rvs(size=POPUPLATION_SIZE, scale=100) expon_pop = stats.expon.rvs(size=POPUPLATION_SIZE, scale=100) skew_pop = stats.skewnorm.rvs(a=50, size=POPUPLATION_SIZE, scale=100) lognorm_pop = stats.lognorm.rvs(1, size=POPUPLATION_SIZE, scale=100) # population mean mean_norm_pop = np.mean(norm_pop) mean_expon_pop = np.mean(expon_pop) mean_skew_pop = np.mean(skew_pop) mean_lognorm_pop = np.mean(lognorm_pop) # population variance var_norm_pop = np.var(norm_pop, ddof=0) var_expon_pop = np.var(expon_pop, ddof=0) var_skew_pop = np.var(skew_pop, ddof=0) var_lognorm_pop = np.var(lognorm_pop, ddof=0) # initialize count_mean_norm_analy, count_mean_expon_analy, count_mean_skew_analy, count_mean_lognorm_analy = [], [], [], [] count_mean_norm_boot, count_mean_expon_boot, count_mean_skew_boot, count_mean_lognorm_boot = [], [], [], [] count_var_norm_analy, count_var_expon_analy, count_var_skew_analy, count_var_lognorm_analy = [], [], [], [] count_var_norm_boot, count_var_expon_boot, count_var_skew_boot, count_var_lognorm_boot = [], [], [], [] for i in range(NUM_SAMPLES): # randomly draw samples from population norm_samp = np.random.choice(norm_pop, SAMPLE_SIZE) expon_samp = np.random.choice(expon_pop, SAMPLE_SIZE) skew_samp = np.random.choice(skew_pop, SAMPLE_SIZE) lognorm_samp = np.random.choice(lognorm_pop, SAMPLE_SIZE) # 95% ANALYTICAL confidence interval of MEAN mean_lo_norm_analy, mean_hi_norm_analy = ci_mean_analy(norm_samp, 0.05) mean_lo_expon_analy, mean_hi_expon_analy = ci_mean_analy(expon_samp, 0.05) mean_lo_skew_analy, mean_hi_skew_analy = ci_mean_analy(skew_samp, 0.05) mean_lo_lognorm_analy, mean_hi_lognorm_analy = ci_mean_analy(lognorm_samp, 0.05) # 95% BOOTSTRAP confidence interval of MEAN mean_lo_norm_boot, mean_hi_norm_boot = ci_mean_boot(norm_samp, 2.5, 97.5) mean_lo_expon_boot, mean_hi_expon_boot = ci_mean_boot(expon_samp, 2.5, 97.5) mean_lo_skew_boot, mean_hi_skew_boot = ci_mean_boot(skew_samp, 2.5, 97.5) mean_lo_lognorm_boot, mean_hi_lognorm_boot = ci_mean_boot(lognorm_samp, 2.5, 97.5) # 95% ANALYTICAL confidence interval of VARIANCE var_lo_norm_analy, var_hi_norm_analy = ci_var_analy(norm_samp, 0.05) var_lo_expon_analy, var_hi_expon_analy = ci_var_analy(expon_samp, 0.05) var_lo_skew_analy, var_hi_skew_analy = ci_var_analy(skew_samp, 0.05) var_lo_lognorm_analy, var_hi_lognorm_analy = ci_var_analy(lognorm_samp, 0.05) # 95% BOOSTRAP confidence interval of VARIANCE var_lo_norm_boot, var_hi_norm_boot = ci_var_boot(norm_samp, 2.5, 97.5) var_lo_expon_boot, var_hi_expon_boot = ci_var_boot(expon_samp, 2.5, 97.5) var_lo_skew_boot, var_hi_skew_boot = ci_var_boot(skew_samp, 2.5, 97.5) var_lo_lognorm_boot, var_hi_lognorm_boot = ci_var_boot(lognorm_samp, 2.5, 97.5) # coverage test for ANALYTICAL CI of MEAN count_mean_norm_analy.append(coverage_test(mean_lo_norm_analy, mean_hi_norm_analy, mean_norm_pop)) count_mean_expon_analy.append(coverage_test(mean_lo_expon_analy, mean_hi_expon_analy, mean_expon_pop)) count_mean_skew_analy.append(coverage_test(mean_lo_skew_analy, mean_hi_skew_analy, mean_skew_pop)) count_mean_lognorm_analy.append(coverage_test(mean_lo_lognorm_analy, mean_hi_lognorm_analy, mean_lognorm_pop)) # coverage test for BOOTSTRAP CI of MEAN count_mean_norm_boot.append(coverage_test(mean_lo_norm_boot, mean_hi_norm_boot, mean_norm_pop)) count_mean_expon_boot.append(coverage_test(mean_lo_expon_boot, mean_hi_expon_boot, mean_expon_pop)) count_mean_skew_boot.append(coverage_test(mean_lo_skew_boot, mean_hi_skew_boot, mean_skew_pop)) count_mean_lognorm_boot.append(coverage_test(mean_lo_lognorm_boot, mean_hi_lognorm_boot, mean_lognorm_pop)) # coverage test for ANALYTICAL CI of VARIANCE count_var_norm_analy.append(coverage_test(var_lo_norm_analy, var_hi_norm_analy, var_norm_pop)) count_var_expon_analy.append(coverage_test(var_lo_expon_analy, var_hi_expon_analy, var_expon_pop)) count_var_skew_analy.append(coverage_test(var_lo_skew_analy, var_hi_skew_analy, var_skew_pop)) count_var_lognorm_analy.append(coverage_test(var_lo_lognorm_analy, var_hi_lognorm_analy, var_lognorm_pop)) # coverage test for BOOTSTRAP CI of VARIANCE count_var_norm_boot.append(coverage_test(var_lo_norm_boot, var_hi_norm_boot, var_norm_pop)) count_var_expon_boot.append(coverage_test(var_lo_expon_boot, var_hi_expon_boot, var_expon_pop)) count_var_skew_boot.append(coverage_test(var_lo_skew_boot, var_hi_skew_boot, var_skew_pop)) count_var_lognorm_boot.append(coverage_test(var_lo_lognorm_boot, var_hi_lognorm_boot, var_lognorm_pop)) print() print('###########################################################') print(' Coverage Test (%) ') print('###########################################################') print() print() print('------------------ Parametric CI of mean ------------------') print() print('{:<40} {:>15} %'.format('Normal Population: ', round(sum(count_mean_norm_analy) / NUM_SAMPLES * 100, 1))) print('{:<40} {:>15} %'.format('Exponential Population: ', round(sum(count_mean_expon_analy) / NUM_SAMPLES * 100, 1))) print('{:<40} {:>15} %'.format('Skewed Population: ', round(sum(count_mean_skew_analy) / NUM_SAMPLES * 100, 1))) print('{:<40} {:>15} %'.format('Lognormal Population: ', round(sum(count_mean_lognorm_analy) / NUM_SAMPLES * 100, 1))) print() print('------------------ Non-parametric CI of mean --------------') print() print('{:<40} {:>15} %'.format('Normal Population: ', round(sum(count_mean_norm_boot) / NUM_SAMPLES * 100, 1))) print('{:<40} {:>15} %'.format('Exponential Population: ', round(sum(count_mean_expon_boot) / NUM_SAMPLES * 100, 1))) print('{:<40} {:>15} %'.format('Skewed Population: ', round(sum(count_mean_skew_boot) / NUM_SAMPLES * 100, 1))) print('{:<40} {:>15} %'.format('Lognormal Population: ', round(sum(count_mean_lognorm_boot) / NUM_SAMPLES * 100, 1))) print() print('---------------- Parametric CI of variance ----------------') print() print('{:<40} {:>15} %'.format('Normal Population: ', round(sum(count_var_norm_analy) / NUM_SAMPLES * 100, 1))) print('{:<40} {:>15} %'.format('Exponential Population: ', round(sum(count_var_expon_analy) / NUM_SAMPLES * 100, 1))) print('{:<40} {:>15} %'.format('Skewed Population: ', round(sum(count_var_skew_analy) / NUM_SAMPLES * 100, 1))) print('{:<40} {:>15} %'.format('Lognormal Population: ', round(sum(count_var_lognorm_analy) / NUM_SAMPLES * 100, 1))) print() print('---------------- Non-parametric CI of variance ------------') print() print('{:<40} {:>15} %'.format('Normal Population: ', round(sum(count_var_norm_boot) / NUM_SAMPLES * 100, 1))) print('{:<40} {:>15} %'.format('Exponential Population: ', round(sum(count_var_expon_boot) / NUM_SAMPLES * 100, 1))) print('{:<40} {:>15} %'.format('Skewed Population: ', round(sum(count_var_skew_boot) / NUM_SAMPLES * 100, 1))) print('{:<40} {:>15} %'.format('Lognormal Population: ', round(sum(count_var_lognorm_boot) / NUM_SAMPLES * 100, 1))) Highlights of the simulation result There are lots of things we can learn from the simulation result, shown in figure (24) . I want to mainly emphasize on the distinction between the parametric vs. non-parametric methods here. Note that bootstrap is one kind of non-parametric methods Some statistics are sensitive to deviation from normality Mean is resistent to deviation from normality, while variance is not. One of the reason that t-test is so popular is because the C.I. of mean is resistent to deviation from normality (which is translated as broader applicability). On the other hand, C.I. of variance is rarely used due to its extreme sensitivity (check out Section 4.3. Confidence interval of variance above ). Similar idea applies to the other statistics (Ex: covariance, regression coefficient, f-statistic). Do not blindly use parametric methods if you are not sure if a population satisfies the assumptions. Parametric methods acquire 95% nominal coverage rate ONLY under normality This is an expected behavior, as I specified the confidence level to be 95% in the simulation setting; it means that I want the C.I. to corretly guess the population parameter 19/20 times. Parametric C.I.'s were able to acquire the norminal 95% coverage rate under normality, but when the population was not normal, it acquired lower coverage rate due to the violation of normality. Making an inference about heavy-tailed distribution is difficult. Exponential and lognormal distributions have heavier tails than normal distribution. This means that they have extreme data points that significantly affect the population parameter, and yet the occurrence rate of those extreme points is so low that they are unlikely to be included in the collected samples, which in turn increases bias. Theoretically, exponential is heavier-tailed than normal, and lognormal is heavier-tailed than exponential. In the simulation result, you can see that the coverage rate drops in order: normal -> exponential -> lognormal. For more information check out Problems with kurtosis (heavy-tails) above . Bootstrap outperforms parametric method under non-normality This is the whole point of using non-parametric methods. We can see this by comparing the coverage rate of variance for exponential and lognormal populations. For the variance of lognormal distribution, the difference in coverage rate is almost double. Parametric method outperforms non-parametric method under normality You don't want to blindly always use non-parametric methods. If you are confident that your population is normally distributed, use parametric method. Not doing so results in abandoning a piece of information which is perfectly fine. You can see this by comparing the coverage rate of normal distribution for both parametric and non-parametric methods. An exception to this rule is when a sample size very large. In that case, you can blindly use bootstrap, as the samples are diverse enough to account for all possible variations within a population. Check out figure (13) ; bootstrap C.I. gets approximates the analytical C.I. really well with the increasing sample size. Non-parametric methods yield \"approximate\" solutions with smaller risk The central idea behind most non-parametric methods is to approximate the near-exact solution with asymptotic process — their accuracy increases with the increasing sample size. Pay attention to the term, \"near-exact\" . Parametric methods are statistically more powerful when their assumptions are met; they guess the exact solution. But in return, when the assumptions are not met, they suffer massive loss in coverage rate, as it can be seen in the C.I. of variance of lognormal & exponential distributions. On the other hand, non-parametric methods highlight on broader applicability with smaller risk, which can be overcome by increasing sample size. They aim to acquire near-exact solutions by making less assumptions about the population.","tags":"Statistics","url":"https://aegis4048.github.io/comprehensive_confidence_intervals_for_python_developers","loc":"https://aegis4048.github.io/comprehensive_confidence_intervals_for_python_developers"},{"title":"Optimize Computational Efficiency of Skip-Gram with Negative Sampling","text":"Contents 1. Review on Word2Vec Skip-Gram 1.1. Review on Softmax 1.2. Softmax is computationally very expensive 2. Skip-Gram negative sampling Notes: Choice of $K$ 2.1. How does negative sampling work? Notes: Sigmoid function $\\sigma(x)$ Notes: Drawing random negative samples 2.1.1. What is a positive word $c_{pos}$? 2.1.2. What is a negative word $c_{neg}$? 2.1.3. What is a noise distribution $P_n(w)$? Notes: Incremental noise distribution 2.1.4. How are negative samples drawn? 2.2. Derivation of cost function in negative sampling Notes: Probability product 2.3. Derivation of gradients 2.3.1. Gradients with respect to output weight matrix $\\frac{\\partial-J}{\\partial-W_{output}}$ Notes: Prediction error 2.3.2. Gradients with respect to input weight matrix $\\frac{\\partial-J}{\\partial-W_{input}}$ 2.4. Negative sampling algorithm 2.5. Numerical demonstration The code snippet assumes Anaconda 5.2.0 version of Python virtual environment Acknowledgement The materials on this post are based the on five NLP papers, Distributed Representations of Words and Phrases and their Compositionality (Mikolov et al., 2013), word2vec Parameter Learning Explained (Rong, 2014), Distributed Negative Sampling for Word Embeddings (Stergiou et al., 2017), Incremental Skip-gram Model with Negative Sampling (Kaji and Kobayashi, 2017), and word2vec Explained: Deriving Mikolov et al.'s Negative-Sampling Word-Embedding Method (Goldberg and Levy, 2014). 1. Review on Word2Vec Skip-Gram In my previous post , I illustrated the neural network structure of Skip-Gram Word2Vec model that represents words in a vector space. Figure 1: Neural network structure of Skip-Gram I also derived the cost function of Skip-Gram in Derivation of Cost Function : $$\\begin{align} J(\\theta) = & - \\frac{1}{T} \\sum_{t=1}&#94;{T} \\sum_{-c\\leq j \\leq c,j\\neq 0} \\log p(w_{t+j} \\mid w_t ; \\, \\theta) \\\\ \\end{align} \\tag{1} $$ where $p(w_{t+j} \\mid w_t ; \\, \\theta)$ is a probability of observing $w_{t+j}$ given $w_{t}$ with parameters $\\theta$. In vanilla Skip-Gram, the probability is computed with softmax. I also noted that stochastic gradient descent (SGD) is used to mitigate computational burden — the size of $T$ in $\\frac{1}{T} \\sum&#94;T_{t=1}$ can be billions or more in NLP applications. The new cost function using SGD is: $$J(\\theta; w&#94;{(t)}) = - \\sum_{c=1}&#94;{C} log \\frac{exp(W_{output_{(c)}} \\cdot h)}{\\sum&#94;V_{i=1}exp(W_{output_{(i)}} \\cdot h)} \\tag{2}$$ where $T$ is the size of training samples, $C$ is the window size , $V$ is the size of unique vocab in the corpus, and $W_{input}$, $W_{output}$ and $h$ are illustrated in Figure 1 . 1.1. Review on Softmax Softmax is a multinomial regression classifier. It means that it classifies multiple labels, such as predicting if an hand-written digit is $0,\\,1,\\,2,\\,...\\,8\\,$ or $9$. In case of binary classification (True or False), such as classifying fraud or not-fraud in bank transactions, binomial regression classifier called Sigmoid function is used. In eq-2 , the fraction inside the summation of log yields the probability distribution of all $V$-vocabs in the corpus, given the input word. In statistics, the conditional probability of $A$ given $B$ is denoted as $p(A|B)$. In Skip-Gram, we use the notation, $p(w_{context}| w_{center})$, to denote the conditional probability of observing a context word given a center word. It is obtained by using the softmax function: $$ p(w_{context}|w_{center}) = \\frac{exp(W_{output_{(context)}} \\cdot h)}{\\sum&#94;V_{i=1}exp(W_{output_{(i)}} \\cdot h)} \\in \\mathbb{R}&#94;{1} \\tag{3} $$ Exponentiation ensures that the transformed values are positive, and the normalization factor in the denominator ensures that the values have a range of $[0, 1)$ and their sum equals $1$. Figure 2: softmax function transformation The probability is computed $V$ times using eq-3 to obtain a conditional probability distribution of observing all $V$-unique vocabs in the corpus, given a center word ($w&#94;{(t)}$). $$ \\left[ \\begin{array}{c} p(w_{1}|w&#94;{(t)}) \\\\ p(w_{2}|w&#94;{(t)}) \\\\ p(w_{3}|w&#94;{(t)}) \\\\ \\vdots \\\\ p(w_{V}|w&#94;{(t)}) \\end{array} \\right] = \\frac{exp(W_{output} \\cdot h)}{\\sum&#94;V_{i=1}exp(W_{output_{(i)}} \\cdot h)} \\in \\mathbb{R}&#94;{V}\\tag{4} $$ 1.2. Softmax is computationally very expensive There is an issue with the vanilla Skip-Gram — softmax is computationally very expensive, as it requires scanning through the entire output embedding matrix ($W_{output}$) to compute the probability distribution of all $V$ words, where $V$ can be millions or more. Figure 3: Algorithm complexity of vanilla Skip-Gram Furtheremore, the normalization factor in the denominator also requires $V$ iterations. In mathematical context, the normalization factor needs to be computed for each probability p($w_{context}| w_{center}$), making the alogrithm complexity = $O(V \\times V)$. However, when implemented on code, the normalization factor is computed only once and cached as a Python variable, making the alogrithm complexity = $O(V + V) \\approx O(V)$. This is possible because normalization factor is the same for all words. Due to this computational inefficiency, softmax is not used in most implementaions of Skip-Gram . Instead we use an alternative called negative sampling with sigmoid function , which rephrases the problem into a set of independent binary classification task of algorithm complexity = $O(K \\, + \\, 1)$, where $K$ typically has a range of $[5, 20]$. 2. Skip-Gram negative sampling In Skip-Gram, assuming stochastic gradient descent , weight marices in the neural network are updated for each training sample to correctly predict output. Let's assume that the training corpus has 10,000 unique vocabs ($V$ = 10000) and the hidden layer is 300-dimensional ($N$ = 300). This means that there are 3,000,000 neurons in the output weight matrix ($W_{output}$) that need to be updated for each training sample (Notes: for the input weight matrix ($W_{input}$), only 300 neurons are updated for each training sample. This is illustrated in figure 18 of my previous post .) Since the size of the training corpus ($T$) is very large, updating 3M neurons for each training sample is unrealistic in terms of computational efficiency. Negative sampling addresses this issue by updating only a small fraction of the output weight neurons for each training sample. In negative sampling, $K$ negative samples are randomly drawn from a noise distribution . $K$ is a hyper-parameter that can be empirically tuned, with a typical range of $[5,\\, 20]$. For each training sample (positive pair: $w$ and $c_{pos}$), you randomly draw $K$ number of negative samples from a noise distribution $P_n(w)$, and the model will update $(K+1) \\times N$ neurons in the output weight matrix ($W_{output}$). $N$ is the dimension of the hidden layer ($h$), or the size of a word vector. $+1$ accounts for a positive sample . With the above assumption, if you set K=9 , the model will update $(9 + 1) \\times 300 = 3000$ neurons, which is only 0.1% of the 3M neurons in $W_{output}$. This is computationally much cheaper than the original Skip-Gram, and yet maintains a good quality of word vectors. The below figure has 3-dimensional hidden layer ($N=3$), 11 vocabs ($V=11$), and 3 negative samples ($K=3$). Figure 4: Skip-Gram model structure Notes: Choice of $K$ The paper (Mikolov et al., 2013) says that K=2 ~ 5 works for large data sets, and K=5 ~ 20 for small data sets. 2.1. How does negative sampling work? With negative sampling, word vectors are no longer learned by predicting context words of a center word. Instead of using softmax to compute the $V$-dimensional probability distribution of observing an output word given an input word, $p(w_O|w_I)$, the model uses sigmoid function to learn to differentiate the actual context words ( positive ) from randomly drawn words ( negative ) from the noise distribution $P_n(w)$. Assume that the center word is \"regression\" . It is likely to observe \"regression\" + { \"logistic\" , \"machine\" , \"sigmoid\" , \"supervised\" , \"neural\" } pairs, but it is unlikely to observe \"regression\" + { \"zebra\" , \"pimples\" , \"Gangnam-Style\" , \"toothpaste\" , \"idiot\" }. The model maximizes the probability $p(D=1|w,c_{pos})$ of observing positive pairs, while minimizing the probability $p(D=1|w,c_{neg})$ of observing negative pairs. The idea is that if the model can distinguish between the likely (positive) pairs vs unlikely (negative) pairs, good word vectors will be learned. Figure 5: Binomial classification of negative sampling Negative sampling converts multi-classification task into binary-classification task. The new objective is to predict, for any given word-context pair ($w,\\,c$), whether the word ($c$) is in the context window of the the center word ($w$) or not. Since the goal is to identify a given word as True ( positive , $D=1$) or False ( negative , $D=0$), we use sigmoid function instead of softmax function. The probability of a word ($c$) appearing within the context of the center word ($w$) can be defined as the following: $$ p(D=1|w,c;\\theta)=\\frac{1}{1+exp(-\\bar{c}_{output_{(j)}} \\cdot w)} \\in \\mathbb{R}&#94;{1} \\tag{5} $$ where $c$ is the word you want to know whether it came from the context window or the noise distribution. $w$ is the input (center) word, and $\\theta$ is the weight matrix passed into the model. Note that $w$ is equivalent to the hidden layer ($h$). $\\bar{c}_{output_{(j)}}$ is the word vector from the output weight matrix ($W_{output}$) of Figure 1 . Eq-5 computes the probability that the given word ($c$) is a positive word ($D=1$). It only needs to be applied $K + 1$ times instead of $V$ times for every word in the vocabulary, because $\\bar{c}_{output_{(j)}}$ comes from the concatenation of a true context word ($c_{pos}$) and $K$ negative words ($\\bar{W}_{neg} = \\{ \\bar{c}_{neg, j}|j=1,\\cdots,K \\}$): $$ \\bar{c}_{output{(j)}} \\in \\{\\bar{c}_{pos}\\} \\cup \\bar{W}_{neg} \\tag{6} $$ This probability is computed $K + 1$ times to obtain a probability distribution of a true context word and $K$ negative samples: $$ \\left[ \\begin{array}{c} p(D=1|w,c_{pos}) \\\\ p(D=1|w,c_{neg, 1}) \\\\ p(D=1|w,c_{neg, 2}) \\\\ p(D=1|w,c_{neg, 3}) \\\\ \\vdots \\\\ p(D=1|w,c_{neg, K}) \\end{array} \\right] = \\frac{1}{1+exp(-(\\{\\bar{c}_{pos}\\} \\cup \\bar{W}_{neg}) \\cdot h)} \\in \\mathbb{R}&#94;{K+1}\\tag{7} $$ Compare this equation with eq-4 — you will notice that eq-7 is computationally much cheaper because $K$ is between 5 ~ 20, whereas $V$ can be millions. Moreover, no extra iterations are necessary to compute the normalization factor in the denominator of eq-4 , because sigmoid function is a binary regression classifier. The algorithm complexity for probability distribution of vanilla Skip-Gram is $O(V)$, whereas negative sampling's is $O(K+1)$. This shows why negative sampling saves a significant amount of computational cost per iteration. Notes: Sigmoid function $\\sigma(x)$ Sigmoid function is used for two-class logistic regression. $$ \\sigma(x) = \\frac{1}{1+exp(-x))}$$ It is used to classify if a given sample is True or False based on the computed probability. The sample is classified as True if the value is greater then 0.5, and vice versa. For example, if you want to classify if a certain bank transaction is fraud or not, you will use sigmoid for binary classification. If you are working on a multi-class task, such as hand-written digit classification, you will use softmax regression classifier. from matplotlib import pylab import pylab as plt import numpy as np %matplotlib notebook #sigmoid = lambda x: 1 / (1 + np.exp(-x)) def sigmoid(x): return (1 / (1 + np.exp(-x))) mySamples = [] mySigmoid = [] x = plt.linspace(-10,10,10) y = plt.linspace(-10,10,100) plt.plot(y, sigmoid(y)) plt.title('Sigmoid Function $\\sigma(x)$') plt.text(4, 0.8, r'$\\sigma(x)=\\frac{1}{1+e&#94;{-x}}$', fontsize=15) plt.xlabel('X', fontsize=12) plt.ylabel('$\\sigma(x)$', fontsize=14) Figure 6: Choice of negative samples (Text source: Petrowiki ) For the purpose of illustration, consider the above paragraphs. Assume that our center word ($w$) is drilling , window size is $3$, and the number of negative samples ($K$) is $5$. With the window size of $3$, the contexts words are: \"engineer\" , \"traditionally\" , \"designs\" , \"fluids\" , \"with\" , and \"two\" . These context words are considered as positive labels ($D = 1$). Our current context word ($c_{pos}$) is engineer . We also need negative words. We randomly pick $5$-words from the noise distribution $P_n(w)$ of the corpus for each context word, and consider them as negative samples ($D = 0$). For the current context word, engineer , the 5 randomly drawn negative words ($c_{neg}$) are: \"minimized\" , \"primary\" , \"concerns\" , \"led\" , and \"page\" . The idea of negative sampling is that it is more likely to observe positive word pairs ($w$, $c_{pos}$) together than negative word pairs ($w$, $c_{neg}$) together in the corpus. The model attempts to maximize the the probability of observing positive pairs $p(c_{pos}|w) \\rightarrow 1$ and minimize the probability of observing negative pairs $p(c_{neg}|w) \\rightarrow 0$ simultaneously by iterating through the training samples and updating the weights ($\\theta$). Note that the sum of the probability distribution obtained by sigmoid function ( eq-7 ) does not need to equal $1$, unlike softmax ( eq-4 ). Figure 7: maximizing postive pairs and minimizing negative pairs By the time the output probability distribution is nearly one-hot-encoded as in $iter = 4$ of the above figure, weight matrices $\\theta$ are optimized and good word vectors are learned. This optimization is achieved by maximizing the dot product of positive pairs ($\\bar{c}_{pos}\\cdot \\bar{w}$) and minimizing the dot product of negative pairs ($c_{neg}\\cdot w$) in eq-18 . Notes: Drawing random negative samples For each positive word-context pair ($w,\\,c_{pos}$), $K$ new negative samples are randomly drawn from a noise distribution . In Figure 6 , there are $6$ positive context words ( \"engineer\" , \"traditionally\" , \"designs\" , \"fluids\" , \"with\" , and \"two\" ) for one center word ( \"drilling\" ), and $K$ is $5$. This means that a total of $6 \\times 5 = 30$ word vectors are updated for each center word $w$. 2.1.1. What is a positive word $c_{pos}$? Words that actually appear within the context window of the center word ($w$). After the model is optimized, the probability computed with eq-5 for positive words $c_{pos}$ will output $\\approx$ 1 as shown in Figure 7 . A word vector of a center word ($w$) will be more similar to a word vector of positive word ($c_{pos}$) than of randomly drawn negative words ($c_{neg}$). This is because words that frequently appear together show strong correlation with each other. Therefore, once the model is optimized: $p(D=1|w,c_{pos})\\approx1$. 2.1.2. What is a negative word $c_{neg}$? Words that are randomly drawn from a noise distribution $P_n(w)$. After the model is optimized, the probability computed with eq-5 for negative words $c_{neg}$ will output $\\approx$ 0 as shown in Figure 7 . When training an Word2Vec model, the vocab size ($V$) easily exceeds tens of thousands. When 5 ~ 20 negative samples are randomly drawn among the vocabs, it is unlikely to observe the random word with a center word together in the corpus. Therefore, once the model is optimized: $p(D=1|w,c_{neg})\\approx0$. 2.1.3. What is a noise distribution $P_n(w)$? Imagine a distribution of words based on how many times each word appeared in a corpus, denoted as $U(w)$ (this is called unigram distribution). For each word $w$, divide the number of times it appeared in a corpus by a normalization factor $Z$ so that the distribution becomes a probability distribution of range $[0, 1)$ and sums up to $1$. Raise the normalized distribution to the power of $\\alpha$ so that the distribution is \"smoothed-out\". Then this becomes your noise distribution $P_n(w)$ — normalized frequency distribution of words raised to the power of $\\alpha$. Mathematically, it can be expressed as: $$ P_n(w) = \\left(\\frac{U(w)}{Z}\\right)&#94;{\\alpha} \\tag{8} $$ Raising the unigram distribution $U(w)$ to the power of $\\alpha$ has an effect of smoothing out the distribution. It attempts to combat the imbalance between common words and rare words by decreasing the probability of drawing common words, and increasing the probability drawing rare words. $\\alpha$ is a hyper-parameter that can be empirically tuned. The authors of the original Word2Vec paper claims that the unigram distribution $U(w)$ raised to the $3/4$rd power (i.e., $U(w)&#94;{3/4}/Z$) yielded the best result. Figure 8: Effect of raising power of unigram distribution $U(w)$ Notes: Incremental noise distribution Noise distribution $P_n(w)$ needs to be pre-computed by iterating through the entire corpus to obtain word frequency $U(w)$ and normalization factor $Z$ of the distribution. If an additional training data is added to the corpus, $P_n(w)$ needs to be computed all over again. To address this problem, a simple incremental extension of negative sampling was provided in this paper (Kaji and Kobayashi, 2017). import numpy as np import matplotlib.pyplot as plt from scipy import stats %matplotlib notebook # sample data generation data = sorted(stats.norm.rvs(size=1000) + 5) # fit normal distribution mean, std = stats.norm.fit(data, loc=0) pdf_norm = stats.norm.pdf(data, mean, std) temp = np.power(data, 3/4) temp_mean, temp_std = stats.norm.fit(temp, loc=0) temp_pdf_norm = stats.norm.pdf(temp, temp_mean, temp_std) fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(8, 4), sharey=True) ax1.set_title('$U(w)$') ax1.hist(temp, bins='auto', density=True) ax1.set_xticklabels(['apple', 'desk', 'cup', 'chair', 'zebra', 'room', 'pencil', 'water', 'coin']) ax1.plot(temp, temp_pdf_norm, label='norm') ax2.set_title('$U(w)&#94;{3/4}$') ax2.hist(data, bins='auto', density=True) ax2.plot(data, pdf_norm, label='norm') ax2.set_xticklabels(['apple', 'desk', 'cup', 'chair', 'zebra', 'room', 'pencil', 'water', 'coin']) 2.1.4. How are negative samples drawn? $K$-negative samples are randomly drawn from a noise distribution $P_n(w)$. The noise distribution is generated with eq-8 and the random samples are drawn with np.random.choice . In [2]: import numpy as np In [7]: unig_dist = { 'apple' : 0.023 , 'bee' : 0.12 , 'desk' : 0.34 , 'chair' : 0.517 } sum ( unig_dist . values ()) Out[7]: 1.0 In [5]: alpha = 3 / 4 noise_dist = { key : val ** alpha for key , val in unig_dist . items ()} Z = sum ( noise_dist . values ()) noise_dist_normalized = { key : val / Z for key , val in noise_dist . items ()} noise_dist_normalized Out[5]: {'apple': 0.044813853132981724, 'bee': 0.15470428538870049, 'desk': 0.33785130228003507, 'chair': 0.4626305591982827} In [8]: sum ( noise_dist_normalized . values ()) Out[8]: 1.0 In the initial unigram distribution, chair appeared the most in the corpus, and had 0.517 chance of being drawn as a negative sample. However, the unigram distribution is raised to the power of $3/4$rd to combat the imbalance between common vs rare words, as shown in Figure 8 . After unig_dist was raised to the power of alpha = 3/4 and normalized, chair now has 0.463 chance of being drawn. On the other hand, apple had the lowest probability ( 0.023 ) of being drawn. After the transformation, it now has a bit higher probability ( 0.049 ) of being drawn. The imbalance between the most common word ( chair ) and the least common word ( apple ) was mitigated. Once the noise distribution ( noise_dist_normalized ) is generated, you randomly draw $K$ negative samples according to each word's probability. In [6]: K = 10 np . random . choice ( list ( noise_dist_normalized . keys ()), size = K , p = list ( noise_dist_normalized . values ())) Out[6]: array(['apple', 'chair', 'bee', 'desk', 'chair', 'bee', 'bee', 'chair', 'desk', 'chair'], dtype=' 2.2. Derivation of cost function in negative sampling The derivations written here are based on the work of word2vec Explained: Deriving Mikolov et al.'s Negative-Sampling Word-Embedding Method (Goldberg and Levy, 2014). Consider a pair ($w,\\,c$) of center word and its context. Did this pair come from the context window or the noise distribution ? Let $p(D=1|w,c)$ be the probability that ($w,\\,c$) is observed in the true corpus, and $p(D=0|w,c) = 1 - p(D=1|w,c)$ the probability that ($w,\\,c$) is non-observed. There are parameters $\\theta$ controlling the probability distribution: $p(D=1|w,c;\\theta)$. Negative sampling attempts to optimize the parameters $\\theta$ by maximizing the probability of observing positive pairs ($w,\\,c_{pos}$) while minimizing the probability of observing negative pairs ($w,\\,c_{neg}$). For each positive pair, the model randomly draws $K$ negative words $W_{neg} = \\{ c_{neg, j}|j=1,\\cdots,K \\}$. Our objective function is then: $$ \\begin{align} & \\underset{\\theta}{\\text{argmax}} \\,\\,\\, p(D=1|w,c_{pos};\\theta) \\prod_{c_{neg} \\in W_{neg}} p(D=0|w,c_{neg};\\theta) \\label{}\\tag{9}\\\\ =\\quad & \\underset{\\theta}{\\text{argmax}} \\,\\,\\, p(D=1|w,c_{pos};\\theta) \\prod_{c_{neg} \\in W_{neg}} (1 - p(D=1|w,c_{neg};\\theta))\\label{}\\tag{10} \\end{align} $$ Notes: Probability product In statistics, probability of observing $C$ multiple events at the same time is computed by the product of each event's probability. $$p(x_{1}, x_{2} ... x_{C}) = p(x_{1}) \\times p(x_{2}) \\, \\times \\, ... \\, \\times \\, p(x_{C})$$ This can be shortened with a product notation: $$p(x_{1}, x_{2} ... x_{C}) = \\prod_{c=1}&#94;{C}p(x_{c})$$ It is a common practice in machine learning to take a natural log to the objective function to simplify derivations. This does not affect the optimized weights ($\\theta$) because natural log is a monotonically increasing function. It ensures that the maximum value of the original probability function occurs at the same point as the log probability function. Therefore: $$ \\begin{align} =\\quad & \\underset{\\theta}{\\text{argmax}} \\,\\,\\, log \\, p(D=1|w,c_{pos};\\theta) \\prod_{c_{neg} \\in W_{neg}} (1 - p(D=1|w,c_{neg};\\theta)) \\label{}\\tag{11} \\end{align} $$ Using the property of logs, the objective function can be simplified: $$ \\begin{align} =\\quad & \\underset{\\theta}{\\text{argmax}} \\,\\,\\, log \\, p(D=1|w,c_{pos};\\theta) + log \\, \\prod_{c_{neg} \\in W_{neg}} (1 - p(D=1|w,c_{neg};\\theta)) \\label{}\\tag{12}\\\\ =\\quad & \\underset{\\theta}{\\text{argmax}} \\,\\,\\, log \\, p(D=1|w,c_{pos};\\theta) + \\sum_{c_{neg} \\in W_{neg}} log \\, (1 - p(D=1|w,c_{neg};\\theta)) \\label{}\\tag{13} \\end{align} $$ The binomial probability $p(D=1|w,c;\\theta)$ can be replaced with eq-5 : $$ \\begin{align} =\\quad & \\underset{\\theta}{\\text{argmax}} \\,\\,\\, log \\, \\frac{1}{1+exp(-\\bar{c}_{pos} \\cdot \\bar{w})} + \\sum_{c_{neg} \\in W_{neg}} log \\, (1 - \\frac{1}{1+exp(-\\bar{c}_{neg} \\cdot \\bar{w})}) \\label{}\\tag{14}\\\\ =\\quad & \\underset{\\theta}{\\text{argmax}} \\,\\,\\, log \\, \\frac{1}{1+exp(-\\bar{c}_{pos} \\cdot \\bar{w})} + \\sum_{c_{neg} \\in W_{neg}} log \\, \\frac{1}{1+exp(\\bar{c}_{neg} \\cdot \\bar{w})} \\label{}\\tag{15}\\\\ \\end{align} $$ Using the definition of sigmoid function $\\sigma(x)=\\frac{1}{1+exp(-x)}$: $$ \\begin{align} =\\quad & \\underset{\\theta}{\\text{argmax}} \\,\\,\\, log \\, \\sigma(\\bar{c}_{pos} \\cdot \\bar{w}) + \\sum_{c_{pos} \\in W_{neg}} log \\, \\sigma(-\\bar{c}_{neg} \\cdot \\bar{w}) \\label{}\\tag{16} \\end{align} $$ According to Mikolov, eq-16 replaces every $log \\, p(w_O|w_I)$ in the vanilla Skip-Gram cost function defined in eq-1 . Then, the cost function we want to minimize becomes: $$J(\\theta) = - \\frac{1}{T} \\sum_{i = 1}&#94;T \\sum_{-c\\leq j \\leq c,j\\neq 0}( log \\, \\sigma(\\bar{c}_{pos} \\cdot \\bar{w}) + \\sum_{c_{neg} \\in W_{neg}} log \\, \\sigma(-\\bar{c}_{neg} \\cdot \\bar{w})) \\tag{17}$$ However, when implemented in codes, batch gradient descent (making one update after iterating through the entire $T$ corpus) is almost never used due to its high computational cost. Instead, we use stochastic gradient descent . Also, for negative sampling, gradients are calculated and weights are updated for each positive training pairs ($w,\\, c_{pos}$). In the other words, one update for each word within the context window of a center word $w$. This is shown below . The new cost function is then: $$J(\\theta; w,c_{pos}) = - log \\, \\sigma(\\bar{c}_{pos} \\cdot \\bar{w}) - \\sum_{c_{neg} \\in W_{neg}} log \\, \\sigma(-\\bar{c}_{neg} \\cdot \\bar{w}) \\tag{18}$$ Note that $w$ is a word vector for an input word, and that it is equivalent to a hidden layer ($w = h$). For clarification: $$J(\\theta; w,c_{pos}) = - log \\, \\sigma(\\bar{c}_{pos} \\cdot h) - \\sum_{c_{neg} \\in W_{neg}} log \\, \\sigma(-\\bar{c}_{neg} \\cdot h)\\tag{19}$$ Notes: Notations used in different papers Different papers use different notations. In word2vec Parameter Learning Explained (Rong, 2014): $$J(\\theta; w_I, w_O) = - log \\, \\sigma(v&#94;{'}_{w_{O}} \\cdot h) - \\sum_{w_j \\in W_{neg}} log \\, \\sigma(-v&#94;{'}_{w_{j}} \\cdot h)$$ where $w_I$ is the input (center) word in the corpus. $w_{O}$ is a word found in the context window of $w_I$ and is a positive word. $v&#94;{'}$ is a word vector in the output weight matrix ($v&#94;{'} \\in W_{output}$), $w_j$ is a randomly drawn negative word from the noise distribution , and $v&#94;{'}_{w_{j}}$ is a $j$-th word vector in $W_{output}$ that corresponds to the negative word $w_j$. $h$ is a hidden layer . In the original Word2Vec paper, Distributed Representations of Words and Phrases and their Compositionality (Mikolov et al., 2013): $$J(\\theta; w_I, w_O) = - log \\, \\sigma(v&#94;{'}_{w_{O}} \\top v_I) - \\sum&#94;k_{i=1} \\mathbb{E}_{w_{i} \\sim P_n(w)}\\big[ log \\, \\sigma(-v&#94;{'}_{w_{i}} \\top v_I) \\big]$$ $k$ is the number of negative samples and $w_i$ is an $i$-th negative word drawn from the noise distribution $P_n(w)$. $v_{w_{I}}$ is a word vector in the input weight matrix $W_{input}$ for the input word $w_I$ and is equivalent to the hidden layer $h$. These are all equivalent to eq-19 . 2.3. Derivation of gradients The goal of any machine learning model is to find the optimal values of a weight matrix ($\\theta$) to minimize prediction error. A general update equation for weight matrix looks like the following: $$ \\theta&#94;{(new)}=\\theta&#94;{(old)}-\\eta\\cdot\\frac{\\partial J}{\\partial \\theta} \\tag{20}$$ $\\theta$ is a parameter that needs to be optimized, and $\\eta$ is a learning rate. In negative sampling, we take the derivative to the cost function $J(\\theta; w, c_{pos})$ defined in eq-19 with respect to $\\theta$. Note that the derivative of a sigmoid function is $\\frac{\\partial \\sigma}{\\partial x} = \\sigma(x)(1–\\sigma(x))$. $$ \\frac{\\partial J}{\\partial \\theta} = (\\sigma(\\bar{c}_{pos} \\cdot h)) - 1)\\frac{\\partial \\bar{c}_{pos} \\cdot h}{\\partial \\theta} + \\sum_{c_{neg} \\in W_{neg}} \\sigma(\\bar{c}_{neg} \\cdot h) \\frac{\\partial \\bar{c}_{neg} \\cdot h}{\\partial \\theta} \\tag{21}$$ Since the parameter $\\theta$ is a concatenation of the input and output weight matrix $[W_{input} \\quad W_{output}]$, the cost function needs to be differentiated with respect to the both matrices — $[\\frac{\\partial J}{\\partial W_{input}} \\quad \\frac{\\partial J}{\\partial W_{output}}]$. Notes: Clarification on notation $\\bar{c}$ represents a word vector in the output weight matrix ($\\bar{c} \\in W_{output}$) for a context word. The context word can be a positive word ($c_{pos}$) from a context window or a negative word ($c_{neg} \\in W_{neg}$) from a noise distribution . $h$ is a hidden layer . Recall that the hidden layer is essentially a word vector for the input word that is looked up from the input weight matrix $W_{input}$. 2.3.1. Gradients with respect to output weight matrix $\\frac{\\partial J}{\\partial W_{output}}$ With negative sampling, we do not update the entire output weight matrix $W_{output}$, but only a fraction of it. We update $K + 1$ word vectors in the output weight matrix — $\\bar{c}_{pos}$, $\\bar{c}_{neg,1},\\,...\\,\\bar{c}_{neg,K}$. We take partial derivatives to the cost function defined in eq-19 with respect to positive words and negative words. This can be done by replacing $\\theta$ in eq-21 with $\\bar{c}_{pos}$ and $\\bar{c}_{neg}$ each. $$ \\begin{align} \\frac{\\partial J}{\\partial \\bar{c}_{pos}} &= (\\sigma(\\bar{c}_{pos} \\cdot h) - 1)\\cdot h \\label{}\\tag{22}\\\\[5pt] \\frac{\\partial J}{\\partial \\bar{c}_{eng}} &= \\sigma(\\bar{c}_{neg} \\cdot h)\\cdot h \\label{}\\tag{23} \\end{align} $$ The update equations are then: $$ \\begin{align} \\bar{c}_{pos}&#94;{(new)} &= \\bar{c}_{pos}&#94;{(old)} - \\eta \\cdot (\\sigma(\\bar{c}_{pos} \\cdot h) - 1)\\cdot h \\label{}\\tag{24}\\\\[6pt] \\bar{c}_{neg}&#94;{(new)} &= \\bar{c}_{neg}&#94;{(old)} - \\eta \\cdot \\sigma(\\bar{c}_{neg} \\cdot h) \\label{}\\tag{25} \\end{align} $$ The gradients for positive and negative words can be merged for brevity: $$ \\bar{c}_{j}&#94;{(new)} = \\bar{c}_{j}&#94;{(old)}-\\eta\\cdot (\\sigma(\\bar{c}_{j} \\cdot h) - t_j) \\cdot h \\tag{26}$$ where $t_j = 1$ for positive words ($c_j = c_{pos}$) and $t_j = 0$ for negative words ($c_j = c_{neg} \\in W_{neg}$). $\\bar{c}_{j}$ is the $j$-th word vector in the output word matrix ($\\bar{c}_j \\in W_{output}$). For each positive pairs ($w$, $c_{pos}$), eq-26 is applied to $K + 1$ word vectors in $W_{output}$ as shown in Figure 4 . Notes: Prediction error In e-26 , $\\sigma(\\bar{c}_{j} \\cdot h) - t_j$ is called a prediction error . Recall that negative sampling attempts to maximize the the probability of observing positive pairs $p(c_{pos}|w) \\rightarrow 1$ while minimizing the probability of observing negative pairs $p(c_{neg}|w) \\rightarrow 0$. If good word vectors are learned, $\\sigma(\\bar{c}_{pos}\\cdot h) \\approx 1$ for positive pairs, and $\\sigma(\\bar{c}_{neg}\\cdot h) \\approx 0$ for negative pairs as shown in Figure 7 . The prediction error will gradually approach zero $\\sigma(\\bar{c}_{pos}\\cdot h) - t_j \\approx 0$, as the model iterates through the training samples (positive pairs) and optimizes the weights. 2.3.2. Gradients with respect to input weight matrix $\\frac{\\partial J}{\\partial W_{input}}$ Just like vanilla Skip-Gram, only one word vector that corresponds to the input word $w$ in $W_{input}$ is updated with negative sampling. This is because the input layer is an one-hot-encoded vector (however, the equation for the gradient descent is different.) Therefore, taking the derivative for the input weight matrix is equivalent to taking the derivative to the hidden layer ($\\frac{\\partial J}{\\partial W_{input}} = \\frac{\\partial J}{\\partial h}$). We replace $\\theta$ in eq-21 with $h$, and differentiate it: $$ \\begin{align} \\frac{\\partial J}{\\partial h} &= (\\sigma(\\bar{c}_{pos} \\cdot h) - 1) \\cdot \\bar{c}_{pos} + \\sum_{c_{eng} \\in W_{neg}} \\sigma(\\bar{c}_{neg} \\cdot h) \\cdot \\bar{c}_{neg} \\label{}\\tag{27}\\\\ &= \\sum_{c_j \\in \\{c_{pos}\\} \\cup W_{neg}} (\\sigma(\\bar{c}_{j} \\cdot h) - t_j) \\cdot \\bar{c}_{j} \\label{}\\tag{28} \\end{align} $$ Same as eq-26 , $t_j = 1$ for positive words ($c_j = c_{pos}$) and $t_j = 0$ for negative words ($c_j = c_{neg} \\in W_{neg}$). The update equation is then: $$ \\bar{w}&#94;{(new)} = \\bar{w}&#94;{(old)}-\\eta\\cdot \\sum_{c_j \\in \\{c_{pos}\\} \\cup W_{neg}} (\\sigma(\\bar{c}_{j} \\cdot h) - t_j) \\cdot \\bar{c}_{j} \\tag{29}$$ Recall that $w$ is a word vector in the input weight matrix ($\\bar{w} \\in W_{input}$) and $\\bar{c}_j$ is a $j$-th word vector in the output weight matrix ($\\bar{c}_j \\in W_{output}$). 2.4 Negative sampling algorithm For each positive word-context pair ($w, c_{pos}$), $1$ word vector that corresponds to the center word ($w$) is updated in the input weight matrix $W_{input}$ as shown in Figure 15 . In the output weight matrix $W_{output}$, $1+K$ word vectors that correspond to the positive and negative words ($\\{\\bar{c}_{pos}\\} \\cup \\bar{W}_{neg} \\in \\mathbb{R}&#94;{1+K} $) are updated as shown in Figure 4 . Since Skip-Gram uses SGD to reduce computational cost, negative sampling also uses SGD too. Then, the training for Skip-Gram negative sampling has the following algorithm structure: Algorithm 1: Skip-Gram Negative Sampling P_nw = # generate noise distribution for word in corpus: for context in context_window: # draw K negative samples from P_nw W_neg = np.random.choice(Pn_w.keys(), size=K, p=Pn_w.values()) # compute gradients. w is a input word vector = hidden layer grad_V_output_pos = (sigmoid(c_pos * h) - 1) * w grad_V_input = (sigmoid(c_pos * h) - 1) * c_pos grad_V_output_neg_list = [] for c_neg in W_neg: grad_V_output_neg_list.append(sigmoid(c_neg * h) * h) grad_V_input += sigmoid(c_neg * h) * c_neg # use SGD to update w, c_pos, and c_neg_1, ... , c_neg_K V_output_pos = V_output_pos - alpha * grad_V_output_pos V_input = V_input - alpha * grad_V_input for grad_V_output_neg in grad_V_output_neg_list: V_output_neg = V_output_neg - alpha * grad_V_output_neg The Python implementation of negative sampling here is based on the interpretation of Algorithm 1 SGNS Word2Vec in Distributed Negative Sampling for Word Embeddings (Stergiou et al., 2017). In vanilla Skip-Gram, one update is made for the entire weight matrices $[W_{input} \\quad W_{output}]$ for each input word. Each update involves summing up dot products for all context words within the context window of size $C$ as shown in eq (20) and eq (21) of my previous post . In negative sampling, $C$ updates are made for a fraction of the weights for each input word. This is because negative sampling treats each positive pair ($w$, $c_{pos}$) as one training sample, whereas vanilla Skip-Gram treats a center word ($w$) and its $C$ neighboring context words ($c_{pos, 1}$, $...$, $c_{pos,C}$) all together as one training sample for SGD. 2.5. Numerical demonstration For the ease of illustration, screenshots from Excel will be used to demonstrate the concept of updating weight matrices through forward and backward propagations. Description of the Corpus Assume that the training corpus is the entire text in the book, \" A Song of Ice and Fire .\" Unlike the simple one-sentence training corpus used in my previous post , the training corpus needs to be much bigger to illustrate negative sampling, because we need to randomly draw negative samples that are unlikely to be observed in a pair with a center word. The sentence that has the current center word is \"Ned Stark is the most honorable man\" . Center (input) word is Ned , and window size is $C = 2$, making Stark and is context words. Number of negative samples drawn from the noise distribution for each positive pair is $K = 3$. Figure 9: Training corpus for negative sampling For your information, Ned Stark is a fictional character from the book, A Song of Ice and Fire . The book also has a TV show adaptation, known as the Game of Thrones (GoT). Ned Stark is a noble lord of his land and has a reputation of being the most honorable man in the kingdom. So here's the idea. The center word Ned will be observed in a pair with context words (postive) like Stark , because it is his last name. The same thing goes for is too, because is is a verb tense used to describe a singular object. However, Ned most likely won't be observed in a pair with random words (negative) like pimples , zebra , donkey within the book. If the model can differentiate between positive pairs and negative pairs as shown in Figure 5 , good word vectors will be learned. Positive word pair: ( Ned , Stark ) Recall that in negative sampling, one update is made for each of the positive training pairs. This means that $C$ weight updates are made for each input (center) word, where $C$ is the window size. Our current positive word pair is ( Ned , Stark ). For the current positive pair, we randomly draw $K=3$ negative words from the noise distribution: pimples , zebra , idiot Forward Propagation: Computing hidden (projection) layer Hidden layer ($h$) is looked up from $W_{input}$ by multiplying the one-hot-encoded input vector with the input weight matrix $W_{input}$. Figure 10: Computing hidden (projection) layer Forward Propagation: Sigmoid output layer Output layer is a probability distribution of positive and negative words ($c_{pos} \\cup W_{neg}$), given a center word ($w$). It is computed with eq-7 . Recall that sigmoid function has $\\sigma(x) = \\frac{1}{1+exp(-x)}$. Figure 11: Sigmoid output layer Backward Propagation: Prediction Error The details about the prediction error is described above . Since our current positive word is Stark , $t_j = 1$ for Stark and $t_j=0$ for other negative words ( pimples , zebra , idiot ). Figure 12: Prediction errors of positive and negative words Backward Propagation: Computing $\\nabla W_{input}$ Gradients of input weight matrix ($\\frac{\\partial J}{\\partial W_{input}}$) are computed using eq-28 . Just like vanilla Skip-Gram, only the word vector in the input weight matrix $W_{input}$ that corresponds to the input (center) word $w$ is updated. Figure 13: Computing input weight matrix gradient $\\nabla W_{input}$ Backward Propagation: Computing $\\nabla W_{output}$ With negative sampling, only a fraction of word vectors in the output weight matrix $W_{output}$ is updated. Gradients for $K + 1$ word vectors for positive and negative words in the $W_{output}$ are computed using eq-22 and eq-23 . Recall that $K$ is the number of negative samples drawn from a noise distribution, and that $K = 3$ in our example. Figure 14: Computing output weight matrix gradient $\\nabla W_{output}$ Backward Propagation: Updating Weight matrices Input and output weight matrices ($[W_{input} \\quad W_{output}]$) are updated using eq-26 and eq-29 . Figure 15: Updating $W_{input}$ Figure 16: Updating $W_{output}$ Positive word pair: ( Ned , is ) The center word Ned has two context words: Stark and is . This means that we have two positive pairs = two updates to make. Since we already update the matrices $[W_{input} \\quad W_{output}]$ using ( Ned , Stark ), we will use ( Ned , is ) to update weight matrices this time. In negative sampling, we draw new $K$ negative words for each positive pairs. Assume that we randomly drew coins , donkey , and machine as our negative words this time. Forward Propagation: Computing hidden (projection) layer Figure 17: Computing hidden (projection) layer Forward Propagation: Sigmoid output layer Figure 18: Sigmoid output layer Backward Propagation: Prediction Error Our current positive word is Stark : $t_j = 1$ for Stark and $t_j=0$ for other negative words ( coins , donkey , and machine ). Figure 19: Prediction errors of positive and negative words Backward Propagation: Computing $\\nabla W_{input}$ Figure 20: Computing input weight matrix gradient $\\nabla W_{input}$ Backward Propagation: Computing $\\nabla W_{output}$ Figure 21: Computing output weight matrix gradient $\\nabla W_{output}$ Backward Propagation: Updating Weight matrices Figure 22: Updating $W_{input}$ Figure 23: Updating $W_{output}$","tags":"Natural Language Processing","url":"https://aegis4048.github.io/optimize_computational_efficiency_of_skip-gram_with_negative_sampling","loc":"https://aegis4048.github.io/optimize_computational_efficiency_of_skip-gram_with_negative_sampling"},{"title":"Demystifying Neural Network in Skip-Gram Language Modeling","text":"Contents 1. Paradigm shift in word embedding: count-based to prediction-based 2. Prediction-based word-embedding: Word2Vec Skip-Gram Notes: CBOW and Skip-Gram 2.1. Why predict context words? 2.2. What is the application of vector representations of words? 3. Derivation of cost function Notes: Why take a natural log? Notes: Probability product 4. Window size of Skip-Gram 5. Neural network structure of Skip-Gram 5.1. Training: forward propagation 5.1.1. Input layer ($x$) Notes: Stochastic gradient descent 5.1.2. Input and output weight matrix ($W_{input}$,-$W_{output}$) Notes: $\\theta$ in cost function 5.1.3. Hidden (projection) layer ($h$) 5.1.4. Softmax output layer ($y_{pred}$) Notes: Negative sampling 5.2. Training: backward propagation Notes: Applying Softmax 5.1.1. Prediction error ($y_{pred} - y_{true}$) 6. Numerical demonstration Acknowledgement The materials on this post are based the on two NLP papers, Distributed Representations of Words and Phrases and their Compositionality (Mikolov et al., 2013) and word2vec Parameter Learning Explained (Rong, 2014). 1. Paradigm shift in word embedding: count-based to prediction-based Up until 2013, the traditional models for NLP tasks were count-based models. They mainly involve computing a co-occurence matrix to capture meaningful relationships among words (If you are interested in how co-occurrence matrix is used for language modeling, check out Understanding Multi-Dimensionality in Vector Space Modeling ). For example: Document 1: \"all that glitters is not gold\" Document 2: \"all is well that ends well\" * START all that glitters is not gold well ends END START 0 2 0 0 0 0 0 0 0 0 all 2 0 1 0 1 0 0 0 0 0 that 0 1 0 1 0 0 0 1 1 0 glitters 0 0 1 0 1 0 0 0 0 0 is 0 1 0 1 0 1 0 1 0 0 not 0 0 0 0 1 0 1 0 0 0 gold 0 0 0 0 0 1 0 0 0 1 well 0 0 1 0 1 0 0 0 1 1 ends 0 0 1 0 0 0 0 1 0 0 END 0 0 0 0 0 0 1 1 0 0 Table 1: Co-Occurence Matrix Count-based language modeling is easy to comprehend — related words are observed (counted) together more often than unrelated words. Many attempts were made to improve the performance of the model to the state-of-art, using SVD, ramped window, and non-negative matrix factorization ( Rohde et al. ms., 2005 ), but the model did not do well in capturing complex relationships among words. Then, the paradigm started to change in 2013, when Thomas Mikolov proposed the prediction-based modeling technique, called Word2Vec, in his famous paper, Distributed Representations of Words and Phrases and their Compositionality . Unlike counting word co-occurrences, the model uses neural networks to learn intelligent representation of words in a vector space. Then, the paper submitted to ACL in 2014, Don't count, predict! A systematic comparison of context-counting vs. context-predicting semantic vectors , quantified & compared the performances of count-based vs prediction-based models. Figure 1: Performance comparison of models ( source ) The blue bars represent the count-based models, and the red bars are for prediction-based models. The full summary of the paper and more detailed description about the result graph can be found here . Long story short, prediction-based models outperformed count-based models by a large margin on various language tasks. 2. Prediction-based word-embedding: Word2Vec Skip-Gram One of the prediction-based language model introduced by Mikolov is Skip-Gram: Figure 2: Original Skip-gram model architecture Figure 2 is a diagram presented in the original Word2Vec paper. It is essentially describing that the model uses a neural network of one hidden (projection) layer to correctly predict context words $w(t-2)$, $w(t-1)$, $w(t+1)$, $w(t+2)$ of an input word $w(t)$. In the other words, the model attempts to maximize the probability of observing all four context words together, given a center word. Mathematically, it can be denoted as eq-1 . The training objective is to learn word vector representations that are good at predicting the nearby words. Notes: CBOW and Skip-Gram There are two models for Word2Vec: Continous Bag Of Words (CBOW) and Skip-Gram . While Skip-Gram model predicts context words given a center word, CBOW model predicts a center word given context words. According to Mikolov: Skip-gram : works well with small amount of the training data, represents well even rare words or phrases CBOW : several times faster to train than the skip-gram, slightly better accuracy for the frequent words Skip-Gram model is a better choice most of the time due to its ability to predict infrequent words, but this comes at the price of increased computational cost. If training time is a big concern, and you have large enough data to overcome the issue of predicting infrequent words, CBOW model may be a more viable choice. The details of CBOW model won't be covered in this post. 2.1. Why predict context words? A natural question is, why do we predict context words? One must understand that the ultimate goal of Skip-Gram model is not to predict context words, but to learn intelligent vector representation of words. It just happens that predicting context words inevitably results in good vector representations of words, because of the neural network structure of Skip-Gram. Neural network at its essence is just optimizing weight marices $\\theta$ to correctly predict output. In Word2Vec Skip-Gram, the weight matrices are, in fact, the vector representations of words. Therefore, optimizing weight matrix = good vector representations of words. This is described in detail below . 2.2. What is the application of vector representations of words? In Word2Vec, words are represented as vectors, and related words are placed closed to each other on a vector space. Mathematically, this means that the vector distance between related words are smaller than the vector distance between unrelated words. Figure 3: Vector distance between two words For example in Figure 3 , correlation between \"success\" and \"achieve\" can be quantified by computing the vector distance between them (Notes: For illustration purpose, three-dimensional word vectors are assumed in the figure, because higher dimensional vectors can't be visualized. Also, distance annotated in the figure is Euclidean, but in real-life, we use Cosine distance to evaluate vector correlations). One interesting application of vector representaion of words is that it can be used to solve analogy tasks. Let's assume the following word vectors for \"Germany\" , \"capital\" , and \"Berlin\" . $$ \\begin{align*} vec(\\text{Germany}) & = [1.22 \\quad 0.34 \\quad -3.82] \\\\ vec(\\text{capital}) & = [3.02 \\quad -0.93 \\quad 1.82] \\\\ vec(\\text{Berlin}) & = [4.09 \\quad -0.58 \\quad 2.01] \\end{align*} $$ To find out the capital of Germany, the word vector of \"capital\" can be added to the word vector of \"Germany\" . $$ \\begin{align*} vec(\\text{Germany}) + vec(\\text{capital}) &= [1.22 \\quad 0.34 \\quad -3.82] + [3.02 \\quad -0.93 \\quad 1.82] \\\\ &= [4.24 \\quad -0.59 \\quad -2.00] \\end{align*} $$ Since the sum of the word vectors of \"Germany\" and \"capital\" is similar to the word vector of \"Berlin\" , the model may conclude that the capital of Germany is Berlin. $$ \\begin{align*} [4.24 \\quad -0.59 \\quad -2.00] & \\cong [4.09 \\quad -0.58 \\quad 2.01] \\\\ vec(\\text{Germany}) + vec(\\text{capital}) & \\cong vec(\\text{Berlin}) \\end{align*} $$ Notes: Analogy tasks don't always work Not all analogy tasks can be solved like this. The above illustration works like a magic, but there are many analogy problems that can't be solved with Word2Vec. Think of the above illustration as just one use case of Word2Vec. 3. Derivation of cost function Skip-Gram model seeks to optimize the word weight (embedding) matrix by correctly predicting context words, given a center word. In the other words, the model wants to maximize the probability of correctly predicting all context words at the same time, given a center word. Maximizing the probability of predicting context words leads to optimizing the weight matrix ($\\theta$) that best represents words in a vector space. $\\theta$ is a concatenation of input and output weight matrices — $[W_{input} \\quad W_{output}]$, as described below . It is passed into the cost function ($J$) as a variable and optimized. Mathematically, it can be expressed as: $$ \\underset{\\theta}{\\text{argmax}} \\,\\, p(w_{1}, w_{2}, ... , w_{C}|w_{center}; \\, \\theta) \\tag{1} $$ where $C$ is the window size, and $w$ is a word vector (which can be a context or a center word). Recall that in statistics, the probability of $A$ given $B$ is expressed as $P(A|B)$. Then, natural log is taken on eq-1 to simplify taking derivatives. $$ \\underset{\\theta}{\\text{argmax}} \\,\\, log \\, p(w_{1}, w_{2}, ... , w_{C}|w_{center}; \\, \\theta) \\tag{2} $$ Notes: Why take a natural log? In machine learning, it is a common practice to take a natural log to the objective function to simplify taking derivatives. For example, a multinomial regression classifer called Softmax (details explained below ) has the following probability function: $p(x_i) = \\frac{e&#94;{x_i}}{\\sum_{j=1}e&#94;{x_{j}}}$ Taking a log simplifies the function: $log \\, p(x_i) = x_i - log \\, {\\sum_{j=1}e&#94;{x_{j}}}$ Depending on a model, the argument ($x_i$) passed into the probability function ($p$) can be complicated, and simplifying the original softmax function helps with taking the derivatives in the future. Taking a log does not affect the optimized weights ($\\theta$), because natural log is a monotonically increasing function. This means that increasing the value of $x$-axis results in increasing the value of $y$-axis. This is important because it ensures that the maximum value of the original probability function occurs at the same point as the log probability function. Therefore: $\\underset{\\theta}{\\text{argmax}} \\,\\, p(x_i) = \\underset{\\theta}{\\text{argmax}} \\,\\, log \\, p(x_i)$ In Skip-Gram, softmax function is used for context words classfication. The details are explained below . Softmax in Skip-Gram has the following equation: $$ p(w_{context}|w_{center}; \\, \\theta) = \\frac{exp(W_{output_{(context)}} \\cdot h)}{\\sum&#94;V_{i=1}exp(W_{output_{(i)}} \\cdot h)} \\tag{3} $$ $W_{output_{(context)}}$ is a row vector for a context word from the output embedding matrix (see below ), and $h$ is the hidden (projection) layer word vector for a center word (see below ). Softmax function is then plugged into the eq-2 to yield a new objective function that maximizes the probability of observing all $C$ context words, given a center word: $$ \\underset{\\theta}{\\text{argmax}} \\,\\, log \\, \\prod_{c=1}&#94;{C} \\frac{exp(W_{output_{(c)}} \\cdot h)}{\\sum&#94;V_{i=1}exp(W_{output_{(i)}} \\cdot h)} \\tag{4} $$ Notes: Probability product In statistics, probability of observing $C$ multiple events at the same time is computed by the product of each event's probability. $$p(x_{1}, x_{2} ... x_{C}) = p(x_{1}) \\times p(x_{2}) \\, \\times \\, ... \\, \\times \\, p(x_{C})$$ This can be shortened with a product notation: $$p(x_{1}, x_{2} ... x_{C}) = \\prod_{c=1}&#94;{C}p(x_{c})$$ However, in machine learning, the convention is to minimize the cost function, not to maximize it. To stick to the convention, we add a negative sign to eq-4 . This can be done because minimizing a negative log-likelihood is equivalent to maximizing a positive log-likelihood. Therefore, the cost function we want to minimize becomes: $$ J(\\theta; w&#94;{(t)}) = -log \\, \\prod_{c=1}&#94;{C} \\frac{exp(W_{output_{(c)}} \\cdot h)}{\\sum&#94;V_{i=1}exp(W_{output_{(i)}} \\cdot h)} \\tag{5} $$ where $c$ is the index of the context word around the center word ($w_{t}$). $t$ is the index of the center word within a corpus of size $T$. Using the property of log, it can be changed to: $$J(\\theta; w&#94;{(t)}) = - \\sum_{c=1}&#94;{C} log \\frac{exp(W_{output_{(c)}} \\cdot h)}{\\sum&#94;V_{i=1}exp(W_{output_{(i)}} \\cdot h)} \\tag{6}$$ Taking a log to the softmax function allows us to simplify the expression into simpler forms because we can split the fraction into addtion of the numerator and the denominator: $$ J(\\theta; w&#94;{(t)}) = - \\sum_{c=1}&#94;{C}(W_{output_{(c)}} \\cdot h) + C \\cdot log \\sum&#94;V_{i=1}exp(W_{output_{(i)}} \\cdot h) \\tag{7} $$ Different paper uses different notations for the cost function. To stick to the notation used in the Word2Vec original paper , some of the notations in eq-7 can be changed. However, they are all equivalent: $$J(\\theta;w&#94;{(t)}) = -\\sum_{-c\\leq j \\leq c,j\\neq 0} \\log p(w_{t+j} \\mid w_t ; \\, \\theta) \\tag{8}$$ Note that eq-7 and eq-8 are equivalent. They both assume stochastic gradient descent , which means that for each training sample (center word) $w&#94;{(t)}$ in the corpus of size $T$, one update is made to the weight matrix ($\\theta$). The cost function expressed in the paper shows batch gradient descent eq-9 , which means that only one update is made for all $T$ training samples: $$J(\\theta) = -\\frac{1}{T} \\sum&#94;T_{t=1} \\sum_{-c\\leq j \\leq c,j\\neq 0} \\log p(w_{t+j} \\mid w_t ;\\, \\theta) \\tag{9}$$ However, in Word2Vec, batch gradient descent is almost never used due to its high computational cost. The author of the paper stated that he used stochastic gradient descent for training. Read the below notes for more information about stochastic gradient descent. 4. Window size of Skip-Gram Softmax regression (or multinomial logistic regression) is a generalization of logistic regression to the case where we want to handle multiple classes. A general form of the softmax regression looks like this: $$J(\\theta) = -\\frac{1}{T} \\sum&#94;T_{t=1} \\sum&#94;K_{k=1} log \\frac {exp(\\theta&#94;{(k)\\top}x&#94;{(t)})} {\\sum&#94;K_{i=1} exp(\\theta&#94;{(i)\\top}x&#94;{(t)})} \\tag{10}$$ where $T$ is the number of training samples, and $K$ is the number of labels to classify. In NLP applications, $K = V$, because there are $V$ unique vocabulary we need to classify in a vector space. $V$ can easily exceed tens of thousands. Skip-Gram tweaks this a little, and replaces $K$ with a variable called window size $C$. Window size is a hyper parameter of the model with a typical range of $[1, 10]$ (see Figure 4 ). Recall that Skip-Gram is a model that attempts to predict neighboring words of a center word. It doesn't have to predict all $V$ vocab in the corpus that may be 100 or more words away from it, but instead predict only a few, 1~10 neighboring context words. This is also intuitive, considering how words that are far away carry less information about each another. Thus, the adapted form of the softmax regression equation for Skip-Gram becomes: $$J(\\theta) = -\\frac{1}{T} \\sum&#94;T_{t=1} \\sum_{-c\\leq j \\leq c,j\\neq 0} log \\frac {exp(\\theta&#94;{(t+j)\\top}x&#94;{(t)})} {\\sum&#94;K_{i=1} exp(\\theta&#94;{(i)\\top}x&#94;{(t)})} \\tag{11}$$ This is equivalent to eq-9 . Note that the $K$ in the denominator is still equal to $V$, because the denominator acts as a normalization factor, as described below . However, the size of $K$ in the denominator can still be reduced to smaller size using negative sampling . 5. Neural network structure of Skip-Gram How is neural network used to minimize the cost functoin described in eq-11 ? One needs to look into the structure of the Skip-Gram model to gain insights about their correlation. For illustration purpose, let's assume that the entire corpus is composed of the quote from the Game of Thrones, \"The man who passes the sentence should swing the sword\" , by Ned Stark. There are 10 words ($T = 10$), and 8 unique words ($V = 8$). Note that in real life, the corpus is much bigger than just one sentence. The man who passes the sentence should swing the sword. - Ned Stark We will use window=1 , and assume that 'passes' is the current center word, making 'who' and 'the' context words. window is a hyper-parameter that can be empirically tuned. It typically has a range of $[1, 10]$. Figure 4 : Training Window For illustration purpose, a three-dimensional neural net will be constructed. In gensim , this can be implemented by setting size=3 . This makes $N = 3$. Note that size is also a hyper-parameter that can be empirically tuned. In real life, a typical Word2Vec model has 200-600 neurons. from gensim.models import Word2Vec model = Word2Vec(corpus, size=3, window=1) This means that the input weight matrix ($W_{input}$) will have a size of $8 \\times 3$, and output weight matrix ($W_{output}&#94;T$) will have a size of $3 \\times 8$. Recall that the corpus, \"The man who passes the sentence should swing the sword\" , has 8 unique vocabularies ($V = 8$). Figure 5: Skip-Gram model structure. Current center word is \"passes\" 5.1. Training: forward propagation The word embedding matrices ($W_{input}$, $W_{output}$) in Skip-Gram are optimized through forward and backward propagations. For each iteration of forward + backward propagations, the model learns to reduce prediction error by optimizing the weight matrix ($\\theta$), thus acquiring higher quality embedding matrices that better capture relationships among words. Forward propagation includes obtaining the probability distribution of words ($y_{pred}$ in Figure 5 ) given a center word, and backward propagation includes calculating the prediction error, and updating the weight (embedding) matrices to minimize the prediction error. 5.1.1. Input layer ($x$) The input layer is a $V$-dim one-hot encoded vector. Every element in the vector is 0 except one element that corresponds to the center (input) word. Input vector is multiplied with the input weight matrix ($W_{input}$) of size $V \\times N$, and yields a hidden (projection) layer ($h$) of $N$-dim vector. Because the input layer is one-hot encoded, it makes the input weight matrix ($W_{input}$) to behave like a look-up table for the center word. Assuming epoch number of 1 ( iter=1 in gensim Word2Vec implementation) and stochastic gradient descent, the input vector is injected into the network $T$ times for every word in the corpus and makes $T$ updates to the weight matrix ($\\theta$) to learn from the training samples. Derivation of the stochasitc update equations are explained below . Figure 6: One-hot encoded input vector and parameter update Notes: Stochastic gradient descent The goal of any machine learning model is to find the optimal values of a weight matrix ($\\theta$) to minimize prediction error. A general update equation for weight matrix looks like the following: $\\theta&#94;{(new)}=\\theta&#94;{(old)}-\\eta\\cdot\\nabla_{J(\\theta)}$ $\\eta$ is learning rate, $\\nabla_{J(\\theta)}$ is gradient for the weight matrix, and $J(\\theta)$ is the cost function that has different forms for each model. The cost function for the Skip-Gram model proposed in the Word2Vec original paper has the following equation: $$J(\\theta) = -\\frac{1}{T} \\sum&#94;T_{t=1} \\sum_{-c\\leq j \\leq c,j\\neq 0} \\log p(w_{t+j} \\mid w_t ; \\theta)$$ Here, what gives us headache is the expression, $\\frac{1}{T} \\sum&#94;T_{t=1}$, because $T$ can be larger than billions or more in many NLP applications. It is basically telling us that billions of iterations need to be computed to make just one update to the weight matrix ($\\theta$). In order to mitigate this computational burden, the author of the paper states that Stochastic Gradient Descent (SGD) was used for parameter optimization. SGD removes the expression, $\\frac{1}{T} \\sum&#94;T_{t=1}$, from the cost function and performs parameter update for each training example, $w&#94;{(t)}$: $$J(\\theta;w&#94;{(t)}) = -\\sum_{-c\\leq j \\leq c,j\\neq 0} \\log p(w_{t+j} \\mid w_t ; \\theta)$$ Then, the new parameter update equation for SGD becomes: $\\theta&#94;{(new)}=\\theta&#94;{(old)}-\\eta\\cdot\\nabla_{J(\\theta;w&#94;{(t)})}$ The original vanilla graident descent makes $1$ parameter update for $T$ training samples, but the new update equation using SGD makes $T$ parameter update for $T$ training samples. However, this comes at the price of higher fluctuation (or variance) in minimizing prediction error. 5.1.2. Input and output weight matrix ($W_{input}$, $W_{output}$) Why does Skip-Gram model attempt to predict context words given a center word? How does predicting context words help with quantifying words and representing them in a vector space? In fact, the ultimate goal of the model is not to predict context words, but to construct the word embedding matrices ($W_{input}$, $W_{output}$) that best caputure relationship among words in a vector space. Skip-Gram achieves this by using a neural net — it optimizes the weight (embedding) matrices by adjusting the weight matrix to minimize the prediction error ($y_{pred} - y_{true}$). This will make more sense once you understand how the embedding matrix behaves like a look-up table . Each row in a word-embedding matrix is a word-vector for each word. Consider the following word-embedding matrix, $W_{input}$. Figure 7: Word-embedding matrix, $W_{input}$ The words of our interest are \"passes\" and \"should\" . \"passes\" has a word vector of $[0.1 \\quad 0.2 \\quad 0.7]$ and \"should\" has $[-2 \\quad 0.2 \\quad 0.8]$. Since we set the size of the weight matrix to be size=3 , the matrix is three-dimensional, and can be visualized in a 3D vector space: Figure 8: 3D visualization of word vectors in embedding matrix Optimizing the embedding (weight) matrices $\\theta$ results in representing words in a high quality vector space, and the model will be able to capture meaningful relationships among words. Notes: $\\theta$ in cost function There are two weight matrices that need to be optimized in Skip-Gram model: $W_{input}$ and $W_{output}$. Often times in neural net, the weights are expressed as $\\theta$. In Skip-Gram, $\\theta$ is a concatenation of input and output weight matrices — $[W_{input} \\quad W_{output}]$. $$ \\theta = [W_{input} \\quad W_{output}] = \\left[ \\begin{array}{l} u_{the} \\\\ u_{passes} \\\\ \\vdots \\\\ u_{who} \\\\ v_{the} \\\\ v_{passes} \\\\ \\vdots \\\\ v_{who} \\end{array} \\right] \\in \\mathbb{R}&#94;{2NV}$$ $\\theta$ has a size of $2V \\times N$, where $V$ is the number of unique vocab in a corpus, and $N$ is the dimension of word vectors in the embedding matrices. $2$ is multipled to $V$ because there are two weight matrices, $W_{input}$ and $W_{output}$. $u$ is a word vector from $W_{input}$ and $v$ is a word vector from $W_{output}$. Each word vectors are $N$-dim row vectors from input and output embedding matrices. 5.1.3. Hidden (projection) layer ($h$) Skip-Gram uses a neural net with one hidden layer. In the context of natural language processing, hidden layer is often referred to as a projection layer, because $h$ is essentially an N-dim vector projected by the one-hot encoded input vector. Figure 9: Computing projection layer $h$ is obtained by multiplying the input word embedding matrix with the $V$-dim input vector. $$h = W_{input}&#94;T \\cdot x \\in \\mathbb{R}&#94;{N} \\tag{12}$$ 5.1.4. Softmax output layer ($y_{pred}$) The output layer is a $V$-dim probability distribution of all unique words in the corpus, given a center word. In statistics, the conditional probability of $A$ given $B$ is denoted as $p(A|B)$. In Skip-Gram, we use the notation, $p(w_{context}| w_{center})$, to denote the conditional probability of observing a context word given a center word. It is obtained by using the softmax function, $$ p(w_{context}|w_{center}) = \\frac{exp(W_{output_{(context)}} \\cdot h)}{\\sum&#94;V_{i=1}exp(W_{output_{(i)}} \\cdot h)} \\in \\mathbb{R}&#94;{1} \\tag{13} $$ where $W_{output_{(i)}}$ is the $i$-th row vector of size $1 \\times N$ from the output embedding matrix, $W_{output_{context}}$ is also a row vector of size $1 \\times N$ from the output embedding matrix corresponding to the context word. $V$ is the size of unique vocab in the corpus, and $h$ is the hidden (projection) layer of size ($N \\times 1$). The output is an $1 \\times 1$ scalar value of probability of range $[0, 1)$. This probability is computed $V$ times to obtain a conditional probability distribution of observing each unique vocabs in the corpus, given a center word. $$ \\left[ \\begin{array}{c} p(w_{1}|w_{center}) \\\\ p(w_{2}|w_{center}) \\\\ p(w_{3}|w_{center}) \\\\ \\vdots \\\\ p(w_{V}|w_{center}) \\end{array} \\right] = \\frac{exp(W_{output} \\cdot h)}{\\sum&#94;V_{i=1}exp(W_{output_{(i)}} \\cdot h)} \\in \\mathbb{R}&#94;{V}\\tag{14} $$ $W_{output}$ in the denominator of eq 13 has size $V \\times N$. Multiplying $W_{output}$ with $h$ of size $N \\times 1$ will yield a dot product vector of size $V \\times 1$. This dot product vector goes through the softmax function: Figure 10: softmax function transformation The exponentiation ensures that the transformed values are positive, and the normalization factor in the denominator ensures that the values have a range of $[0, 1)$. The result is a conditional probability distribution of observing each unique vocabs in the corpus, given a center word. Notes: Negative sampling Softmax function in Skip-Gram has the following equation: $$ P = \\frac{exp(W_{output} \\cdot h)}{\\sum&#94;V_{i=1}exp(W_{output_{(i)}} \\cdot h)} \\in \\mathbb{R}&#94;{V}$$ There is an issue with softmax in Skip-Gram — it is computationally very expensive, as it requires scanning through the entire output embedding matrix ($W_{output}$) to compute the probability distribution of all $V$ words, where $V$ can be millions or more. Furtheremore, the normalization factor in the denominator also requires $V$ iterations. When implemented in codes, the normalization factor is computed only once and cached as a Python variable, making the alogrithm complexity = $O(V+V)\\approx O(V)$. Due to this computational inefficiency, softmax is not used in most implementaions of Skip-Gram . Instead we use an alternative called negative sampling with sigmoid function, which rephrases the problem into a set of independent binary classification task of algorithm complexity = $O(K+1)$, where $K$ typically has a range of $[5,20]$. Then, the new probability distribution is defined as: $$ P = \\frac{1}{1+exp(-(\\{c_{pos}\\} \\cup W_{neg}) \\cdot h)} \\in \\mathbb{R}&#94;{K+1}$$ $K=20$ is used for small corpus, and $K=5$ is used for big corpus. Negative sampling is much cheaper than vanilla Skip-Gram with softmax, because $K$ is between 5 ~ 20, whereas $V$ can be millions. Moreover, no extra iterations are necessary to compute the normalization factor in the denominator, because sigmoid function is a binary regression classifier. The algorithm complexity of the probability distribution of vanilla Skip-Gram is $O(V)$, whereas negative sampling's is $O(K+1)$. This shows why negative sampling saves a significant amount of computational cost per iteration. In gensim , negative sampling is applied by default with Word2Vec(negative=5, ns_exponent=0.75) , where negative is the number of $K$-negative samples, and ns_exponent is a hyperparameter related to negative sampling, of range $(0, 1)$. The details of the methodology behind negative sampling deserves another fully devoted post, and as such, covered in a different post . 5.2. Training: backward propagation Backward propagation involves computing prediction errors, and updating the weight matrix ($\\theta$) to optimize vector representation of words. Assuming stochastic gradient descent , we have the following general update equations for the weight matrix ($\\theta$): $$ \\theta_{new}=\\theta_{old}-\\eta\\cdot\\nabla_{J(\\theta;w&#94;{(t)}}) \\tag{15} $$ $\\eta$ is learning rate, $\\nabla_{J(\\theta;w&#94;{(t)})}$ is gradient for the weight matrix, and $J(\\theta;w&#94;{(t)})$ is the cost function defined in eq-6 . Since the $\\theta$ is a concatenation of input and output weight matrices ($[W_{input} \\quad W_{output}]$) as described above , there are two update equations for each embedding matrix: $$ W_{input}&#94;{(new)}=W_{input}&#94;{(old)}- \\eta \\cdot \\frac{\\partial J}{\\partial W_{input}} \\tag{16} $$ $$ W_{output}&#94;{(new)}=W_{output}&#94;{(old)}- \\eta \\cdot \\frac{\\partial J}{\\partial W_{output}} \\tag{17} $$ Mathematically, it can be shown that the gradients of $W_{input}$ $W_{output}$ have the following forms: $$ \\frac{\\partial J}{\\partial W_{input}} = x \\cdot (W_{output}&#94;T \\sum&#94;C_{c=1} e_c) \\tag{18}$$ $$ \\frac{\\partial J}{\\partial W_{output}} = h \\cdot \\sum&#94;C_{c=1} e_c \\tag{19}$$ The gradients can be substitued into eq-16 and eq-17 : $$ W_{input}&#94;{(new)}=W_{input}&#94;{(old)}- \\eta \\cdot x \\cdot (W_{output}&#94;T \\sum&#94;C_{c=1} e_c) \\tag{20} $$ $$ W_{output}&#94;{(new)}=W_{output}&#94;{(old)}- \\eta \\cdot h \\cdot \\sum&#94;C_{c=1} e_c \\tag{21} $$ $W_{input}$ and $W_{output}$ are the input and output weight matrices , $x$ is one-hot encoded input layer , $C$ is window size , and $e_{c}$ is prediction error for $c$-th context word in the window. Note that $h$ (hidden layer) is equivalent to $W_{input}&#94;T x$. Notes: Applying Softmax Although eq-21 does not explicitly show it, softmax function is applied in the prediction error ($e_c$). Prediction error is the difference between the predicted and true probability ($y_{pred} - y_{true}$) as illustrated below . The predicted probability $y_{pred}$ is computed using softmax function using eq-13 . 5.1.1. Prediction error ($y_{pred} - y_{true}$) Skip-Gram model optimizes the weight matrix ($\\theta$) to reduce the prediction error. Prediction error is the difference between the probability distribution of words computed from the softmax output layer ($y_{pred}$) and the true probability distribution ($y_{true}$) of the $c$-th context word. Just like the input layer, $y_{true}$ is one-hot encoded vector, in which only one element in the vector that corresponds to the $c$-th context word is $1$, and the rest is all $0$. Figure 11: Prediction error window The figure has a window size of $2$, so two prediction errors were computed. Recall from the above notes about the window size that the original softmax regression classifier ( eq-10 ) has $K$ labels to classify, in which $K = V$ in NLP applications because there are $V$ words to classify. Employing window size transforms eq-10 into eq-11 significantly reduces the algorithm complexity because the model only needs to compute prediction errors for 2~10 (this is a hyperparameter $C$) neighboring words, instead of computing all $V$-prediction errors for all vocabs that can be millions or more. Then, prediction errors for all $C$ context words are summed up to compute weight gradients to the update weight matrices, according to eq-18 and eq-19 . Figure 12: Sum of prediction errors As the weight matrices are optimized, the prediction error for all words in the prediction error vector $\\sum_{(c=1)}&#94;C e_c$ converges to 0. Figure 13: Prediction errors converging to zero with optimization 6. Numerical demonstration For the ease of illustration, screenshots from Excel will be used to demonstrate the concept of updating weight matrices through forward and backward propagations. Forward propagation: computing hidden (projection) layer Center word is \"passes\" . Window size is size=1 , making \"the\" and \"who\" context words. Hidden layer ($h$) is looked up from the input weight matrix. It is computed with eq-12 . Figure 14: Computing hidden (projection) layer Forward propagation: softmax output layer Output layer is a probability distribution of all words, given a center word. It is computed with eq-14 . Note that all context windows share the same output layer ($y_{pred}$). Only the errors ($e_c$) are different. Figure 15: Softmax output layer Backward propagation: sum of prediction errors $C$ different prediction errors are computed, then summed up. In this case, since we set window=1 above , only two errors are computed. Figure 16: Prediction errors of context words Backward propagation: computing $\\nabla W_{input}$ Gradients of input weight matrix ($\\frac{\\partial J}{\\partial W_{input}}$) are computed using eq-18 . Note that multiplying $W_{output}&#94;T \\sum&#94;C_{c=1} e_c$ with the one-hot-encoded input vector ($x$) makes the neural net to update only one word vector that corresponds to the input (center) word. Figure 17: Computing input weight matrix gradient $\\nabla W_{input}$ Backward propagation: computing $\\nabla W_{output}$ Gradients of output weight matrix ($\\frac{\\partial J}{\\partial W_{output}}$) are computed using eq-19 . Unlike the input weight matrix ($W_{input}$), all word vectors in the output weight matrix ($W_{output}$) are updated. Figure 18: Computing output weight matrix gradient $\\nabla W_{output}$ Backward propagation: updating weight matrices Input and output weight matrices ($[W_{input} \\quad W_{output}]$) are updated using eq-20 and eq-21 . Figure 19: Updating $W_{input}$ Figure 20: Updating $W_{output}$ Note that for each iteration in the learning process, all weights in $W_{output}$ are updated, but only one row vector that corresponds to the center word is updated in $W_{input}$. When the model finishes updating both of the weight matrices, then one iteration is completed. The model then moves to the next iteration with the next center word. However, remember that this uses eq-8 as the cost function and assumes stochastic gradient descent . This means that one update is made for each training example. If eq-9 is used as a cost function instead (which is almost never the case), then one update is made for all $T$ training examples in the corpus.","tags":"Natural Language Processing","url":"https://aegis4048.github.io/demystifying_neural_network_in_skip_gram_language_modeling","loc":"https://aegis4048.github.io/demystifying_neural_network_in_skip_gram_language_modeling"},{"title":"Understanding Multi-Dimensionality in Vector Space Modeling","text":"Contents 1. Introduction 2. Review on vector space modeling techniques 2.1. Co-occurence matrix Notes: START and END tokens 2.2. Word2Vec 3. Why is multi-dimensionality important? 4. Vector space modeling: crude oil news 4.1. Data preparation 4.2. Constructing co-occurence matrix 4.2.1. Tokenization 4.2.2. Choice of window size 4.2.3. Matrix construction 4.3. Dimensionality reduction Notes: Two algorithms of TruncatedSVD Notes: explained_variance_ratio_ 5. Word vectors visualization 6. Dimensionality analysis on visualizations 7. Conclusion The code snippet assumes Anaconda 5.2.0 version of Python virtual environment Acknowledgement Some materials on this post are from CS224n: Natural Language Processing with Deep Learning at the Stanford University. Check out the YouTube Lecture on Word Vectors and Word Senses taught by Dr. Chris Manning . 1. Introduction One of the critical components in Natural Langauge Processing (NLP) is to encode text information in a numerical format that can be fed into an NLP model. Such technique, representing words in a numerical vector space, is called Vector Space Modeling . It is often synonymous to word embedding . A typical vector space model that haven't went through dimensional reduction has a dimension of $V \\times N$, where $V$ is a size of unique vocabulary, and $N$ varies based on the choice of modeling method (Notes: in document-to-word embedding model like TF-IDF, $V$ is a number of documents and and $N$ is a size of unique vocabulary). In this context, $V$ is basically a sample data size — the larger the $V$, the bigger the training data set. It is always good to have more training data than the otherwise. Therefore, our prime interest lies within the size of $N$, which affects the multi-dimensionality of a vector space model. Here is a simple illustration of how words look like in a high dimensional vector space. Figure 1: simple 10-dimensional word vectors Consider the two semantically similar words, \"Success\" and \"Achieve\". When converted into 10-dimensional word vectors using a vector space model of one's choice (Ex: Word2Vec), each word is a $1 \\times 10$ vector where each value in a vector represent the word's position in a 10D space. When projected on this high dimensional vector space, the similarity between words can be quantified by evaluating the similarity between these two word vectors. Observe that the vectors in the illustration above looks similar to each other: positions of non-zero values, and values of each cell. Similar word vectors will put similar words close to each other in a vector space, and as a result, \"Success\" and \"Achieve\" will have small Euclidean or Cosine Distance. One might experience difficulty in trying to visualize the Euclidean or Cosine distance of the word vectors in a 10D vector space. In fact, you can't visualize anything bigger then 3D. If one attempts to visualize the word vectors in a 2D or 3D space, he will have to represent the word vectors in 2D or 3D space first using dimensional reduction. Let's assume that such dimensional reduction was performed and the word vectors for \"Success\" and \"Achieve\" are reduced to 3D vectors. The word vectors will then look like this: Figure 2: Dimensional-reduced word vectors visualization in 3D Observe the dissimilarity between two word vectors and their positions within the 3D vector space. This is because 3 dimensions are not enough to capture all relationship among words and as a result fails to maintain the semantic relationship between two similar words, \"Success\" and \"Achieve\". Multi-dimensionality in vector space modeling has great significance because it directly affects the performance of any NLP model. In this post, the concept and effect of multi-dimensionality in NLP will be illustrated using mainly Co-Occurence Matrix and some Word2Vec models. 2. Review on vector space modeling techniques Before we talk about the significance of the size of dimensions ($N$), let us review how text information is transformed into a numerical matrix. Please feel free to skip this part if you are already knowledgable about this topic. There are two types of methods for word embedding: Frequency-Based Methods and Prediction-Based Methods . The below table lists some options we have for each type of embedding method. Frequency-Based Methods Prediction-Based Methods Count Vector Continuous Bag of words TF-IDF Doc2Vec Co-Occurence Matrix Word2Vec Frequency-based methods are pretty straightforward to understand. It counts how many times each word appeared in each document, or how many times each word appeared together with each words. Co-Occurence Matrix is a type of frequency-based methods. 2.1. Co-occurence matrix The value of $N$ for co-occurence matrix is the size of unique vocabulary. In the other words, co-occurence matrix is a square matrix of size $V \\times V$. Consider a co-occurence matrix with a fixed window size of $n=1$. Setting window size $n=1$ will tell the model to search adjacent context words that are positioned directly left or right of a center word. The matrix is contructed using the following two input documents: Document 1: \"all that glitters is not gold\" Document 2: \"all is well that ends well\" * START all that glitters is not gold well ends END START 0 2 0 0 0 0 0 0 0 0 all 2 0 1 0 1 0 0 0 0 0 that 0 1 0 1 0 0 0 1 1 0 glitters 0 0 1 0 1 0 0 0 0 0 is 0 1 0 1 0 1 0 1 0 0 not 0 0 0 0 1 0 1 0 0 0 gold 0 0 0 0 0 1 0 0 0 1 well 0 0 1 0 1 0 0 0 1 1 ends 0 0 1 0 0 0 0 1 0 0 END 0 0 0 0 0 0 1 1 0 0 Table 1: Co-Occurence Matrix Notes: START and END tokens In NLP, we often add START and END tokens to represent the beginning and end of sentences, paragraphs or documents. In thise case we imagine START and END tokens encapsulating each document, e.g., \"START All that glitters is not gold END\", and include these tokens in our co-occurrence counts. This co-occurence matrix is essentially a vector space model of $V$-dimensional ($V$ columns) matrix, in which $V = 10$. However, in most NLP tasks, this co-occurence matrix goes through PCA or SVD for dimensional reduction and decomposed into a new $k$-dimensional matrix. * $k_{1}$ $k_{2}$ START 0.705 0.484 all 0.705 -0.484 that 0.654 -0.783 glitters 0.52 0 is 1.027 0 not 0.654 0.783 gold 0.382 0.656 well 0.382 -0.656 ends 1.394 -1.061 END 1.394 1.061 Table 2: Dimension Reduced Co-Occurence Matrix The original matrix was 10D matrix (10 columns) — this can't be visualized. Humans can understand only up to 3D visualizations. However, dimensional reduction was performed with sklearn.decomposition.TruncatedSVD(n_components=2) , and the output table yielded a new matrix with reduced dimension of $k = 2$. This was because I set n_components = 2 . The word vectors can now be visualized in a 2D space. Further discussions about the choice of n_components and dimensional reduction will be followed in the later section of this post. Figure 3: Dimensional-reduced word vectors visualization in 2D 2.2. Word2Vec Contrary to frequency-based methods, prediction-based methods are more difficult to understand. As the name 'prediction' implies, their methodologies are based on predicting context words given a center word ( Word2Vec Skip-Gram: $P(w_{context} \\mid w_{center})$), or a center word given context words ( Continuous Bag of Words: $P(w_{center} \\mid w_{context})$). Prediction-based methods use neural network algorithm, which means that we have to worry about the number of neurons (weights) in a network. In Word2Vec model, the model matrix has a dimension of $V \\times N$, where $V$ is the size of unique vocabulary and the size of $N$ is the number of neurons in a network. Figure 3: Skip-Gram algorithm structure for Word2Vec During the forward and back propagation process, the weights in Matrix $W$ ( Embedding matrix ) of size $V \\times N$ and Matrix $W'$ ( Context matrix ) of size $N \\times V$ are optimized to minimize a loss function. Recall that the number of neurons ($N$) is a hyper-parameter that needs to be empirically optimized. Choosing different values for $N$ will yield different output performances. $N = 300$ is a dimensional parameter known to work well with Word2Vec models. Note that a matrix with a dimension of 300 cannot be visualized. However, the dimension can be reduced down to 2D or 3D using t-distributed stochastic neighbor embedding (t-SNE) , or PCA. For NLP visualization purpose, T-SNE is often preferred over PCA or SVD due to its ability to reduce high dimensions to low dimensions while capturing complex relationships with neighboring wods. More comparison about PCA vs T-SNE will be illustrated later. Notes: Word2Vec algorithm The theory behind Word2Vec skip-gram algorithm maybe complex and difficult to understand for beginners. I encourage you to read the following two and articles that explain the theory. They do it quite well. 3. Why is multi-dimensionality important? First , high dimensionality leads to high computational cost. This is especially true in the case of co-occurence matrix, in which it has a dimension of $V \\times V$, where $V$ is the size of vocabulary in a corpus. The previous example shown in Table 1 had a corpus size of 12 words, and vocab size of 10 words. In real-life applications, corpus size easily exceeds 10's or 100 GB's. For example, Gensim 's pre-trained Word2Vec model trained from Google News had a vocab size of three million ( Github Source ). If we obtain a co-occurence matrix and feed it into an NLP model without dimensional reduction, we will be training our model with a matrix size of $3M \\times 3M$. This is unrealistic. We need to choose an optimal value for reduced dimension $k$ that will best describe the variability of the data while significantly cutting down the computational cost. On the other hand, computational cost due to high dimensionality is NOT a big concern with Word2Vec, because the most optimal dimension for Word2Vec are already known to be between 200 - 600 (Note: this doesn't mean that Word2Vec is superior to co-occurence matrix. Each has its own pros and cons). It is natural to think that high dimension would lead to higher accuracy, but this happens at the cost of increased computation time. One needs to find a \"sweet spot\" that optimizes the trade-off between accuracy vs. computation time. The recent paper submitted in Dec 2018 proposed a deterministic way to compute the optimal number of $k$-dimensions. It's code implementation is available on this Github repo . Second , dimension is a critical parameter in word embedding. Too low dimension leads to underfitting and makes your model not expressive enough to capture all possible word relations. On the other hand, too high dimension results in overfitting. However, in the recent paper , it was discovered that Word2Vec and GloVe models are not sensitive to overfitting. Figure 4: skip-gram Word2Vec: over-parametrization does not significantly hurt performance Figure 5: GloVe: over-parametrization does not significantly hurt performance The spikes of the skip gram Word2Vec and and GloVe models revealed the existence of the \"sweet spot\" in which a certain number of dimensions leads to the highest accuracy. The sharp rise in accuracy in the far left zone showed that too low number of dimensions results in underfitting. However, it is interesting to note that the flat right tail of the charts showed that indefinitely increasing the number of dimensions did not really result in overfitting. Often times in neural net applications, too high number of dimensions (neurons) results in overfitting, but in skip-gram Word2Vec and GloVe, this was not the case. A typical good NLP model trained on a wide variety of corpora has a dimension in the order of hundreds. The famous Mikolov et al. 2013 paper on skip-gram Word2Vec model suggests 300 neurons, and Rohde et al. ms., 2005 paper on co-occurence/correlation matrix suggests 600-800 dimensions (columns) as the optimum parameters for vector space modeling. 4. Vector space modeling: crude oil news Figure 6: International news organization, Reuters Enough of theories. Now let's dive into the actual application of vector space modeling with Python code implementation. We will explore multi-dimensionality in NLP task using Reuters news articles on crude oil . Take a glance at the overall content of the crude oil articles with the following WordCloud visualizaiton. Figure 7: WordCloud of crude oil articles 4.1. Data preparation Imports Here is all the imports you need to follow with this tutorial. In [2]: import pandas as pd import numpy as np import nltk nltk . download ( 'reuters' ) from nltk.corpus import reuters from sklearn.decomposition import TruncatedSVD import matplotlib.pyplot as plt from mpl_toolkits.mplot3d import Axes3D % matplotlib notebook [nltk_data] Downloading package reuters to [nltk_data] C:\\Users\\EricKim\\AppData\\Roaming\\nltk_data... [nltk_data] Package reuters is already up-to-date! Sample data description First, you will have to download the articles. NLTK provides a nice api that allows us to download the digitized articles. In [3]: import nltk nltk . download ( 'reuters' ) from nltk.corpus import reuters [nltk_data] Downloading package reuters to [nltk_data] C:\\Users\\EricKim\\AppData\\Roaming\\nltk_data... [nltk_data] Package reuters is already up-to-date! In [4]: data = [] for fileid in reuters . fileids (): category = reuters . categories ( fileid ) text = reuters . raw ( fileid ) data . append ([ fileid , category , text ]) df_reuters = pd . DataFrame ( data , columns = [ 'File ID' , 'Category' , 'Text' ]) In [5]: df_reuters . head ( 10 ) Out[5]: File ID Category Text 0 test/14826 [trade] ASIAN EXPORTERS FEAR DAMAGE FROM U.S.-JAPAN RI... 1 test/14828 [grain] CHINA DAILY SAYS VERMIN EAT 7-12 PCT GRAIN STO... 2 test/14829 [crude, nat-gas] JAPAN TO REVISE LONG-TERM ENERGY DEMAND DOWNWA... 3 test/14832 [corn, grain, rice, rubber, sugar, tin, trade] THAI TRADE DEFICIT WIDENS IN FIRST QUARTER\\n ... 4 test/14833 [palm-oil, veg-oil] INDONESIA SEES CPO PRICE RISING SHARPLY\\n Ind... 5 test/14839 [ship] AUSTRALIAN FOREIGN SHIP BAN ENDS BUT NSW PORTS... 6 test/14840 [coffee, lumber, palm-oil, rubber, veg-oil] INDONESIAN COMMODITY EXCHANGE MAY EXPAND\\n Th... 7 test/14841 [grain, wheat] SRI LANKA GETS USDA APPROVAL FOR WHEAT PRICE\\n... 8 test/14842 [gold] WESTERN MINING TO OPEN NEW GOLD MINE IN AUSTRA... 9 test/14843 [acq] SUMITOMO BANK AIMS AT QUICK RECOVERY FROM MERG... In [6]: len ( df_reuters ) Out[6]: 10788 There are a total of 10,788 articles. To our convinience, the articles are already labeled with their respective categories. This helps us in a case we want to perform further analysis and run supervised learning. But we won't be running any supervised learning in this post. But instead, we will extract only the articles that are in the category of crude . In the other words, only the articles that talk about crude oil. In [7]: df_crude = df_reuters [ df_reuters [ 'Category' ] . apply ( lambda x : 'crude' in x )] df_crude . head ( 10 ) Out[7]: File ID Category Text 2 test/14829 [crude, nat-gas] JAPAN TO REVISE LONG-TERM ENERGY DEMAND DOWNWA... 123 test/15063 [acq, crude, earn, pet-chem] ENERGY/U.S. PETROCHEMICAL INDUSTRY\\n Cheap oi... 187 test/15200 [crude] TURKEY CALLS FOR DIALOGUE TO SOLVE DISPUTE\\n ... 205 test/15230 [crude] IRAQI TROOPS REPORTED PUSHING BACK IRANIANS\\n ... 209 test/15238 [crude, earn] UNION TEXAS OIL RESERVES DROPPED IN 1986\\n Un... 214 test/15244 [crude] GHANA TO BUY CRUDE OIL FROM IRAN\\n Ghana will... 257 test/15322 [crude, nat-gas] U.S.SENATE LIFTS SOME BANS ON NATURAL GAS\\n T... 266 test/15339 [crude, gas] EIA SAYS DISTILLATE STOCKS UNCHANGED, GASOLINE... 268 test/15344 [crude, gas] EIA SAYS DISTILLATE STOCKS UNCHANGED IN WEEK\\n... 272 test/15351 [crude, gas] RECENT U.S. OIL DEMAND OFF 2.6 PCT FROM YEAR A... In [8]: df_crude . shape Out[8]: (578, 3) There are 578 articles that talk about crude oil. 4.2. Constructing co-occurence matrix We will build co-occurence matrix that looks like Table 1 above. Before we build the matrix, we need to consider tokenization and window size . 4.2.1. Tokenization Tokenization is a process of splitting sentences into separate words. Tokenization usually isn't as simple as splitting senteces based on space bars. It is usually a very complicated process that involves heavy regex manipulation, lemmatization, stemming, and some hard-coding with different rules for different cases. For example, converting 'unchanged' to 'unchange', 'stocks' to 'stock', 'pushing' to 'push', 'is, was, being', to 'be'. Or treating multi-words as one word, such as 'in spite of', 'no matter of', 'Panda Express', 'North America', or 'beat down'. Or dealing with punctuations, such as 'Parskin-disease', 'one-to-one' or 'O'reilly'. And many more. Tokenization won't be covered in detail in this post, because complex tokenization is not necessary for the purpose of explaining multi-dimensinoality in NLP. However, please do note that you will need advanced tokenization scheme in real-life applications. On the other hand, NLTK has a convinient package that allows you to check if a word is within the corpora of English. However, this does not remove punctuations. The basic use case looks like this: >>> from nltk.corpus import words >>> \"fine\" in words.words() True We will split the sentences using nltk.corpus.reuters.words() . In [52]: def read_corpus ( category = '' ): files = reuters . fileids ( category ) return [[ 'START' ] + [ w . lower () for w in list ( reuters . words ( f ))] + [ 'END' ] for f in files ] In [53]: corpus_crude = read_corpus ( category = [ 'crude' ]) In [54]: pd . DataFrame ( corpus_crude ) . head ( 10 ) Out[54]: 0 1 2 3 4 5 6 7 8 9 ... 991 992 993 994 995 996 997 998 999 1000 0 START japan to revise long - term energy demand downwards ... None None None None None None None None None None 1 START energy / u . s . petrochemical industry cheap ... . economy continues its modest rate of growth . END 2 START turkey calls for dialogue to solve dispute turkey said ... None None None None None None None None None None 3 START iraqi troops reported pushing back iranians iraq said today ... None None None None None None None None None None 4 START union texas oil reserves dropped in 1986 union texas ... None None None None None None None None None None 5 START ghana to buy crude oil from iran ghana will ... None None None None None None None None None None 6 START u . s . senate lifts some bans on ... None None None None None None None None None None 7 START eia says distillate stocks unchanged , gasoline off 200 ... None None None None None None None None None None 8 START eia says distillate stocks unchanged in week distillate fuel ... None None None None None None None None None None 9 START recent u . s . oil demand off 2 ... None None None None None None None None None None 10 rows × 1001 columns 4.2.2. Choice of window size On of the critical variable in co-occurence matrix is window_size . The below illustration has a window size of 2. Figure 8: Illustration of window size A typical window size is chosen between 2-10. However, window size may exceed way over 10 in case of special circumstances, such as having too few data. Increasing window size may increase the accuracy of the model, but it comes at the price of computational cost, and sometimes loss in accuracy due to noise. The author of this paper claims that he observed decrease in model performance of his co-occurrence matrix when he used high window size with large corpus. However, he observed the opposite when high window size was used with smaller corpus. Figure 9: Effect of window size and corpus size 4.2.3. Matrix construction In [55]: def compute_co_occurrence_matrix ( corpus , window_size = 4 ): distinct_words = sorted ( list ( set ([ word for sentence in corpus for word in sentence ]))) num_words = len ( distinct_words ) word2Ind = { word : index for index , word in enumerate ( distinct_words )} M = np . zeros (( num_words , num_words )) for sentence in corpus : for i , word in enumerate ( sentence ): begin = max ( i - window_size , 0 ) end = min ( i + window_size , num_words ) context = sentence [ begin : end + 1 ] context . remove ( sentence [ i ]) current_row = word2Ind [ word ] for token in context : current_col = word2Ind [ token ] M [ current_row , current_col ] += 1 return M , word2Ind In [56]: reuters_corpus = read_corpus ( 'crude' ) M_co_occurrence , word2Ind_co_occurrence = compute_co_occurrence_matrix ( reuters_corpus , window_size = 5 ) In [57]: pd . DataFrame ( M_co_occurrence , index = word2Ind_co_occurrence . keys (), columns = word2Ind_co_occurrence . keys ()) . head ( 10 ) Out[57]: \" \"( \", \"... $ & ' ( ) )\" ... zinc zoete zollinger zone zones zubedei zubeidi zuheir zulia zverev \" 88.0 0.0 1.0 0.0 1.0 2.0 78.0 17.0 7.0 0.0 ... 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 \"( 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 ... 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 \", 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 ... 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 \"... 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 ... 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 $ 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 ... 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 & 2.0 0.0 0.0 0.0 0.0 12.0 31.0 8.0 4.0 0.0 ... 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 ' 78.0 0.0 0.0 0.0 0.0 31.0 22.0 8.0 5.0 0.0 ... 1.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 ( 17.0 0.0 0.0 0.0 0.0 8.0 8.0 6.0 220.0 1.0 ... 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 ) 7.0 1.0 0.0 0.0 0.0 4.0 5.0 220.0 2.0 0.0 ... 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 )\" 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 ... 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 10 rows × 8185 columns Notes: Effect of punctuatoins Be cautious when dealing with punctuations. Although the effect of punctuations in NLP is beyond the scope of this post, please note that punctuations often have their own roles to play in NLP applications. Ex: Parkinson-disease vs Parkinson's disease. Sometimes they may be simply removed, and sometimes they shouldn't be. 4.3. Dimensionality reduction Dimensional reduction is necessary in most of the machine learning problems. It is used to reduce computational cost of optimization model, and to mitigate the issue of correlated features. For example, column 1 and column 2 are independent, but column 2, 3 and 4 may be correlated with one another by some relationship like: $$col_2 = log{(col_3 + col_4 ) / 2}$$ This kind of correlationship may not be visible to an engineer, and may cause problems when left unmitigated. Dimensional reduction is especially necessary in the case of co-occurence matrix due to the size of the matrix, as it was mentioned above . Recall that co-occurence matrix has a dimension of $V \\times V$, where $V$ is the size of unique vocabulary. Often times vocabulary size easily exceeds tens of thousand, and it is unrealistic to run a machine learning model on such a matrix due to time constraint. Moreover, co-occurrence matrix is a very sparse matrix. Take a look at the above Pandas DataFrame of the co-occurrence matrix. You will notice that most of the values are zero, which doesn't add much value to the matrix. Dimensional reduction will make the matrix more compact to convey \"most\" of the \"useful\" information. sklearn.decomposition.TruncatedSVD will be used for dimensional reduction. Notes: Two algorithms of TruncatedSVD TruncatedSVD has two options of algorithms. \"arpack\" and \"randomized\" . \"arpack\" has an algorithm complexity of $O(m \\times n \\times k)$, and \"randomized\" has $O(m \\times n \\times log(k))$. In theory, \"arpack\" is supposed to be better in terms of accuracy, but \"randomized\" is much faster by an order of magnitude, and claimed to be as nearly accurate as \"arpack\" . The default of Scipy implementation is \"randomized\" . In [15]: def reduce_to_k_dim ( M , n_components = 2 ): svd = TruncatedSVD ( n_components = n_components , n_iter = 10 , random_state = 42 ) M_reduced = svd . fit_transform ( M_co_occurrence ) print ( 'n_components =' , n_components ) print ( 'Explained Variance =' , round ( svd . explained_variance_ratio_ . sum (), 3 )) return M_reduced In [16]: reuters_corpus = read_corpus ( 'crude' ) M_co_occurrence , word2Ind_co_occurrence = compute_co_occurrence_matrix ( reuters_corpus , window_size = 5 ) M_reduced_2 = reduce_to_k_dim ( M_co_occurrence , n_components = 2 ) n_components = 2 Explained Variance = 0.906 In [17]: pd . DataFrame ( M_reduced_2 , columns = [ '$k_ {1} $' , '$k_ {2} $' ]) . head ( 10 ) Out[17]: $k_{1}$ $k_{2}$ 0 861.598711 -92.837976 1 1.523025 -0.235919 2 0.299002 0.054687 3 0.895219 -0.383224 4 1.626508 -0.125964 5 158.617460 -26.523809 6 975.388537 250.237303 7 211.345647 30.294191 8 162.468186 -12.582009 9 1.272724 -0.197958 The original $8185 \\times 8185$ co-occurence matrix was reduced into 2-dimensional matrix of $8185 \\times 2$, using dimensional reduction with n_components=2 . According to the explained_variance_ratio_ , the new reduced matrix captures 90.6% variability of the original data with window_size=5 . This may not be satisfactory, but good enough to obtain a reasonable word vector visualizaiton on a 2D space. We can also try different values of n_components . In [18]: M_reduced_3 = reduce_to_k_dim ( M_co_occurrence , n_components = 3 ) n_components = 3 Explained Variance = 0.923 In [19]: M_reduced_10 = reduce_to_k_dim ( M_co_occurrence , n_components = 10 ) n_components = 10 Explained Variance = 0.972 In [20]: M_reduced_50 = reduce_to_k_dim ( M_co_occurrence , n_components = 50 ) n_components = 50 Explained Variance = 0.993 In [21]: M_reduced_100 = reduce_to_k_dim ( M_co_occurrence , n_components = 100 ) n_components = 100 Explained Variance = 0.996 Notes: explained_variance_ratio_ Performing dimensional reduction inevitably results in loss of some data in the original matrix. It is a natural phenomenon considering how we are reducing $8185 \\times 8185$ matrix all the way down to $8185 \\times 2$ matrix. We can quantify the loss in data using explained_variance_ratio_ . I won't cover the details of this property, but long story short, it's good if explained_variance_ratio_ is close to 100%. Reducing the matrix to high $k$-dimension will result in smaller loss of data at the cost of high computational load, and vice versa. If you are interested in learning a deterministic way to decide the optimal value of $k$, take a look at this paper . 5. Word vectors visualization The original word vector had 8185-dimension, but now its reduced down to 2-dimension and 3-dimension, which can be visualized on a 2D and 3D plane. Normalization Rescaling (normalization) needs to be done on rows to make each of them unit-length. Skipping this step will result in your visualization looking unbalanced. In [23]: # normalize M_lengths_2 = np . linalg . norm ( M_reduced_2 , axis = 1 ) M_normalized_2 = M_reduced_2 / M_lengths_2 [:, np . newaxis ] M_lengths_3 = np . linalg . norm ( M_reduced_3 , axis = 1 ) M_normalized_3 = M_reduced_3 / M_lengths_3 [:, np . newaxis ] Visualization In [24]: # Axes3D needs to be imported in case of plotting 3D visualizations from mpl_toolkits.mplot3d import Axes3D In [25]: def plot_embeddings ( M_reduced , word2Ind , words , ax ): dimension = M_reduced . shape [ 1 ] assert ( dimension == 3 or dimension == 2 ) for i , word in enumerate ( words ): index = word2Ind [ word ] embedding = M_reduced [ index ] if dimension == 3 : x , y , z = embedding [ 0 ], embedding [ 1 ], embedding [ 2 ] ax . scatter ( x , y , z , color = 'red' ) ax . text ( x , y , z , word ) else : x , y = embedding [ 0 ], embedding [ 1 ] ax . scatter ( x , y , marker = 'x' , color = 'red' ) ax . text ( x , y , word ) return fig , ax Choice of words to visualize Recall that there are 8185 unique vocabulary in our data. Visualizing all of them on a plot will won't be very informative, because readers won't be able to distinguish between words because they are too densely plotted. In [26]: # choose words to visualize words = [ 'bank' , 'barrels' , 'bpd' , 'ecuador' , 'energy' , 'industry' , 'oil' , 'petroleum' , 'output' , 'produce' , 'occidental' , 'mobil' , 'exxon' , 'electricity' , 'kilowatt' , 'china' , 'paris' , 'saudi' , 'norway' , 'blockading' , 'expert' , 'yen' , 'kuwaiti' , 'kuwait' , 'persian' , 'eia' , 'gulf' , 'bp' , 'uk' , 'gas' , 'europe' , 'allocated' , 'lacks' , 'militarily' , 'discouraged' , 'violations' , 'possibly' ] In [27]: fig = plt . figure () fig . suptitle ( 'Word Vector Visualizations' ) # First subplot ax1 = fig . add_subplot ( 1 , 2 , 1 ) ax1 = plot_embeddings ( M_normalized_2 , word2Ind_co_occurrence , words , ax1 ) # Second subplot ax2 = fig . add_subplot ( 1 , 2 , 2 , projection = '3d' ) ax2 = plot_embeddings ( M_normalized_3 , word2Ind_co_occurrence , words , ax2 ) 6. Dimensionality analysis on visualizations 3D visualizations can be viewed from different angles. What will they look like in different angles? Let's take a look. Figure 10: 3D plot - angle 1 Figure 11: 3D plot - angle 2 Figure 12: 3D plot - angle 3 Interesting Observations: First , when viewed from a certain angle, the 3D plot looks exactly like the 2D plot. However, different patterns can be observed when viewed from a different angle. Second , the distance of \"violations\" and \"discouraged\" are different. They are close to each other on Figure 10 , but far away from each other on Figure 11 & 12 . Third , similar pattern was found between \"kilowatt\" and \"electricity\" . As you know, \"kilowatt\" is an unit of \"electricity\" . That is why they are close on Figure 11 & 12 , but are separated in Figure 10 . Observe how \"electricity\" is close, but \"kilowatt\" is not to the cluster of oil-producing entities in Figure 10 . A possible explanation is that \"electricity\" is not oil, but similar to oil in a sense that it is a \"type\" of energy consumed. On the other hand, although \"kilowatt\" is a unit of energy, but it is not a type of energy. Fourth , \"barrels\" and \" bpd \" are always close to one another. They seem very close on all three plots shown above, and indeed they look close on other angles as well. Due to time and space constraint, I can't take a screenshot of every possible angles and put it here, but I observed that they are always close on any angles. This makes sense, considering \"bpd\" stands for \"barrels per day\". Fifth , \"paris\" was group together with \"barrels\" and \"bpd\" . They have nothing in common, and yet they were grouped together. The model failed to project \"paris\" on a right vector space. Sixth , the co-occurrence vector space model was able to capture the cluster of oil producers. \"occidental\" is for Occidental Petroleum Corporation (or Oxy), and \"exxon\" is for Exxon Mobil, the two major oil-producing companies in the US. And \"gulf\" for the Gulf of Mexico, one of the biggest offshore oil-producing region in the world. Seventh , \"eia\" was not inside the cluster of oil producers in most of the angles. This is noteworthy because EIA (U.S. Energy Information Administration) provides energy statistics, such as oil & gas production, consumption, export and export rate. However, it is not a \"producing\" entity. The model was able to differentiate between an entity who \"talks\" about the oil-production, and the entities who actually \"produce\" oil. Eighth , the model was not able to differentiate nations (Kuwait, Norway, China, UK) from the other entities. A well trained Word2Vec model with 300-dimensions is able to distinguish among nations, cities, and companies, but this one couldn't. WARNING! Previously I said that a good co-occurrence matrix model has a dimension of 600-800. However, the explained_variance_ratio_ was 92.3%, which is reasonably good, and the 3D visualizations seemed to capture the relationship among words pretty well. This happend because: 1. the purpose of this post was to explain dimensionality If you attempt to feed the co-occurrence matrix of 3-dimensions into a machine learning model, it will NOT perform well. It \"seems\" to work well because it was used only for visualizations. 2. the data had limited scope of topics: crude oil Having limited scope of topics really simplfies the complexity of problem. If the training data had all sorts of articles (Ex: toiles, water bottles, nuclear, desk, paper, insects, milk, gas, school, trash), this will not work well. 7. Conclusion It is known that a well trained Word2Vec model has 300 dimensions. Think of each dimension in Word2Vec vector space as an entity that represents word relationships: man vs woman, objects vs living, humans vs animals, nation vs city, drink vs food, action vs concept, and many more. Assume that that there are 300 of these kinds of relationships among all words in this world, represented by a single dimension per relationship. The 3D visualization from the co-occurrence matrix model was able to capture the cluster of oil-producing entities ( Sixth observation). But it wasn't able to differentiate between nations and companies, because it simply didn't have a dimension that captures that kind of relationship ( Eighth observation). In Figure 10 , the model sensed the negative feelings that \"discouraged\" and \"violations\" conveyed, and put them in a close vector space ( Second observation). But when viewed from a different angle, or to put it better, viewed from a different dimension, the model put them far apart because they do not convey similar meanings ( Figure 11 & 12 ). On the other hand, the model completely failed to distinguish between \"paris\" vs \"barrels\" + \"bpd\" ( Fifth observation). Recall that our explained_variance_ratio_ for 3-dimension was 92.3% above . Some information about \"paris\" could've been lost during dimensional reduction and that might have caused the error in word vector projection on a 3D vector space. Or, we simply did not have sufficient training data. Having more dimensions allows a model to capture more complex relationships among words, and that's precisly why Google's Word2Vec model had 300 dimensions.","tags":"Natural Language Processing","url":"https://aegis4048.github.io/understanding_multi-dimensionality_in_vector_space_modeling","loc":"https://aegis4048.github.io/understanding_multi-dimensionality_in_vector_space_modeling"},{"title":"Transforming Non-Normal Distribution to Normal Distribution","text":"In the field of statistics, the assumption of normality is important because many statistical techniques perform calculations assuming the data is normally distributed. The techniques that assume Gaussian or Gaussian-like distribution are listed below: Techniques That Assume Normality Hypothesis testing through t-test and z-test Analysis of variance (ANOVA) Sequential Gaussian simulation in spatial analysis Control limits in control chart Unfortunately, many real-life data are not normal. Permeability distribution of rock samples is lognormal. Time required to repair a malfunctioning component follows exponential distribution, and reliability analysis for machine performance with respect to time follows Weibull distribution. What should you do if your data fails a normality test, or is not Gaussian-like? You have three options: Use it as it is or fit non-normal distribution Try non-parametric method Transform the data into normal distribution 1. Use it as it is or fit non-normal distribution Altough your data is known to follow normal distribution, it is possible that your data does not look normal when plotted, because there are too few samples. For example, test scores of college students follow a normal distribution. If you know for certain that your data is normally distributed by nature, then according to the Central Limit Theorem, your data will eventually become normal when you obtain a greater number of sample data. This means that you can still use the famous standard deviation method to assign letter grades to students as shown in figure (1) , even if your students' test scores do not look normally distributed. If you increase the number of students that takes your exam, the test score distribution will become more normal according to the Central Limit Theorem. Figure 1: assigning letter grades with standard deviation On the other hand, if you have plenty enough samples to represent the true population, you can fit different types of distributions to better describe your data. Different methods exist for different distributions and maybe you will be able to achieve your goal without using techniques that strictly require Gaussian distribution. The code snippet below fits three different distributions on the sample data: lognormal, normal, and Weibull distributions. Through a visual inspection, it can be observed that the sample data is the best represented by a lognormal distribution . Once we know that the sample data follows lognormal distribution, we can move forward by employing techniques that assume lognormal distribution. In [12]: import numpy as np import matplotlib.pyplot as plt from scipy import stats % matplotlib notebook In [4]: # sample data generation np . random . seed ( 42 ) data = sorted ( stats . lognorm . rvs ( s = 0.5 , loc = 1 , scale = 1000 , size = 1000 )) # fit lognormal distribution shape , loc , scale = stats . lognorm . fit ( data , loc = 0 ) pdf_lognorm = stats . lognorm . pdf ( data , shape , loc , scale ) # fit normal distribution mean , std = stats . norm . fit ( data , loc = 0 ) pdf_norm = stats . norm . pdf ( data , mean , std ) # fit weibull distribution shape , loc , scale = stats . weibull_min . fit ( data , loc = 0 ) pdf_weibull_min = stats . weibull_min . pdf ( data , shape , loc , scale ) In [5]: # visualize fig , ax = plt . subplots ( figsize = ( 8 , 4 )) ax . hist ( data , bins = 'auto' , density = True ) ax . plot ( data , pdf_lognorm , label = 'lognorm' ) ax . plot ( data , pdf_norm , label = 'normal' ) ax . plot ( data , pdf_weibull_min , label = 'Weibull_Min' ) ax . set_xlabel ( 'X values' ) ax . set_ylabel ( 'probability' ) ax . legend (); Notes: Normality test with hypothesis testing Visual inspection is one option to assess the performance of the fitted distributions. The other option is to use hypothesis testing with Q-Q plots to numerically assess the performance of the fitted distribution. For example, if you want to numerically assess how well your data matches Gaussian distribution, you can test your hypothesis through D'Agostino-Pearson normality test, Anderson-Darling Test, or Shapiro-Wilk Test. Normality test with D'Agostino using scipty.stats.normaltest() is covered below . 2. Try non-parametric method There are pros and cons for using non-parametric methods. The biggest pros is that it does not assume anything about the distribution. They are distribution-free . You do not need to know distribution shape, mean, standard devation, skewness, kurtosis, etc... All you need is just a set of sample data that is representative of a population. The fact that it does not assume anything about the distribution has another implication when you have small number of data - there's no need for Central Limit Theorem to be applied. Recall that the Central Limit Theorem states that the data will become more and more Gaussian-distributed as the number of samples increases. Techniques that assume normality of a distribution expect the sample data to follow Central Limit Theorem. Non-parametric methods improves the performance of statistical calculation when there are too few number of samples that the Central Limit Theorem can't be applied. However, it is important that those few samples are reasonably representative of the true population. If they are not, your result will be biased. Non-parametric methods are geared toward hypothesis testing rather than estimation. Disadvantages of non-parametric methods include lack of power compared to more traditional approaches that require prior knowledge of a distribution. If you knew the distribution of your data with 100% certainty, there is no reason to use a non-parametric method. Doing so would be a waste of perfectly good prior knowledge. Another disadvantage is that many non-parametric methods are computation intensive. For example, Boostrapping is a non-parametric resampling method. It can be used to compute confidence interval of statistics, but requires numerical iterations, whereas computing confidence interval with parametric methods does not require iterations. The following table lists non-parametric alternatives to techniques that assume normality of a distribution: Techniques That Assume Normality Non-Parametric Alternatives Confidence Interval with z-test Bootstrapping T-test Mann-Whitney test; Mood's median test; Kruskal-Wallis test ANOVA Mood's median test; Kruskal-Willis test Paired t-test One-sample sign test F-test; Bartlett's test Levene's test Individuals control chart Run Chart Notes Parametric methods are the type of methods that assume a certain shape of a distribution. For example, the following equation is used to calculate the confidence interval of a mean of a distribution: $$\\text{CI of mean} = \\text{sample mean} \\pm (\\text{distribution score} \\times \\text{Standard Error} )$$ The variable in the equation, distribution score , depends on the type of the distribution. If you do not know the distribution shape of your data, it is very difficult to obtain the value of the distribution score. On the other hand, non-parametric methods do not assume anything about a distribution. A non-parametric alternative to calculate confidencer interval of mean is to use Bootstrapping . 3. Transform the data into normal distribution The data is actually normally distributed, but it might need transformation to reveal its normality. For example, lognormal distribution becomes normal distribution after taking a log on it. The two plots below are plotted using the same data, just visualized in different x-axis scale. Observe how lognormal distribution looks normal when log is taken on the x-axis. In [6]: import numpy as np import matplotlib.pyplot as plt from scipy import stats % matplotlib notebook In [7]: # sample data generation np . random . seed ( 42 ) data = sorted ( stats . lognorm . rvs ( s = 0.5 , loc = 1 , scale = 1000 , size = 1000 )) # fit lognormal distribution shape , loc , scale = stats . lognorm . fit ( data , loc = 0 ) pdf_lognorm = stats . lognorm . pdf ( data , shape , loc , scale ) In [9]: # visualize fig , ( ax1 , ax2 ) = plt . subplots ( 1 , 2 , figsize = ( 8 , 4 )) ax1 . hist ( data , bins = 'auto' , density = True ) ax1 . plot ( data , pdf_lognorm ) ax1 . set_ylabel ( 'probability' ) ax1 . set_title ( 'Linear Scale' ) ax2 . hist ( data , bins = 'auto' , density = True ) ax2 . plot ( data , pdf_lognorm ) ax2 . set_xscale ( 'log' ) ax2 . set_title ( 'Log Scale' ); Similar transformations can be done on the sample data to convert non-normal to normal distribution. Lognormal transformation is used to convert rock permeability distributions to normal distribution, and square root transformation is used to analyze biological population growth, such as bacterial colonies per petri dish. These types of transformations - rescaling the distribution by taking exponents or log - are called Power Transformations. Box-Cox transformation is the most popular technique within the family of power transformations. Box-Cox Transformation: Theory Box-Cox Transformation is a type of power transformation to convert non-normal data to normal data by raising the distribution to a power of lambda ($\\lambda$). The algorithm can automatically decide the lambda ($\\lambda$) parameter that best transforms the distribution into normal distribution. Box-Cox transformation is a statistical technique known to have remedial effects on highly skewed data. Essentially it's just raising the distribution to a power of lambda ($\\lambda$) to transform non-normal distribution into normal distribution. The lambda ($\\lambda$) parameter for Box-Cox has a range of -5 < $\\lambda$ < 5 . If the lambda ($\\lambda$) parameter is determined to be 2, then the distribution will be raised to a power of 2 — $Y&#94;2$. The exception to this rule is when the lambda ($\\lambda$) parameter is 0 - log will be taken to the distribution — log($Y$). The below table shows how Box-Cox transformation raises the power of a distribution to different lambda ($\\lambda$) values: Lambda ($\\lambda$) Transformed Distribution ($Y&#94;{'}$) -2 $Y&#94;{'} = \\frac{1}{Y&#94;2}$ -1 $Y&#94;{'} = \\frac{1}{Y&#94;1}$ -0.5 $Y&#94;{'} = \\frac{1}{sqrt(Y)}$ 0 $Y&#94;{'} = log(Y)$ 0.5 $Y&#94;{1} = sqrt(Y)$ 1 $Y&#94;{'} = Y$ 2 $Y&#94;{'} = Y&#94;2$ Although in the table lambda ($\\lambda$) values of only -2 < $\\lambda$ < 2 were displayed, the actual algorithm has a range of -5 < $\\lambda$ < 5 . Also note that using a lambda ($\\lambda$) value of 1 does not do anything to the distribution. If the Box-Cox algorithm spits out $\\lambda = 1$, it probably means that your data is Gaussian-like or Gaussian enough to an extent that there is no need for transformation. All data to be positive and greater than 0 (Y > 0) Box-Cox transformation does not work if data is smaller than 0. This can easily be fixed by adding a constant ($C$) that will make all your data greater than zero. The transformation equation is then: $Y&#94;{'} = (Y + C)&#94;{\\lambda}$ Python Code Implementation The code implementation for Box-Cox transformation is very simple with the help of scipy.stats.boxcox() . from scipy import stats xt, lmbda = stats.boxcox(x) xt is the transformed data, and lmbda is the lambda ($\\lambda$) parameter. More detailed usage & analysis of Box-Cox will be covered in the next section. Box-Cox Transformation: Phone Call Duration - Gamma Distribution The distribution for phone call duration follows Erlang distribution, a member of a family of Gamma distribution. When the shape parameter of Gamma distribution has an integer value, the distribution is the Erlang disribution. Since power transformation is known to work well with Gamma distribution, we can try Box-Cox transformation to turn non-normal data into normal data. The below code snippet demonstrates how a typical Gamma distribution looks like when plotted: In [10]: from scipy import stats import matplotlib.pyplot as plt import numpy as np % matplotlib notebook In [11]: # random variable generation for gamma distribution def generate_gamma_dist ( shape ): dist_gamma = sorted ( stats . gamma . rvs ( shape , loc = 0 , scale = 1000 , size = 5000 )) shape , loc , scale = stats . gamma . fit ( dist_gamma , loc = 0 ) pdf_gamma = stats . gamma . pdf ( dist_gamma , shape , loc , scale ) return dist_gamma , pdf_gamma In [12]: # visualize fig , ax = plt . subplots ( figsize = ( 8 , 4 )) for i in range ( 1 , 5 ): x , y = generate_gamma_dist ( i ) ax . plot ( x , y , label = 'shape parameter = %s ' % i ) ax . set_xlabel ( 'X values' ) ax . set_ylabel ( 'probability' ) ax . set_ylim ( 0 , 0.0004 ) ax . set_xlim ( 0 , 10000 ) ax . set_title ( 'Gamma (Erlang) Distribution' ) ax . legend (); 1. Data Preparation We will use phone calls data from Enigma Public . Enigma Public is a website that processes & hosts various public data and allows people to obtain them through file downloads or API access. The sample data originally comes from the National Response Center (NRC). They receive phone calls from anyone witnessing an oil spill, chemical release or maritime security incident and record that data. For your convenience, I already downloaded the sample data and hosted it on this website. You can access the sample data directly by importing the file through requests . In [2]: import requests import io import pandas as pd In [3]: base_url = 'https://aegis4048.github.io/downloads/notebooks/sample_data/' filename = '08c32c03-9d88-42a9-b8a1-f493a644b919_NRCEventReporting-Calls-2010.csv' data = requests . get ( base_url + filename ) . content df = pd . read_csv ( io . StringIO ( data . decode ( 'utf-8' ))) df . head () Out[3]: seqnos date_time_received date_time_complete calltype responsible_company responsible_org_type responsible_city responsible_state responsible_zip source serialid 0 946479 2010-07-03T21:11:31+00:00 2010-07-03T21:19:57+00:00 INC NaN UNKNOWN NaN XX NaN TELEPHONE 15900 1 946480 2010-07-03T20:59:29+00:00 2010-07-03T21:16:22+00:00 INC CHEVRON PRIVATE ENTERPRISE NaN HI NaN WEB REPORT 15901 2 946481 2010-07-03T21:42:43+00:00 2010-07-03T21:53:07+00:00 INC BP PRIVATE ENTERPRISE NaN LA NaN TELEPHONE 15902 3 946482 2010-07-03T22:22:41+00:00 2010-07-03T22:34:07+00:00 INC CHEVRON PRIVATE ENTERPRISE SAN LUIS OBISPO CA 93401 TELEPHONE 15903 4 946483 2010-07-03T22:46:13+00:00 2010-07-03T22:50:24+00:00 INC NaN UNKNOWN NaN XX NaN TELEPHONE 15904 1.1. Sample Data Processing Since we are interested in the time ellapsed for each phone call, the primary columns of our interest are date_time_received and date_time_complete . However, the raw data is not in a numerical format that can be directly plotted on histogram; we will need to parse & process the time data. I chose .iloc[11000: 12000, :] because it would take too long time to process all ~30,000 rows of the original data. The data is then sorted by the timestamp column. Process DateTime In [4]: import datetime In [5]: def process_time ( row ): call_received = datetime . datetime . strptime ( row [ 'date_time_received' ] . split ( '+' )[ 0 ], '%Y-%m- %d T%H:%M:%S' ) call_ended = datetime . datetime . strptime ( row [ 'date_time_complete' ] . split ( '+' )[ 0 ], '%Y-%m- %d T%H:%M:%S' ) time_ellapsed = call_ended - call_received row [ 'Parsed Call Received' ] = str ( call_received ) row [ 'Parsed Call Ended' ] = str ( call_ended ) row [ 'Time Ellapsed' ] = str ( time_ellapsed ) row [ 'Time Ellapsed (minutes)' ] = round ( time_ellapsed . total_seconds () / 60 , 1 ) return row In [6]: # df was defined above parsed_df = df . iloc [ 11000 : 12000 , :] . apply ( process_time , axis = 1 ) . iloc [:, - 4 :] parsed_df [ 'Parsed Call Received' ] = pd . to_datetime ( parsed_df [ 'Parsed Call Received' ], format = '%Y-%m- %d %H:%M:%S' ) parsed_df [ 'Parsed Call Ended' ] = pd . to_datetime ( parsed_df [ 'Parsed Call Ended' ], format = '%Y-%m- %d %H:%M:%S' ) parsed_df = parsed_df . sort_values ( by = 'Parsed Call Received' ) parsed_df . head () Out[6]: Parsed Call Received Parsed Call Ended Time Ellapsed Time Ellapsed (minutes) 11000 2010-05-21 19:32:09 2010-05-21 19:43:35 0:11:26 11.4 11001 2010-05-21 19:54:40 2010-05-21 19:58:40 0:04:00 4.0 11002 2010-05-21 20:03:14 2010-05-21 20:09:11 0:05:57 6.0 11003 2010-05-21 20:04:26 2010-05-21 20:07:39 0:03:13 3.2 11004 2010-05-21 20:18:38 2010-05-21 20:27:45 0:09:07 9.1 Drop Duplicate Rows The original data does not record the time to a precision of microseconds ( 2010-07-03T21:11:31+00:00 ). Due to the imprecision of the recorded data, there will be rows with duplicate date_time_received and date_time_complete . The data itself is not duplicate, but the data in datetime column is duplicate. Leaving them as they are and plotting them might mess up your plots. Observe the change in the row number of the data frame. It's the indication that there are rows with the same date_time_received values. In [7]: parsed_df . shape Out[7]: (1000, 4) In [8]: new_parsed_df = parsed_df . drop_duplicates ( subset = [ 'Parsed Call Received' ], keep = False ) new_parsed_df . shape Out[8]: (998, 4) 2. Transformatoin It can be observed that the phone call duration data does not follow normal distribution. In [9]: time_duration_orig = new_parsed_df [ 'Time Ellapsed (minutes)' ] . values In [40]: fig , ax = plt . subplots ( figsize = ( 8 , 4 )) ax . hist ( time_duration_orig , bins = 'auto' , density = True ) ax . set_xlabel ( 'Call Duration (minutes)' ) ax . set_ylabel ( 'probability' ) ax . set_title ( 'Non-normal Distribution of Phone Call Duration' ); 2.1. Box-Cox Transformation The Python code implementation for Box-Cox is actually very simple. The below one-line code is it for transformation. More information about the Box-Cox function can be found in the scipy documentaion . In [10]: time_duration_trans , lmbda = stats . boxcox ( time_duration_orig ) How does Box-Cox determine the best transformation parameter to obtain a distribution that is close to normal? It calculates correlation coefficient for different lambda ($\\lambda$) values, and finds the one that maximizes the correlation coefficient. In our case, we find that the best lambda parameter is $\\lambda = -0.322$ In [13]: print ( 'Best lambda parameter = %s ' % round ( lmbda , 3 )) fig , ax = plt . subplots ( figsize = ( 8 , 4 )) prob = stats . boxcox_normplot ( time_duration_orig , - 20 , 20 , plot = ax ) ax . axvline ( lmbda , color = 'r' ); Best lambda parameter = -0.322 2.2. Visual Inspection by Fitting Gaussian Distribution One can visually inspect how good the transformation was by fitting a Gaussian distribution function. In [14]: # fit Gaussian distribution time_duration_trans . sort () mean , std = stats . norm . fit ( time_duration_trans , loc = 0 ) pdf_norm = stats . norm . pdf ( time_duration_trans , mean , std ) In [44]: # visual inspection fig , ax = plt . subplots ( figsize = ( 8 , 4 )) ax . hist ( time_duration_trans , bins = 'auto' , density = True ) ax . plot ( time_duration_trans , pdf_norm , label = 'Fitted normal distribution' ) ax . set_xlabel ( 'Call Duration (minutes)' ) ax . set_ylabel ( 'Transformed Probability' ) ax . set_title ( 'Box-Cox Transformed Distribution of Phone Call Duration' ) ax . legend (); Based on the transformed historgram and the respective fitted normal distribuion, it seems that our Box-Cox transformation with $\\lambda = -0.322$ worked well. 2.3. Visual Inspection with Q-Q Plots Visual inspection can be done in a different way with Q-Q plots. The red straight line is the fitted theoretical Gaussian distribution function. If the scatter plot is closer to the red straight line, it means that the data is very close to Gaussian distribution. Deviation from the red line indicates that the data is most likely not Gaussian. Recall that time_duration_orig is the original sample data, and time_duration_trans is the Box-Cox transformed data. In [26]: fig , ( ax1 , ax2 ) = plt . subplots ( 1 , 2 , figsize = ( 8 , 4 )) prob = stats . probplot ( time_duration_orig , dist = stats . norm , plot = ax1 ) prob = stats . probplot ( time_duration_trans , dist = stats . norm , plot = ax2 ) ax1 . set_title ( 'Original Data' ) ax1 . set_ylabel ( 'Call Duration (minutes)' ) ax2 . set_title ( 'Transforamed Data, λ = %s ' % - 0.322 ); ax2 . set_ylabel ( '' ); No significant deviation is observed in Q-Q plots for the transformed data. The transformed data seems to follow Gaussian distribution well. 2.4. Normality Test with Hypothesis Testing Sometimes one might prefer not to evaluate the normality of the transformed data with visual inspection. It is possible to run formal hypothesis testing and check normality in terms of statistical values with scipy.stats.normaltest . It is based on D'Agostino and Pearson's test that combines skew and kurtosis to produce an omnibus test of normality. scipy.stats.normaltest() returns a 2-tuple of the chi-squared statistic, and the associated p-value. Given the null hypothesis that x came from a normal distribution, if the p-value is very small, we reject the null hypothesis. It means that it is unlikely that the data came from a normal distribution. In [15]: k2 , p = stats . normaltest ( time_duration_trans ) print ( ' \\n Chi-squared statistic = %.3f , p = %.3f ' % ( k2 , p )) alpha = 0.05 if p > alpha : print ( ' \\n The transformed data is Gaussian (fails to reject the null hypothesis)' ) else : print ( ' \\n The transformed data does not look Gaussian (reject the null hypothesis)' ) Chi-squared statistic = 2.453, p = 0.293 The transformed data is Gaussian (fails to reject the null hypothesis) The traditional alpha value of 5% was assumed ($\\alpha = 0.05$). Based on the result of the hypothesis testing, it seems that the transformed data does not significantly deviate from a theoretical Gaussian distribution. 3. Back Transformation - Control Chart Analysis One might wonder why we ever want to transform data into something different. What's the point of running analysis on transformed data that significantly deviates from the original data? Let's say that you have a sample data for human's life expectancy, which ranges between 0 and 100. Let's say that the distribution is not Gaussian, so you raised it to a power of 2 to convert it to Gaussian, making the transformed range to be between 0 to 10000. You calculate the mean of the transformed data and find out that the mean is 4,900 years. It is unreasonble to think that average life span of humans is 4,900 years. $$ \\text{Transformed average life span} = 4,900 \\,\\, \\text{years} $$ One must note that the whole point of data transformation is not to transform the data itself, but to use techniques that require a certain form of a distribution and acquire correct statistical values of your interest. This is where Back Transformation comes into play. You raised your sample data to a power of 2, and obtained the mean value of 4,900 years. Since you raised it to a power of 2, you will back transform it by lowering its power by 2. $$ \\text{original average life span} = \\sqrt{\\text{transformed average life span}} = \\sqrt{4,900 \\,\\, \\text{years}} = 70 \\,\\, \\text{years} $$ The concept of back transformation will be illustrated with control chart analysis. 3.1. Control Chart Basics If you don't have a good understanding of what control chart is, I recommend you to read this article . It is well written with clean, illustrative visualizations. The upper and lower control limits (UCL and LCL) in control charts are defined as values that are three standard deviations from a mean ($\\mu \\space \\pm \\space 3 \\sigma$). The control limits can be plotted on control chart with the following code snippet (note that new_parsed_df and time_duration_orig were defined above): In [29]: y = new_parsed_df [ 'Parsed Call Received' ] . values In [30]: mean = np . mean ( time_duration_orig ) std = np . std ( time_duration_orig ) upper_limit = mean + 3 * std lower_limit = mean - 3 * std In [31]: fig , ax = plt . subplots ( figsize = ( 8 , 4 )) ax . plot ( y , time_duration_orig ) ax . axhline ( mean , color = 'C1' ) ax . axhline ( upper_limit , color = 'r' ) ax . axhline ( lower_limit , color = 'r' ); ax . text ( y [ - 200 ], upper_limit + 3 , 'Upper Control Limit' , color = 'r' ) ax . text ( y [ - 200 ], lower_limit + 3 , 'Lower Control Limit' , color = 'r' ) ax . text ( y [ 3 ], mean + 3 , 'Mean' , color = 'C1' ) ax . set_ylabel ( 'Call duration (minutes)' ); ax . set_title ( 'Control Chart for Phone Call Duration - Original' ); A few interesting observations could be drawn from the control chart visualization. Intervals dense number of phone calls are from daytime, and intervals with sparse number of phone calls are made from night time. The peaks seem to happen quite regularly. This makes sense considering how some phone calls take much longer than the others due to special circumstances No phone call duration is smaller than 0, and yet the lower control limit is -10, because the traditional control limit computation assumes normality of data, when phone call duration is not normally distributed. 3.2. Why Is Transformation Necessary? The upper control limit plotted on the above visualization defines any phone calls that take longer than 26 minutes to be an outlier. But, are they really outliers? Some phone calls might take longer than 26 minutes due to some extreme circumstances. Moreover, those \"outliers\" seem to be happening too often to be considred outliers. This is happening because the calculation of control limits through plus/minus three standard deviation ($\\pm \\space 3 \\sigma$) assumes that the data is normally distributed. The standard deviation method fails because the assumption of normality is not valid for the phone call duration distribution. Box-Cox transformation is necessary. In [32]: time_duration_trans , lmbda = stats . boxcox ( time_duration_orig ) In [33]: mean_trans = np . mean ( time_duration_trans ) std_trans = np . std ( time_duration_trans ) upper_limit_trans = mean_trans + 3 * std_trans lower_limit_trans = mean_trans - 3 * std_trans In [34]: fig , ax = plt . subplots ( figsize = ( 8 , 4 )) ax . plot ( y , time_duration_trans ) ax . axhline ( mean_trans , color = 'C1' ) ax . axhline ( upper_limit_trans , color = 'r' ) ax . axhline ( lower_limit_trans , color = 'r' ); ax . text ( y [ - 200 ], upper_limit_trans - 0.15 , 'Upper Control Limit' , color = 'r' ) ax . text ( y [ - 200 ], lower_limit_trans + 0.15 , 'Lower Control Limit' , color = 'r' ) ax . text ( y [ 3 ], mean_trans + 0.1 , 'Mean' , color = 'C1' ) ax . set_ylabel ( 'Call duration (minutes)' ); ax . set_title ( 'Control Chart for Phone Call Duration - Transformed' ); A quick glance at the control chart of the transformed data tells us that the most of the phone calls were actually within the upper and lower control limit boundaries. $\\pm \\space 3 \\sigma$ standard deviation method is now working because the assumption of normality is satisfied. 3.3. Back Transforming Control Limits It is difficult for non-statisticians to understand that we are drawing conclusions from the transformed data. We need to back transform the calculated upper and lower control limits by taking the inverse of the lambda ($\\lambda$) parameter we applied for Box-Cox transformation. scipy.special.inv_boxcox will do the job. In [35]: from scipy.special import inv_boxcox In [36]: back_trans_upper_limits = inv_boxcox ( upper_limit_trans , lmbda ) back_trans_lower_limits = inv_boxcox ( lower_limit_trans , lmbda ) mean = np . mean ( time_duration_orig ) In [37]: fig , ax = plt . subplots ( figsize = ( 8 , 4 )) ax . plot ( y , time_duration_orig ) ax . axhline ( mean , color = 'C1' ) ax . axhline ( back_trans_upper_limits , color = 'r' ) ax . axhline ( back_trans_lower_limits , color = 'r' ); ax . text ( y [ - 200 ], back_trans_upper_limits + 3 , 'Upper Control Limit' , color = 'r' ) ax . text ( y [ - 200 ], back_trans_lower_limits + 3 , 'Lower Control Limit' , color = 'r' ) ax . text ( y [ 3 ], mean + 3 , 'Mean' , color = 'C1' ) ax . set_ylabel ( 'Call duration (minutes)' ); ax . set_title ( 'Control Chart for Phone Call Duration - Back Transformed' ); After back-transforming the Box-Cox transformed data, we can now draw a conclusion that all of the phone calls, except for one, made to the National Response Center between 2010-05-22 to 2010-06-01 were within the control limits.","tags":"Statistics","url":"https://aegis4048.github.io/transforming-non-normal-distribution-to-normal-distribution","loc":"https://aegis4048.github.io/transforming-non-normal-distribution-to-normal-distribution"},{"title":"Parse PDF Files While Retaining Structure with Tabula-py","text":"If you've ever tried to do anything with data provided to you in PDFs, you know how painful it is — it's hard to copy-and-paste rows of data out of PDF files. It's especially hard if you want to retain the formats of the data in PDF file while extracting text. Most of the open source PDF parsers available are good at extracting text. But when it comes to retaining the the file's structure, eh, not really. Try tabula-py to extract data into a CSV or Excel spreadsheet using a simple, easy-to-use interface. One look is worth a thousand words. Take a look at the demo screenshot. Installations This installation tutorial assumes that you are using Windows. However, according to the offical tabula-py documentation , it was confirmed that tabula-py works on macOS and Ubuntu. 1. Download Java Tabula-py is a wrapper for tabula-java, which translates Python commands to Java commands. As the name \"tabula-java\" suggests, it requires Java. You can download Java here . 2. Set environment PATH variable (Windows) One thing that I don't like about Windows is that it's difficult to use a new program I downloaded in a console environment like Python or CMD window. But oh well, if you are a Windows user, you have to go through this extra step to allow Python to use Java. If you are a macOS or Ubuntu user, you probably don't need this step. Find where Java is installed, and go to Control Panel > System and Security > System > Advanced system settings > Advanced > Environment Variables... to set environment PATH variable for Java. Make sure you have Java\\jdk1.8.0_201\\bin and Java\\jre1.8.0_201\\bin in the environment path variable. Then, type java -version on CMD window. If you successfully installed Java and configured the environment variable, you should see something like this: java -version java version \"1.8.0_201\" Java(TM) SE Runtime Environment (build 1.8.0_201-b09) Java HotSpot(TM) 64-Bit Server VM (build 25.201-b09, mixed mode) If you don't see something like this, it means that you didn't properly configure environment PATH variable for Java. 3. Re-start Your Command Prompt Any program invoked from the command prompt will be given the environment variables that was at the time the command prompt was invoked. If you launched your Python console or Jupyter Notebook before you updated your environment PATH variable, you need to re-start again. Otherwise the change in the environment variable will not be reflected. If you are experiencing FileNotFoundError or 'java' is not recognized as an internal or external command, operable program or batch file inside Jupyter or Python console, it's the issue of environment variable. Either you set it wrong, or your command prompt is not reflecting the change you made in the environment variable. To check if the change in the environment variable was reflected, run the following code in Jupyter or Python console: import os s = os.environ[\"PATH\"].split(';') for item in s: print(item) Something like these must be in the output if everything is working fine: C:\\Program Files\\Java\\jdk1.8.0_201\\bin C:\\Program Files\\Java\\jre1.8.0_201\\bin 4. Install Tabula-py This is the last step: pip install tabula-py Make sure that you install tabula-py , not tabula . Failing to do so will result in AttributeError: module 'tabula' has no attribute 'read_pdf' , as discussed in this thread . More detailed instructions are provided in the github repo of tabula-py Tabula Web Application Tabula supports web application to parse PDF files. You do not need this to use tabula-py, but from my personal experience I strongly recommend you to use this tool because it really helps you debugging issues when using tabula-py. For example, I was tring to parse 100s of PDF files at once, and for some reason tabula-py would return an NoneType object instead of pd.DataFrame object (by default, tabula-py extracts tables in dataframe) for one PDF file. There was nothing wrong with my codes, and yet it would just not parse the file. So I tried opening it on the tabula web-app, and realized that it was actually a scanned PDF file and that tabula is unable to parse scanned PDFs. Long story short, if it can be parsed with tabula web-app, you can replicate it with tabula-py. If tabula web-app can't, you should probably look for a different tool. Installations If you already configured the environment PATH variable for Java, all you need to do is downloading the .zip file here and running tabula.exe . That's it. Tabula has really nice web UI that allows you to parse tables from PDFs by just clicking buttons. Note The web-app will automatically open in your browser with 127.0.0.1:8080 local host. If port 8080 is already being used by another process, you will need to shut it down. But normally you don't have to worry about this. Screenshots This is what you will see when you launch tabula.exe . Browse... the PDF file you want to parse, and import . You can either use Autodetect Tables or drag your mouse to choose the area of your interest. If the PDF file has a complicated structure, it is usually better to manually choose the area of your interest. Also, note the option Repeat to All Pages . Selecting this option will apply the area you chose for all pages. Here's the output. More explanation about Lattice and Stream options will be discussed in detail later. Template JSON Files Tabula web-app accepts the user's drag & click as input and translates it into Java arguments that are actually used behind the scenes to parse PDF files. The translated Java arguments are accessible to users in a JSON format. Select the area you want to parse, and click Save Selections as Template . Then, Download the translated Java arguments in a text JSON file. These arguments are useful when coding arguments for tabula.read_pdf() later. template.json { \"page\": 2, \"extraction_method\": \"guess\", \"x1\": 24.785995330810547, \"x2\": 589.3559953308105, \"y1\": 390.5325, \"y2\": 695.0025, \"width\": 564.57, \"height\": 304.47 } Running Tabula-py Tabula-py enables you to extract tables from PDFs into DataFrame and JSON. It can also extract tables from PDFs and save files as CSV, TSV or JSON. Some basic code examples are as follows: import tabula # Read pdf into DataFrame df = tabula.read_pdf(\"test.pdf\", options) # Read remote pdf into DataFrame df2 = tabula.read_pdf(\"https://github.com/tabulapdf/tabula-java/raw/master/src/test/resources/technology/tabula/arabic.pdf\") # convert PDF into CSV tabula.convert_into(\"test.pdf\", \"output.csv\", output_format=\"csv\") # convert all PDFs in a directory tabula.convert_into_by_batch(\"input_directory\", output_format='csv') Area Selection You can select portions of PDFs you want to analyze by setting area (top,left,bottom,right) option in tabula.read_pdf() . This is equivalent to dragging your mouse and setting the area of your interest in tabula web-app as it was mentioned above. Default is the entire page. Also note that you can choose the page, or pages you want to parse with pages option. The sample PDF file can be downloaded from here . In [1]: import tabula import pandas as pd In [37]: file = 'pdf_parsing/lattice-timelog-multiple-pages.pdf' df = tabula . read_pdf ( file , lattice = True , pages = 2 , area = ( 406 , 24 , 695 , 589 )) df Out[37]: Start Date End Date (hr) Activity Activity Detail Operation Com 0 12/13/2014\\r06:00 12/13/2014\\r09:00 3.0 SURF-DRILL DRILL SURFACE DRL Rotate from 1600' to 1859' (259' @ 8 fph). WOB... 1 12/13/2014\\r09:00 12/13/2014\\r11:00 2.0 SURF-CIRC CIRCULATE CIRC Pump 2- 50 bbl hi vis sweep; Circulate to surface 2 12/13/2014\\r11:00 12/13/2014\\r14:00 3.0 SURF-TRIP TOOH TRIP TOOH (Slick off bottom) f/1859' to 108' (SLM)... 3 12/13/2014\\r14:00 12/13/2014\\r16:00 2.0 PLAN EQUIP BHA PJSA - Break bit & L/D directional BHA, clean ... 4 12/13/2014\\r16:00 12/13/2014\\r17:30 1.5 PLAN DRLG CSG PTJSA / R/U Pipe Pros.Csg. tools / PTJSA on ru... 5 12/13/2014\\r17:30 12/13/2014\\r18:00 0.5 PLAN DRLG CSG Make up 13 3/8 Gemco PDC drillable float shoe;... 6 12/13/2014\\r18:00 12/13/2014\\r18:30 0.5 PLAN PERS SFTY HJSM with Morning tour crew, Pipe Pro casing c... 7 12/13/2014\\r18:30 12/13/2014\\r23:30 5.0 PLAN DRLG CSG Make up 13 /8\" PDC drillable float collar onto... 8 12/13/2014\\r23:30 12/14/2014\\r01:30 2.0 SURF-CIRC CIRCULATE CIRC HJSM on Hoisting personal; Make up Swedge in ... 9 12/14/2014\\r01:30 12/14/2014\\r03:30 2.0 PLAN DRLG CSG Run 13 3/8\"J-55 54.5 BTC f/ 1,639' to 1,819';... 10 12/14/2014\\r03:30 12/14/2014\\r04:30 1.0 SURF-CIRC CIRCULATE CIRC Circulate Bttms up while Rigging down csg crew... 11 12/14/2014\\r04:30 12/14/2014\\r06:00 1.5 SURF-CMT CEMENT SURFACE\\rCASING CMT HJSM w/ Basic Cementer, H&P rig crew & PNR; D... Alternatively, you can set area with percentage scale by setting relative_area=True . For this specific PDF file, the below area=(50, 5, 92, 100), relative_area=True option is equivalent to area=(406, 24, 695, 589) above. In [38]: file = 'pdf_parsing/lattice-timelog-multiple-pages.pdf' df = tabula . read_pdf ( file , lattice = True , pages = 2 , area = ( 50 , 5 , 92 , 100 ), relative_area = True ) df Out[38]: Start Date End Date Dur (hr) Activity Activity Detail Operation Com 0 2/13/2014\\r6:00 12/13/2014\\r09:00 3.0 SURF-DRILL DRILL SURFACE DRL Rotate from 1600' to 1859' (259' @ 8 fph). WOB... 1 2/13/2014\\r9:00 12/13/2014\\r11:00 2.0 SURF-CIRC CIRCULATE CIRC Pump 2- 50 bbl hi vis sweep; Circulate to surface 2 2/13/2014\\r1:00 12/13/2014\\r14:00 3.0 SURF-TRIP TOOH TRIP TOOH (Slick off bottom) f/1859' to 108' (SLM)... 3 2/13/2014\\r4:00 12/13/2014\\r16:00 2.0 PLAN EQUIP BHA PJSA - Break bit & L/D directional BHA, clean ... 4 2/13/2014\\r6:00 12/13/2014\\r17:30 1.5 PLAN DRLG CSG PTJSA / R/U Pipe Pros.Csg. tools / PTJSA on ru... 5 2/13/2014\\r7:30 12/13/2014\\r18:00 0.5 PLAN DRLG CSG Make up 13 3/8 Gemco PDC drillable float shoe;... 6 2/13/2014\\r8:00 12/13/2014\\r18:30 0.5 PLAN PERS SFTY HJSM with Morning tour crew, Pipe Pro casing c... 7 2/13/2014\\r8:30 12/13/2014\\r23:30 5.0 PLAN DRLG CSG Make up 13 /8\" PDC drillable float collar onto... 8 2/13/2014\\r3:30 12/14/2014\\r01:30 2.0 SURF-CIRC CIRCULATE CIRC HJSM on Hoisting personal; Make up Swedge in ... 9 2/14/2014\\r1:30 12/14/2014\\r03:30 2.0 PLAN DRLG CSG Run 13 3/8\"J-55 54.5 BTC f/ 1,639' to 1,819';... 10 2/14/2014\\r3:30 12/14/2014\\r04:30 1.0 SURF-CIRC CIRCULATE CIRC Circulate Bttms up while Rigging down csg crew... 11 2/14/2014\\r4:30 12/14/2014\\r06:00 1.5 SURF-CMT CEMENT SURFACE\\rCASING CMT HJSM w/ Basic Cementer, H&P rig crew & PNR; D... Notes on Escape Characters When used as lattice mode, tabula replaces abnormally large spacing between texts and newline within a cell with \\r . This can be fixed with a simple regex manipulation. In [41]: clean_df = df . replace ( ' \\r ' , ' ' , regex = True ) clean_df Out[41]: Start Date End Date Dur (hr) Activity Activity Detail Operation Com 0 2/13/2014 6:00 12/13/2014 09:00 3.0 SURF-DRILL DRILL SURFACE DRL Rotate from 1600' to 1859' (259' @ 8 fph). WOB... 1 2/13/2014 9:00 12/13/2014 11:00 2.0 SURF-CIRC CIRCULATE CIRC Pump 2- 50 bbl hi vis sweep; Circulate to surface 2 2/13/2014 1:00 12/13/2014 14:00 3.0 SURF-TRIP TOOH TRIP TOOH (Slick off bottom) f/1859' to 108' (SLM)... 3 2/13/2014 4:00 12/13/2014 16:00 2.0 PLAN EQUIP BHA PJSA - Break bit & L/D directional BHA, clean ... 4 2/13/2014 6:00 12/13/2014 17:30 1.5 PLAN DRLG CSG PTJSA / R/U Pipe Pros.Csg. tools / PTJSA on ru... 5 2/13/2014 7:30 12/13/2014 18:00 0.5 PLAN DRLG CSG Make up 13 3/8 Gemco PDC drillable float shoe;... 6 2/13/2014 8:00 12/13/2014 18:30 0.5 PLAN PERS SFTY HJSM with Morning tour crew, Pipe Pro casing c... 7 2/13/2014 8:30 12/13/2014 23:30 5.0 PLAN DRLG CSG Make up 13 /8\" PDC drillable float collar onto... 8 2/13/2014 3:30 12/14/2014 01:30 2.0 SURF-CIRC CIRCULATE CIRC HJSM on Hoisting personal; Make up Swedge in ... 9 2/14/2014 1:30 12/14/2014 03:30 2.0 PLAN DRLG CSG Run 13 3/8\"J-55 54.5 BTC f/ 1,639' to 1,819';... 10 2/14/2014 3:30 12/14/2014 04:30 1.0 SURF-CIRC CIRCULATE CIRC Circulate Bttms up while Rigging down csg crew... 11 2/14/2014 4:30 12/14/2014 06:00 1.5 SURF-CMT CEMENT SURFACE CASING CMT HJSM w/ Basic Cementer, H&P rig crew & PNR; D... Lattice Mode vs Stream Mode Tabula supports two primary modes of table extraction — Lattice mode and Stream mode. Lattice Mode lattice=True forces PDFs to be extracted using lattice-mode extraction. It recognizes each cells based on ruling lines, or borders of each cell. Stream Mode stream=True forces PDFs to be extracted using stream-mode extraction. This mode is used when there are no ruling lines to differentiate one cell from the other. Instead, it uses spacings among each cells to recognize each cell. PDF File 1 : Lattice mode recommended PDF file 2 : Stream mode recommended How would it look like if PDF File 1 and PDF file 2 are each extracted in both stream mode and lattice mode? In [51]: # PDF File 1: lattice mode file = 'pdf_parsing/lattice-timelog-multiple-pages.pdf' df = tabula . read_pdf ( file , lattice = True , pages = 2 , area = ( 406 , 24 , 695 , 589 )) df . head () Out[51]: Start Date End Date (hr) Activity Activity Detail Operation Com 0 12/13/2014\\r06:00 12/13/2014\\r09:00 3.0 SURF-DRILL DRILL SURFACE DRL Rotate from 1600' to 1859' (259' @ 8 fph). WOB... 1 12/13/2014\\r09:00 12/13/2014\\r11:00 2.0 SURF-CIRC CIRCULATE CIRC Pump 2- 50 bbl hi vis sweep; Circulate to surface 2 12/13/2014\\r11:00 12/13/2014\\r14:00 3.0 SURF-TRIP TOOH TRIP TOOH (Slick off bottom) f/1859' to 108' (SLM)... 3 12/13/2014\\r14:00 12/13/2014\\r16:00 2.0 PLAN EQUIP BHA PJSA - Break bit & L/D directional BHA, clean ... 4 12/13/2014\\r16:00 12/13/2014\\r17:30 1.5 PLAN DRLG CSG PTJSA / R/U Pipe Pros.Csg. tools / PTJSA on ru... In [57]: # PDF File 1: stream mode file = 'pdf_parsing/lattice-timelog-multiple-pages.pdf' df = tabula . read_pdf ( file , stream = True , guess = False , pages = 2 , area = ( 406 , 24 , 695 , 589 )) df . head ( 11 ) Out[57]: Start Date End Date (hr) Activity Activity Detail Operation Com 0 12/13/2014 12/13/2014 3.0 SURF-DRILL DRILL SURFACE DRL Rotate from 1600' to 1859' (259' @ 8 fph). WOB... 1 06:00 09:00 NaN NaN NaN NaN SPP 2300, motor diff 650, 800 GPM, torque 18k. 2 NaN NaN NaN NaN NaN NaN (T.D. Surface @ 09:00 12-13-14) 3 12/13/2014 12/13/2014 2.0 SURF-CIRC CIRCULATE CIRC Pump 2- 50 bbl hi vis sweep; Circulate to surface 4 09:00 11:00 NaN NaN NaN NaN NaN 5 12/13/2014 12/13/2014 3.0 SURF-TRIP TOOH TRIP TOOH (Slick off bottom) f/1859' to 108' (SLM)... 6 11:00 14:00 NaN NaN NaN NaN Hole taking correct fill 7 12/13/2014 12/13/2014 2.0 PLAN EQUIP BHA PJSA - Break bit & L/D directional BHA, clean ... 8 14:00 16:00 NaN NaN NaN NaN ext. 9 12/13/2014 12/13/2014 1.5 PLAN DRLG CSG PTJSA / R/U Pipe Pros.Csg. tools / PTJSA on ru... 10 16:00 17:30 NaN NaN NaN NaN csg. In [62]: # PDF File 2: lattice mode file = 'pdf_parsing/stream-railroad-pages-1-4.pdf' df = tabula . read_pdf ( file , lattice = True , pages = 1 , area = ( 209 , 12.5 , 387.3 , 386 )) df Out[62]: WELL INFORMATION In [64]: # PDF File 2: stream mode file = 'pdf_parsing/stream-railroad-pages-1-4.pdf' df = tabula . read_pdf ( file , stream = True , guess = False , pages = 1 , area = ( 209 , 12.5 , 387.3 , 386 )) df Out[64]: Unnamed: 0 WELL INFORMATION 0 API No.: 42-003-46352 County: A 1 Well No.:22H RRC Distri 2 Lease Name: UNIVERSITY \"7-43\" Field Name 3 RRC Lease No.: 40532 Field No.: 4 Location: Section: 35, Block: 7, Survey: UN... NaN 5 Latitude: Longitude: 6 This well is located 17.2 miles in a SE 7 direction from ANDREWS, NaN 8 which is the nearest town in the county. NaN Observe how lattice mode extraction for PDF file 2 was able to extract only \"WELL INFORMATION\" string. This is not an error. Recall that lattice mode identifies cells by ruling lines. Notes About guess option According to the offical documentation , guess is known to make a conflict between stream option. If you feel something strange with your result, try setting guess=False . For example, for PDF File 1 , if stream mode is used without setting guess=False , it would look like this: In [66]: # PDF File 1: stream mode, guess=True file = 'pdf_parsing/lattice-timelog-multiple-pages.pdf' df = tabula . read_pdf ( file , stream = True , pages = 2 , area = ( 406 , 24 , 695 , 589 )) df . head ( 11 ) Out[66]: Report #:3 Daily Operation:12/13/2014 06:00 - 12/14/2014 06:00 Unnamed: 2 Unnamed: 3 Unnamed: 4 Unnamed: 5 Unnamed: 6 0 Job Category NaN Primary Job Type NaN NaN AFE Number NaN 1 ORIG DRILLING NaN ODR NaN NaN 033402 NaN 2 Days From Spud (days) Days on Location (days) End Depth (ftKB) End Depth (TVD) (ftKB) Dens Last Mud (lb/gal) Rig NaN NaN 3 1 3 1,859.0 1,858.5 8.60 H & P, 637 NaN NaN 4 Operations Summary NaN NaN NaN NaN NaN NaN 5 Drld. Surface f/1600' to 1859' T.D. @ 09:00... NaN NaN NaN NaN NaN NaN 6 S/M, R/U csg running equip & Run 45 jts. of... NaN NaN NaN NaN NaN NaN 7 Surface Float shoe @ 1,857.5' NaN NaN NaN NaN NaN Surface 8 Float collar @ 1,817.9' NaN NaN NaN NaN NaN NaN 9 Remarks NaN NaN NaN NaN NaN NaN 10 Rig (H&P 637), Well (University 7-43 # 22H) NaN NaN NaN NaN NaN NaN Pandas Option Pandas arguments can be passed into tabula.read_pdf() as a dictionary object. In [74]: file = 'pdf_parsing/lattice-timelog-multiple-pages.pdf' df = tabula . read_pdf ( file , lattice = True , pages = 2 , area = ( 406 , 24 , 695 , 589 ), pandas_options = { 'header' : None }) df . head () Out[74]: 0 1 2 3 4 5 6 0 Start Date End Date (hr) Activity Activity Detail Operation Com 1 12/13/2014\\r06:00 12/13/2014\\r09:00 3 SURF-DRILL DRILL SURFACE DRL Rotate from 1600' to 1859' (259' @ 8 fph). WOB... 2 12/13/2014\\r09:00 12/13/2014\\r11:00 2 SURF-CIRC CIRCULATE CIRC Pump 2- 50 bbl hi vis sweep; Circulate to surface 3 12/13/2014\\r11:00 12/13/2014\\r14:00 3 SURF-TRIP TOOH TRIP TOOH (Slick off bottom) f/1859' to 108' (SLM)... 4 12/13/2014\\r14:00 12/13/2014\\r16:00 2 PLAN EQUIP BHA PJSA - Break bit & L/D directional BHA, clean ... More Documentation Further instructions about tabula-py can be found on its official github repo .","tags":"Others","url":"https://aegis4048.github.io/parse-pdf-files-while-retaining-structure-with-tabula-py","loc":"https://aegis4048.github.io/parse-pdf-files-while-retaining-structure-with-tabula-py"},{"title":"Creating a Jupyter Notebook-Powered Data Science Blog with Pelican","text":"What powers this blog, Pythonic Excursions? - Pelican. - Me Pelican is a static site genertor, written in Python. It is a Python library used to auto-generate HTML elements that are used to run websites. Pelican-powered blogs are light and easy to host with no scaling concerns. It pre-generates HTML files and responds with the existing files during a typical HTTP request-response cycle. So, why should you use Pelican? It's GREAT for Blogging You can write your content directly with the editor of your choice in reStructuredText or Markdown formats. Select a favorite theme of your choice from a collection of Pelican-themes , and write articles. The CSS and Javascript contained in the theme will handle the rest and output your article nice and clean. One look is worth a thousand words. Take a look at this markdown file that is used to render this article through Github Pages by Jake VanderPlas . His blog is made with Pelican, after modifying some codes in Octopress theme (you can also write your articles in other formats, such as Jupyter Notebook, which powers this blog). Minimal Learning Curves Going through the official documenations, user-made tutorials, or YouTube videos can be painful. Using Pelican will minimize wasting your time dealing with the learning curves. One thing that makes it very easy to learn & modify is that there already are lots blogs that run on Pelican, and their source codes are open to public. Pythonic Excursions -- source code , Aegis-Jupyter Theme by Me onCrash = 'reboot();' -- source code , Elegant Theme by Talha Mansoor Pythonic Perambulations -- source code , Adapted Octopress Theme by Jake VanderPlas ... and many more If you don't want to learn Pelican from scratch, you can download these open source repos and start from there. You will only need to learn how to tweak some settings to meet your needs. Completely Static Output Is Easy To Host Anywhere The output of Pelican is all HTML. You don't have to worry about configuring a complicated database and optimizing connections. Let's take a look at how Pelican works. Most Pelican blogs have the following directory tree. blog content articles article_1.md article_2.md article_3.md figures images ... output category images figures index.html archives.html article_1.html article_2.html article_3.html plugins themes custom_theme static templates Makefile pelicanconf.py publishconf.py output directory is the folder where Pelican stores the auto-generated HTML files, and those existing files are returned to the user who sent an HTTP request to view your website. The other directories are tools and templates used to generate the HTML files in the output folder. You do not need to configure a SQL database, or execute any codes on the server. All outputs are completely static. Can Pelican Be Used In Dynamic Websites Too? Yes, it can. Although Pelican is a static site generator, that does not mean that you can't have dynamic backend features on your website. You can pre-generate the output HTML files with Pelican, and just wrap it around with the backend framework of your choice. Let's say that you are developing a web-app with Django, and you want part of your website to be a static blog. You have a Pelican-generated output HTML file called article_1.html . In Django, you can render your Pelican-generated HTML file using a basic Class-Based-View like this: views.py from django.views.generic import TemplateView class PelicanView(TemplateView): template_name = 'article_1.html' urls.py from django.urls import re_path from your_awesome_app import views app_name = 'your_awesome_app' urlpatterns = [ re_path('&#94;$', views.PelicanView.as_view(), name='pelican'), ] And that's all it takes to integrate Pelican with Django. Part of your website can be static pages where it doesn't need to execute any code on a server, but the other part of your website can be dynamic pages where you can send queries to your server. Of course, the methodology to combine Pelican with dynamic backend will differ for each backend framework of your choice, but you get the idea. Here is the point: Pelican is a static site generator, but that does not mean that Pelican can't be used in dynamic websites. And Pelican is GREAT for blogging. Introducing Aegis-Jupyter Theme Aegis-Jupyter theme is a custom Pelican theme I made to easily host & maintain a Jupyter Notebook powered data science blog. I borrowed some CSS design of the articles from Jake VanderPlas , and improved the rendering of Jupyter Notebook files by adding custom CSS & JS codes. Every articles you see in archives page is rendered using Jupyter Notebook .ipynb files, even this very article you are reading right now! There are several reason's you why you might wanna consider using Aegis-Jupyter theme. Jupyter-Notebook-Based Articles First and foremost, your articles are be rendered by Jupyter Notebook. The question of \"Why would you want to use Aegis-Jupyter theme?\" is synonymous to \"Why would you want to use Jupyter Notebook?\" - it allows you to create and share documents that contain live code, equations, visualizations and narrative text. You don't have to go through the hassel of writing Python input codes in HTML, save the output visualizations in jpg or png files, and then render it on a browser using an image tag like this: &ltimg src=\"your_awesome_visualization.png\"> No, don't do this. This is bad. You don't want to be keep doing this for every single output of your code block. This is too much work. Simply write codes in Jupyter, save it, and render your article. Aegis-Jupyter theme was built for that purpose. Mobile Device Friendly The theme renders very nicely on all resolutions, screenwidth, and mobile devices. Try viewing this website on your phone. If you are on PC, try stretching & collapsing the browser size and see how it responsively re-aligns itself. Google Analytics Support If you own any kind of website, not just a data science blog, at some point in your life you would be wondering about the behaviors of the viewers. How many people visit my website every week? How many of them are unique visitors? From what region do I get the most number of visitors? On average, how many minutes do people stay on my website? Which post was the most popular? From what social media platform do I get the most number of visitors from? These kinds of questions can be answered by leveraging the power of Google Analytics, FOR FREE . All you need to do is to create a Google Analytics account, get a tracking ID, and put that on publishconf.py file. For example, if your Google Analytics tracking ID is UA-1XXXXXXXX-1 , then you set GOOGLE_ANALYTICS variable liks this: publishconf.py GOOGLE_ANALYTICS = \"UA-1XXXXXXXX-1\" That's it. Aegis-Jupyter theme will take care of the rest. More detailed tutorials on how to create Google Analytics account and tracking ID will come later. Easy to Manage Your Articles Meta properties of your article can easily be managed my changing attributes in markdown files. The below markdown is the actual .md file that renders this article . non-parametric-confidence-interval-with-bootstrap.md Title: Non-Parametric Confidence Interval with Bootstrap Tags: non-parametric, confidence-interval, bootstrap(stats), statistics Date: 2019-01-04 09:00 Slug: non-parametric-confidence-interval-with-bootstrap Subtitle: Keywords: Featured_Image: images/featured_images/bootstrap.png Social_Media_Description: Bootstrapping can calculate uncertainty in any confidence interval of any kind of distribution. It's great because it is distribution-free. Summary: {% notebook downloads/notebooks/Non-ParametricConfidenceIntervalswithBootstrap.ipynb cells[1:2] %} {% notebook downloads/notebooks/Non-ParametricConfidenceIntervalswithBootstrap.ipynb cells[:] %} The below screenshot is the preview of the article on the landing page of this blog. Observe how each attribute in the markdown file is used to render the output preview page. You can declare additional attributes as much as you want. Share Your Posts on Social Media Aegis-Jupyter theme leverages the power of Open Graph Meta Tags and renders the preview of your website nicely when shared on social media. You can set a preview image by declaring Featured_Image and set preview descriptions by declaring Social_Media_Description for each article in its respective markdown files. If you do not specify Featured_Image attribute in the markdown file, a default featured image will show up when shared on social media. Default featured image can be set up in pelicanconf.py file. This is what I have for my blog: pelicanconf.py FEATURED_IMAGE = SITEURL + '/theme/img/logo_icon_background.png' Search Box Most static websites do not support search box functionality out of the box. However, Pelican supports Tipue Search , a jQuery site search plugin. Talha Mansoor made a pelican plugin that allows Pelican to leverage the power of Tipue Search, and Aegis-Jupyter integrated it to work with articles written in Jupyter Notebook. Take a look at it with your own eyes by scrolling up and actually using the search box in this blog! Disqus Comment Box Being able to communicate with the audiences is a quintessential component of a blog. Create an account in Disqus and get your Disqus Website Name here . Then, declare DISQUS_SITENAME variable in publishconf.py . That's all it takes to have a comment box feature for your blog. Aegis-Jupyter will handle the rest. And, of course, IT'S FREE publishconf.py DISQUS_SITENAME = \"pythonic-excursions\" In [ ]:","tags":"Others","url":"https://aegis4048.github.io/creating-a-jupyternotebook-powered-data-science-blog-with-pelican","loc":"https://aegis4048.github.io/creating-a-jupyternotebook-powered-data-science-blog-with-pelican"},{"title":"Non-Parametric Confidence Interval with Bootstrap","text":"The code snippet assumes Anaconda 5.2.0 version of Python virtual environment Acknowledgement I would like to acknowledge Micahel Pyrcz , Associate Professor at the University of Texas at Austin in the Petroleum and Geosystems Engineering, for developing course materials that helped me write this article. Check out his Youtube Lecture on Bootstrap , and Boostrap Excel numerical demo on his Github repo to help yourself better understand the statistical theories and concepts. Bootstrap is a non-parametric statistical technique to resample from known samples to estimate uncertainty in summary statistics. When there are small, limited number of samples, it gives a more accurate forecast model than directly obtaining a forecast model from the limited sample pool (assuming that the sample set of data is reasonable representation of the population). It is non-parametric because it does not require any prior knowledge of the distribution (shape, mean, standard devation, etc..). Advantages of Bootstrap One great thing about Bootstrapping is that it is distribution-free . You do not need to know distribution shape, mean, standard devation, skewness, kurtosis, etc... All you need is just a set of sample data that is representative of a population. The fact that Bootstrapping does not depend on a type of distribution leads to another great advantage - It can calculate uncertainty in any confidence interval of any kind of distribution . For example, the analytical solution to calculate a confidence interval in any statistics of a distribution is as follows: CI of mean = stats of interest $\\pm$ $($distribution score $\\times$ Standard Error $)$ There are three problems with analytically solving for confidence interval of a statistic. First, the variable in the equation, distribution score , depends on the type of the distribution. If you do not know the distribution shape of your population, it is very difficult to calculate the confidence interval of a statistic. Second, not all statistics have a formula to calculate its Standard Error . For example, there exists an equation to calculate the standard error of a mean: Standard Error = $\\sigma_{sample} \\ \\mathbin{/} \\ \\sqrt{N}$ But there is no equation to calculate the standard error of a median. If you want to obtain confidence intervals for other statistics (ex: skewness, kurtosis, IQR, etc...), it will be very difficult to do so, simply because there are no equations for them. Third, some statistics have analytical solutions for its standard error calculation, but it is so convoluted that Bootstrapping is simpler. A classic example is obtaining a CI for the correlation coefficient given a sample from a bivariate normal distribution. Bootstrapping calculates confidence intervals for summary statistics numerically, not analytically , and this is why it can calculate ANY summary stats for ANY distribution. Methodology One goal of inferential statistics is to determine the value of a parameter of an entire population. It is typically too expensive or even impossible to measure this directly. So we use statistical sampling. We sample a population, measure a statistic of this sample, and then use this statistic to say something about the corresponding parameter of the population. Bootstrapping is a type of resampling method to save time and money taking measurements. From a sample pool of size N, it picks a random value N times with replacement , and create M number of new Bootstrapped-sample pools. The term with replacement here means that you put back the sample you drew to the original sample pool after adding it to a new Bootstrapped-sample pool. Think of it this way: you randomly choose a file from a folder in your PC, and you copy and paste the randomly-chosen file into a new folder. You do not cut and paste the file, but you copy and paste the file into a new folder. You will have M number of folders (M is an arbitrary number of your choice), each containing N number of files. Bootstrapping resamples the original sample pool to generate multiple smaller population of the true population. Each Bootstrap simulation is done by selecting a random value from the sample pool. For example, lets assume that you have the following sample pool of integers: Sample Integers = [12, 433, 533, 14, 65, 42, 64] From the sample pool of size N=7, you choose a random value N=7 times, and create a new sample pool of size N=7. In Bootstrap, each newly created sample pool is called a realization . You generate many of these realizations, and use them to calculate uncertainties in summary stats. Realization 1 = [12, 533, 533, 533, 12, 14, 42] Realization 2 = [65, 14, 14, 65, 433, 64, 14] Realization 3 = [433, 64, 533, 14, 14, 64, 12] Realization 4 = [14, 65, 65, 433, 533, 42, 12] Notice the duplicate data in the realizations (Ex: 533, 533, 533). Duplicates in realizations exist because each data in realization is randomly chosen from the original sample pool with replacement . Warning! It is extremly important that the N size for each Bootstrap realization matches the N size of the original sample pool. We use Bootstrap to numerically estimate the confidence interval (CI). It's an alternative tool to analytically solve for CI. Observing how CI is analytically calculated may help one to understand why the value of N is important. Let's take the CI of a mean for example. Recall that the CI of a mean represents how far a sample mean can deviate from the true population mean. In case of a Gaussian, or Gaussian-like distribution (ex: student-t), the equation to analytically solve for confidence interval of a mean is as follows: CI of mean = sample mean $\\pm$ $($z-score $\\times$ Standard Error $)$ Standard Error = $\\sigma_{sample} \\ \\mathbin{/} \\ \\sqrt{N}$ where $N$ is the number of measured samples. If you increase the number of samples, the standard error of a mean decreases. This logically makes sense, because the more samples you have, the more accurate the estimation of the true population mean becomes. The size of each Bootstrap realization, N, works the similar way, except that the random sample in each realization is not from the true population, but from a measured sample pool. Increasing the N-value will falsely make you to calculate smaller confidence interval. It can be observed that the CI obtained by using a wrong N-value for Bootstrap generates narrower CI. As a result, the CI of the sample mean does not cover the true population mean, returning a misleading estimation. In summary, Bootstrapping is used for three reasons: Bootstrap can obtain confidence interval in any statistics. Bootstrap does not assume anything about a distribution. Bootstrap helps when there are too few number of samples. Imports In [2]: import pandas as pd import numpy as np import scipy.stats import matplotlib.pyplot as plt import matplotlib.gridspec as gridspec % matplotlib notebook 1.A. Confidence Intervals in Summary Stats: US Male Height - Gaussian Distribution Bootstrap simulation can be run to obtain confidence intervals in various population parameters: mean, stdev, variance, min, or max. In this example, we will work with the height distribution of the US Male population, which tends to be Gaussian. However, the fact that the distribution Gaussian is totally unrelated to Bootstrap simulation, because it does not assume anything about the distribution. Bootstrapping can give us confidence intervals in any summary statistics like the following: By 95% chance, the following statistics will fall within the range of: Mean : 75.2 ~ 86.2, with 80.0 being the average Standard Deviation : 2.3 ~ 3.4 with 2.9 being the average Min : 54.3 ~ 57.2, with 55.2 being the average Max : 77.8 ~ 82.4, with 79.8 being the average Skew : -0.053 ~ 0.323, with 0.023 being the average 1.A.0. Bootstrap Scripts Bootstrap Simulator In [3]: def bootstrap_simulation ( sample_data , num_realizations ): n = sample_data . shape [ 0 ] boot = [] for i in range ( num_realizations ): real = np . random . choice ( sample_data . values . flatten (), size = n ) boot . append ( real ) columns = [ 'Real ' + str ( i + 1 ) for i in range ( num_realizations )] return pd . DataFrame ( boot , index = columns ) . T Summary Statistics Calculator In [4]: def calc_sum_stats ( boot_df ): sum_stats = boot_df . describe () . T [[ 'mean' , 'std' , 'min' , 'max' ]] sum_stats [ 'median' ] = boot_df . median () sum_stats [ 'skew' ] = boot_df . skew () sum_stats [ 'kurtosis' ] = boot_df . kurtosis () sum_stats [ 'IQR' ] = boot_df . quantile ( 0.75 ) - boot_df . quantile ( 0.25 ) return sum_stats . T Visualization Script In [5]: def visualize_distribution ( dataframe , ax_ ): dataframe = dataframe . apply ( lambda x : x . sort_values () . values ) for col , label in zip ( dataframe , dataframe . columns ): fit = scipy . stats . norm . pdf ( dataframe [ col ], np . mean ( dataframe [ col ]), np . std ( dataframe [ col ])) ax_ . plot ( dataframe [ col ], fit ) ax_ . set_ylabel ( 'Probability' ) Generate Confidence Intervals In [6]: def calc_bounds ( conf_level ): assert ( conf_level < 1 ), \"Confidence level must be smaller than 1\" margin = ( 1 - conf_level ) / 2 upper = conf_level + margin lower = margin return margin , upper , lower def calc_confidence_interval ( df_sum_stats , conf_level ): margin , upper , lower = calc_bounds ( conf_level ) conf_int_df = df_sum_stats . T . describe ( percentiles = [ lower , 0.5 , upper ]) . iloc [ 4 : 7 , :] . T conf_int_df . columns = [ 'P' + str ( round ( lower * 100 , 1 )), 'P50' , 'P' + str ( round ( upper * 100 , 1 ))] return conf_int_df def print_confidence_interval ( conf_df , conf_level ): print ( 'By {}% c hance, the following statistics will fall within the range of: \\n ' . format ( round ( conf_level * 100 , 1 ))) margin , upper , lower = calc_bounds ( conf_level ) upper_str = 'P' + str ( round ( upper * 100 , 1 )) lower_str = 'P' + str ( round ( lower * 100 , 1 )) for stat in conf_df . T . columns : lower_bound = round ( conf_df [ lower_str ] . T [ stat ], 1 ) upper_bound = round ( conf_df [ upper_str ] . T [ stat ], 1 ) mean = round ( conf_df [ 'P50' ] . T [ stat ], 1 ) print ( \" {0:<10} : {1:>10} ~ {2:>10} , AVG = {3:>5} \" . format ( stat , lower_bound , upper_bound , mean )) 1.A.1 Sample Data Description 100 samples of US male height data is provided in my Github Repo - sample_data/US_Male_Height.csv . Summary statistics of the sample data can be calculated. Your goal is to calculate the confidence intervals for the summary stats. In [7]: # height data height_data = pd . read_csv ( 'sample_data/US_Male_Height.csv' ) height_data . index = [ 'Male ' + str ( i + 1 ) for i in range ( height_data . shape [ 0 ])] height_data . round ( 1 ) . T Out[7]: Male 1 Male 2 Male 3 Male 4 Male 5 Male 6 Male 7 Male 8 Male 9 Male 10 ... Male 91 Male 92 Male 93 Male 94 Male 95 Male 96 Male 97 Male 98 Male 99 Male 100 Height (in) 70.8 72.8 72.5 67.3 72.7 73.6 65.0 67.1 70.8 70.6 ... 71.7 66.4 72.9 74.5 73.5 70.5 73.1 63.6 68.7 73.0 1 rows × 100 columns In [8]: height_summary_stats = calc_sum_stats ( height_data ) height_summary_stats Out[8]: Height (in) mean 69.881971 std 3.169548 min 63.143732 max 77.762886 median 69.894434 skew -0.059779 kurtosis -0.700743 IQR 5.154145 Visualization In [9]: fig , ax = plt . subplots ( figsize = ( 8 , 4 )) ax . set_xlabel ( 'Height (inches)' ); fig . suptitle ( 'Original Sample Data Distribution: Gaussian Distribution' ) visualize_distribution ( height_data , ax ); Based on the distribution plot of the original sample data, we can observe that the distribution indeed looks Gaussian. However, the fact that it looks like Gaussian does not matter at all when Bootstrapping, because Bootstrapping does not assume anything about the distribution. 1.A.2 Resampling From the Sample Data Each Bootstrap resampling (realization) can be done in one-line with numpy.random.choice() . Each realization is an array of size N, where N is the length of the original sample data. There are M number of realizations, where M is an arbitrary number of your choice. Results In [10]: M = 100 # number of realizations - arbitrary bootstrap_data = bootstrap_simulation ( height_data , M ) bootstrap_data . round ( 1 ) . head ( 10 ) Out[10]: Real 1 Real 2 Real 3 Real 4 Real 5 Real 6 Real 7 Real 8 Real 9 Real 10 ... Real 91 Real 92 Real 93 Real 94 Real 95 Real 96 Real 97 Real 98 Real 99 Real 100 0 72.7 67.9 65.0 69.2 70.4 64.9 70.3 66.3 67.6 72.2 ... 74.1 66.3 73.0 68.5 65.3 72.5 72.7 69.2 66.4 72.3 1 68.4 70.5 65.3 64.5 71.8 69.2 70.5 71.6 65.3 67.9 ... 72.5 68.6 66.1 71.7 67.3 74.1 67.9 71.3 72.9 65.0 2 71.3 74.1 72.8 72.9 68.3 67.9 73.1 65.0 73.6 72.8 ... 69.5 72.5 72.5 73.3 69.2 74.1 73.0 65.5 67.9 63.1 3 72.5 73.0 71.4 68.5 73.3 70.5 70.5 70.6 68.5 69.0 ... 64.5 69.2 66.0 69.5 72.5 70.3 67.9 68.3 73.6 73.5 4 67.2 73.5 73.6 67.2 64.5 72.9 72.8 66.4 69.2 66.8 ... 66.3 73.6 71.3 73.1 71.6 72.2 64.9 69.0 71.7 70.2 5 69.1 66.0 65.5 69.1 71.7 70.6 66.0 73.0 72.2 69.9 ... 73.0 66.3 69.0 67.9 69.4 69.9 69.5 68.7 72.4 67.3 6 72.2 68.5 72.9 63.1 73.6 73.1 70.8 75.5 69.9 70.6 ... 68.4 65.0 68.5 68.8 67.2 72.2 65.5 70.6 72.2 66.8 7 73.1 72.5 69.0 72.5 71.6 68.7 73.5 66.2 71.6 74.4 ... 73.6 68.5 72.2 73.1 72.3 72.1 66.8 77.8 72.8 69.5 8 67.3 69.5 74.5 66.8 69.1 65.0 69.9 70.6 65.0 73.1 ... 67.1 72.8 70.5 70.8 73.6 72.5 71.8 67.9 67.2 70.6 9 66.3 72.9 65.5 72.5 72.4 70.6 73.1 69.5 67.2 68.7 ... 71.6 69.2 65.0 71.3 71.4 69.1 73.3 70.2 66.1 67.6 10 rows × 100 columns In [11]: boot_sum_stats = calc_sum_stats ( bootstrap_data ) boot_sum_stats . round ( 1 ) Out[11]: Real 1 Real 2 Real 3 Real 4 Real 5 Real 6 Real 7 Real 8 Real 9 Real 10 ... Real 91 Real 92 Real 93 Real 94 Real 95 Real 96 Real 97 Real 98 Real 99 Real 100 mean 69.4 70.0 69.9 70.1 70.0 69.5 70.2 70.2 69.7 70.5 ... 69.7 69.9 69.7 69.9 70.1 70.0 70.0 69.5 70.1 69.4 std 3.1 3.1 3.2 3.3 3.3 3.1 3.1 3.2 2.9 3.0 ... 3.1 3.3 2.8 3.0 3.2 2.8 3.2 3.2 3.2 3.3 min 63.1 63.6 64.1 63.1 63.1 63.1 63.1 63.1 63.6 63.1 ... 63.1 63.1 63.1 63.6 63.6 64.5 64.1 63.1 63.1 63.1 max 75.5 77.8 76.1 77.8 76.1 76.1 76.1 77.8 77.8 77.8 ... 75.5 77.8 77.8 75.5 77.8 76.1 77.8 77.8 77.8 76.1 median 69.1 70.3 70.5 70.4 70.9 69.2 70.5 70.6 69.5 70.7 ... 69.9 69.5 69.5 69.9 69.9 69.9 70.4 69.2 70.6 69.1 skew -0.1 -0.0 -0.2 -0.0 -0.4 -0.0 -0.3 -0.3 0.2 -0.2 ... -0.4 -0.0 -0.0 -0.3 -0.1 0.1 0.2 -0.1 -0.2 -0.0 kurtosis -1.1 -0.8 -1.1 -0.4 -0.9 -0.7 -0.9 -0.6 -0.4 -0.5 ... -0.9 -0.7 -0.3 -0.9 -0.8 -0.9 -0.6 -0.7 -0.8 -1.0 IQR 5.4 5.2 5.9 4.2 5.2 4.9 5.5 5.0 4.9 4.2 ... 5.2 5.4 4.9 4.7 4.8 4.4 5.1 4.8 5.3 5.7 8 rows × 100 columns Visualize In [13]: fig , ax = plt . subplots ( figsize = ( 8 , 4 )) ax . set_xlabel ( 'Height (inches)' ); fig . suptitle ( 'Distribution of Bootstrap-Simulated Data: Gaussian' ) visualize_distribution ( bootstrap_data , ax ); Each line in the plot represents one Bootstrap realization. There are 100 realizations, each having 100 random samples. 1.A.3 Uncertainty Models in Summary Statistics with Blox Plots In [16]: f = plt . figure () plt . suptitle ( 'Uncertainty Models for Various Statistics: US Male Height - Gaussian' ) gs = gridspec . GridSpec ( 2 , 4 ) ax1 = plt . subplot ( gs [ 0 , 0 : 4 ]) ax2 = plt . subplot ( gs [ 1 , 0 ]) ax3 = plt . subplot ( gs [ 1 , 1 ]) ax4 = plt . subplot ( gs [ 1 , 2 ]) ax5 = plt . subplot ( gs [ 1 , 3 ]) boot_sum_stats . T [[ 'mean' , 'min' , 'max' , 'median' ]] . boxplot ( ax = ax1 ) boot_sum_stats . T [[ 'std' ]] . boxplot ( ax = ax2 ) boot_sum_stats . T [[ 'IQR' ]] . boxplot ( ax = ax3 ) boot_sum_stats . T [[ 'skew' ]] . boxplot ( ax = ax4 ) boot_sum_stats . T [[ 'kurtosis' ]] . boxplot ( ax = ax5 ) ax5 . set_ylim ([ - 3 , 3 ]); 1.A.4 Confidence Interval in Summary Statistics Confidence intervals of summary statistics usually have a confidence level of 90%, 95%, or 99%. In this case, we will choose 95% confidence level . In [17]: confidence_level = 0.95 conf_int = calc_confidence_interval ( boot_sum_stats , confidence_level ) conf_int . round ( 1 ) Out[17]: P2.5 P50 P97.5 mean 69.4 69.9 70.5 std 2.8 3.2 3.4 min 63.1 63.1 64.5 max 75.4 77.8 77.8 median 69.1 70.0 71.1 skew -0.5 -0.1 0.3 kurtosis -1.1 -0.7 -0.1 IQR 4.0 4.9 5.8 In [18]: print_confidence_interval ( conf_int , confidence_level ) By 95.0% chance, the following statistics will fall within the range of: mean : 69.4 ~ 70.5 , AVG = 69.9 std : 2.8 ~ 3.4 , AVG = 3.2 min : 63.1 ~ 64.5 , AVG = 63.1 max : 75.4 ~ 77.8 , AVG = 77.8 median : 69.1 ~ 71.1 , AVG = 70.0 skew : -0.5 ~ 0.3 , AVG = -0.1 kurtosis : -1.1 ~ -0.1 , AVG = -0.7 IQR : 4.0 ~ 5.8 , AVG = 4.9 1.B. Confidence Intervals in Summary Stats: Rock Permeability - Lognormal Distribution It was previously stated that Bootstrapping does not assume anything about the distribution. Is that really true? The previous example of the US Male Height distribution was a Gaussian distribution. But what if the distribution of our interest is not Gaussian? In this example, rock pearmeability, which has a lognormal distribution , will be used to show that Bootstrap does not depend on the type of the distribution. 1.B.0. Bootstrap Scripts The sample scripts used for US Male Height example will be used for Bootstrap simulation. Same scripts can be used for both Gaussian and lognormal distribution because Bootstrapping does not assume anything about the distribution. 1.B.1. Sample Data Description 105 samples of permeability data is provided in Github Repo - sample_data/PoroPermSampleData.xlsx . Permeability data is taken at many times at different depth of a wellbore. Summary statistics of the sample data can be calculated. Your goal is to calculate the confidence intervals for the summary stats. In [19]: # permeability data perm_depth_data = pd . read_excel ( 'sample_data/PoroPermSampleData.xlsx' , sheet_name = 'Sheet1' )[[ 'Depth' , 'Permeability (mD)' ]] perm_data = perm_depth_data [ 'Permeability (mD)' ] . to_frame () # visualize fig = plt . figure () ax = plt . axes () ax . plot ( perm_depth_data [ 'Permeability (mD)' ], perm_depth_data [ 'Depth' ]); ax . invert_yaxis () ax . set_title ( 'Permeability Along A Wellbore' ) ax . set_xlabel ( 'Permeability (mD)' ) ax . set_ylabel ( 'Depth (ft)' ); In [20]: perm_summary_stats = calc_sum_stats ( perm_data ) perm_summary_stats Out[20]: Permeability (mD) mean 161.008972 std 80.900128 min 43.534147 max 573.461883 median 144.329837 skew 1.625086 kurtosis 5.498080 IQR 102.580432 Visualization In [21]: fig , ax = plt . subplots ( figsize = ( 8 , 4 )) ax . set_xlabel ( 'Permeability (mD)' ) fig . suptitle ( 'Original Sample Data Distribution: Lognormal Distribution' ) visualize_distribution ( perm_data , ax ); Based on the distribution of the original sample data, we can observe that the distribution looks lognormal. The uncertainty in summary statistics can be calculated using Bootstrap the same way it was done for the US Male Height (Gaussian) distribution, because Bootstrap does not depend on the shape of the distribution. Warning! Outlier removal on rock permeability cannot be done directly, as this is a lognormal distribution. Recall that the typical outlier removal method assumes the distribution to be Gaussian. If you want to detect outliers for non-Gaussian distributions, you have to first transform the distribution into Gaussian. 1.B.2 Resampling From the Sample Data Each Bootstrap resampling (realization) can be done in one-line with numpy.random.choice() . Each realization is an array of size N, where N is the length of the original sample data. There are M number of realizations, where M is an arbitrary number of your choice. Results In [22]: M = 100 # number of realizations - arbitrary boot_perm_data = bootstrap_simulation ( perm_data , M ) boot_perm_data . round ( 1 ) . head ( 10 ) Out[22]: Real 1 Real 2 Real 3 Real 4 Real 5 Real 6 Real 7 Real 8 Real 9 Real 10 ... Real 91 Real 92 Real 93 Real 94 Real 95 Real 96 Real 97 Real 98 Real 99 Real 100 0 61.9 258.6 138.8 61.9 285.3 156.1 179.7 125.9 58.8 89.6 ... 151.0 227.4 59.1 573.5 258.6 56.3 240.6 89.6 132.7 182.6 1 170.5 61.0 143.7 214.1 264.2 244.1 144.7 160.5 83.9 258.6 ... 58.8 279.8 244.1 92.1 213.7 160.5 240.6 146.0 141.8 138.8 2 143.7 86.8 117.6 92.7 83.9 104.0 187.9 138.8 162.3 132.1 ... 258.4 380.1 89.6 89.6 123.3 77.5 102.7 193.1 133.2 234.5 3 166.9 151.0 240.6 265.5 183.1 65.6 59.1 305.1 103.5 131.6 ... 214.9 128.9 210.8 108.6 193.1 125.9 77.5 151.5 112.3 58.8 4 161.0 146.0 89.6 84.9 129.1 43.5 170.5 97.7 190.9 197.8 ... 56.3 85.0 53.4 79.4 58.8 92.7 102.7 190.9 126.3 161.0 5 104.0 132.1 129.1 144.4 184.8 263.5 151.0 170.5 162.9 311.6 ... 61.0 156.1 170.5 264.2 244.1 85.0 112.3 117.6 224.4 265.5 6 305.1 77.5 213.7 84.9 240.6 58.8 224.4 234.5 128.9 193.1 ... 66.9 138.8 240.6 66.9 166.9 84.7 305.1 80.4 53.4 264.2 7 286.8 171.4 92.1 84.9 116.9 245.7 141.8 135.8 206.6 116.9 ... 162.3 244.1 187.9 151.0 84.9 85.0 573.5 170.5 83.3 117.6 8 142.7 187.9 131.6 117.6 244.1 214.1 182.6 134.7 132.7 132.7 ... 123.3 104.0 65.6 86.8 84.9 193.1 56.3 136.9 156.1 311.6 9 58.8 132.1 380.1 136.9 65.6 244.1 134.7 77.8 321.1 79.4 ... 128.9 83.9 182.6 132.0 117.6 234.5 227.4 187.9 80.4 138.8 10 rows × 100 columns In [23]: boot_perm_sum_stats = calc_sum_stats ( boot_perm_data ) boot_perm_sum_stats . round ( 1 ) Out[23]: Real 1 Real 2 Real 3 Real 4 Real 5 Real 6 Real 7 Real 8 Real 9 Real 10 ... Real 91 Real 92 Real 93 Real 94 Real 95 Real 96 Real 97 Real 98 Real 99 Real 100 mean 159.5 152.6 146.9 155.7 160.3 161.7 161.5 164.4 146.6 162.7 ... 143.9 150.0 166.7 155.6 155.2 151.5 164.7 171.3 143.8 148.1 std 80.0 64.1 72.6 77.2 72.1 98.0 88.0 90.7 65.3 73.3 ... 78.4 68.0 64.7 78.0 68.8 74.8 106.0 96.3 63.1 62.0 min 56.3 43.5 43.5 43.5 53.4 43.5 43.5 43.5 43.5 43.5 ... 43.5 43.5 53.4 43.5 43.5 53.4 53.4 43.5 43.5 43.5 max 573.5 305.1 380.1 573.5 380.1 573.5 573.5 573.5 321.1 321.1 ... 573.5 380.1 380.1 573.5 380.1 321.1 573.5 573.5 380.1 380.1 median 143.7 138.8 132.1 144.4 138.8 143.7 142.7 138.8 136.9 145.6 ... 132.0 133.2 151.0 141.8 138.8 133.2 133.2 144.7 132.7 138.8 skew 1.8 0.6 1.1 1.7 0.7 1.6 2.2 2.0 0.6 0.4 ... 2.1 0.8 0.6 1.8 0.7 0.6 1.9 2.2 0.9 0.8 kurtosis 6.1 -0.5 1.0 6.9 -0.3 4.3 7.7 6.4 -0.2 -0.9 ... 8.1 0.5 0.3 6.7 0.2 -0.7 4.6 7.0 1.0 1.1 IQR 79.3 80.8 92.7 113.0 102.4 129.2 102.4 86.7 95.2 114.0 ... 77.3 98.2 78.7 102.6 86.7 112.7 132.3 79.2 82.1 80.8 8 rows × 100 columns Visualize In [29]: fig , ax = plt . subplots ( figsize = ( 8 , 4 )) fig . suptitle ( 'Distribution of Bootstrap-Simulated Data: Lognormal' ) ax . set_xlabel ( 'Permeability (mD)' ) visualize_distribution ( boot_perm_data , ax ); 1.B.3 Uncertainty Models in Summary Statistics with Blox Plots In [25]: f = plt . figure () plt . suptitle ( 'Uncertainty Models for Various Statistics: Rock Permeability - Lognormal' ) gs = gridspec . GridSpec ( 2 , 4 ) ax1 = plt . subplot ( gs [ 0 , 0 : 4 ]) ax2 = plt . subplot ( gs [ 1 , 0 ]) ax3 = plt . subplot ( gs [ 1 , 1 ]) ax4 = plt . subplot ( gs [ 1 , 2 ]) ax5 = plt . subplot ( gs [ 1 , 3 ]) boot_perm_sum_stats . T [[ 'mean' , 'min' , 'max' , 'median' ]] . boxplot ( ax = ax1 ) boot_perm_sum_stats . T [[ 'std' ]] . boxplot ( ax = ax2 ) boot_perm_sum_stats . T [[ 'IQR' ]] . boxplot ( ax = ax3 ) boot_perm_sum_stats . T [[ 'skew' ]] . boxplot ( ax = ax4 ) boot_perm_sum_stats . T [[ 'kurtosis' ]] . boxplot ( ax = ax5 ) ax4 . set_ylim ([ - 3 , 3 ]) ax5 . set_ylim ([ - 10 , 10 ]); Observe the positive skewness in the boxplot summary statistics. This is consistent with the left-justified lognormal distribution of the permeability plot. 1.B.4 Confidence Interval in Summary Statistics Confidence intervals of summary statistics usually have a confidence level of 90%, 95%, or 99%. In this case, we will choose 90% confidence level . In [26]: confidence_level = 0.9 conf_int_perm = calc_confidence_interval ( boot_perm_sum_stats , confidence_level ) conf_int_perm . round ( 1 ) Out[26]: P5.0 P50 P95.0 mean 146.1 160.0 171.3 std 64.4 77.4 96.4 min 43.5 43.5 54.3 max 321.1 573.5 573.5 median 132.7 144.3 156.1 skew 0.4 1.4 2.2 kurtosis -0.6 4.0 8.6 IQR 78.4 98.1 120.9 In [27]: print_confidence_interval ( conf_int_perm , confidence_level ) By 90.0% chance, the following statistics will fall within the range of: mean : 146.1 ~ 171.3 , AVG = 160.0 std : 64.4 ~ 96.4 , AVG = 77.4 min : 43.5 ~ 54.3 , AVG = 43.5 max : 321.1 ~ 573.5 , AVG = 573.5 median : 132.7 ~ 156.1 , AVG = 144.3 skew : 0.4 ~ 2.2 , AVG = 1.4 kurtosis : -0.6 ~ 8.6 , AVG = 4.0 IQR : 78.4 ~ 120.9 , AVG = 98.1","tags":"Statistics","url":"https://aegis4048.github.io/non-parametric-confidence-interval-with-bootstrap","loc":"https://aegis4048.github.io/non-parametric-confidence-interval-with-bootstrap"},{"title":"Uncertainty Modeling with Monte-Carlo Simulation","text":"The code snippet assumes Anaconda 5.2.0 version of Python virtual environment Acknowledgement I would like to acknowledge Micahel Pyrcz , Associate Professor at the University of Texas at Austin in the Petroleum and Geosystems Engineering, for developing course materials that helped me write this article. Check out his Youtube Lecture on Monte-Carlo Simulation to help yourself better understand the statistical theories and concepts. Monte Carlo Simulation is a random sampling method to model uncertainty of a population estimation. When given only population parameters (mean, standard deviation, degrees of freedom, etc..), but not the sample data itself, it generates random samples based on the distribution parameters to create a sample pool that is representative of the true population. Uncertainty models can be created from the newly generated sample pool. Based on historical data, expertise in the field, or past experience, you might know the typical values of population mean, standard deviation and degrees of freedom. While these parameters are useful for developing a model, they do not tell you the uncertainties in a population. In a financial market, you might know the distribution of possible values through the mean and standard deviation of returns. By using a range of possible values, instead of a single guess, you can create a more realistic picture of what might happen in the future. Let's assume that your consultant recommended you a certain investment program that has a mean return rate of 10% and a standard deviation of 1% However, You do not have an access to the actual sample data that is used to obtain the mean, and standard deviation . You made 100 investments through this program, but your 100 investments had an average rate of return of 3%. Did the consultant lie to you, or is it one of the possible corner cases that you can have if you are unlucky? What is the P10, P50, P90 value of this investment program? What are the the most plausible range of rate of return? Does your 3% rate of return fall within that range ? In order to answer these questions, you need sample data that is representative of the population. Monte-Carlo simulation takes population parameters as arguments, and generates series of random samples to investigate a range of possible outcomes . Methodology Monte-Carlo simulation is one of the random sampling method that generates a new set of random samples from statistic parameters of a population. It assumes a certain distribution shape, and population parameters as input and returns a random sample based on the distribution shape and parameters. The most simple examples are as follows: Excel Gaussian: NORM.INV(RAND(), mean, stdev) Lognormal: LOGNORM.INV(RAND(), mean, stdev) Chi-Square: CHISQ.INV(RAND(), degree_freedom) F-distribution: F_INV(RAND(), degree_freedom_numerator, degree_freedom_denominator) Python Gaussian: np.random.normal(mean, stdev) Lognormal: np.random.lognormal(mean, stdev) Chi-Square: np.random.chisquare(degree_freedom) F-distribution: np.random.f(degree_freedom_numerator, degree_freedom_denominator) These examples are the most simple cases of generating random samples vis Monte-Carlo simulation. Random samples can be generated as many times as desired. Based on the N-number of random samples generated, you can draw a CDF or boxplot to model uncertainty in prediction. Warning! In order to use Monte-Carlo simulation, you must know the distribution shape (normal, lognormal, chi-square, etc..) and distribution parameters (mean, standard deviation, degrees of freedom, etc..) of the data. If you do not have enough samples to draw an uncertainty model, or do not know the distribution shape and parameters, Bootstrap simulation may address your issue. Random samples of interest can can be created via an applied form of Monte-Carlo simulation. For example, the Casino Dice Roll Example simulates a game 1,000 times for a single player, and calculates a player's final fund at the end. The final fund of a single player is one random Monte-Carlo sample. The process is repeated 100 times to account for 100 players' final fund, and now we have 100 random Monte-Carlo samples. Total Thickness of Two Formations Example generates two sets of Monte-Carlo formation samples N times (where N is arbitrary number of your choice) to account for Formation A, and Formation B. The two sets of Monte-Carlo formation data are then added together to obtain Monte-Carlo data for total thickness. 1. Casino Dice Roll Example How do casinos earn money? The answer is simple - the longer you play, the bigger the chance of you losing money. Let's assume an imaginary dice roll game between a casino house and a player. The rules are simple. Dice Roll Game Rules There is an imaginary dice that rolls between 1 to 100. If a player rolls between 1 to 51, the house wins. If a player rolls between 52 to 100, the player wins. A player can bet as many times as he wants. With the above rules, the house has 2% higher chance of winning over a player . As a financial analyst of the house, upper management wants you to create a Dice Roll game profit forecast model. Question : If a certain game is configured so that the house has 2% higher chance of winning over a player , what is the expected profit forecast model for the game? Monte-Carlo simulation can be used to simulate the possible outcomes of dice roll game, and generate a forecast model. 1.0 Game Simulator Scripts Imports In [2]: import random import scipy import matplotlib.pyplot as plt import pandas as pd import numpy as np % matplotlib notebook Dice Roll Simulation In [3]: def rolldice (): dice = random . randint ( 1 , 100 ) if dice <= 51 : # Player loses return False elif dice > 51 & dice <= 100 : # Player wins return True Single Game Simulation In [4]: def play ( total_funds , wager_amount , total_plays , final_fund ): play_num = [] # x-axis of the plot funds = [] # y-axis of the plot play = 1 while play <= total_plays : if rolldice (): # Player wins total_funds = total_funds + wager_amount # updates current total funds play_num . append ( play ) funds . append ( total_funds ) else : # Player loses total_funds = total_funds - wager_amount play_num . append ( play ) funds . append ( total_funds ) play = play + 1 final_fund . append ( funds [ - 1 ]) # final_fund contains the ending fund of all players return final_fund , play_num , funds Results Visualization In [5]: def simulate_visualize ( init_money , bet , num_bet , num_players = 1 ): # simulates and generates a plot f , ax = plt . subplots () count = 1 ending_fund_all_players = [] while count <= num_players : ending_fund_all_players , num_play , funds_record = play ( init_money , bet , num_bet , ending_fund_all_players ) ax . plot ( num_play , funds_record ) count += 1 ax . set_title ( str ( num_players ) + ' Player(s): ' + 'Change in Total Fund with Each Game' ) ax . set_ylabel ( 'Player \\' s Fund ($)' ) ax . set_xlabel ( 'Number of Bets' ) return ending_fund_all_players In [6]: def simulate ( init_money , bet , num_bet , num_players = 1 ): # simulates, but does not plot count = 1 ending_fund_all_players = [] while count <= num_players : ending_fund_all_players , num_play , funds_record = play ( init_money , bet , num_bet , ending_fund_all_players ) count += 1 return ending_fund_all_players 1.1 Monte-Carlo Simulation: 1 Player Let's say than an imaginary player, 'Eric', visits the house and wants to play the Dice Roll Game. A Monte-Carlo simulation can be run to simulate the result of Eric's game. The simulation will be run with the following conditions: Eric starts with \\$10,000 Eric bets \\$100 each time Eric plays the game 1,000 times In [148]: simulate_visualize ( init_money = 10000 , bet = 100 , num_bet = 1000 , num_players = 1 ) plt . axhline ( 10000 , color = \"red\" , linewidth = 3 ) plt . text ( 780 , 10200 , 'Starting Money $10,000' , color = 'red' ); Eric started with 10,000 dollars. To your surprise, Eric actually ended up earning money from the house by 2,500 dollars after 1,000 games . According to the configuration of the game, the house has 2% higher chance of winning over Eric. Therefore, with such a high number of games, like a thousand, the house was supposed to earn money from the player. But it was not the case here. Was the configuration of the game wrong, or was Eric just really lucky? 1.1 Monte-Carlo Simulation: 100 Players Eric earned $2,500 dollars after running 1,000 games. However, if hundred other players play the Dice Roll game for thousand times each, would the result be different? From the house's perspective, what is the expected profit from the Dice Roll game? To get more accurate estimation of the expected profit, multiple Monte-Carlo simulation will be run. In this case, hundred. The simulation will be run with the following conditions: Hundred players each start with \\$10,000 Hundred players bet \\$100 each time Hundred players play the game 1,000 times In [7]: simulate_visualize ( init_money = 10000 , bet = 100 , num_bet = 1000 , num_players = 100 ) plt . axhline ( 10000 , color = \"white\" , linewidth = 3 ) plt . text ( 100 , 10400 , 'Starting Money $10,000' , color = 'white' , weight = 'bold' ); As it can be shown on the plots, Eric's earning 2,500 dollars after 1,000 games was a plausible outcome. There was even a player who eanred ended up with 16,500 dollars, which means that he earned 6,500 dollars ! However, this does not mean that the house will earn negative profit. The plot clearly indicates overall trend in the house earning money over the players as the number of bets increases. 1.3 Uncertainty Modeling The previous simulation results represent the outcome of 100 players each playing 1,000 games . One hundred Monte-Carlo simulations were run, and now we have one hundred samples of 1,000 game simulations data. To obtain more accurate uncertainty model for the Dice Roll game, further simulations will be run for 1,000 players each playing 100, 1,000, 10,000, and 100,000 games . In [6]: df = pd . DataFrame () for num_games in [ 100 , 1000 , 5000 , 10000 ]: result = simulate ( init_money = 10000 , bet = 100 , num_bet = num_games , num_players = 1000 ) col_name = str ( num_games ) + ' Games ($)' df [ col_name ] = result In [7]: df . index . name = 'Player Number' df . head ( 10 ) Out[7]: 100 Games ($) 1000 Games ($) 5000 Games ($) 10000 Games ($) Player Number 0 8400 7000 5800 -15600 1 8600 8000 16000 -34800 2 9600 7600 -400 5000 3 9400 10400 -6600 -11200 4 9400 10600 -400 0 5 8600 7200 -200 -19600 6 10800 7800 5600 -14000 7 9800 12400 -4000 -6200 8 10600 7600 24600 -9400 9 7400 7400 1800 -19200 In [10]: ax = df . boxplot ( grid = False ) ax . set_title ( 'Uncertainty Model for Dice Roll Game Profit: 1000 Players' ) ax . set_ylabel ( 'Player \\' s Fund ($)' ) ax . axhline ( 10000 , color = \"red\" , linewidth = 3 ); ax . text ( 3.5 , 11500 , 'Starting Money $10,000' , color = 'red' ); The generated box plot is the forecast model for the Dice Roll game profit generation. It tells you the most likely range of profit expected for N number of games played for each player. Based on the box plot uncertainty model, you can confirm that the longer you play, the bigger chance of you losing money. Although some lucky players may double, or even triple their money at the casino, far bigger population of the players will end up losing money to the casino. Recall that the Dice Roll game was configured so that the Casino has 2% higher chance of winning the game over a player. Summary: A player starts with 10,000 dollars and bets 100 dollar for each game. If a player plays 100 games, he will most likely end up between 12,500 to 6800 dollars If a player plays 1000 games, he will most likely end up between 15,800 to $-$360 dollars If a player plays 5,000 games, he will most likely end up between 19,200 to $-$18,900 dollars If a player plays 10,000 games, he will most likely end up between 15,200 to $-$36,000 dollars 1.4 Outlier Removal and Mean of the Prediction The uncertainty model generated by Monte-Carlo simulations gives you a range of possible outcome. But what if you want a single value of the outcome? One simple way to address this question is to just calculate the average of the simulated data. Means of simulated data BEFORE outlier removal In [192]: raw_mean = pd . DataFrame ( df . describe () . T [ 'mean' ]) . T raw_mean . rename ( index = { 'mean' : 'original mean' }, inplace = True ) raw_mean Out[192]: 100 Games ($) 1000 Games ($) 5000 Games ($) 10000 Games ($) original mean 9812.4 7930.8 214.0 -9865.2 But as it can be observed in the boxplot, the simulated data contains outliers (circled points). One might want to remove these outliers before calculating the average of the data to improve accuracy. The traditional IQR outlier detection method can be implemented. IQR = P75 - P25 Lower Fence = P25 - 1.5 $\\times$ IQR Upper Fence = P75 + 1.5 $\\times$ IQR In [1]: def get_outlier_params ( orig_data ): iqr_params = orig_data . describe () . T [[ '25%' , '75%' ]] iqr_params [ 'IQR' ] = iqr_params [ '75%' ] - iqr_params [ '25%' ] iqr_params [ 'Lower Fence' ] = iqr_params [ '25%' ] - 1.5 * iqr_params [ 'IQR' ] iqr_params [ 'Upper Fence' ] = iqr_params [ '75%' ] + 1.5 * iqr_params [ 'IQR' ] return iqr_params In [194]: iqr_params = get_outlier_params ( df ) iqr_params Out[194]: 25% 75% IQR Lower Fence Upper Fence 100 Games ($) 9200.0 10600.0 1400.0 7100.0 12700.0 1000 Games ($) 6000.0 10200.0 4200.0 -300.0 16500.0 5000 Games ($) -4450.0 5200.0 9650.0 -18925.0 19675.0 10000 Games ($) -16600.0 -3150.0 13450.0 -36775.0 17025.0 Means of simulated data AFTER outlier removal In [195]: def remove_outliers ( outlier_params , data ): outlier_removed_df = pd . DataFrame () for column in data . columns : outlier_removed_df [ column ] = data [ column ] . apply ( lambda x : x if x > outlier_params [ 'Lower Fence' ][ column ] else np . nan ) outlier_removed_df [ column ] = data [ column ] . apply ( lambda x : x if x < outlier_params [ 'Upper Fence' ][ column ] else np . nan ) return outlier_removed_df In [196]: new_df = remove_outliers ( iqr_params , df ) new_mean = pd . DataFrame ( new_df . describe () . round ( 1 ) . T [ 'mean' ]) . T new_mean . rename ( index = { 'mean' : 'outlier-removed mean' }, inplace = True ) pd . concat ([ raw_mean , new_mean ]) Out[196]: 100 Games ($) 1000 Games ($) 5000 Games ($) 10000 Games ($) original mean 9812.4 7930.8 214.0 -9865.2 outlier-removed mean 9800.2 7892.8 172.7 -9950.1 Based on the simulated mean of each players Dice Roll game result, it can be observed that a player will lose ~20,000 dollars if he plays the 10,000 games , betting 100 dollars each game. 2. Oil Field Example: Total Thickness of Two Formations Your company is about to drill into two formations: formation A and formation B . From the previous experiences within the asset, you know the the distribution of each formation's thickness (which is rarely the case...). In order to develop production / facility plans, you need to draw an uncertainty model for the total thickness of formation A + formation B . 2.0.1 Assumptions Before Monte-Carlo simulation is run to develop the uncertainty model, a few assumptions will be made. The formation thickness in the asset has Gaussian distribution Formation A has a mean value of 10 ft, and standard deviation of 2 ft. Formation B has a mean value of 24 ft, and standard deviation of 4 ft. The mean and standard deviation were calculated from large enough samples, and their values are reliable. We are not given any sample data set. We are only given mean and standard deviations. In [18]: assumptions = pd . DataFrame ( data = [[ 10 , 24 ],[ 2 , 4 ]], columns = [ 'Formation A (ft)' , 'Formation B (ft)' ], index = [ 'mean' , 'stdev' ]) assumptions Out[18]: Formation A (ft) Formation B (ft) mean 10 24 stdev 2 4 Recall that Monte-Carlo simulation requires the distribution shape and distribution parameters of the population. If we know the distribution shape, but do not have large enough samples to estimate reasonable values for the mean and the standard deviation of the population, Monte-Carlo simulation for Gaussian distribution may return inaccurate results. This can't really be helped since we just don't have enough samples. Furthurmore, if we have reasonably large enough samples, but do not know the distribution shape, Monte-Carlo simulation cannot be run . Recall that when generating random samples, it assumes a certain form of a distribution. (Ex: numpy.random.normal() , numpy.random.lognormal() , numpy.random.chiquare() ). Notes If Monte-Carlo simulation cannot be run because the distribution shape is unknown, non-parametric Bootstrap simulation can be used to generate random samples. 2.0.2 Why Use Monte-Carlo Simulation? One might ask why Monte-Carlo simulation is needed for this task. Why can't we just add the provided means of the two formations and use it for our thickness model? Total Thickness = Form. A Mean Thickness + Form. B Mean Thickness Total Thickness = 10 ft + 24 ft = 34 ft However, this simple forecast model does not give any information about the uncertainty in the total thickness of the formation. That is, we only know the overall mean thickness, but nothing about the possible range of thickness of the formations. Ideally we want to formulate something like the following: The total formation thickness will fall within the range of 27 ~ 41 ft by 80% chance, with 34 ft being the mean of the distribution. When we are given only the estimated mean and standard deviation of the population, uncertainty model cannot be formulated without some kind of random sampling method. Monte-Carlo simulation can be used to generate a pool of random samples. 2.1 Monte-Carlo Simulation for Gaussian Distribution Steps Using the provided mean and standard deviation, generate a random Gaussian distribution of Formation A and B thickness. Recall that we assumed the thickness distribution to be Gaussian. Generate random thickness values N times. Add the randomly generated thickness values for Formation A and B. Generate visualizations (CDF, boxplot, etc...) The distribution is Gaussian, and therefore np.random.normal() will be used to generate random normal distribution of formation thickness. If the distribution was assumed to be non-Gaussian, other function will be used to create random samples. For more information, check the numpy documentation of random sampling for various distributions . In [19]: mean_A = assumptions [ 'Formation A (ft)' ][ 'mean' ] mean_B = assumptions [ 'Formation B (ft)' ][ 'mean' ] std_A = assumptions [ 'Formation A (ft)' ][ 'stdev' ] std_B = assumptions [ 'Formation B (ft)' ][ 'stdev' ] iteration = 1000 monte_A = np . random . normal ( mean_A , std_A , iteration ) monte_B = np . random . normal ( mean_B , std_B , iteration ) total_thic = monte_A + monte_B df_thic = pd . DataFrame ([ monte_A , monte_B , total_thic ], index = [ 'Formation A (ft)' , 'Formation B (ft)' , 'Total Thickness (ft)' ]) . T df_thic . index . name = 'Iteration' df_thic . round ( 1 ) . head ( 10 ) Out[19]: Formation A (ft) Formation B (ft) Total Thickness (ft) Iteration 0 7.3 29.4 36.6 1 9.4 28.7 38.1 2 7.7 18.5 26.1 3 11.0 33.1 44.1 4 8.1 21.8 29.9 5 10.0 23.2 33.2 6 10.2 26.5 36.7 7 10.8 25.5 36.3 8 10.8 22.7 33.4 9 10.1 23.1 33.3 Visualizations Cumulative probablity function (CDF) and boxplot can be used to visualize the simulation result. In [33]: def visualize_distribution ( dataframe , ax_ ): dataframe = dataframe . apply ( lambda x : x . sort_values () . values ) for col , label in zip ( dataframe , dataframe . columns ): fit = scipy . stats . norm . pdf ( dataframe [ col ], np . mean ( dataframe [ col ]), np . std ( dataframe [ col ])) ax_ . plot ( dataframe [ col ], fit ) ax_ . set_ylabel ( 'Probability' ) In [34]: fig , ( ax1 , ax2 ) = plt . subplots ( 1 , 2 , figsize = ( 8 , 4 )) fig . suptitle ( 'Uncertainty Models for Total Formation Thickness' ) visualize_distribution ( df_thic [ 'Total Thickness (ft)' ] . to_frame (), ax1 ) ax1 . set_title ( 'Probability Distribution Function' ) ax1 . set_ylabel ( 'Probability' ) ax1 . set_xlabel ( 'Total Thickness (ft)' ) ax2 . boxplot ( df_thic [ 'Total Thickness (ft)' ]) ax2 . set_title ( 'Boxplot' ) ax2 . set_ylabel ( 'Total Thickness (ft)' ); ax2 . set_xticklabels ([]); Business Decision on P10, P50, and P90 Statistics Many of the business decisions are made on P10, P50, and P90 values. When reporting your statistical analysis to the management, you want to provide them the most likely range of outcome. In [36]: pd . DataFrame ( df_thic [ 'Total Thickness (ft)' ] . describe ( percentiles = [ 0.1 , 0.9 ])) . T . iloc [:, 4 : 7 ] . round ( 1 ) Out[36]: 10% 50% 90% Total Thickness (ft) 28.6 34.2 39.9 Based on the obtained P10, P50, and P90 values, the following forcast can be constructed: The total formation thickness will fall within the range of 28.6 ~ 39.9 ft by 80% chance, with 34.2 ft being the mean of the distribution.","tags":"Statistics","url":"https://aegis4048.github.io/uncertainty-modeling-with-monte-carlo-simulation","loc":"https://aegis4048.github.io/uncertainty-modeling-with-monte-carlo-simulation"}]};