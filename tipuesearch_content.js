var tipuesearch = {"pages":[{"title":"Creating a Jupyter Notebook-Powered Data Science Blog with Pelican","text":"What powers this blog, Pythonic Excursions? - Pelican. - Me Pelican is a static site genertor, written in Python. It is a Python library used to auto-generate HTML elements that are used to run websites. Pelican-powered blogs are light and easy to host with no scaling concerns. It pre-generates HTML files and responds with the existing files during a typical HTTP request-response cycle. So, why should you use Pelican? It's GREAT for Blogging You can wrtie your content directly with the editor of your choice in reStructuredText or Markdown formats. Select a favorite theme of your choice from a collection of Pelican-themes , and write articles. The CSS and Javascript contained in the theme will handle the rest and output your article nice and clean. One look is worth a thousand words. Take a look at this markdown file that is used to render this article through Github Pages by Jake VanderPlas . His blog is made with Pelican, after modifying some codes in Octopress theme (you can also write your articles in other formats, such as Jupyter Notebook, which powers this blog). Minimal Learning Curves Going through the official documenations, user-made tutorials, YouTube videos are painful. Using Pelican will minimize wasting your time dealing with the learning curves. One thing that makes it very easy to learn & modify is that there already are lots blogs that runs on Pelican, and their source codes are open to public. Pythonic Excursions -- source code , Aegis-Jupyter Theme by Me onCrash = 'reboot();' -- source code , Elegant Theme by Talha Mansoor Pythonic Perambulations -- source code , Adapted Octopress Theme by Jake VanderPlas ... and many more If you don't want to learn Pelican from scratch, you can download these open source repos and start from there. You will only need to learn how to tweak some settings to meet your needs. Completely Static Output Is Easy To Host Anywhere The output of Pelican is all HTML. You don't have to worry about configuring a complicated database and optimizing connections. Let's take a look at how Pelican works. Most Pelican blogs have the following directory tree. blog content articles article_1.md article_2.md article_3.md figures images ... output category images figures index.html archives.html article_1.html article_2.html article_3.html plugins themes custom_theme static templates Makefile pelicanconf.py publishconf.py output directory is the folder where Pelican stores the auto-generated HTML files, and those existing files are returned to the user who sent an HTTP request to view your website. The other directories are tools and templates used to generate the HTML files in the output folder. You do not need to configure a SQL database, or execute any codes on the server. All outputs are completely static. Can Pelican Be Used In Dynamic Websites Too? Yes, it can. Although Pelican is a static site generator, that does not mean that you can't have dynamic backend features on your website. You can pre-generate the output HTML files with Pelican, and just wrap it around with the backend framework of your choice. Let's say that you are developing a web-app with Django, and you want part of your website to be a static blog. You have a Pelican-generated output HTML file called article_1.html . In Django, you can render your Pelican-generated HTML file using a basic Class-Based-View like this: views.py from django.views.generic import TemplateView class PelicanView(TemplateView): template_name = 'article_1.html' urls.py from django.urls import re_path from your_app import views urlpatterns = [ re_path('&#94;$', views.PelicanView.as_view(), name='pelican'), ] And that's all it takes to integrate Pelican with Django. Part of your website can be static pages where it doesn't need to execute any code on a server, but the other part of your website can be dynamic pages where you can send queries to your server. Of course, the methodology to combine Pelican with dynamic backend will differ for each backend framework of your choice, but you get the idea. Here is the point: Pelican is a static site generator, but that does not mean that Pelican can't be used in dynamic websites. And Pelican is GREAT for blogging. Introducing Aegis-Jupyter Theme Aegis-Jupyter theme is a custom Pelican theme I made to easily host & maintain a Jupyter Notebook powered data science blog. I burrowed the some CSS design of the articles from Jake VanderPlas , and improved the rendering of Jupyter Notebook files by adding custom CSS & JS codes. Every articles you see in archives page is rendered using Jupyter Notebook .ipynb files, even this very article you are reading right now! Mobile Device Friendly The theme renders very nicely on all resolutions, screenwidth, and mobile devices. Try viewing this website on your phone. If you are on PC, try stretching & collapsing the browser size and see how it responsively re-aligns itself. Google Analytics Support If you own any kind of website, not just a data science blog, at some point in your life you would be wondering about the behaviors of the viewers. How many people visit my website every week? How many of them are UNIQUE visitor? From what region do I get the most visitors? On average, how many minutes do people stay on my website? Which post was the most popular? From what social media platform do I get the most visitors from? These kinds of questions can be answered by leveraging the power of Google Analytics, FOR FREE . All you need to do is to create a Google Analytics account, get a tracking ID, and put that on publishconf.py file. For example, if your Google Analytics tracking ID is UA-1XXXXXXXX-1 , then you set GOOGLE_ANALYTICS variable liks this: GOOGLE_ANALYTICS = \"UA-1XXXXXXXX-1\" That's it. Aegis-Jupyter theme will take care of the rest. More detailed tutorials on how to create Google Analytics account and tracking ID will come later. Easy to Manage Your Articles Meta properties of your article can easily be managed my changing variables inside markdown files. The below markdown is the actual .md file that renders this article . non-parametric-confidence-interval-with-bootstrap.md Title: Non-Parametric Confidence Interval with Bootstrap Tags: non-parametric, confidence-interval, bootstrap(stats), statistics Date: 2019-01-04 09:00 Slug: non-parametric-confidence-interval-with-bootstrap Subtitle: Keywords: Featured_Image: images/featured_images/bootstrap.png Social_Media_Description: Bootstrapping can calculate uncertainty in any confidence interval of any kind of distribution. It's great because it is distribution-free. Summary: {% notebook downloads/notebooks/Non-ParametricConfidenceIntervalswithBootstrap.ipynb cells[1:2] %} {% notebook downloads/notebooks/Non-ParametricConfidenceIntervalswithBootstrap.ipynb cells[:] %} The below screenshot is the preview of the article on the landing page of this blog. Observe how each attributes in the markdown file are used to render the output preview page. You can declare additional attributes as much as you want. Share Your Posts on Social Media Aegis-Jupyter theme leverages the power of Open Graph Meta Tags and renders the preview of your website nicely when shared on social media. You can set preview image by declaring Featured_Image and set preview descriptions by declaring Social_Media_Description for each article in each markdown files. If you do not specify Featured_Image attribute in the markdown file, a default featured image will show up when shared on social media. Default featured image can be set up in publishconf.py file. This is what I have for my blog: publishconf.py FEATURED_IMAGE = SITEURL + '/theme/img/logo_icon_background.png' Search Box Most static websites do not support search box functionality out of the box. However, Pelican supports Tipue Search , a jQuery site search plugin. Talha Mansoor made a pelican plugin that allows Pelican to leverage the power of Tipue Search, and Aegis-Jupyter integrated it to work with articles written in Jupyter Notebook. You can Disqus Comment Box Being able to communicate with the audiences is a quintessential component of a blog. Create an account in Disqus and get your Disqus Website Name here Then, declare DISQUS_SITENAME variable in publishconf.py . This all it takes to have a comment box feature for your blog. publishconf.py DISQUS_SITENAME = \"pythonic-excursions\"","tags":"WebDev","url":"https://aegis4048.github.io/creating-a-jupyternotebook-powered-data-science-blog-with-pelican","loc":"https://aegis4048.github.io/creating-a-jupyternotebook-powered-data-science-blog-with-pelican"},{"title":"Non-Parametric Confidence Interval with Bootstrap","text":"The codes were developed on Windows 10, and were not tested on other machines. Anaconda 5.2.0 is chosen as a Python interpreter. Bootstrap is a non-parametric statistical technique to resample from known samples to estimate uncertainty in summary statistics. When there are small, limited number of samples, it gives a more accurate forecast model than directly obtaining a forecast model from the limited sample pool (assuming that the sample set of data is reasonable representation of the population). It is non-parametric because it does not require any prior knowledge of the distribution (shape, mean, standard devation, etc..). Advantages of Bootstrap One great thing about Bootstrapping is that it is distribution-free . You do not need to know distribution shape, mean, standard devation, skewness, kurtosis, etc... All you need is just a set of sample data that is representative of a population. The fact that Bootstrapping does not depend on a type of distribution leads to another great advantage - It can calculate uncertainty in any confidence interval of any kind of distribution . For example, the analytical solution to calculate a confidence interval in any statistics of a distribution is as follows: CI of mean = stats of interest $\\pm$ $($distribution score $\\times$ Standard Error $)$ There are three problems with analytically solving for confidence interval of a statistic. First, the variable in the equation, distribution score , depends on the type of the distribution. If you do not know the distribution shape of your population, it is very difficult to calculate the confidence interval of a statistic. Second, not all statistics have a formula to calculate its Standard Error . For example, there exists an equation to calculate the standard error of a mean: Standard Error = $\\sigma_{sample} \\ \\mathbin{/} \\ \\sqrt{N}$ But there is no equation to calculate the standard error of a median. If you want to obtain confidence intervals for other statistics (ex: skewness, kurtosis, IQR, etc...), it will be very difficult to do so, simply because there are no equations for them. Third, some statistics have analytical solutions for its standard error calculation, but it is so convoluted that Bootstrapping is simpler. A classic example is obtaining a CI for the correlation coefficient given a sample from a bivariate normal distribution. Bootstrapping calculates confidence intervals for summary statistics numerically, not analytically , and this is why it can calculate ANY summary stats for ANY distribution. Methodology One goal of inferential statistics is to determine the value of a parameter of an entire population. It is typically too expensive or even impossible to measure this directly. So we use statistical sampling. We sample a population, measure a statistic of this sample, and then use this statistic to say something about the corresponding parameter of the population. Bootstrapping is a type of resampling method to save time and money taking measurements. From a sample pool of size N, it picks a random value N times with replacement , and create M number of new Bootstrapped-sample pools. The term with replacement here means that you put back the sample you drew to the original sample pool after adding it to a new Bootstrapped-sample pool. Think of it this way: you randomly choose a file from a folder in your PC, and you copy and paste the randomly-chosen file into a new folder. You do not cut and paste the file, but you copy and paste the file into a new folder. You will have M number of folders (M is an arbitrary number of your choice), each containing N number of files. Bootstrapping resamples the original sample pool to generate multiple smaller population of the true population. Each Bootstrap simulation is done by selecting a random value from the sample pool. For example, lets assume that you have the following sample pool of integers: Sample Integers = [12, 433, 533, 14, 65, 42, 64] From the sample pool of size N=7, you choose a random value N=7 times, and create a new sample pool of size N=7. In Bootstrap, each newly created sample pool is called a realization . You generate many of these realizations, and use them to calculate uncertainties in summary stats. Realization 1 = [12, 533, 533, 533, 12, 14, 42] Realization 2 = [65, 14, 14, 65, 433, 64, 14] Realization 3 = [433, 64, 533, 14, 14, 64, 12] Realization 4 = [14, 65, 65, 433, 533, 42, 12] Notice the duplicate data in the realizations (Ex: 533, 533, 533). Duplicates in realizations exist because each data in realization is randomly chosen from the original sample pool with replacement . Warning! It is extremly important that the N size for each Bootstrap realization matches the N size of the original sample pool. We use Bootstrap to numerically estimate the confidence interval (CI). It's an alternative tool to analytically solve for CI. Observing how CI is analytically calculated may help one to understand why the value of N is important. Let's take the CI of a mean for example. Recall that the CI of a mean represents how far a sample mean can deviate from the true population mean. In case of a Gaussian, or Gaussian-like distribution (ex: student-t), the equation to analytically solve for confidence interval of a mean is as follows: CI of mean = sample mean $\\pm$ $($z-score $\\times$ Standard Error $)$ Standard Error = $\\sigma_{sample} \\ \\mathbin{/} \\ \\sqrt{N}$ where $N$ is the number of measured samples. If you increase the number of samples, the standard error of a mean decreases. This logically makes sense, because the more samples you have, the more accurate the estimation of the true population mean becomes. The size of each Bootstrap realization, N, works the similar way, except that the random sample in each realization is not from the true population, but from a measured sample pool. Increasing the N-value will falsely make you to calculate smaller confidence interval. It can be observed that the CI obtained by using a wrong N-value for Bootstrap generates narrower CI. As a result, the CI of the sample mean does not cover the true population mean, returning a misleading estimation. In summary, Bootstrapping is used for three reasons: Bootstrap can obtain confidence interval in any statistics. Bootstrap does not assume anything about a distribution. Bootstrap helps when there are too few number of samples. Imports In [2]: import pandas as pd import numpy as np import scipy.stats import matplotlib.pyplot as plt import matplotlib.gridspec as gridspec % matplotlib notebook 1.A. Confidence Intervals in Summary Stats: US Male Height - Gaussian Distribution Bootstrap simulation can be run to obtain confidence intervals in various population parameters: mean, stdev, variance, min, or max. In this example, we will work with the height distribution of the US Male population, which tends to be Gaussian. However, the fact that the distribution Gaussian is totally unrelated to Bootstrap simulation, because it does not assume anything about the distribution. Bootstrapping can give us confidence intervals in any summary statistics like the following: By 95% chance, the following statistics will fall within the range of: Mean : 75.2 ~ 86.2, with 80.0 being the average Standard Deviation : 2.3 ~ 3.4 with 2.9 being the average Min : 54.3 ~ 57.2, with 55.2 being the average Max : 77.8 ~ 82.4, with 79.8 being the average Skew : -0.053 ~ 0.323, with 0.023 being the average 1.A.0. Bootstrap Scripts Bootstrap Simulator In [3]: def bootstrap_simulation ( sample_data , num_realizations ): n = sample_data . shape [ 0 ] boot = [] for i in range ( num_realizations ): real = np . random . choice ( sample_data . values . flatten (), size = n ) boot . append ( real ) columns = [ 'Real ' + str ( i + 1 ) for i in range ( num_realizations )] return pd . DataFrame ( boot , index = columns ) . T Summary Statistics Calculator In [4]: def calc_sum_stats ( boot_df ): sum_stats = boot_df . describe () . T [[ 'mean' , 'std' , 'min' , 'max' ]] sum_stats [ 'median' ] = boot_df . median () sum_stats [ 'skew' ] = boot_df . skew () sum_stats [ 'kurtosis' ] = boot_df . kurtosis () sum_stats [ 'IQR' ] = boot_df . quantile ( 0.75 ) - boot_df . quantile ( 0.25 ) return sum_stats . T Visualization Script In [5]: def visualize_distribution ( dataframe , ax_ ): dataframe = dataframe . apply ( lambda x : x . sort_values () . values ) for col , label in zip ( dataframe , dataframe . columns ): fit = scipy . stats . norm . pdf ( dataframe [ col ], np . mean ( dataframe [ col ]), np . std ( dataframe [ col ])) ax_ . plot ( dataframe [ col ], fit ) ax_ . set_ylabel ( 'Probability' ) Generate Confidence Intervals In [6]: def calc_bounds ( conf_level ): assert ( conf_level < 1 ), \"Confidence level must be smaller than 1\" margin = ( 1 - conf_level ) / 2 upper = conf_level + margin lower = margin return margin , upper , lower def calc_confidence_interval ( df_sum_stats , conf_level ): margin , upper , lower = calc_bounds ( conf_level ) conf_int_df = df_sum_stats . T . describe ( percentiles = [ lower , 0.5 , upper ]) . iloc [ 4 : 7 , :] . T conf_int_df . columns = [ 'P' + str ( round ( lower * 100 , 1 )), 'P50' , 'P' + str ( round ( upper * 100 , 1 ))] return conf_int_df def print_confidence_interval ( conf_df , conf_level ): print ( 'By {}% c hance, the following statistics will fall within the range of: \\n ' . format ( round ( conf_level * 100 , 1 ))) margin , upper , lower = calc_bounds ( conf_level ) upper_str = 'P' + str ( round ( upper * 100 , 1 )) lower_str = 'P' + str ( round ( lower * 100 , 1 )) for stat in conf_df . T . columns : lower_bound = round ( conf_df [ lower_str ] . T [ stat ], 1 ) upper_bound = round ( conf_df [ upper_str ] . T [ stat ], 1 ) mean = round ( conf_df [ 'P50' ] . T [ stat ], 1 ) print ( \" {0:<10} : {1:>10} ~ {2:>10} , AVG = {3:>5} \" . format ( stat , lower_bound , upper_bound , mean )) 1.A.1 Sample Data Description 100 samples of US male height data is provided in my Github Repo - sample_data/US_Male_Height.csv . Summary statistics of the sample data can be calculated. Your goal is to calculate the confidence intervals for the summary stats. In [7]: # height data height_data = pd . read_csv ( 'sample_data/US_Male_Height.csv' ) height_data . index = [ 'Male ' + str ( i + 1 ) for i in range ( height_data . shape [ 0 ])] height_data . round ( 1 ) . T Out[7]: Male 1 Male 2 Male 3 Male 4 Male 5 Male 6 Male 7 Male 8 Male 9 Male 10 ... Male 91 Male 92 Male 93 Male 94 Male 95 Male 96 Male 97 Male 98 Male 99 Male 100 Height (in) 70.8 72.8 72.5 67.3 72.7 73.6 65.0 67.1 70.8 70.6 ... 71.7 66.4 72.9 74.5 73.5 70.5 73.1 63.6 68.7 73.0 1 rows × 100 columns In [8]: height_summary_stats = calc_sum_stats ( height_data ) height_summary_stats Out[8]: Height (in) mean 69.881971 std 3.169548 min 63.143732 max 77.762886 median 69.894434 skew -0.059779 kurtosis -0.700743 IQR 5.154145 Visualization In [9]: fig , ax = plt . subplots ( figsize = ( 8 , 4 )) ax . set_xlabel ( 'Height (inches)' ); fig . suptitle ( 'Original Sample Data Distribution: Gaussian Distribution' ) visualize_distribution ( height_data , ax ); Based on the distribution plot of the original sample data, we can observe that the distribution indeed looks Gaussian. However, the fact that it looks like Gaussian does not matter at all when Bootstrapping, because Bootstrapping does not assume anything about the distribution. 1.A.2 Resampling From the Sample Data Each Bootstrap resampling (realization) can be done in one-line with numpy.random.choice() . Each realization is an array of size N, where N is the length of the original sample data. There are M number of realizations, where M is an arbitrary number of your choice. Results In [10]: M = 100 # number of realizations - arbitrary bootstrap_data = bootstrap_simulation ( height_data , M ) bootstrap_data . round ( 1 ) . head ( 10 ) Out[10]: Real 1 Real 2 Real 3 Real 4 Real 5 Real 6 Real 7 Real 8 Real 9 Real 10 ... Real 91 Real 92 Real 93 Real 94 Real 95 Real 96 Real 97 Real 98 Real 99 Real 100 0 72.7 67.9 65.0 69.2 70.4 64.9 70.3 66.3 67.6 72.2 ... 74.1 66.3 73.0 68.5 65.3 72.5 72.7 69.2 66.4 72.3 1 68.4 70.5 65.3 64.5 71.8 69.2 70.5 71.6 65.3 67.9 ... 72.5 68.6 66.1 71.7 67.3 74.1 67.9 71.3 72.9 65.0 2 71.3 74.1 72.8 72.9 68.3 67.9 73.1 65.0 73.6 72.8 ... 69.5 72.5 72.5 73.3 69.2 74.1 73.0 65.5 67.9 63.1 3 72.5 73.0 71.4 68.5 73.3 70.5 70.5 70.6 68.5 69.0 ... 64.5 69.2 66.0 69.5 72.5 70.3 67.9 68.3 73.6 73.5 4 67.2 73.5 73.6 67.2 64.5 72.9 72.8 66.4 69.2 66.8 ... 66.3 73.6 71.3 73.1 71.6 72.2 64.9 69.0 71.7 70.2 5 69.1 66.0 65.5 69.1 71.7 70.6 66.0 73.0 72.2 69.9 ... 73.0 66.3 69.0 67.9 69.4 69.9 69.5 68.7 72.4 67.3 6 72.2 68.5 72.9 63.1 73.6 73.1 70.8 75.5 69.9 70.6 ... 68.4 65.0 68.5 68.8 67.2 72.2 65.5 70.6 72.2 66.8 7 73.1 72.5 69.0 72.5 71.6 68.7 73.5 66.2 71.6 74.4 ... 73.6 68.5 72.2 73.1 72.3 72.1 66.8 77.8 72.8 69.5 8 67.3 69.5 74.5 66.8 69.1 65.0 69.9 70.6 65.0 73.1 ... 67.1 72.8 70.5 70.8 73.6 72.5 71.8 67.9 67.2 70.6 9 66.3 72.9 65.5 72.5 72.4 70.6 73.1 69.5 67.2 68.7 ... 71.6 69.2 65.0 71.3 71.4 69.1 73.3 70.2 66.1 67.6 10 rows × 100 columns In [11]: boot_sum_stats = calc_sum_stats ( bootstrap_data ) boot_sum_stats . round ( 1 ) Out[11]: Real 1 Real 2 Real 3 Real 4 Real 5 Real 6 Real 7 Real 8 Real 9 Real 10 ... Real 91 Real 92 Real 93 Real 94 Real 95 Real 96 Real 97 Real 98 Real 99 Real 100 mean 69.4 70.0 69.9 70.1 70.0 69.5 70.2 70.2 69.7 70.5 ... 69.7 69.9 69.7 69.9 70.1 70.0 70.0 69.5 70.1 69.4 std 3.1 3.1 3.2 3.3 3.3 3.1 3.1 3.2 2.9 3.0 ... 3.1 3.3 2.8 3.0 3.2 2.8 3.2 3.2 3.2 3.3 min 63.1 63.6 64.1 63.1 63.1 63.1 63.1 63.1 63.6 63.1 ... 63.1 63.1 63.1 63.6 63.6 64.5 64.1 63.1 63.1 63.1 max 75.5 77.8 76.1 77.8 76.1 76.1 76.1 77.8 77.8 77.8 ... 75.5 77.8 77.8 75.5 77.8 76.1 77.8 77.8 77.8 76.1 median 69.1 70.3 70.5 70.4 70.9 69.2 70.5 70.6 69.5 70.7 ... 69.9 69.5 69.5 69.9 69.9 69.9 70.4 69.2 70.6 69.1 skew -0.1 -0.0 -0.2 -0.0 -0.4 -0.0 -0.3 -0.3 0.2 -0.2 ... -0.4 -0.0 -0.0 -0.3 -0.1 0.1 0.2 -0.1 -0.2 -0.0 kurtosis -1.1 -0.8 -1.1 -0.4 -0.9 -0.7 -0.9 -0.6 -0.4 -0.5 ... -0.9 -0.7 -0.3 -0.9 -0.8 -0.9 -0.6 -0.7 -0.8 -1.0 IQR 5.4 5.2 5.9 4.2 5.2 4.9 5.5 5.0 4.9 4.2 ... 5.2 5.4 4.9 4.7 4.8 4.4 5.1 4.8 5.3 5.7 8 rows × 100 columns Visualize In [13]: fig , ax = plt . subplots ( figsize = ( 8 , 4 )) ax . set_xlabel ( 'Height (inches)' ); fig . suptitle ( 'Distribution of Bootstrap-Simulated Data: Gaussian' ) visualize_distribution ( bootstrap_data , ax ); Each line in the plot represents one Bootstrap realization. There are 100 realizations, each having 100 random samples. 1.A.3 Uncertainty Models in Summary Statistics with Blox Plots In [16]: f = plt . figure () plt . suptitle ( 'Uncertainty Models for Various Statistics: US Male Height - Gaussian' ) gs = gridspec . GridSpec ( 2 , 4 ) ax1 = plt . subplot ( gs [ 0 , 0 : 4 ]) ax2 = plt . subplot ( gs [ 1 , 0 ]) ax3 = plt . subplot ( gs [ 1 , 1 ]) ax4 = plt . subplot ( gs [ 1 , 2 ]) ax5 = plt . subplot ( gs [ 1 , 3 ]) boot_sum_stats . T [[ 'mean' , 'min' , 'max' , 'median' ]] . boxplot ( ax = ax1 ) boot_sum_stats . T [[ 'std' ]] . boxplot ( ax = ax2 ) boot_sum_stats . T [[ 'IQR' ]] . boxplot ( ax = ax3 ) boot_sum_stats . T [[ 'skew' ]] . boxplot ( ax = ax4 ) boot_sum_stats . T [[ 'kurtosis' ]] . boxplot ( ax = ax5 ) ax5 . set_ylim ([ - 3 , 3 ]); 1.A.4 Confidence Interval in Summary Statistics Confidence intervals of summary statistics usually have a confidence level of 90%, 95%, or 99%. In this case, we will choose 95% confidence level . In [17]: confidence_level = 0.95 conf_int = calc_confidence_interval ( boot_sum_stats , confidence_level ) conf_int . round ( 1 ) Out[17]: P2.5 P50 P97.5 mean 69.4 69.9 70.5 std 2.8 3.2 3.4 min 63.1 63.1 64.5 max 75.4 77.8 77.8 median 69.1 70.0 71.1 skew -0.5 -0.1 0.3 kurtosis -1.1 -0.7 -0.1 IQR 4.0 4.9 5.8 In [18]: print_confidence_interval ( conf_int , confidence_level ) By 95.0% chance, the following statistics will fall within the range of: mean : 69.4 ~ 70.5 , AVG = 69.9 std : 2.8 ~ 3.4 , AVG = 3.2 min : 63.1 ~ 64.5 , AVG = 63.1 max : 75.4 ~ 77.8 , AVG = 77.8 median : 69.1 ~ 71.1 , AVG = 70.0 skew : -0.5 ~ 0.3 , AVG = -0.1 kurtosis : -1.1 ~ -0.1 , AVG = -0.7 IQR : 4.0 ~ 5.8 , AVG = 4.9 1.B. Confidence Intervals in Summary Stats: Rock Permeability - Lognormal Distribution It was previously stated that Bootstrapping does not assume anything about the distribution. Is that really true? The previous example of the US Male Height distribution was a Gaussian distribution. But what if the distribution of our interest is not Gaussian? In this example, rock pearmeability, which has a lognormal distribution , will be used to show that Bootstrap does not depend on the type of the distribution. 1.B.0. Bootstrap Scripts The sample scripts used for US Male Height example will be used for Bootstrap simulation. Same scripts can be used for both Gaussian and lognormal distribution because Bootstrapping does not assume anything about the distribution. 1.B.1. Sample Data Description 105 samples of permeability data is provided in Github Repo - sample_data/PoroPermSampleData.xlsx . Permeability data is taken at many times at different depth of a wellbore. Summary statistics of the sample data can be calculated. Your goal is to calculate the confidence intervals for the summary stats. In [19]: # permeability data perm_depth_data = pd . read_excel ( 'sample_data/PoroPermSampleData.xlsx' , sheet_name = 'Sheet1' )[[ 'Depth' , 'Permeability (mD)' ]] perm_data = perm_depth_data [ 'Permeability (mD)' ] . to_frame () # visualize fig = plt . figure () ax = plt . axes () ax . plot ( perm_depth_data [ 'Permeability (mD)' ], perm_depth_data [ 'Depth' ]); ax . invert_yaxis () ax . set_title ( 'Permeability Along A Wellbore' ) ax . set_xlabel ( 'Permeability (mD)' ) ax . set_ylabel ( 'Depth (ft)' ); In [20]: perm_summary_stats = calc_sum_stats ( perm_data ) perm_summary_stats Out[20]: Permeability (mD) mean 161.008972 std 80.900128 min 43.534147 max 573.461883 median 144.329837 skew 1.625086 kurtosis 5.498080 IQR 102.580432 Visualization In [21]: fig , ax = plt . subplots ( figsize = ( 8 , 4 )) ax . set_xlabel ( 'Permeability (mD)' ) fig . suptitle ( 'Original Sample Data Distribution: Lognormal Distribution' ) visualize_distribution ( perm_data , ax ); Based on the distribution of the original sample data, we can observe that the distribution looks lognormal. The uncertainty in summary statistics can be calculated using Bootstrap the same way it was done for the US Male Height (Gaussian) distribution, because Bootstrap does not depend on the shape of the distribution. Warning! Outlier removal on rock permeability cannot be done directly, as this is a lognormal distribution. Recall that the typical outlier removal method assumes the distribution to be Gaussian. If you want to detect outliers for non-Gaussian distributions, you have to first transform the distribution into Gaussian. 1.B.2 Resampling From the Sample Data Each Bootstrap resampling (realization) can be done in one-line with numpy.random.choice() . Each realization is an array of size N, where N is the length of the original sample data. There are M number of realizations, where M is an arbitrary number of your choice. Results In [22]: M = 100 # number of realizations - arbitrary boot_perm_data = bootstrap_simulation ( perm_data , M ) boot_perm_data . round ( 1 ) . head ( 10 ) Out[22]: Real 1 Real 2 Real 3 Real 4 Real 5 Real 6 Real 7 Real 8 Real 9 Real 10 ... Real 91 Real 92 Real 93 Real 94 Real 95 Real 96 Real 97 Real 98 Real 99 Real 100 0 61.9 258.6 138.8 61.9 285.3 156.1 179.7 125.9 58.8 89.6 ... 151.0 227.4 59.1 573.5 258.6 56.3 240.6 89.6 132.7 182.6 1 170.5 61.0 143.7 214.1 264.2 244.1 144.7 160.5 83.9 258.6 ... 58.8 279.8 244.1 92.1 213.7 160.5 240.6 146.0 141.8 138.8 2 143.7 86.8 117.6 92.7 83.9 104.0 187.9 138.8 162.3 132.1 ... 258.4 380.1 89.6 89.6 123.3 77.5 102.7 193.1 133.2 234.5 3 166.9 151.0 240.6 265.5 183.1 65.6 59.1 305.1 103.5 131.6 ... 214.9 128.9 210.8 108.6 193.1 125.9 77.5 151.5 112.3 58.8 4 161.0 146.0 89.6 84.9 129.1 43.5 170.5 97.7 190.9 197.8 ... 56.3 85.0 53.4 79.4 58.8 92.7 102.7 190.9 126.3 161.0 5 104.0 132.1 129.1 144.4 184.8 263.5 151.0 170.5 162.9 311.6 ... 61.0 156.1 170.5 264.2 244.1 85.0 112.3 117.6 224.4 265.5 6 305.1 77.5 213.7 84.9 240.6 58.8 224.4 234.5 128.9 193.1 ... 66.9 138.8 240.6 66.9 166.9 84.7 305.1 80.4 53.4 264.2 7 286.8 171.4 92.1 84.9 116.9 245.7 141.8 135.8 206.6 116.9 ... 162.3 244.1 187.9 151.0 84.9 85.0 573.5 170.5 83.3 117.6 8 142.7 187.9 131.6 117.6 244.1 214.1 182.6 134.7 132.7 132.7 ... 123.3 104.0 65.6 86.8 84.9 193.1 56.3 136.9 156.1 311.6 9 58.8 132.1 380.1 136.9 65.6 244.1 134.7 77.8 321.1 79.4 ... 128.9 83.9 182.6 132.0 117.6 234.5 227.4 187.9 80.4 138.8 10 rows × 100 columns In [23]: boot_perm_sum_stats = calc_sum_stats ( boot_perm_data ) boot_perm_sum_stats . round ( 1 ) Out[23]: Real 1 Real 2 Real 3 Real 4 Real 5 Real 6 Real 7 Real 8 Real 9 Real 10 ... Real 91 Real 92 Real 93 Real 94 Real 95 Real 96 Real 97 Real 98 Real 99 Real 100 mean 159.5 152.6 146.9 155.7 160.3 161.7 161.5 164.4 146.6 162.7 ... 143.9 150.0 166.7 155.6 155.2 151.5 164.7 171.3 143.8 148.1 std 80.0 64.1 72.6 77.2 72.1 98.0 88.0 90.7 65.3 73.3 ... 78.4 68.0 64.7 78.0 68.8 74.8 106.0 96.3 63.1 62.0 min 56.3 43.5 43.5 43.5 53.4 43.5 43.5 43.5 43.5 43.5 ... 43.5 43.5 53.4 43.5 43.5 53.4 53.4 43.5 43.5 43.5 max 573.5 305.1 380.1 573.5 380.1 573.5 573.5 573.5 321.1 321.1 ... 573.5 380.1 380.1 573.5 380.1 321.1 573.5 573.5 380.1 380.1 median 143.7 138.8 132.1 144.4 138.8 143.7 142.7 138.8 136.9 145.6 ... 132.0 133.2 151.0 141.8 138.8 133.2 133.2 144.7 132.7 138.8 skew 1.8 0.6 1.1 1.7 0.7 1.6 2.2 2.0 0.6 0.4 ... 2.1 0.8 0.6 1.8 0.7 0.6 1.9 2.2 0.9 0.8 kurtosis 6.1 -0.5 1.0 6.9 -0.3 4.3 7.7 6.4 -0.2 -0.9 ... 8.1 0.5 0.3 6.7 0.2 -0.7 4.6 7.0 1.0 1.1 IQR 79.3 80.8 92.7 113.0 102.4 129.2 102.4 86.7 95.2 114.0 ... 77.3 98.2 78.7 102.6 86.7 112.7 132.3 79.2 82.1 80.8 8 rows × 100 columns Visualize In [29]: fig , ax = plt . subplots ( figsize = ( 8 , 4 )) fig . suptitle ( 'Distribution of Bootstrap-Simulated Data: Lognormal' ) ax . set_xlabel ( 'Permeability (mD)' ) visualize_distribution ( boot_perm_data , ax ); 1.B.3 Uncertainty Models in Summary Statistics with Blox Plots In [25]: f = plt . figure () plt . suptitle ( 'Uncertainty Models for Various Statistics: Rock Permeability - Lognormal' ) gs = gridspec . GridSpec ( 2 , 4 ) ax1 = plt . subplot ( gs [ 0 , 0 : 4 ]) ax2 = plt . subplot ( gs [ 1 , 0 ]) ax3 = plt . subplot ( gs [ 1 , 1 ]) ax4 = plt . subplot ( gs [ 1 , 2 ]) ax5 = plt . subplot ( gs [ 1 , 3 ]) boot_perm_sum_stats . T [[ 'mean' , 'min' , 'max' , 'median' ]] . boxplot ( ax = ax1 ) boot_perm_sum_stats . T [[ 'std' ]] . boxplot ( ax = ax2 ) boot_perm_sum_stats . T [[ 'IQR' ]] . boxplot ( ax = ax3 ) boot_perm_sum_stats . T [[ 'skew' ]] . boxplot ( ax = ax4 ) boot_perm_sum_stats . T [[ 'kurtosis' ]] . boxplot ( ax = ax5 ) ax4 . set_ylim ([ - 3 , 3 ]) ax5 . set_ylim ([ - 10 , 10 ]); Observe the positive skewness in the boxplot summary statistics. This is consistent with the left-justified lognormal distribution of the permeability plot. 1.B.4 Confidence Interval in Summary Statistics Confidence intervals of summary statistics usually have a confidence level of 90%, 95%, or 99%. In this case, we will choose 90% confidence level . In [26]: confidence_level = 0.9 conf_int_perm = calc_confidence_interval ( boot_perm_sum_stats , confidence_level ) conf_int_perm . round ( 1 ) Out[26]: P5.0 P50 P95.0 mean 146.1 160.0 171.3 std 64.4 77.4 96.4 min 43.5 43.5 54.3 max 321.1 573.5 573.5 median 132.7 144.3 156.1 skew 0.4 1.4 2.2 kurtosis -0.6 4.0 8.6 IQR 78.4 98.1 120.9 In [27]: print_confidence_interval ( conf_int_perm , confidence_level ) By 90.0% chance, the following statistics will fall within the range of: mean : 146.1 ~ 171.3 , AVG = 160.0 std : 64.4 ~ 96.4 , AVG = 77.4 min : 43.5 ~ 54.3 , AVG = 43.5 max : 321.1 ~ 573.5 , AVG = 573.5 median : 132.7 ~ 156.1 , AVG = 144.3 skew : 0.4 ~ 2.2 , AVG = 1.4 kurtosis : -0.6 ~ 8.6 , AVG = 4.0 IQR : 78.4 ~ 120.9 , AVG = 98.1","tags":"Statistics","url":"https://aegis4048.github.io/non-parametric-confidence-interval-with-bootstrap","loc":"https://aegis4048.github.io/non-parametric-confidence-interval-with-bootstrap"},{"title":"Uncertainty Modeling with Monte-Carlo Simulation","text":"The codes were developed on Windows 10, and were not tested on other machines. Anaconda 5.2.0 is chosen as a Python interpreter. Monte Carlo Simulation is a random sampling method to model uncertainty of a population estimation. When given only population parameters (mean, standard deviation, degrees of freedom, etc..), but not the sample data itself, it generates random samples based on the distribution parameters to create a sample pool that is representative of the true population. Uncertainty models can be created from the newly generated sample pool. Based on historical data, expertise in the field, or past experience, you might know the typical values of population mean, standard deviation and degrees of freedom. While these parameters are useful for developing a model, they do not tell you the uncertainties in a population. In a financial market, you might know the distribution of possible values through the mean and standard deviation of returns. By using a range of possible values, instead of a single guess, you can create a more realistic picture of what might happen in the future. Let's assume that your consultant recommended you a certain investment program that has a mean return rate of 10% and a standard deviation of 1% However, You do not have an access to the actual sample data that is used to obtain the mean, and standard deviation . You made 100 investments through this program, but your 100 investments had an average rate of return of 3%. Did the consultant lie to you, or is it one of the possible corner cases that you can have if you are unlucky? What is the P10, P50, P90 value of this investment program? What are the the most plausible range of rate of return? Does your 3% rate of return fall within that range ? In order to answer these questions, you need sample data that is representative of the population. Monte-Carlo simulation takes population parameters as arguments, and generates series of random samples to investigate a range of possible outcomes . Methodology Monte-Carlo simulation is one of the random sampling method that generates a new set of random samples from statistic parameters of a population. It assumes a certain distribution shape, and population parameters as input and returns a random sample based on the distribution shape and parameters. The most simple examples are as follows: Excel Gaussian: NORM.INV(RAND(), mean, stdev) Lognormal: LOGNORM.INV(RAND(), mean, stdev) Chi-Square: CHISQ.INV(RAND(), degree_freedom) F-distribution: F_INV(RAND(), degree_freedom_numerator, degree_freedom_denominator) Python Gaussian: np.random.normal(mean, stdev) Lognormal: np.random.lognormal(mean, stdev) Chi-Square: np.random.chisquare(degree_freedom) F-distribution: np.random.f(degree_freedom_numerator, degree_freedom_denominator) These examples are the most simple cases of generating random samples vis Monte-Carlo simulation. Random samples can be generated as many times as desired. Based on the N-number of random samples generated, you can draw a CDF or boxplot to model uncertainty in prediction. Warning! In order to use Monte-Carlo simulation, you must know the distribution shape (normal, lognormal, chi-square, etc..) and distribution parameters (mean, standard deviation, degrees of freedom, etc..) of the data. If you do not have enough samples to draw an uncertainty model, or do not know the distribution shape and parameters, Bootstrap simulation may address your issue. Random samples of interest can can be created via an applied form of Monte-Carlo simulation. For example, the Casino Dice Roll Example simulates a game 1,000 times for a single player, and calculates a player's final fund at the end. The final fund of a single player is one random Monte-Carlo sample. The process is repeated 100 times to account for 100 players' final fund, and now we have 100 random Monte-Carlo samples. Total Thickness of Two Formations Example generates two sets of Monte-Carlo formation samples N times (where N is arbitrary number of your choice) to account for Formation A, and Formation B. The two sets of Monte-Carlo formation data are then added together to obtain Monte-Carlo data for total thickness. 1. Casino Dice Roll Example How do casinos earn money? The answer is simple - the longer you play, the bigger the chance of you losing money. Let's assume an imaginary dice roll game between a casino house and a player. The rules are simple. Dice Roll Game Rules There is an imaginary dice that rolls between 1 to 100. If a player rolls between 1 to 51, the house wins. If a player rolls between 52 to 100, the player wins. A player can bet as many times as he wants. With the above rules, the house has 2% higher chance of winning over a player . As a financial analyst of the house, upper management wants you to create a Dice Roll game profit forecast model. Question : If a certain game is configured so that the house has 2% higher chance of winning over a player , what is the expected profit forecast model for the game? Monte-Carlo simulation can be used to simulate the possible outcomes of dice roll game, and generate a forecast model. 1.0 Game Simulator Scripts Imports In [2]: import random import scipy import matplotlib.pyplot as plt import pandas as pd import numpy as np % matplotlib notebook Dice Roll Simulation In [3]: def rolldice (): dice = random . randint ( 1 , 100 ) if dice <= 51 : # Player loses return False elif dice > 51 & dice <= 100 : # Player wins return True Single Game Simulation In [4]: def play ( total_funds , wager_amount , total_plays , final_fund ): play_num = [] # x-axis of the plot funds = [] # y-axis of the plot play = 1 while play <= total_plays : if rolldice (): # Player wins total_funds = total_funds + wager_amount # updates current total funds play_num . append ( play ) funds . append ( total_funds ) else : # Player loses total_funds = total_funds - wager_amount play_num . append ( play ) funds . append ( total_funds ) play = play + 1 final_fund . append ( funds [ - 1 ]) # final_fund contains the ending fund of all players return final_fund , play_num , funds Results Visualization In [5]: def simulate_visualize ( init_money , bet , num_bet , num_players = 1 ): # simulates and generates a plot f , ax = plt . subplots () count = 1 ending_fund_all_players = [] while count <= num_players : ending_fund_all_players , num_play , funds_record = play ( init_money , bet , num_bet , ending_fund_all_players ) ax . plot ( num_play , funds_record ) count += 1 ax . set_title ( str ( num_players ) + ' Player(s): ' + 'Change in Total Fund with Each Game' ) ax . set_ylabel ( 'Player \\' s Fund ($)' ) ax . set_xlabel ( 'Number of Bets' ) return ending_fund_all_players In [6]: def simulate ( init_money , bet , num_bet , num_players = 1 ): # simulates, but does not plot count = 1 ending_fund_all_players = [] while count <= num_players : ending_fund_all_players , num_play , funds_record = play ( init_money , bet , num_bet , ending_fund_all_players ) count += 1 return ending_fund_all_players 1.1 Monte-Carlo Simulation: 1 Player Let's say than an imaginary player, 'Eric', visits the house and wants to play the Dice Roll Game. A Monte-Carlo simulation can be run to simulate the result of Eric's game. The simulation will be run with the following conditions: Eric starts with \\$10,000 Eric bets \\$100 each time Eric plays the game 1,000 times In [148]: simulate_visualize ( init_money = 10000 , bet = 100 , num_bet = 1000 , num_players = 1 ) plt . axhline ( 10000 , color = \"red\" , linewidth = 3 ) plt . text ( 780 , 10200 , 'Starting Money $10,000' , color = 'red' ); Eric started with 10,000 dollars. To your surprise, Eric actually ended up earning money from the house by 2,500 dollars after 1,000 games . According to the configuration of the game, the house has 2% higher chance of winning over Eric. Therefore, with such a high number of games, like a thousand, the house was supposed to earn money from the player. But it was not the case here. Was the configuration of the game wrong, or was Eric just really lucky? 1.1 Monte-Carlo Simulation: 100 Players Eric earned $2,500 dollars after running 1,000 games. However, if hundred other players play the Dice Roll game for thousand times each, would the result be different? From the house's perspective, what is the expected profit from the Dice Roll game? To get more accurate estimation of the expected profit, multiple Monte-Carlo simulation will be run. In this case, hundred. The simulation will be run with the following conditions: Hundred players each start with \\$10,000 Hundred players bet \\$100 each time Hundred players play the game 1,000 times In [7]: simulate_visualize ( init_money = 10000 , bet = 100 , num_bet = 1000 , num_players = 100 ) plt . axhline ( 10000 , color = \"white\" , linewidth = 3 ) plt . text ( 100 , 10400 , 'Starting Money $10,000' , color = 'white' , weight = 'bold' ); As it can be shown on the plots, Eric's earning 2,500 dollars after 1,000 games was a plausible outcome. There was even a player who eanred ended up with 16,500 dollars, which means that he earned 6,500 dollars ! However, this does not mean that the house will earn negative profit. The plot clearly indicates overall trend in the house earning money over the players as the number of bets increases. 1.3 Uncertainty Modeling The previous simulation results represent the outcome of 100 players each playing 1,000 games . One hundred Monte-Carlo simulations were run, and now we have one hundred samples of 1,000 game simulations data. To obtain more accurate uncertainty model for the Dice Roll game, further simulations will be run for 1,000 players each playing 100, 1,000, 10,000, and 100,000 games . In [6]: df = pd . DataFrame () for num_games in [ 100 , 1000 , 5000 , 10000 ]: result = simulate ( init_money = 10000 , bet = 100 , num_bet = num_games , num_players = 1000 ) col_name = str ( num_games ) + ' Games ($)' df [ col_name ] = result In [7]: df . index . name = 'Player Number' df . head ( 10 ) Out[7]: 100 Games ($) 1000 Games ($) 5000 Games ($) 10000 Games ($) Player Number 0 8400 7000 5800 -15600 1 8600 8000 16000 -34800 2 9600 7600 -400 5000 3 9400 10400 -6600 -11200 4 9400 10600 -400 0 5 8600 7200 -200 -19600 6 10800 7800 5600 -14000 7 9800 12400 -4000 -6200 8 10600 7600 24600 -9400 9 7400 7400 1800 -19200 In [10]: ax = df . boxplot ( grid = False ) ax . set_title ( 'Uncertainty Model for Dice Roll Game Profit: 1000 Players' ) ax . set_ylabel ( 'Player \\' s Fund ($)' ) ax . axhline ( 10000 , color = \"red\" , linewidth = 3 ); ax . text ( 3.5 , 11500 , 'Starting Money $10,000' , color = 'red' ); The generated box plot is the forecast model for the Dice Roll game profit generation. It tells you the most likely range of profit expected for N number of games played for each player. Based on the box plot uncertainty model, you can confirm that the longer you play, the bigger chance of you losing money. Although some lucky players may double, or even triple their money at the casino, far bigger population of the players will end up losing money to the casino. Recall that the Dice Roll game was configured so that the Casino has 2% higher chance of winning the game over a player. Summary: A player starts with 10,000 dollars and bets 100 dollar for each game. If a player plays 100 games, he will most likely end up between 12,500 to 6800 dollars If a player plays 1000 games, he will most likely end up between 15,800 to $-$360 dollars If a player plays 5,000 games, he will most likely end up between 19,200 to $-$18,900 dollars If a player plays 10,000 games, he will most likely end up between 15,200 to $-$36,000 dollars 1.4 Outlier Removal and Mean of the Prediction The uncertainty model generated by Monte-Carlo simulations gives you a range of possible outcome. But what if you want a single value of the outcome? One simple way to address this question is to just calculate the average of the simulated data. Means of simulated data BEFORE outlier removal In [192]: raw_mean = pd . DataFrame ( df . describe () . T [ 'mean' ]) . T raw_mean . rename ( index = { 'mean' : 'original mean' }, inplace = True ) raw_mean Out[192]: 100 Games ($) 1000 Games ($) 5000 Games ($) 10000 Games ($) original mean 9812.4 7930.8 214.0 -9865.2 But as it can be observed in the boxplot, the simulated data contains outliers (circled points). One might want to remove these outliers before calculating the average of the data to improve accuracy. The traditional IQR outlier detection method can be implemented. IQR = P75 - P25 Lower Fence = P25 - 1.5 $\\times$ IQR Upper Fence = P75 + 1.5 $\\times$ IQR In [1]: def get_outlier_params ( orig_data ): iqr_params = orig_data . describe () . T [[ '25%' , '75%' ]] iqr_params [ 'IQR' ] = iqr_params [ '75%' ] - iqr_params [ '25%' ] iqr_params [ 'Lower Fence' ] = iqr_params [ '25%' ] - 1.5 * iqr_params [ 'IQR' ] iqr_params [ 'Upper Fence' ] = iqr_params [ '75%' ] + 1.5 * iqr_params [ 'IQR' ] return iqr_params In [194]: iqr_params = get_outlier_params ( df ) iqr_params Out[194]: 25% 75% IQR Lower Fence Upper Fence 100 Games ($) 9200.0 10600.0 1400.0 7100.0 12700.0 1000 Games ($) 6000.0 10200.0 4200.0 -300.0 16500.0 5000 Games ($) -4450.0 5200.0 9650.0 -18925.0 19675.0 10000 Games ($) -16600.0 -3150.0 13450.0 -36775.0 17025.0 Means of simulated data AFTER outlier removal In [195]: def remove_outliers ( outlier_params , data ): outlier_removed_df = pd . DataFrame () for column in data . columns : outlier_removed_df [ column ] = data [ column ] . apply ( lambda x : x if x > outlier_params [ 'Lower Fence' ][ column ] else np . nan ) outlier_removed_df [ column ] = data [ column ] . apply ( lambda x : x if x < outlier_params [ 'Upper Fence' ][ column ] else np . nan ) return outlier_removed_df In [196]: new_df = remove_outliers ( iqr_params , df ) new_mean = pd . DataFrame ( new_df . describe () . round ( 1 ) . T [ 'mean' ]) . T new_mean . rename ( index = { 'mean' : 'outlier-removed mean' }, inplace = True ) pd . concat ([ raw_mean , new_mean ]) Out[196]: 100 Games ($) 1000 Games ($) 5000 Games ($) 10000 Games ($) original mean 9812.4 7930.8 214.0 -9865.2 outlier-removed mean 9800.2 7892.8 172.7 -9950.1 Based on the simulated mean of each players Dice Roll game result, it can be observed that a player will lose ~20,000 dollars if he plays the 10,000 games , betting 100 dollars each game. 2. Oil Field Example: Total Thickness of Two Formations Your company is about to drill into two formations: formation A and formation B . From the previous experiences within the asset, you know the the distribution of each formation's thickness (which is rarely the case...). In order to develop production / facility plans, you need to draw an uncertainty model for the total thickness of formation A + formation B . 2.0.1 Assumptions Before Monte-Carlo simulation is run to develop the uncertainty model, a few assumptions will be made. The formation thickness in the asset has Gaussian distribution Formation A has a mean value of 10 ft, and standard deviation of 2 ft. Formation B has a mean value of 24 ft, and standard deviation of 4 ft. The mean and standard deviation were calculated from large enough samples, and their values are reliable. We are not given any sample data set. We are only given mean and standard deviations. In [18]: assumptions = pd . DataFrame ( data = [[ 10 , 24 ],[ 2 , 4 ]], columns = [ 'Formation A (ft)' , 'Formation B (ft)' ], index = [ 'mean' , 'stdev' ]) assumptions Out[18]: Formation A (ft) Formation B (ft) mean 10 24 stdev 2 4 Recall that Monte-Carlo simulation requires the distribution shape and distribution parameters of the population. If we know the distribution shape, but do not have large enough samples to estimate reasonable values for the mean and the standard deviation of the population, Monte-Carlo simulation for Gaussian distribution may return inaccurate results. This can't really be helped since we just don't have enough samples. Furthurmore, if we have reasonably large enough samples, but do not know the distribution shape, Monte-Carlo simulation cannot be run . Recall that when generating random samples, it assumes a certain form of a distribution. (Ex: numpy.random.normal() , numpy.random.lognormal() , numpy.random.chiquare() ). Notes If Monte-Carlo simulation cannot be run because the distribution shape is unknown, non-parametric Bootstrap simulation can be used to generate random samples. 2.0.2 Why Use Monte-Carlo Simulation? One might ask why Monte-Carlo simulation is needed for this task. Why can't we just add the provided means of the two formations and use it for our thickness model? Total Thickness = Form. A Mean Thickness + Form. B Mean Thickness Total Thickness = 10 ft + 24 ft = 34 ft However, this simple forecast model does not give any information about the uncertainty in the total thickness of the formation. That is, we only know the overall mean thickness, but nothing about the possible range of thickness of the formations. Ideally we want to formulate something like the following: The total formation thickness will fall within the range of 27 ~ 41 ft by 80% chance, with 34 ft being the mean of the distribution. When we are given only the estimated mean and standard deviation of the population, uncertainty model cannot be formulated without some kind of random sampling method. Monte-Carlo simulation can be used to generate a pool of random samples. 2.1 Monte-Carlo Simulation for Gaussian Distribution Steps Using the provided mean and standard deviation, generate a random Gaussian distribution of Formation A and B thickness. Recall that we assumed the thickness distribution to be Gaussian. Generate random thickness values N times. Add the randomly generated thickness values for Formation A and B. Generate visualizations (CDF, boxplot, etc...) The distribution is Gaussian, and therefore np.random.normal() will be used to generate random normal distribution of formation thickness. If the distribution was assumed to be non-Gaussian, other function will be used to create random samples. For more information, check the numpy documentation of random sampling for various distributions . In [19]: mean_A = assumptions [ 'Formation A (ft)' ][ 'mean' ] mean_B = assumptions [ 'Formation B (ft)' ][ 'mean' ] std_A = assumptions [ 'Formation A (ft)' ][ 'stdev' ] std_B = assumptions [ 'Formation B (ft)' ][ 'stdev' ] iteration = 1000 monte_A = np . random . normal ( mean_A , std_A , iteration ) monte_B = np . random . normal ( mean_B , std_B , iteration ) total_thic = monte_A + monte_B df_thic = pd . DataFrame ([ monte_A , monte_B , total_thic ], index = [ 'Formation A (ft)' , 'Formation B (ft)' , 'Total Thickness (ft)' ]) . T df_thic . index . name = 'Iteration' df_thic . round ( 1 ) . head ( 10 ) Out[19]: Formation A (ft) Formation B (ft) Total Thickness (ft) Iteration 0 7.3 29.4 36.6 1 9.4 28.7 38.1 2 7.7 18.5 26.1 3 11.0 33.1 44.1 4 8.1 21.8 29.9 5 10.0 23.2 33.2 6 10.2 26.5 36.7 7 10.8 25.5 36.3 8 10.8 22.7 33.4 9 10.1 23.1 33.3 Visualizations Cumulative probablity function (CDF) and boxplot can be used to visualize the simulation result. In [33]: def visualize_distribution ( dataframe , ax_ ): dataframe = dataframe . apply ( lambda x : x . sort_values () . values ) for col , label in zip ( dataframe , dataframe . columns ): fit = scipy . stats . norm . pdf ( dataframe [ col ], np . mean ( dataframe [ col ]), np . std ( dataframe [ col ])) ax_ . plot ( dataframe [ col ], fit ) ax_ . set_ylabel ( 'Probability' ) In [34]: fig , ( ax1 , ax2 ) = plt . subplots ( 1 , 2 , figsize = ( 8 , 4 )) fig . suptitle ( 'Uncertainty Models for Total Formation Thickness' ) visualize_distribution ( df_thic [ 'Total Thickness (ft)' ] . to_frame (), ax1 ) ax1 . set_title ( 'Probability Distribution Function' ) ax1 . set_ylabel ( 'Probability' ) ax1 . set_xlabel ( 'Total Thickness (ft)' ) ax2 . boxplot ( df_thic [ 'Total Thickness (ft)' ]) ax2 . set_title ( 'Boxplot' ) ax2 . set_ylabel ( 'Total Thickness (ft)' ); ax2 . set_xticklabels ([]); Business Decision on P10, P50, and P90 Statistics Many of the business decisions are made on P10, P50, and P90 values. When reporting your statistical analysis to the management, you want to provide them the most likely range of outcome. In [36]: pd . DataFrame ( df_thic [ 'Total Thickness (ft)' ] . describe ( percentiles = [ 0.1 , 0.9 ])) . T . iloc [:, 4 : 7 ] . round ( 1 ) Out[36]: 10% 50% 90% Total Thickness (ft) 28.6 34.2 39.9 Based on the obtained P10, P50, and P90 values, the following forcast can be constructed: The total formation thickness will fall within the range of 28.6 ~ 39.9 ft by 80% chance, with 34.2 ft being the mean of the distribution.","tags":"Statistics","url":"https://aegis4048.github.io/uncertainty-modeling-with-monte-carlo-simulation","loc":"https://aegis4048.github.io/uncertainty-modeling-with-monte-carlo-simulation"}]};