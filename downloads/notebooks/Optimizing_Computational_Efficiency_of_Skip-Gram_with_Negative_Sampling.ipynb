{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*The code snippet assumes Anaconda 5.2.0 version of Python virtual environment*\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "    <h4>Acknowledgement</h4>\n",
    "    <p>The materials on this post are based the on five NLP papers, <a href=\"https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf\" target=\"_blank\">Distributed Representations of Words and Phrases and their Compositionality</a> (Mikolov et al., 2013), <a href=\"https://arxiv.org/pdf/1411.2738.pdf\" target=\"_blank\">word2vec Parameter Learning Explained</a> (Rong, 2014), <a href=\"https://www.aaai.org/ocs/index.php/AAAI/AAAI17/paper/viewFile/14956/14446\" target=\"_blank\">Distributed Negative Sampling for Word Embeddings</a> (Stergiou et al., 2017), <a href=\"https://aclweb.org/anthology/D17-1037\" target=\"_blank\">Incremental Skip-gram Model with Negative Sampling</a> (Kaji and Kobayashi, 2017), and <a href=\"https://arxiv.org/pdf/1402.3722.pdf\" target=\"_blank\">word2vec Explained: Deriving Mikolov et al.’s\n",
    "Negative-Sampling Word-Embedding Method</a> (Goldberg and Levy, 2014).</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review on Word2Vec Skip-Gram\n",
    "\n",
    "In my <a href=\"https://aegis4048.github.io/demystifying_neural_network_in_skip_gram_language_modeling\" target=\"_blank\">previous post</a>, I illustrated the neural network structure of Skip-Gram Word2Vec model that represents words in a vector space.\n",
    "\n",
    "<div class=\"row\" id=\"fig1\">\n",
    "    <div class=\"col\"><img src=\"jupyter_images/word2vec_skip-gram.png\" style=\"margin: 0;\"></div>\n",
    "    <div class=\"col-12\"><p class=\"image-description\">Figure 1: Neural network structure of Skip-Gram</p></div>\n",
    "</div>\n",
    "\n",
    "I also derived the cost function of Skip-Gram in <a href=\"https://aegis4048.github.io/demystifying_neural_network_in_skip_gram_language_modeling#Derivation-of-Cost-Function\" target=\"_blank\">Derivation of Cost Function</a>:\n",
    "\n",
    "<div id=\"eq-1\" style=\"font-size: 1rem;\">\n",
    "$$\\begin{align}\n",
    "  J(\\theta) = & - \\frac{1}{T} \\sum_{t=1}^{T} \\sum_{-c\\leq j \\leq c,j\\neq 0} \\log p(w_{t+j} \\mid w_t ; \\, \\theta) \\\\\n",
    "\\end{align}\n",
    "\\tag{1} $$\n",
    "</div>\n",
    "\n",
    "where $p(w_{t+j} \\mid w_t ; \\, \\theta)$ is a probability of observing $w_{t+j}$ given $w_{t}$ with parameters $\\theta$. In vanilla Skip-Gram, the probability is computed with softmax. I also noted that <a href=\"https://aegis4048.github.io/demystifying_neural_network_in_skip_gram_language_modeling#stochastic\" target=\"_blank\">stochastic gradient descent</a> (SGD) is used to mitigate computational burden — the size of $T$ in $\\frac{1}{T} \\sum^T_{t=1}$ can be billions or more in NLP applications. The new cost function using SGD is:\n",
    "\n",
    "<div id=\"eq-2\" style=\"font-size: 1rem;\">\n",
    "$$J(\\theta; w^{(t)}) = -\n",
    "\\sum_{c=1}^{C}\n",
    "log \n",
    "\\frac{exp(W_{output_{(c)}} \\cdot h)}{\\sum^V_{i=1}exp(W_{output_{(i)}} \\cdot h)} \\tag{2}$$\n",
    "</div>\n",
    "\n",
    "where $T$ is the size of training samples, $C$ is the <a href=\"https://aegis4048.github.io/demystifying_neural_network_in_skip_gram_language_modeling#Window-Size-of-Skip-Gram\" target=\"_blank\">window size</a>, $V$ is the size of unique vocab in the corpus, and $W_{input}$, $W_{output}$ and $h$ are illustrated in <a href=\"#fig1\">figure 1</a>. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Review on Softmax\n",
    "\n",
    "Softmax is a multinomial regression classifier. It means that it classifies multiple labels, such as predicting if an hand-written digit is $0,\\,1,\\,2,\\,...\\,8\\,$ or $9$. In case of binary classification (True or False), such as classifying fraud or not-fraud in bank transactions, binomial regression classifier called Sigmoid function is used.\n",
    "\n",
    "In <a href=\"#eq-2\">eq (2)</a>, the fraction inside the summation of log yields the probability distribution of all $V$-vocabs in the corpus, given the input word. In statistics, the conditional probability of $A$ given $B$ is denoted as $p(A|B)$. In Skip-Gram, we use the notation, $p(w_{context}| w_{center})$, to denote the conditional probability of observing a context word given a center word. It is obtained by using the softmax function:\n",
    "\n",
    "<div id=\"eq-3\" style=\"font-size: 1rem;\">$$ p(w_{context}|w_{center}) = \\frac{exp(W_{output_{(context)}} \\cdot h)}{\\sum^V_{i=1}exp(W_{output_{(i)}} \\cdot h)} \\in \\mathbb{R}^{1} \\tag{3} $$</div>\n",
    "\n",
    "Exponentiation ensures that the transformed values are positive, and the normalization factor in the denominator ensures that the values have a range of $[0, 1)$ and their sum equals $1$. \n",
    "\n",
    "<div id=\"fig2\" class=\"row full_screen_margin mobile_responsive_plot_full_width\">\n",
    "    <div class=\"col\"><img src=\"jupyter_images/softmax_function.png\" style=\"height: 300px;\"></div>\n",
    "    <div class=\"col-12\"><p class=\"image-description\">Figure 2: softmax function transformation</p></div>\n",
    "</div>\n",
    "\n",
    "The probability is computed $V$ times using <a href=\"#eq-3\">eq (3)</a> to obtain a conditional probability distribution of observing all $V$-unique vocabs in the corpus, given a center word ($w^{(t)}$). \n",
    "\n",
    "<div id=\"eq-4\" style=\"font-size: 1rem;\">$$ \\left[ \\begin{array}{c} p(w_{1}|w^{(t)}) \\\\ p(w_{2}|w^{(t)}) \\\\ p(w_{3}|w^{(t)}) \\\\ \\vdots \\\\ p(w_{V}|w^{(t)}) \\end{array} \\right] = \\frac{exp(W_{output} \\cdot h)}{\\sum^V_{i=1}exp(W_{output_{(i)}} \\cdot h)} \\in \\mathbb{R}^{V}\\tag{4} $$</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax is computationally very expensive\n",
    "\n",
    "There is an issue with the vanilla Skip-Gram — softmax is computationally very expensive, as it requires scanning through the entire output embedding matrix ($W_{output}$) to compute the probability distribution of all $V$ words, where $V$ can be millions or more.\n",
    "\n",
    "<div class=\"row full_screen_margin mobile_responsive_plot_full_width\">\n",
    "    <div class=\"col\"><img src=\"jupyter_images/vanilla-skip-gram-complexity.png\"></div>\n",
    "    <div class=\"col-12\"><p class=\"image-description\">Figure 3: Algorithm complexity of vanilla Skip-Gram</p></div>\n",
    "</div>\n",
    "\n",
    "Furtheremore, the normalization factor in the denominator also requires $V$ iterations. In mathematical context, the normalization factor needs to be computed for each probability p($w_{context}| w_{center}$), making the alogrithm complexity = $O(V \\times V)$. However, when implemented on code, the normalization factor is computed only once and cached as a Python variable, making the alogrithm complexity = $O(V + V) \\approx O(V)$. This is possible because normalization factor is the same for all words.\n",
    "\n",
    "Due to this computational inefficiency, **softmax is not used in most implementaions of Skip-Gram**. Instead we use an alternative called *negative sampling* with <a href=\"#sigmoid\">sigmoid function</a>, which rephrases the problem into a set of independent binary classification task of algorithm complexity = $O(K \\, + \\, 1)$, where $K$ typically has a range of $[5, 20]$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Skip-Gram Negative Sampling\n",
    "\n",
    "In Skip-Gram, assuming <a href=\"https://aegis4048.github.io/demystifying_neural_network_in_skip_gram_language_modeling#stochastic\" target=\"_blank\">stochastic gradient descent</a>, weight marices in the neural network are updated for each training sample to correctly predict output. Let's assume that the training corpus has 10,000 unique vocabs ($V$ = 10000) and the hidden layer is 300-dimensional ($N$ = 300). This means that there are 3,000,000 neurons in the output weight matrix ($W_{output}$) that need to be updated for each training sample (Notes: for the input weight matrix ($W_{input}$), only 300 neurons are updated for each training sample. This is illustrated in *figure 18* of my <a href=\"https://aegis4048.github.io/demystifying_neural_network_in_skip_gram_language_modeling#weight_update\" target=\"_blank\">previous post</a>.) Since the size of the training corpus ($T$) is very large, updating 3M neurons for each training sample is unrealistic in terms of computational efficiency. Negative sampling addresses this issue by updating only a small fraction of the output weight neurons for each training sample. \n",
    "\n",
    "In negative sampling, $K$ <a href=\"#neg_word\">negative samples</a> are <a href=\"#neg_drawn\">randomly drawn</a> from a <a href=\"#noise_dist\">noise distribution</a>. $K$ is a hyper-parameter that can be empirically tuned, with a typical range of $[5,\\, 20]$. For each training sample (positive pair: $w$ and $c_{pos}$), you randomly draw $K$ number of negative samples from a noise distribution $P_n(w)$, and the model will update $(K+1) \\times N$ neurons in the output weight matrix ($W_{output}$). $N$ is the dimension of the hidden layer ($h$), or the size of a word vector. $+1$ accounts for a <a href=\"#pos_word\">positive sample</a>. \n",
    "\n",
    "With the above assumption, if you set <code>K=9</code>, the model will update $(9 + 1) \\times 300 = 3000$ neurons, which is only 0.1% of the 3M neurons in $W_{output}$. This is computationally much cheaper than the original Skip-Gram, and yet maintains a good quality of word vectors.\n",
    "\n",
    "The below figure has 3-dimensional hidden layer ($N=3$), 11 vocabs ($V=11$), and 3 negative samples ($K=3$). \n",
    "\n",
    "<div class=\"row\" id=\"fig4\" style=\"margin-top: 20px;\">\n",
    "    <div class=\"col\"><img src=\"jupyter_images/neg_vs_skip.png\" style=\"margin: 0;\"></div>\n",
    "    <div class=\"col-12\"><p class=\"image-description\">Figure 4: Skip-Gram model structure</p></div>\n",
    "</div>\n",
    "\n",
    "<div id=\"negsample\" class=\"alert alert-info\">\n",
    "    <h4>Notes: Choice of $K$</h4>\n",
    "    <p>The <a href=\"https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf\" target=\"_blank\">paper</a> (Mikolov et al., 2013) says that K=2 ~ 5 works for large data sets, and K=5 ~ 20 for small data sets.\n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How does negative sampling work?\n",
    "\n",
    " With negative sampling, word vectors are no longer learned by predicting context words of a center word. Instead of using softmax to compute the $V$-dimensional probability distribution of observing an output word given an input word, $p(w_O|w_I)$, the model uses sigmoid function to learn to differentiate the actual context words (*positive*) from randomly drawn words (*negative*) from the <a href=\"#noise_dist\">noise distribution</a> $P_n(w)$. \n",
    " \n",
    " Assume that the center word is *\"regression\"*. It is likely to observe *\"regression\"* + {*\"logistic\"*, *\"machine\"*, *\"sigmoid\"*, *\"supervised\"*, *\"neural\"*} pairs, but it is unlikely to observe *\"regression\"* + {*\"zebra\"*, *\"pimples\"*, *\"Gangnam-Style\"*, *\"toothpaste\"*, *\"idiot\"*}. The model maximizes the probability $p(D=1|w,c_{pos})$ of observing positive pairs, while minimizing the probability $p(D=1|w,c_{neg})$ of observing negative pairs. The idea is that <b>if the model can distinguish between the likely (positive) pairs vs unlikely (negative) pairs, good word vectors will be learned.</b>\n",
    " \n",
    "<div class=\"row full_screen_margin_md mobile_responsive_plot_full_width\" id=\"fig5\" style=\"margin-top: 20px;\">\n",
    "    <div class=\"col\"><img src=\"jupyter_images/neg_binomial.png\"></div>\n",
    "    <div class=\"col-12\"><p class=\"image-description\">Figure 5: Binomial classification of negative sampling</p></div>\n",
    "</div> \n",
    "\n",
    "Negative sampling converts multi-classification task into binary-classification task. The new objective is to predict, for any given word-context pair ($w,\\,c$), whether the word ($c$) is in the context window of the the center word ($w$) or not. Since the goal is to identify a given word as True (*positive*, $D=1$) or False (*negative*, $D=0$), we use sigmoid function instead of softmax function. The probability of a word ($c$) appearing within the context of the center word ($w$) can be defined as the following:\n",
    "\n",
    "<div id=\"eq-5\" style=\"font-size: 1rem;\">\n",
    "$$\n",
    "p(D=1|w,c;\\theta)=\\frac{1}{1+exp(-\\bar{c}_{output_{(j)}} \\cdot w)}\n",
    "\\in \\mathbb{R}^{1}\n",
    "\\tag{5} \n",
    "$$</div>\n",
    "\n",
    "where $c$ is the word you want to know whether it came from the context window or the noise distribution. $w$ is the input (center) word, and $\\theta$ is the <a href=\"https://aegis4048.github.io/demystifying_neural_network_in_skip_gram_language_modeling#theta_in_cost\" target=\"_blank\">weight matrix</a> passed into the model. Note that $w$ is equivalent to the hidden layer ($h$). $\\bar{c}_{output_{(j)}}$ is the word vector from the output weight matrix ($W_{output}$) of <a href=\"#fig1\">figure 1</a>. \n",
    "\n",
    "<a href=\"#eq-5\">Eq (5)</a> computes the probability that the given word ($c$) is a <a href=\"#pos_word\">positive word</a> ($D=1$). It only needs to be applied $K + 1$ times instead of $V$ times for every word in the vocabulary, because $\\bar{c}_{output_{(j)}}$ comes from the concatenation of a true context word ($c_{pos}$) and $K$ <a href=\"#neg_word\">negative words</a> ($\\bar{W}_{neg} = \\{ \\bar{c}_{neg, j}|j=1,\\cdots,K \\}$):\n",
    "\n",
    "<div id=\"eq-6\" style=\"font-size: 1rem;\">\n",
    "$$\n",
    "\\bar{c}_{output{(j)}} \\in \\{\\bar{c}_{pos}\\} \\cup \\bar{W}_{neg}\n",
    "\\tag{6} \n",
    "$$\n",
    "</div>\n",
    "\n",
    "This probability is computed $K + 1$ times to obtain a probability distribution of a true context word and $K$ negative samples:\n",
    "\n",
    "<div id=\"eq-7\" style=\"font-size: 1rem;\">\n",
    "$$\n",
    "\\left[ \\begin{array}{c} p(D=1|w,c_{pos}) \\\\ p(D=1|w,c_{neg, 1}) \\\\ p(D=1|w,c_{neg, 2}) \\\\ p(D=1|w,c_{neg, 3}) \\\\ \\vdots \\\\ p(D=1|w,c_{neg, K}) \\end{array} \\right] \n",
    "= \n",
    "\\frac{1}{1+exp(-(\\{\\bar{c}_{pos}\\} \\cup \\bar{W}_{neg}) \\cdot h)}\n",
    "\\in \\mathbb{R}^{K+1}\\tag{7} \n",
    "$$\n",
    "</div>\n",
    "\n",
    "Compare this equation with <a href=\"#eq-4\">eq (4)</a> — you will notice that <a href=\"#eq-7\">eq (7)</a> is **computationally much cheaper because $K$ is between 5 ~ 20, whereas $V$ can be millions. Moreover, no extra iterations are necessary to compute the normalization factor** in the denominator of <a href=\"#eq-4\">eq (4)</a>, because sigmoid function is a binary regression classifier. The algorithm complexity for probability distribution of vanilla Skip-Gram is $O(V)$, whereas negative sampling's is $O(K+1)$. This shows why negative sampling saves a significant amount of computational cost per iteration.\n",
    "\n",
    "<div id=\"sigmoid\" class=\"alert alert-info\">\n",
    "    <h4>Notes: Sigmoid function $\\sigma(x)$</h4>\n",
    "    <p>Sigmoid function is used for two-class logistic regression.</p>\n",
    "    <p><div style=\"font-size: 1rem; margin-top: 20px;\">$$ \\sigma(x) = \\frac{1}{1+exp(-x))}$$</div></p>\n",
    "    <p>It is used to classify if a given sample is True or False based on the computed probability. The sample is classified as True if the value is greater then 0.5, and vice versa. For example, if you want to classify if a certain bank transaction is fraud or not, you will use sigmoid for binary classification.</p>\n",
    "    <img class=\"admonition-image\" style=\"border: 1px solid #ddd;\" src=\"jupyter_images/sigmoid.png\"/>\n",
    "    <p>If you are working on a multi-class task, such as hand-written digit classification, you will use softmax regression classifier.</p>\n",
    "</div>\n",
    "\n",
    "<div style=\"display: none\">\n",
    "from matplotlib import pylab\n",
    "import pylab as plt\n",
    "import numpy as np\n",
    "%matplotlib notebook\n",
    "\n",
    "#sigmoid = lambda x: 1 / (1 + np.exp(-x))\n",
    "def sigmoid(x):\n",
    "    return (1 / (1 + np.exp(-x)))\n",
    "\n",
    "mySamples = []\n",
    "mySigmoid = []\n",
    "\n",
    "x = plt.linspace(-10,10,10)\n",
    "y = plt.linspace(-10,10,100)\n",
    "\n",
    "plt.plot(y, sigmoid(y))\n",
    "plt.title('Sigmoid Function $\\sigma(x)$')\n",
    "plt.text(4, 0.8, r'$\\sigma(x)=\\frac{1}{1+e^{-x}}$', fontsize=15)\n",
    "plt.xlabel('X', fontsize=12)\n",
    "plt.ylabel('$\\sigma(x)$', fontsize=14)\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div><hr></div>\n",
    "<div id=\"fig6\" class=\"row give-margin-inline-big-plot mobile_responsive_plot_full_width\">\n",
    "    <div class=\"col\"><img src=\"jupyter_images/negative_sample_text.png\"></div>\n",
    "    <div class=\"col-12\"><p class=\"image-description\">Figure 6: Choice of negative samples (Text source: <a href=\"https://petrowiki.org/Drilling_induced_formation_damage\" target=\"_blank\">Petrowiki</a>)</p></div>\n",
    "</div>\n",
    "\n",
    "For the purpose of illustration, consider the above paragraphs. Assume that our center word ($w$) is <code>drilling</code>, window size is $3$, and the number of negative samples ($K$) is $5$. With the window size of $3$, the contexts words are: *\"engineer\"*, *\"traditionally\"*, *\"designs\"*, *\"fluids\"*, *\"with\"*, and *\"two\"*. These context words are considered as positive labels ($D = 1$). Our current context word ($c_{pos}$) is <code>engineer</code>. We also need negative words. We randomly pick $5$-words from the <a href=\"#noise_dist\">noise distribution</a> $P_n(w)$ of the corpus for each context word, and consider them as negative samples ($D = 0$). For the current context word, <code>engineer</code>, the 5 randomly drawn negative words ($c_{neg}$) are: *\"minimized\"*, *\"primary\"*, *\"concerns\"*, *\"led\"*, and *\"page\"*. \n",
    "\n",
    "The idea of negative sampling is that it is more likely to observe positive word pairs ($w$, $c_{pos}$) together than negative word pairs ($w$, $c_{neg}$) together in the corpus. The model attempts to maximize the the probability of observing positive pairs $p(c_{pos}|w) \\rightarrow 1$ and minimize the probability of observing negative pairs $p(c_{neg}|w) \\rightarrow 0$ simultaneously by iterating through the training samples and updating the weights ($\\theta$). Note that the sum of the probability distribution obtained by sigmoid function (<a href=\"#eq-7\">eq (7)</a>) does not need to equal $1$, unlike softmax (<a href=\"#eq-4\">eq (4)</a>).\n",
    "\n",
    "<div id=\"fig7\" class=\"row\">\n",
    "    <div class=\"col\"><img src=\"jupyter_images/neg_opt_1.png\"></div>\n",
    "    <div class=\"col-12\"><p class=\"image-description\">Figure 7: maximizing postive pairs and minimizing negative pairs</p></div>\n",
    "</div>\n",
    "\n",
    "By the time the output probability distribution is nearly one-hot-encoded as in $iter = 4$ of the above figure, weight matrices $\\theta$ are optimized and good word vectors are learned. This optimization is achieved by maximizing the dot product of positive pairs ($\\bar{c}_{pos}\\cdot \\bar{w}$) and minimizing the dot product of negative pairs ($c_{neg}\\cdot w$) in <a href=\"#eq-18\">eq (18)</a>. \n",
    "\n",
    "<div id=\"negsample\" class=\"alert alert-info\">\n",
    "    <h4>Notes: Drawing random negative samples</h4>\n",
    "    <p>For each positive word-context pair ($w,\\,c_{pos}$), $K$ new negative samples are <a href=\"#neg_drawn\">randomly drawn</a> from a <a href=\"#noise_dist\">noise distribution</a>. In <a href=\"#fig6\">figure 6</a>, there are $6$ positive context words (<i>\"engineer\"</i>, <i>\"traditionally\"</i>, <i>\"designs\"</i>, <i>\"fluids\"</i>, <i>\"with\"</i>, and <i>\"two\"</i>) for one center word (<i>\"drilling\"</i>), and $K$ is $5$. This means that a total of $6 \\times 5 = 30$ word vectors are updated for each center word $w$.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=\"pos_word\"></div>\n",
    "\n",
    "#### What is a positive word $c_{pos}$?\n",
    "\n",
    "Words that actually appear within the context window of the center word ($w$). After the model is optimized, the probability computed with <a href=\"#eq-5\">eq (5)</a> for positive words $c_{pos}$ will output $\\approx$ 1 as shown in <a href=\"#fig7\">fig 7</a>. A word vector of a center word ($w$) will be more similar to a word vector of positive word ($c_{pos}$) than of randomly drawn negative words ($c_{neg}$). This is because words that frequently appear together show strong correlation with each other. Therefore, once the model is optimized: $p(D=1|w,c_{pos})\\approx1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=\"neg_word\"></div>\n",
    "\n",
    "#### What is a negative word $c_{neg}$?\n",
    "\n",
    "Words that are randomly drawn from a noise distribution $P_n(w)$. After the model is optimized, the probability computed with <a href=\"#eq-5\">eq (5)</a> for negative words $c_{neg}$ will output $\\approx$ 0 as shown in <a href=\"#fig7\">fig 7</a>. When training an Word2Vec model, the vocab size ($V$) easily exceeds tens of thousands. When 5 ~ 20 negative samples are randomly drawn among the vocabs, it is unlikely to observe the random word with a center word together in the corpus. Therefore, once the model is optimized: $p(D=1|w,c_{neg})\\approx0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=\"noise_dist\"></div>\n",
    "\n",
    "#### What is a noise distribution $P_n(w)$?\n",
    "\n",
    "Imagine a distribution of words based on how many times each word appeared in a corpus, denoted as $U(w)$ (this is called unigram distribution). For each word $w$, divide the number of times it appeared in a corpus by a normalization factor $Z$ so that the distribution becomes a probability distribution of range $[0, 1)$ and sums up to $1$. Raise the normalized distribution to the power of $\\alpha$ so that the distribution is \"smoothed-out\". Then this becomes your noise distribution $P_n(w)$ — normalized frequency distribution of words raised to the power of $\\alpha$. Mathematically, it can be expressed as:\n",
    "\n",
    "<div id=\"eq-8\" style=\"font-size: 1rem;\">\n",
    "$$\n",
    "P_n(w) = \\left(\\frac{U(w)}{Z}\\right)^{\\alpha}\n",
    "\\tag{8} \n",
    "$$</div>\n",
    "\n",
    "Raising the unigram distribution $U(w)$ to the power of $\\alpha$ has an effect of smoothing out the distribution. **It attempts to combat the imbalance between common words and rare words** by decreasing the probability of drawing common words, and increasing the probability drawing rare words. \n",
    "\n",
    "$\\alpha$ is a hyper-parameter that can be empirically tuned. The authors of the original Word2Vec <a href=\"https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf\" target=\"_blank\">paper</a> claims that the unigram distribution $U(w)$ raised to the $3/4$rd power (i.e., $U(w)^{3/4}/Z$) yielded the best result.\n",
    "\n",
    "<div id=\"fig8\" class=\"row\">\n",
    "    <div class=\"col\"><img src=\"jupyter_images/noise_dist.png\"></div>\n",
    "    <div class=\"col-12\"><p class=\"image-description\">Figure 8: Effect of raising power of unigram distribution $U(w)$</p></div>\n",
    "</div>\n",
    "\n",
    "<div id=\"incremental_noise_dist\" class=\"alert alert-info\">\n",
    "    <h4>Notes: Incremental noise distribution</h4>\n",
    "    <p>Noise distribution $P_n(w)$ needs to be pre-computed by iterating through the entire corpus to obtain word frequency $U(w)$ and normalization factor $Z$ of the distribution. If an additional training data is added to the corpus, $P_n(w)$ needs to be computed all over again. To address this problem, a simple incremental extension of negative sampling was provided in this <a href=\"https://aclweb.org/anthology/D17-1037\" target=\"_blank\">paper</a> (Kaji and Kobayashi, 2017).</p>\n",
    "</div>\n",
    "\n",
    "<div style=\"display: none\">\n",
    "    \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "%matplotlib notebook\n",
    "\n",
    "# sample data generation\n",
    "data = sorted(stats.norm.rvs(size=1000) + 5)\n",
    "\n",
    "# fit normal distribution\n",
    "mean, std = stats.norm.fit(data, loc=0)\n",
    "pdf_norm = stats.norm.pdf(data, mean, std)\n",
    "\n",
    "temp = np.power(data, 3/4)\n",
    "temp_mean, temp_std = stats.norm.fit(temp, loc=0)\n",
    "temp_pdf_norm = stats.norm.pdf(temp, temp_mean, temp_std)\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(8, 4), sharey=True)\n",
    "ax1.set_title('$U(w)$')\n",
    "ax1.hist(temp, bins='auto', density=True)\n",
    "ax1.set_xticklabels(['apple', 'desk', 'cup', 'chair', 'zebra', 'room', 'pencil', 'water', 'coin'])\n",
    "ax1.plot(temp, temp_pdf_norm, label='norm')\n",
    "ax2.set_title('$U(w)^{3/4}$')\n",
    "ax2.hist(data, bins='auto', density=True)\n",
    "ax2.plot(data, pdf_norm, label='norm')\n",
    "ax2.set_xticklabels(['apple', 'desk', 'cup', 'chair', 'zebra', 'room', 'pencil', 'water', 'coin'])\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=\"neg_drawn\"></div>\n",
    "#### How are negative samples drawn?\n",
    "\n",
    "$K$-negative samples are randomly drawn from a noise distribution $P_n(w)$. The noise distribution is generated with <a href=\"#eq-8\">eq (8)</a> and the random samples are drawn with <a href=\"https://docs.scipy.org/doc/numpy/reference/generated/numpy.random.choice.html\" target=\"_blank\">np.random.choice</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unig_dist  = {'apple': 0.023, 'bee': 0.12, 'desk': 0.34, 'chair': 0.517}\n",
    "\n",
    "sum(unig_dist.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'apple': 0.044813853132981724,\n",
       " 'bee': 0.15470428538870049,\n",
       " 'desk': 0.33785130228003507,\n",
       " 'chair': 0.4626305591982827}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alpha      = 3 / 4\n",
    "\n",
    "noise_dist = {key: val ** alpha for key, val in unig_dist.items()}\n",
    "Z = sum(noise_dist.values())\n",
    "noise_dist_normalized = {key: val / Z for key, val in noise_dist.items()}\n",
    "noise_dist_normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(noise_dist_normalized.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"margin-top: -25px;\"><hr></hr></div>\n",
    "\n",
    "In the initial unigram distribution, <code>chair</code> appeared the most in the corpus, and had <code>0.517</code> chance of being drawn as a negative sample. However, the unigram distribution is raised to the power of $3/4$rd to combat the imbalance between common vs rare words, as shown in <a href=\"#fig8\">figure 8</a>. After <code>unig_dist</code> was raised to the power of <code>alpha = 3/4</code> and normalized, <code>chair</code> now has <code>0.463</code> chance of being drawn. \n",
    "\n",
    "On the other hand, <code>apple</code> had the lowest probability (<code>0.023</code>) of being drawn. After the transformation, it now has a bit higher probability (<code>0.049</code>) of being drawn. The imbalance between the most common word (<code>chair</code>) and the least common word (<code>apple</code>) was mitigated. \n",
    "\n",
    "Once the noise distribution (<code>noise_dist_normalized</code>) is generated, you randomly draw $K$ negative samples according to each word's probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['apple', 'chair', 'bee', 'desk', 'chair', 'bee', 'bee', 'chair',\n",
       "       'desk', 'chair'], dtype='<U5')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "K = 10\n",
    "\n",
    "np.random.choice(list(noise_dist_normalized.keys()), size=K, p=list(noise_dist_normalized.values()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Derivation of Cost Function in Negative Sampling\n",
    "\n",
    "The derivations written here are based on the work of <a href=\"https://arxiv.org/pdf/1402.3722.pdf\" target=\"_blank\">word2vec Explained: Deriving Mikolov et al.’s\n",
    "Negative-Sampling Word-Embedding Method</a> (Goldberg and Levy, 2014).\n",
    "\n",
    "Consider a pair ($w,\\,c$) of center word and its context. Did this pair come from the context window or the <a href=\"#noise_dist\">noise distribution</a>? Let $p(D=1|w,c)$ be the probability that ($w,\\,c$) is observed in the true corpus, and $p(D=0|w,c) = 1 - p(D=1|w,c)$ the probability that ($w,\\,c$) is non-observed. There are <a href=\"https://aegis4048.github.io/demystifying_neural_network_in_skip_gram_language_modeling#theta_in_cost\" target=\"_blank\">parameters</a> $\\theta$ controlling the probability distribution: $p(D=1|w,c;\\theta)$.\n",
    "\n",
    "Negative sampling attempts to optimize the parameters $\\theta$ by maximizing the probability of observing positive pairs ($w,\\,c_{pos}$) while minimizing the probability of observing negative pairs ($w,\\,c_{neg}$). For each positive pair, the model randomly draws $K$ negative words $W_{neg} = \\{ c_{neg, j}|j=1,\\cdots,K \\}$. Our objective function is then:\n",
    "\n",
    "<div id=\"eq-9\" style=\"font-size: 1rem;\">\n",
    "$$\n",
    "\\begin{align}\n",
    "     & \\underset{\\theta}{\\text{argmax}} \\,\\,\\, p(D=1|w,c_{pos};\\theta) \\prod_{c_{neg} \\in W_{neg}} p(D=0|w,c_{neg};\\theta) \\label{}\\tag{9}\\\\ \n",
    "   =\\quad & \\underset{\\theta}{\\text{argmax}} \\,\\,\\, p(D=1|w,c_{pos};\\theta) \\prod_{c_{neg} \\in W_{neg}} (1 - p(D=1|w,c_{neg};\\theta))\\label{}\\tag{10}\n",
    "\\end{align}\n",
    "$$</div>\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "    <h4>Notes: Probability Product</h4>\n",
    "    <p>In statistics, probability of observing $C$ multiple events at the same time is computed by the product of each event's probability.</p>\n",
    "    <p><center style=\"font-size: 1rem; margin-top: 20px;\">$$p(x_{1}, x_{2} ... x_{C}) = p(x_{1}) \\times p(x_{2}) \\, \\times \\, ... \\, \\times \\, p(x_{C})$$</center></p>\n",
    "    <p>This can be shortened with a product notation:</p>\n",
    "    <p><center style=\"font-size: 1rem; margin-top: 20px;\">$$p(x_{1}, x_{2} ... x_{C}) = \\prod_{c=1}^{C}p(x_{c})$$</center></p>\n",
    "</div>\n",
    "\n",
    "It is a common practice in machine learning to <a href=\"https://aegis4048.github.io/demystifying_neural_network_in_skip_gram_language_modeling#take_natural_log\" target=\"_blank\">take a natural log to the objective function</a> to simplify derivations. This does not affect the optimized weights ($\\theta$) because natural log is a monotonically increasing function. It ensures that the maximum value of the original probability function occurs at the same point as the log probability function. Therefore:\n",
    "\n",
    "<div id=\"eq-11\" style=\"font-size: 1rem;\">\n",
    "$$\n",
    "\\begin{align}\n",
    "   =\\quad  & \\underset{\\theta}{\\text{argmax}} \\,\\,\\, log \\, p(D=1|w,c_{pos};\\theta) \\prod_{c_{neg} \\in W_{neg}} (1 - p(D=1|w,c_{neg};\\theta)) \\label{}\\tag{11}\n",
    "\\end{align}\n",
    "$$</div>\n",
    "\n",
    "Using the property of logs, the objective function can be simplified:\n",
    "\n",
    "<div id=\"eq-12\" style=\"font-size: 1rem;\">\n",
    "$$\n",
    "\\begin{align}\n",
    "   =\\quad  & \\underset{\\theta}{\\text{argmax}} \\,\\,\\, log \\, p(D=1|w,c_{pos};\\theta) + log \\, \\prod_{c_{neg} \\in W_{neg}} (1 - p(D=1|w,c_{neg};\\theta)) \\label{}\\tag{12}\\\\ \n",
    "   =\\quad  & \\underset{\\theta}{\\text{argmax}} \\,\\,\\, log \\, p(D=1|w,c_{pos};\\theta) + \\sum_{c_{neg} \\in W_{neg}} log \\, (1 - p(D=1|w,c_{neg};\\theta)) \\label{}\\tag{13}\n",
    "\\end{align} \n",
    "$$</div>\n",
    "\n",
    "The binomial probability $p(D=1|w,c;\\theta)$ can be replaced with <a href=\"#eq-5\">eq (5)</a>:\n",
    "\n",
    "<div id=\"eq-14\" style=\"font-size: 1rem;\">\n",
    "$$\n",
    "\\begin{align}\n",
    "   =\\quad  & \\underset{\\theta}{\\text{argmax}} \\,\\,\\, log \\, \\frac{1}{1+exp(-\\bar{c}_{pos} \\cdot \\bar{w})} + \\sum_{c_{pos} \\in W_{neg}} log \\, (1 - \\frac{1}{1+exp(-\\bar{c}_{neg} \\cdot \\bar{w})}) \\label{}\\tag{14}\\\\ \n",
    "   =\\quad  & \\underset{\\theta}{\\text{argmax}} \\,\\,\\, log \\, \\frac{1}{1+exp(-\\bar{c}_{pos} \\cdot \\bar{w})} + \\sum_{c_{pos} \\in W_{neg}} log \\, \\frac{1}{1+exp(\\bar{c}_{neg} \\cdot \\bar{w})} \\label{}\\tag{15}\\\\ \n",
    "\\end{align}\n",
    "$$</div>\n",
    "\n",
    "Using the definition of <a href=\"#sigmoid\">sigmoid</a> function $\\sigma(x)=\\frac{1}{1+exp(-x)}$:\n",
    "\n",
    "<div id=\"eq-16\" style=\"font-size: 1rem;\">\n",
    "$$\n",
    "\\begin{align}\n",
    "   =\\quad  & \\underset{\\theta}{\\text{argmax}} \\,\\,\\, log \\, \\sigma(\\bar{c}_{pos} \\cdot \\bar{w}) + \\sum_{c_{pos} \\in W_{neg}} log \\, \\sigma(-\\bar{c}_{neg} \\cdot \\bar{w}) \\label{}\\tag{16}  \n",
    "\\end{align} \n",
    "$$</div>\n",
    "\n",
    "According to Mikolov, <a href=\"#eq-16\">eq (16)</a> replaces every $log \\, p(w_O|w_I)$ in the vanilla Skip-Gram cost function defined in <a href=\"#eq-1\">eq (1)</a>. Then, the cost function we want to minimize becomes: \n",
    "\n",
    "<div id=\"eq-17\" style=\"font-size: 1rem;\">$$J(\\theta) = - \\frac{1}{T} \\sum_{i = 1}^T \\sum_{-c\\leq j \\leq c,j\\neq 0}( log \\, \\sigma(\\bar{c}_{pos} \\cdot \\bar{w}) + \\sum_{c_{neg} \\in W_{neg}} log \\, \\sigma(-\\bar{c}_{neg} \\cdot \\bar{w})) \\tag{17}$$</div>\n",
    "\n",
    "However, when implemented in codes, batch gradient descent (making one update after iterating through the entire $T$ corpus) is almost never used due to its high computational cost. Instead, we use <a href=\"https://aegis4048.github.io/demystifying_neural_network_in_skip_gram_language_modeling#stochastic\" target=\"_blank\">stochastic gradient descent</a>. Also, for negative sampling, gradients are calculated and weights are updated for each positive training pairs ($w,\\, c_{pos}$). In the other words, one update for each word within the context window of a center word $w$. This is shown <a href=\"#Negative-Sampling-Algorithm\">below</a>. The new cost function is then:\n",
    "\n",
    "<div id=\"eq-18\" style=\"font-size: 1rem;\">$$J(\\theta; w,c_{pos}) = - log \\, \\sigma(\\bar{c}_{pos} \\cdot \\bar{w}) - \\sum_{c_{neg} \\in W_{neg}} log \\, \\sigma(-\\bar{c}_{neg} \\cdot \\bar{w}) \\tag{18}$$</div>\n",
    "\n",
    "Note that $w$ is a word vector for an input word, and that it is equivalent to a <a href=\"https://aegis4048.github.io/demystifying_neural_network_in_skip_gram_language_modeling#hidden_layer\" target=\"_blank\">hidden layer</a> ($w = h$). For clarification: \n",
    "\n",
    "<div id=\"eq-19\" style=\"font-size: 1rem;\">$$J(\\theta; w,c_{pos}) = - log \\, \\sigma(\\bar{c}_{pos} \\cdot h) - \\sum_{c_{neg} \\in W_{neg}} log \\, \\sigma(-\\bar{c}_{neg} \\cdot h)\\tag{19}$$</div>\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "    <h4>Notes: Notations used in different papers</h4>\n",
    "    <p>Different papers use different notations. In <a href=\"https://arxiv.org/pdf/1411.2738.pdf\" target=\"_blank\">word2vec Parameter Learning Explained</a> (Rong, 2014):</p>\n",
    "    <p>$$J(\\theta; w_I, w_O) = - log \\, \\sigma(v^{'}_{w_{O}} \\cdot h) - \\sum_{w_j \\in W_{neg}} log \\, \\sigma(-v^{'}_{w_{j}} \\cdot h)$$</p>\n",
    "    <p>where $w_I$ is the input (center) word in the corpus. $w_{O}$ is a word found in the context window of $w_I$ and is a positive word. $v^{'}$ is a word vector in the output weight matrix ($v^{'} \\in W_{output}$), $w_j$ is a randomly drawn negative word from the <a href=\"#noise_dist\">noise distribution</a>, and $v^{'}_{w_{j}}$ is a $j$-th word vector in $W_{output}$ that corresponds to the negative word $w_j$. $h$ is a <a href=\"https://aegis4048.github.io/demystifying_neural_network_in_skip_gram_language_modeling#hidden_layer\" target=\"_blank\">hidden layer</a>.</p>\n",
    "    <p>In the original Word2Vec paper, <a href=\"https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf\" target=\"_blank\">Distributed Representations of Words and Phrases and their Compositionality</a> (Mikolov et al., 2013):</p>\n",
    "    <p>$$J(\\theta; w_I, w_O) = - log \\, \\sigma(v^{'}_{w_{O}} \\top v_I) - \\sum^k_{i=1} \\mathbb{E}_{w_{i} \\sim P_n(w)}\\big[ log \\, \\sigma(-v^{'}_{w_{i}} \\top v_I) \\big]$$</p>\n",
    "    <p>$k$ is the number of negative samples and $w_i$ is an $i$-th negative word drawn from the noise distribution $P_n(w)$. $v_{w_{I}}$ is a word vector in the input weight matrix $W_{input}$ for the input word $w_I$ and is equivalent to the hidden layer $h$. These are all equivalent to <a href=\"#eq-19\">eq (19)</a>.</p>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Derivation of gradients\n",
    "\n",
    "The goal of any machine learning model is to find the optimal values of a weight matrix ($\\theta$) to minimize prediction error. A general update equation for weight matrix looks like the following: \n",
    "\n",
    "<div style=\"font-size: 1rem;\" id=\"eq-20\">$$ \\theta^{(new)}=\\theta^{(old)}-\\eta\\cdot\\frac{\\partial J}{\\partial \\theta} \\tag{20}$$</div>\n",
    "\n",
    "$\\theta$ is a parameter that needs to be optimized, and $\\eta$ is a learning rate. In negative sampling, we take the derivative to the cost function $J(\\theta; w, c_{pos})$ defined in <a href=\"#eq-19\">eq (19)</a> with respect to $\\theta$. Note that the derivative of a sigmoid function is $\\frac{\\partial \\sigma}{\\partial x} = \\sigma(x)(1–\\sigma(x))$.\n",
    "\n",
    "<div style=\"font-size: 1rem;\" id=\"eq-21\">$$ \\frac{\\partial J}{\\partial \\theta} = (\\sigma(\\bar{c}_{pos} \\cdot h)) - 1)\\frac{\\partial \\bar{c}_{pos} \\cdot h}{\\partial \\theta} + \\sum_{c_{neg} \\in W_{neg}} \\sigma(\\bar{c}_{neg} \\cdot h) \\frac{\\partial \\bar{c}_{neg} \\cdot h}{\\partial \\theta} \\tag{21}$$</div>\n",
    "\n",
    "Since the parameter $\\theta$ is a <a href=\"https://aegis4048.github.io/demystifying_neural_network_in_skip_gram_language_modeling#theta_in_cost\">concatenation</a> of the input and output weight matrix $[W_{input} \\quad W_{output}]$, the cost function needs to be differentiated with respect to the both matrices — $[\\frac{\\partial J}{\\partial W_{input}} \\quad \\frac{\\partial J}{\\partial W_{output}}]$.\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "    <h4>Notes: Clarification on notation</h4>\n",
    "    <p>$\\bar{c}$ represents a word vector in the output weight matrix ($\\bar{c} \\in W_{output}$) for a context word. The context word can be a positive word ($c_{pos}$) from a context window or a negative word ($c_{neg} \\in W_{neg}$) from a <a href=\"#noise_dist\">noise distribution</a>.</p>\n",
    "    <p>$h$ is a <a href=\"https://aegis4048.github.io/demystifying_neural_network_in_skip_gram_language_modeling#hidden_layer\" target=\"_blank\">hidden layer</a>. Recall that the hidden layer is essentially a word vector for the input word that is <i>looked up</i> from the input weight matrix $W_{input}$.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=\"grad_W_output\"></div>\n",
    "\n",
    "**Gradients with respect to output weight matrix $\\frac{\\partial J}{\\partial W_{output}}$**\n",
    "\n",
    "With negative sampling, we do not update the entire output weight matrix $W_{output}$, but only a fraction of it. We update $K + 1$ word vectors in the output weight matrix — $\\bar{c}_{pos}$, $\\bar{c}_{neg,1},\\,...\\,\\bar{c}_{neg,K}$. We take partial derivatives to the cost function defined in <a href=\"#eq-19\">eq (19)</a> with respect to positive words and negative words. This can be done by replacing $\\theta$ in <a href=\"#eq-21\">eq (21)</a> with $\\bar{c}_{pos}$ and $\\bar{c}_{neg}$ each.\n",
    "\n",
    "<div id=\"eq-22\" style=\"font-size: 1rem;\">\n",
    "$$\n",
    "\\begin{align}\n",
    "   \\frac{\\partial J}{\\partial \\bar{c}_{pos}} &= (\\sigma(\\bar{c}_{pos} \\cdot h) - 1)\\cdot h \\label{}\\tag{22}\\\\[5pt]\n",
    "   \\frac{\\partial J}{\\partial \\bar{c}_{eng}} &= \\sigma(\\bar{c}_{neg} \\cdot h)\\cdot h \\label{}\\tag{23}\n",
    "\\end{align}\n",
    "$$</div>\n",
    "\n",
    "The update equations are then:\n",
    "\n",
    "<div id=\"eq-24\" style=\"font-size: 1rem;\">\n",
    "$$\n",
    "\\begin{align}\n",
    "   \\bar{c}_{pos}^{(new)} &= \\bar{c}_{pos}^{(old)} - \\eta \\cdot (\\sigma(\\bar{c}_{pos} \\cdot h) - 1)\\cdot h \\label{}\\tag{24}\\\\[6pt]\n",
    "   \\bar{c}_{neg}^{(new)} &= \\bar{c}_{neg}^{(old)} - \\eta \\cdot \\sigma(\\bar{c}_{neg} \\cdot h) \\label{}\\tag{25}\n",
    "\\end{align}\n",
    "$$</div>\n",
    "\n",
    "The gradients for positive and negative words can be merged for brevity:\n",
    "\n",
    "<div style=\"font-size: 1rem;\" id=\"eq-26\">$$ \\bar{c}_{j}^{(new)} = \\bar{c}_{j}^{(old)}-\\eta\\cdot (\\sigma(\\bar{c}_{j} \\cdot h) - t_j) \\cdot h \\tag{26}$$</div>\n",
    "\n",
    "where $t_j = 1$ for positive words ($c_j = c_{pos}$) and $t_j = 0$ for negative words ($c_j = c_{neg} \\in W_{neg}$). $\\bar{c}_{j}$ is the $j$-th word vector in the output word matrix ($\\bar{c}_j \\in W_{output}$). For each positive pairs ($w$, $c_{pos}$), <a href=\"#eq-26\">eq (26)</a> is applied to $K + 1$ word vectors in $W_{output}$ as shown in <a href=\"#fig4\">figure 4</a>.\n",
    "\n",
    "<div id=\"pred_error\"></div>\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "    <h4>Notes: Prediction Error</h4>\n",
    "    <p>In <a href=\"#eq-26\">eq (26)</a>, $\\sigma(\\bar{c}_{j} \\cdot h) - t_j$ is called a <i>prediction error</i>. Recall that negative sampling attempts to maximize the the probability of observing positive pairs $p(c_{pos}|w) \\rightarrow 1$ while minimizing the probability of observing negative pairs $p(c_{neg}|w) \\rightarrow 0$. If good word vectors are learned, $\\sigma(\\bar{c}_{pos}\\cdot h) \\approx 1$ for positive pairs, and $\\sigma(\\bar{c}_{neg}\\cdot h) \\approx 0$ for negative pairs as shown in <a href=\"#fig7\">figure 7</a>.<p> \n",
    "    <p>The prediction error will gradually approach zero $\\sigma(\\bar{c}_{pos}\\cdot h) - t_j \\approx 0$, as the model iterates through the training samples (positive pairs) and optimizes the weights.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=\"grad_W_input\"></div>\n",
    "\n",
    "**Gradients with respect to input weight matrix $\\frac{\\partial J}{\\partial W_{input}}$**\n",
    "\n",
    "Just like vanilla Skip-Gram, only one word vector that corresponds to the input word $w$ in $W_{input}$ is updated with negative sampling. This is because the <a href=\"https://aegis4048.github.io/demystifying_neural_network_in_skip_gram_language_modeling#input_layer\" target=\"_blank\">input layer is an one-hot-encoded vector</a> (however, the equation for the gradient descent is different.) Therefore, taking the derivative for the input weight matrix is equivalent to taking the derivative to the hidden layer ($\\frac{\\partial J}{\\partial W_{input}} = \\frac{\\partial J}{\\partial h}$). We replace $\\theta$ in <a href=\"#eq-21\">eq (21)</a> with $h$, and differentiate it:\n",
    "\n",
    "<div id=\"eq-27\" style=\"font-size: 1rem;\">\n",
    "$$\n",
    "\\begin{align}\n",
    "   \\frac{\\partial J}{\\partial h} &= (\\sigma(\\bar{c}_{pos} \\cdot h) - 1) \\cdot \\bar{c}_{pos} + \\sum_{c_{eng} \\in W_{neg}} \\sigma(\\bar{c}_{neg} \\cdot h) \\cdot \\bar{c}_{neg} \\label{}\\tag{27}\\\\\n",
    "   &= \\sum_{c_j \\in \\{c_{pos}\\} \\cup W_{neg}} (\\sigma(\\bar{c}_{j} \\cdot h) - t_j) \\cdot \\bar{c}_{j} \\label{}\\tag{28}\n",
    "\\end{align}\n",
    "$$</div>\n",
    "\n",
    "Same as <a href=\"#eq-26\">eq (26)</a>, $t_j = 1$ for positive words ($c_j = c_{pos}$) and $t_j = 0$ for negative words ($c_j = c_{neg} \\in W_{neg}$). The update equation is then:\n",
    "\n",
    "<div style=\"font-size: 1rem;\" id=\"eq-29\">$$ \\bar{w}^{(new)} = \\bar{w}^{(old)}-\\eta\\cdot \\sum_{c_j \\in \\{c_{pos}\\} \\cup W_{neg}} (\\sigma(\\bar{c}_{j} \\cdot h) - t_j) \\cdot \\bar{c}_{j} \\tag{29}$$</div>\n",
    "\n",
    "Recall that $w$ is a word vector in the input weight matrix ($\\bar{w} \\in W_{input}$) and $\\bar{c}_j$ is a $j$-th word vector in the output weight matrix ($\\bar{c}_j \\in W_{output}$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=\"algorithm\"></div>\n",
    "\n",
    "## Negative Sampling Algorithm\n",
    "\n",
    "For each positive word-context pair ($w, c_{pos}$), $1$ word vector that corresponds to the center word ($w$) is updated in the input weight matrix $W_{input}$ as shown in <a href=\"#fig15\">figure 15</a>. In the output weight matrix $W_{output}$, $1+K$ word vectors that correspond to the positive and negative words ($\\{\\bar{c}_{pos}\\} \\cup \\bar{W}_{neg} \\in \\mathbb{R}^{1+K} $) are updated as shown in <a href=\"#fig4\">figure 4</a>. \n",
    "\n",
    "Since Skip-Gram uses SGD to reduce computational cost, negative sampling also uses SGD too. Then, the training for Skip-Gram negative sampling has the following algorithm structure:\n",
    "\n",
    "<div id=\"sgns_algorithm\"></div>\n",
    "<p><u>Algorithm 1: Skip-Gram Negative Sampling</u></p>\n",
    "<pre>\n",
    "    <code class=\"language-python\">\n",
    "        P_nw = # generate noise distribution\n",
    "        for word in corpus:\n",
    "            for context in context_window:\n",
    "\n",
    "                # draw K negative samples from P_nw\n",
    "                W_neg = np.random.choice(Pn_w.keys(), size=K, p=Pn_w.values())\n",
    "\n",
    "                # compute gradients. w is a input word vector = hidden layer\n",
    "                grad_V_output_pos = (sigmoid(c_pos * h) - 1) * w\n",
    "                grad_V_input = (sigmoid(c_pos * h) - 1) * c_pos\n",
    "                grad_V_output_neg_list = []\n",
    "                for c_neg in W_neg:\n",
    "                    grad_V_output_neg_list.append(sigmoid(c_neg * h) * h)\n",
    "                    grad_V_input += sigmoid(c_neg * h) * c_neg\n",
    "\n",
    "                # use SGD to update w, c_pos, and c_neg_1, ... , c_neg_K\n",
    "                V_output_pos = V_output_pos - alpha * grad_V_output_pos\n",
    "                V_input = V_input - alpha * grad_V_input\n",
    "                for grad_V_output_neg in grad_V_output_neg_list:\n",
    "                    V_output_neg = V_output_neg - alpha * grad_V_output_neg\n",
    "    </code>\n",
    "</pre>\n",
    "\n",
    "The Python implementation of negative sampling here is based on the interpretation of *Algorithm 1 SGNS Word2Vec* in <a href=\"https://www.aaai.org/ocs/index.php/AAAI/AAAI17/paper/viewFile/14956/14446\" target=\"_blank\">Distributed Negative Sampling for Word Embeddings</a> (Stergiou et al., 2017).\n",
    "\n",
    "In vanilla Skip-Gram, one update is made for the entire weight matrices $[W_{input} \\quad W_{output}]$ for each input word. Each update involves summing up dot products for all context words within the context window of size $C$ as shown in eq (20) and eq (21) of my <a href=\"https://aegis4048.github.io/demystifying_neural_network_in_skip_gram_language_modeling#eq-20\" target=\"_blank\">previous post</a>. \n",
    "\n",
    "In negative sampling, $C$ updates are made for a fraction of the weights for each input word. This is because negative sampling treats each positive pair ($w$, $c_{pos}$) as one training sample, whereas vanilla Skip-Gram treats a center word ($w$) and its $C$ neighboring context words ($c_{pos, 1}$, $...$, $c_{pos,C}$) all together as one training sample for SGD."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numerical Demonstration\n",
    "\n",
    "For the ease of illustration, screenshots from Excel will be used to demonstrate the concept of updating weight matrices through forward and backward propagations.\n",
    "\n",
    "**Description of the Corpus**\n",
    "\n",
    "Assume that the training corpus is the entire text in the book, \"<a href=\"https://en.wikipedia.org/wiki/A_Song_of_Ice_and_Fire\" target=\"_black\">A Song of Ice and Fire</a>.\" Unlike the simple one-sentence training corpus used in my <a href=\"https://aegis4048.github.io/demystifying_neural_network_in_skip_gram_language_modeling#window_id\" target=\"_blank\">previous post</a>, the training corpus needs to be much bigger to illustrate negative sampling, because we need to randomly draw negative samples that are unlikely to be observed in a pair with a center word. The sentence that has the current center word is *\"Ned Stark is the most honorable man\"*. \n",
    "\n",
    "Center (input) word is <code>Ned</code>, and window size is $C = 2$, making <code>Stark</code> and <code>is</code> context words. Number of negative samples drawn from the <a href=\"#noise_dist\">noise distribution</a> for each positive pair is $K = 3$.\n",
    "\n",
    "<div class=\"row give-margin-inline-big-plot mobile_responsive_plot_full_width\">\n",
    "    <div class=\"col\"><img src=\"jupyter_images/neg_training.png\"></div>\n",
    "    <div class=\"col-12\"><p class=\"image-description\">Figure 9: Training corpus for negative sampling</p></div>\n",
    "</div>\n",
    "\n",
    "For your information, Ned Stark is a fictional character from the book, *A Song of Ice and Fire*. The book also has a TV show adaptation, known as the *Game of Thrones* (GoT). Ned Stark is a noble lord of his land and has a reputation of being the most honorable man in the kingdom. \n",
    "\n",
    "So here's the idea. The center word <code>Ned</code> will be observed in a pair with context words (postive) like <code>Stark</code>, because it is his last name. The same thing goes for <code>is</code> too, because <code>is</code> is a verb tense used to describe a singular object. \n",
    "\n",
    "However, <code>Ned</code> most likely won't be observed in a pair with random words (negative) like <code>pimples</code>, <code>zebra</code>, <code>donkey</code> within the book. If the model can differentiate between positive pairs and negative pairs as shown in <a href=\"#fig5\">figure 5</a>, good word vectors will be learned. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Positive word pair: (<code>Ned</code>, <code>Stark</code>)\n",
    "\n",
    "Recall that in negative sampling, one update is made for each of the positive training pairs. This means that $C$ weight updates are made for each input (center) word, where $C$ is the window size. \n",
    "\n",
    "Our current positive word pair is (<code>Ned</code>, <code>Stark</code>). For the current positive pair, we <a href=\"#neg_drawn\">randomly draw</a> $K=3$ negative words from the noise distribution: <code>pimples</code>, <code>zebra</code>, <code>idiot</code>\n",
    "\n",
    "**Forward Propagation: Computing hidden (projection) layer**\n",
    "\n",
    "Hidden layer ($h$) is <i>looked up</i> from $W_{input}$ by multiplying the <a href=\"https://aegis4048.github.io/demystifying_neural_network_in_skip_gram_language_modeling#input_layer\" target=\"_blank\">one-hot-encoded input vector</a> with the <a href=\"https://aegis4048.github.io/demystifying_neural_network_in_skip_gram_language_modeling#weight_matrix\" target=\"_blank\">input weight matrix</a> $W_{input}$.\n",
    "\n",
    "<div class=\"row\">\n",
    "    <div class=\"col\"><img src=\"jupyter_images/neg_hidden.png\"></div>\n",
    "    <div class=\"col-12\"><p class=\"image-description\">Figure 10: Computing hidden (projection) layer</p></div>\n",
    "</div>\n",
    "\n",
    "**Forward Propagation: Sigmoid output layer**\n",
    "\n",
    "Output layer is a probability distribution of positive and negative words ($c_{pos} \\cup W_{neg}$), given a center word ($w$). It is computed with <a href=\"#eq-7\">eq (7)</a>. Recall that <a href=\"#sigmoid\">sigmoid function</a> has $\\sigma(x) = \\frac{1}{1+exp(-x)}$.\n",
    "\n",
    "<div class=\"row\">\n",
    "    <div class=\"col\"><img src=\"jupyter_images/neg_forward_2.png\"></div>\n",
    "    <div class=\"col-12\"><p class=\"image-description\">Figure 11: Sigmoid output layer</p></div>\n",
    "</div>\n",
    "\n",
    "**Backward Propagation: Prediction Error**\n",
    "\n",
    "The details about the prediction error is described <a href=\"#pred_error\">above</a>. Since our current positive word is <code>Stark</code>, $t_j = 1$ for <code>Stark</code> and $t_j=0$ for other negative words (<code>pimples</code>, <code>zebra</code>, <code>idiot</code>).\n",
    "\n",
    "<div class=\"row\">\n",
    "    <div class=\"col\"><img src=\"jupyter_images/neg_backward_1.png\"></div>\n",
    "    <div class=\"col-12\"><p class=\"image-description\">Figure 12: Prediction errors of positive and negative words</p></div>\n",
    "</div>\n",
    "\n",
    "**Backward Propagation: Computing $\\nabla W_{input}$**\n",
    "\n",
    "Gradients of input weight matrix ($\\frac{\\partial J}{\\partial W_{input}}$) are computed using <a href=\"#eq-27\">eq (28)</a>. Just like vanilla Skip-Gram, only the word vector in the input weight matrix $W_{input}$ that corresponds to the input (center) word $w$ is updated.\n",
    "\n",
    "<div class=\"row\">\n",
    "    <div class=\"col\"><img src=\"jupyter_images/neg_backward_2.png\"></div>\n",
    "    <div class=\"col-12\"><p class=\"image-description\">Figure 13: Computing input weight matrix gradient $\\nabla W_{input}$</p></div>\n",
    "</div>\n",
    "\n",
    "**Backward Propagation: Computing $\\nabla W_{output}$**\n",
    "\n",
    "With negative sampling, only a fraction of word vectors in the output weight matrix $W_{output}$ is updated. Gradients for $K + 1$ word vectors for positive and negative words in the $W_{output}$ are computed using <a href=\"#eq-22\">eq (22)</a> and <a href=\"#eq-22\">eq (23)</a>. Recall that $K$ is the number of negative samples drawn from a noise distribution, and that $K = 3$ in our example.\n",
    "\n",
    "<div class=\"row\">\n",
    "    <div class=\"col\"><img src=\"jupyter_images/neg_backward_3.png\"></div>\n",
    "    <div class=\"col-12\"><p class=\"image-description\">Figure 14: Computing output weight matrix gradient $\\nabla W_{output}$</p></div>\n",
    "</div>\n",
    "\n",
    "**Backward Propagation: Updating Weight matrices**\n",
    "\n",
    "Input and output weight matrices ($[W_{input} \\quad W_{output}]$) are updated using <a href=\"#eq-26\">eq (26)</a> and <a href=\"#eq-29\">eq (29)</a>. \n",
    "\n",
    "<div id=\"fig15\" class=\"row\">\n",
    "    <div class=\"col\"><img src=\"jupyter_images/neg_backward_4.png\"></div>\n",
    "    <div class=\"col-12\"><p class=\"image-description\">Figure 15: Updating $W_{input}$</p></div>\n",
    "</div>\n",
    "\n",
    "<div class=\"row\">\n",
    "    <div class=\"col\"><img src=\"jupyter_images/neg_backward_5.png\"></div>\n",
    "    <div class=\"col-12\"><p class=\"image-description\">Figure 16: Updating $W_{output}$</p></div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Positive word pair: (<code>Ned</code>, <code>is</code>)\n",
    "\n",
    "The center word <code>Ned</code> has two context words: <code>Stark</code> and <code>is</code>. This means that we have two positive pairs = two updates to make. Since we already update the matrices $[W_{input} \\quad W_{output}]$ using (<code>Ned</code>, <code>Stark</code>), we will use (<code>Ned</code>, <code>is</code>) to update weight matrices this time. \n",
    "\n",
    "In negative sampling, we draw new $K$ negative words for each positive pairs. Assume that we randomly drew <code>coins</code>, <code>donkey</code>, and <code>machine</code> as our negative words this time. \n",
    "\n",
    "**Forward Propagation: Computing hidden (projection) layer**\n",
    "\n",
    "<div class=\"row\">\n",
    "    <div class=\"col\"><img src=\"jupyter_images/neg_hidden_2.png\"></div>\n",
    "    <div class=\"col-12\"><p class=\"image-description\">Figure 17: Computing hidden (projection) layer</p></div>\n",
    "</div>\n",
    "\n",
    "**Forward Propagation: Sigmoid output layer**\n",
    "\n",
    "<div class=\"row\">\n",
    "    <div class=\"col\"><img src=\"jupyter_images/neg_forward_2_2.png\"></div>\n",
    "    <div class=\"col-12\"><p class=\"image-description\">Figure 18 Sigmoid output layer</p></div>\n",
    "</div>\n",
    "\n",
    "**Backward Propagation: Prediction Error**\n",
    "\n",
    "Our current positive word is <code>Stark</code>: $t_j = 1$ for <code>Stark</code> and $t_j=0$ for other negative words (<code>coins</code>, <code>donkey</code>, and <code>machine</code>).\n",
    "\n",
    "<div class=\"row\">\n",
    "    <div class=\"col\"><img src=\"jupyter_images/neg_backward_1_2.png\"></div>\n",
    "    <div class=\"col-12\"><p class=\"image-description\">Figure 19: Prediction errors of positive and negative words</p></div>\n",
    "</div>\n",
    "\n",
    "**Backward Propagation: Computing $\\nabla W_{input}$**\n",
    "\n",
    "<div class=\"row\">\n",
    "    <div class=\"col\"><img src=\"jupyter_images/neg_backward_2_2.png\"></div>\n",
    "    <div class=\"col-12\"><p class=\"image-description\">Figure 20: Computing input weight matrix gradient $\\nabla W_{input}$</p></div>\n",
    "</div>\n",
    "\n",
    "**Backward Propagation: Computing $\\nabla W_{output}$**\n",
    "\n",
    "<div class=\"row\">\n",
    "    <div class=\"col\"><img src=\"jupyter_images/neg_backward_3_2.png\"></div>\n",
    "    <div class=\"col-12\"><p class=\"image-description\">Figure 21: Computing output weight matrix gradient $\\nabla W_{output}$</p></div>\n",
    "</div>\n",
    "\n",
    "**Backward Propagation: Updating Weight matrices**\n",
    "\n",
    "<div class=\"row\">\n",
    "    <div class=\"col\"><img src=\"jupyter_images/neg_backward_4_2.png\"></div>\n",
    "    <div class=\"col-12\"><p class=\"image-description\">Figure 22: Updating $W_{input}$</p></div>\n",
    "</div>\n",
    "\n",
    "<div class=\"row\">\n",
    "    <div class=\"col\"><img src=\"jupyter_images/neg_backward_5_2.png\"></div>\n",
    "    <div class=\"col-12\"><p class=\"image-description\">Figure 23: Updating $W_{output}$</p></div>\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
