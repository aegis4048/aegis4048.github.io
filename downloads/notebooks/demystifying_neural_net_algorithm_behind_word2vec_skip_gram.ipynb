{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "    <h4>Acknowledgement</h4>\n",
    "    <p>The materials on this post are based the on two NLP papers, <a href=\"https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf\">Distributed Representations of Words and Phrases and their Compositionality</a> (Mikolov et al., 2013) and <a href=\"https://arxiv.org/pdf/1411.2738.pdf\">word2vec Parameter Learning Explained</a> (Rong, 2014).</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paradigm Shift in Word Embedding: Count-Based to Prediction-Based\n",
    "\n",
    "Up until 2013, the traditional models for NLP tasks were count-based models. They mainly involve computing a co-occurence matrix to capture meaningful relationships among words (If you are interested in how co-occurrence matrix is used for language modeling, check out <a href=\"https://aegis4048.github.io/understanding_multi-dimensionality_in_vector_space_modeling\">Understanding Multi-Dimensionality in Vector Space Modeling</a>). For example:\n",
    "\n",
    "*Document 1: \"all that glitters is not gold\"*\n",
    "\n",
    "*Document 2: \"all is well that ends well\"*\n",
    "\n",
    "<table>\n",
    "    <thead>\n",
    "        <tr>\n",
    "            <th>*</th>\n",
    "            <th>START</th>\n",
    "            <th>all</th>\n",
    "            <th>that</th>\n",
    "            <th>glitters</th>\n",
    "            <th>is</th>\n",
    "            <th>not</th>\n",
    "            <th>gold</th>\n",
    "            <th>well</th>\n",
    "            <th>ends</th>\n",
    "            <th>END</th>\n",
    "        </tr>\n",
    "    </thead>\n",
    "    <tbody>\n",
    "        <tr>\n",
    "            <td>START</td>\n",
    "            <td>0</td>\n",
    "            <td>2</td>\n",
    "            <td>0</td>\n",
    "            <td>0</td>\n",
    "            <td>0</td>\n",
    "            <td>0</td>\n",
    "            <td>0</td>\n",
    "            <td>0</td>\n",
    "            <td>0</td>\n",
    "            <td>0</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>all</td>\n",
    "            <td>2</td>\n",
    "            <td>0</td>\n",
    "            <td>1</td>\n",
    "            <td>0</td>\n",
    "            <td>1</td>\n",
    "            <td>0</td>\n",
    "            <td>0</td>\n",
    "            <td>0</td>\n",
    "            <td>0</td>\n",
    "            <td>0</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>that</td>\n",
    "            <td>0</td>\n",
    "            <td>1</td>\n",
    "            <td>0</td>\n",
    "            <td>1</td>\n",
    "            <td>0</td>\n",
    "            <td>0</td>\n",
    "            <td>0</td>\n",
    "            <td>1</td>\n",
    "            <td>1</td>\n",
    "            <td>0</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>glitters</td>\n",
    "            <td>0</td>\n",
    "            <td>0</td>\n",
    "            <td>1</td>\n",
    "            <td>0</td>\n",
    "            <td>1</td>\n",
    "            <td>0</td>\n",
    "            <td>0</td>\n",
    "            <td>0</td>\n",
    "            <td>0</td>\n",
    "            <td>0</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>is</td>\n",
    "            <td>0</td>\n",
    "            <td>1</td>\n",
    "            <td>0</td>\n",
    "            <td>1</td>\n",
    "            <td>0</td>\n",
    "            <td>1</td>\n",
    "            <td>0</td>\n",
    "            <td>1</td>\n",
    "            <td>0</td>\n",
    "            <td>0</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>not</td>\n",
    "            <td>0</td>\n",
    "            <td>0</td>\n",
    "            <td>0</td>\n",
    "            <td>0</td>\n",
    "            <td>1</td>\n",
    "            <td>0</td>\n",
    "            <td>1</td>\n",
    "            <td>0</td>\n",
    "            <td>0</td>\n",
    "            <td>0</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>gold</td>\n",
    "            <td>0</td>\n",
    "            <td>0</td>\n",
    "            <td>0</td>\n",
    "            <td>0</td>\n",
    "            <td>0</td>\n",
    "            <td>1</td>\n",
    "            <td>0</td>\n",
    "            <td>0</td>\n",
    "            <td>0</td>\n",
    "            <td>1</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>well</td>\n",
    "            <td>0</td>\n",
    "            <td>0</td>\n",
    "            <td>1</td>\n",
    "            <td>0</td>\n",
    "            <td>1</td>\n",
    "            <td>0</td>\n",
    "            <td>0</td>\n",
    "            <td>0</td>\n",
    "            <td>1</td>\n",
    "            <td>1</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>ends</td>\n",
    "            <td>0</td>\n",
    "            <td>0</td>\n",
    "            <td>1</td>\n",
    "            <td>0</td>\n",
    "            <td>0</td>\n",
    "            <td>0</td>\n",
    "            <td>0</td>\n",
    "            <td>1</td>\n",
    "            <td>0</td>\n",
    "            <td>0</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>END</td>\n",
    "            <td>0</td>\n",
    "            <td>0</td>\n",
    "            <td>0</td>\n",
    "            <td>0</td>\n",
    "            <td>0</td>\n",
    "            <td>0</td>\n",
    "            <td>1</td>\n",
    "            <td>1</td>\n",
    "            <td>0</td>\n",
    "            <td>0</td>\n",
    "        </tr>\n",
    "    </tbody>\n",
    "</table>\n",
    "<div class=\"col-12\"><p class=\"image-description\">Table 1: Co-Occurence Matrix</p></div>\n",
    "\n",
    "Count-based language modeling is easy to comprehend — related words are observed (counted) together more often than unrelated words. Many attempts were made to improve the performance of the model to the state-of-art, using SVD, ramped window, and non-negative matrix factorization (<a href=\"https://pdfs.semanticscholar.org/73e6/351a8fb61afc810a8bb3feaa44c41e5c5d7b.pdf\">Rohde et al. ms., 2005</a>), but the model did not do well in capturing complex relationships among words. \n",
    "\n",
    "Then, the paradigm started to change in 2013, when Thomas Mikolov proposed the prediction-based modeling technique, called Word2Vec, in his famous paper, <a href=\"https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf\">Distributed Representations of Words and Phrases and their Compositionality</a>. Unlike counting word co-occurrences, the model uses neural networks to learn intelligent representation of words in a vector space. Then, the paper submitted to ACL in 2014, <a href=\"http://www.aclweb.org/anthology/P/P14/P14-1023.pdf\" target=\"_blank\">Don’t count, predict! A systematic comparison of context-counting vs. context-predicting semantic vectors</a>, quantified & compared the performances of count-based vs prediction-based models. \n",
    "\n",
    "<div id=\"fig1\" class=\"row give-margin-inline-big-plot mobile_responsive_plot_full_width\">\n",
    "    <div class=\"col\"><img src=\"jupyter_images/countpredictresults2.png\"></div>\n",
    "    <div class=\"col-12\"><p class=\"image-description\">Figure 1: Performance comparison of models (<a href=\"http://www.marekrei.com/blog/dont-count-predict/\">source</a>)</p></div>\n",
    "</div>\n",
    "\n",
    "The blue bars represent the count-based models, and the red bars are for prediction-based models. The full summary of the paper and more detailed description about the result graph can be found <a href=\"http://www.marekrei.com/blog/dont-count-predict/\">here</a>. Long story short, **prediction-based models outperformed count-based models** by a large margin on various language tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction-based word-embedding: Word2Vec Skip-Gram\n",
    "\n",
    "One of the prediction-based language model introduced by Mikolov is Skip-Gram:\n",
    "\n",
    "<div id=\"fig2\" class=\"row give-margin-inline-big-plot mobile_responsive_plot_full_width\">\n",
    "    <div class=\"col\"><img src=\"jupyter_images/skip-gram-paper.png\" style=\"height: 400px;\"></div>\n",
    "    <div class=\"col-12\"><p class=\"image-description\">Figure 2: Original Skip-gram model architecture</p></div>\n",
    "</div>\n",
    "\n",
    "<a href=\"#fig2\">Figure 2</a> is a diagram presented in the original Word2Vec paper. It is essentially describing that the model uses a neural network of one hidden (projection) layer to correctly predict context words ($w(t-2)$, $w(t-1)$, $w(t+1)$, $w(t+2)$) of an input word ($w(t)$). In the other words, the model attempts to maximize the probability of observing all four context words together, given a center word. Mathematically, it can be denoted as <a href=\"#eq-1\">eq (1)</a>. The training objective is to learn word vector representations that are good at predicting the nearby words.\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "    <h4>Notes: CBOW and Skip-Gram</h4>\n",
    "    <p>There are two models for Word2Vec: <i>Continous Bag Of Words (CBOW)</i> and <i>Skip-Gram</i>. While Skip-Gram model predicts context words given a center word, CBOW model predicts a center word given context words. According to Mikolov:</p>\n",
    "    <p><u>Skip-gram</u>: works well with small amount of the training data, represents well even rare words or phrases</p>\n",
    "    <p><u>CBOW</u>: several times faster to train than the skip-gram, slightly better accuracy for the frequent words</p>\n",
    "    <p>Skip-Gram model is a better choice most of the time due to its ability to predict infrequent words, but this comes at the price of increased computational cost. If training time is a big concern, and you have large enough data to overcome the issue of predicting infrequent words, CBOW model may be a more viable choice. The details of CBOW model won't be covered in this post.</p>\n",
    "</div>\n",
    "\n",
    "**Why predict context words?**\n",
    "\n",
    "A natural question is, why do we predict context words? One must understand that the ultimate goal of Skip-Gram model is not to predict context words, but to learn intelligent vector representation of words. It just happens that predicting context words inevitably results in good vector representations of words, because of the neural network structure of Skip-Gram. Neural network at its essence is just optimizing weight marices ($\\theta$) to correctly predict output. In Word2Vec Skip-Gram, the weight matrices are, in fact, the vector representations of words. Therefore, optimizing weight matrix = good vector representations of words. This is described in detail <a href=\"#weight_matrix\">below</a>.\n",
    "\n",
    "**What is the application of vector representations of words?**\n",
    "\n",
    "In Word2Vec, words are represented as vectors, and related words are placed closed to each other on a vector space. Mathematically, this means that the vector distance between related words are smaller than the vector distance between unrelated words. \n",
    "\n",
    "<div id=\"fig3\" class=\"row give-margin-inline-big-plot mobile_responsive_plot_full_width\">\n",
    "    <div class=\"col\"><img src=\"jupyter_images/vector_distance.png\"></div>\n",
    "    <div class=\"col-12\"><p class=\"image-description\">Figure 3: Vector distance between two words</p></div>\n",
    "</div>\n",
    "\n",
    "For example in <a href=\"#fig3\">figure 3</a>, correlation between *\"success\"* and *\"achieve\"* can be quantified by computing the vector distance between them (Notes: For illustration purpose, three-dimensional word vectors are assumed in the figure, because higher dimensional vectors can't be visualized. Also, distance annotated in the figure is Euclidean, but in real-life, we use Cosine distance to evaluate vector correlations).\n",
    "\n",
    "One interesting application of vector representaion of words is that it can be used to solve analogy tasks. Let's assume the following word vectors for *\"Germany\"*, *\"capital\"*, and *\"Berlin\"*.\n",
    "\n",
    "$$\n",
    "\\begin{align*} \n",
    "vec(\\text{Germany}) & = [1.22 \\quad 0.34 \\quad -3.82] \\\\ \n",
    "vec(\\text{capital}) & = [3.02 \\quad -0.93 \\quad 1.82] \\\\\n",
    "vec(\\text{Berlin})  & = [4.09 \\quad -0.58 \\quad 2.01]\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "To find out the capital of Germany, the word vector of *\"capital\"* can be added to the word vector of *\"Germany\"*.\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "    vec(\\text{Germany}) + vec(\\text{capital}) &= [1.22 \\quad 0.34 \\quad -3.82] + [3.02 \\quad -0.93 \\quad 1.82] \\\\\n",
    "                                              &= [4.24 \\quad -0.59 \\quad -2.00]\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Since the sum of the word vectors of *\"Germany\"* and *\"capital\"* is similar to the word vector of *\"Berlin\"*, the model may conclude that the capital of Germany is Berlin.\n",
    "\n",
    "$$\n",
    "\\begin{align*} \n",
    "[4.24 \\quad -0.59 \\quad -2.00] & \\cong [4.09 \\quad -0.58 \\quad 2.01] \\\\ \n",
    "vec(\\text{Germany}) + vec(\\text{capital}) & \\cong vec(\\text{Berlin})\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "    <h4>Notes: Analogy tasks don't always work</h4>\n",
    "    <p>Not all analogy tasks can be solved like this. The above illustration works like a magic, but there are many analogy problems that can't be solved with Word2Vec. Think of the above illustration as just one use case of Word2Vec.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Derivation of Cost Function\n",
    "\n",
    "Skip-Gram model seeks to optimize the word weight (embedding) matrix by correctly predicting context words, given a center word. In the other words, the model wants to maximize the probability of correctly predicting all context words at the same time, given a center word. Maximizing the probability of predicting context words leads to optimizing the weight matrix ($\\theta$) that best represents words in a vector space. $\\theta$ is a concatenation of input and output weight matrices — $[W_{input} \\quad W_{output}]$, as described <a href=\"#theta_in_cost\">below</a>. It is passed into the cost function ($J$) as a variable and optimized. Mathematically, it can be expressed as:\n",
    "\n",
    "<div id=\"eq-1\" style=\"font-size: 1rem;\">$$ \\underset{\\theta}{\\text{argmax}} \\,\\, p(w_{1}, w_{2}, ... , w_{C}|w_{center}; \\, \\theta) \\tag{1} $$</div>\n",
    "\n",
    "where $C$ is the window size. Recall that in statistics, the probability of $A$ given $B$ is expressed as $P(A|B)$. Then, natural log is taken on <a href=\"#eq-1\">eq (1)</a> to simplify taking derivatives. \n",
    "\n",
    "<div id=\"eq-2\" style=\"font-size: 1rem;\">$$ \\underset{\\theta}{\\text{argmax}} \\,\\, log \\, p(w_{1}, w_{2}, ... , w_{C}|w_{center}; \\, \\theta) \\tag{2} $$</div>\n",
    "\n",
    "<div id=\"take_natural_log\"></div>\n",
    "<div class=\"alert alert-info\">\n",
    "    <h4>Notes: Why take a natural log?</h4>\n",
    "    <p>In machine learning, it is a common practice to take a natural log to the objective function to simplify taking derivatives. For example, a multinomial regression classifer called Softmax (details explained <a href=\"#Softmax-Output-Layer-($y_{pred}$)\">below</a>) has the following probability function:</p>\n",
    "    <p><center style=\"font-size: 1rem; margin-top: 20px;\">$p(x_i) = \\frac{e^{x_i}}{\\sum_{j=1}e^{x_{j}}}$</center></p>\n",
    "    <p>Taking a log simplifies the function:</p>\n",
    "    <p><center style=\"font-size: 1rem; margin-top: 20px;\">$log \\, p(x_i) = x_i - log \\, {\\sum_{j=1}e^{x_{j}}}$</center></p>\n",
    "    <p>Depending on a model, the argument ($x_i$) passed into the probability function ($p$) can be complicated, and simplifying the original softmax function helps with taking the derivatives in the future.</p> \n",
    "    <p>Taking a log does not affect the optimized weights ($\\theta$), because natural log is a <u>monotonically increasing</u> function. This means that increasing the value of $x$-axis results in increasing the value of $y$-axis. This is important because it ensures that the maximum value of the original probability function occurs at the same point as the log probability function. Therefore:</p>\n",
    "    <p><center style=\"font-size: 1rem; margin-top: 20px;\">$\\underset{\\theta}{\\text{argmax}} \\,\\, p(x_i) = \\underset{\\theta}{\\text{argmax}} \\,\\, log \\, p(x_i)$</center></p>\n",
    "</div>\n",
    "\n",
    "In Skip-Gram, softmax function is used for context words classfication. The details are explained <a href=\"#Softmax-Output-Layer-($y_{pred}$)\">below</a>. Softmax in Skip-Gram has the following equation:\n",
    "\n",
    "<div id=\"eq-3\" style=\"font-size: 1rem;\">$$ p(w_{context}|w_{center}; \\, \\theta) = \\frac{exp(W_{output_{(context)}} \\cdot h)}{\\sum^V_{i=1}exp(W_{output_{(i)}} \\cdot h)} \\tag{3} $$</div>\n",
    "\n",
    "$W_{output_{(context)}}$ is a row vector for a context word from the output embedding matrix (see <a href=\"#weight_matrix\">below</a>), and $h$ is the hidden (projection) layer word vector for a center word (see <a href=\"#hidden_layer\">below</a>). Softmax function is then plugged into the <a href=\"#eq-2\">eq (2)</a> to yield a new objective function that maximizes the probability of observing all $C$ context words, given a center word:\n",
    "\n",
    "<div id=\"eq-4\" style=\"font-size: 1rem;\">$$ \\underset{\\theta}{\\text{argmax}} \\,\\, log \\, \\prod_{c=1}^{C} \\frac{exp(W_{output_{(c)}} \\cdot h)}{\\sum^V_{i=1}exp(W_{output_{(i)}} \\cdot h)} \\tag{4} $$</div>\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "    <h4>Notes: Probability Product</h4>\n",
    "    <p>In statistics, probability of observing $C$ multiple events at the same time is computed by the product of each event's probability.</p>\n",
    "    <p><center style=\"font-size: 1rem; margin-top: 20px;\">$$p(x_{1}, x_{2} ... x_{C}) = p(x_{1}) \\times p(x_{2}) \\, \\times \\, ... \\, \\times \\, p(x_{C})$$</center></p>\n",
    "    <p>This can be shortened with a product notation:</p>\n",
    "    <p><center style=\"font-size: 1rem; margin-top: 20px;\">$$p(x_{1}, x_{2} ... x_{C}) = \\prod_{c=1}^{C}p(x_{c})$$</center></p>\n",
    "</div>\n",
    "\n",
    "However, in machine learning, the convention is to minimize the cost function, not to maximize it. To stick to the convention, we add a negative sign to <a href=\"#eq-4\">eq (4)</a>. This can be done because minimizing a negative log-likelihood is equivalent to maximizing a positive log-likelihood. Therefore, the cost function we want to minimize becomes:\n",
    "\n",
    "<div id=\"eq-5\" style=\"font-size: 1rem;\">$$ J(\\theta; w^{(t)}) = -log \\, \\prod_{c=1}^{C} \\frac{exp(W_{output_{(c)}} \\cdot h)}{\\sum^V_{i=1}exp(W_{output_{(i)}} \\cdot h)} \\tag{5} $$</div>\n",
    "\n",
    "where $c$ is the index of the context word around the center word ($w_{t}$). $t$ is the index of the center word within a corpus of size $T$. Using the property of log, it can be changed to:\n",
    "\n",
    "<div id=\"eq-6\" style=\"font-size: 1rem;\">\n",
    "$$J(\\theta; w^{(t)}) = -\n",
    "\\sum_{c=1}^{C}\n",
    "log \n",
    "\\frac{exp(W_{output_{(c)}} \\cdot h)}{\\sum^V_{i=1}exp(W_{output_{(i)}} \\cdot h)} \\tag{6}$$\n",
    "</div>\n",
    "\n",
    "Taking a log to the softmax function allows us to simplify the expression into simpler forms because we can split the fraction into addtion of the numerator and the denominator:\n",
    "\n",
    "<div id=\"eq-7\" style=\"font-size: 1rem;\">\n",
    "$$\n",
    "J(\\theta; w^{(t)}) = - \\sum_{c=1}^{C}(W_{output_{(c)}} \\cdot h) + C \\cdot log \\sum^V_{i=1}exp(W_{output_{(i)}} \\cdot h) \\tag{7}\n",
    "$$\n",
    "</div>\n",
    "\n",
    "Different paper uses different notations for the cost function. To stick to the notation used in the <a href=\"https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf\">Word2Vec original paper</a>, some of the notations in <a href=\"#eq-7\">eq (7)</a> can be changed. However, they are all equivalent:\n",
    "\n",
    "<div id=\"eq-8\" style=\"font-size: 1rem;\">$$J(\\theta;w^{(t)}) = -\\sum_{-c\\leq j \\leq c,j\\neq 0} \\log p(w_{t+j} \\mid w_t ; \\, \\theta) \\tag{8}$$</div>\n",
    "\n",
    "Note that <a href=\"#eq-7\">eq (7)</a> and <a href=\"#eq-8\">eq (8)</a> are equivalent. They both assume **stochastic gradient descent**, which means that for each training sample ($w^{(t)}$) in the corpus of size ($T$), one update is made to the weight matrix ($\\theta$). The cost function expressed in the paper shows **batch gradient descent** <a href=\"#eq-9\">eq (9)</a>, which means that only one update is made for all $T$ training samples:\n",
    "\n",
    "<div id=\"eq-9\" style=\"font-size: 1rem;\">$$J(\\theta) = -\\frac{1}{T} \\sum^T_{t=1} \\sum_{-c\\leq j \\leq c,j\\neq 0} \\log p(w_{t+j} \\mid w_t ;\\, \\theta) \\tag{9}$$</div>\n",
    "\n",
    "However, in Word2Vec, batch gradient descent is almost never used due to its high computational cost. The author of the paper stated that he used stochastic gradient descent for training. Read the below <a href=\"#stochastic\">notes</a> for more information about stochastic gradient descent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=\"window_size\"></div>\n",
    "\n",
    "### Window Size of Skip-Gram\n",
    "\n",
    "Softmax regression (or multinomial logistic regression) is a generalization of logistic regression to the case where we want to handle multiple classes. A general form of the softmax regression looks like this:\n",
    "\n",
    "<div id=\"eq-10\" style=\"font-size: 1rem;\">\n",
    "$$J(\\theta) = \n",
    "-\\frac{1}{T} \n",
    "\\sum^T_{t=1}\n",
    "\\sum^K_{k=1}\n",
    "log \n",
    "\\frac\n",
    "{exp(\\theta^{(k)\\top}x^{(t)})}\n",
    "{\\sum^K_{i=1}\n",
    "exp(\\theta^{(i)\\top}x^{(t)})} \\tag{10}$$</div>\n",
    "\n",
    "where $T$ is the number of training samples, and $K$ is the number of labels to classify. In NLP applications, $K = V$, because there are $V$ unique vocabulary we need to classify in a vector space. $V$ can easily exceed tens of thousands. Skip-Gram tweaks this a little, and replaces $K$ with a variable called **window size** $C$. Window size is a hyper parameter of the model with a typical range of $[1, 10]$ (see <a href=\"#fig4\">figure 4</a>). Recall that Skip-Gram is a model that attempts to predict neighboring words of a center word. It doesn't have to predict all $V$ vocab in the corpus that may be 100 or more words away from it, but instead predict only a few, 1~10 neighboring context words. This is also intuitive, considering how words that are far away carry less information about each another. Thus, the adapted form of the softmax regression equation for Skip-Gram becomes:\n",
    "\n",
    "<div id=\"eq-11\" style=\"font-size: 1rem;\">\n",
    "$$J(\\theta) = \n",
    "-\\frac{1}{T} \n",
    "\\sum^T_{t=1}\n",
    "\\sum_{-c\\leq j \\leq c,j\\neq 0}\n",
    "log \n",
    "\\frac\n",
    "{exp(\\theta^{(t+j)\\top}x^{(t)})}\n",
    "{\\sum^K_{i=1}\n",
    "exp(\\theta^{(i)\\top}x^{(t)})} \\tag{11}$$</div>\n",
    "\n",
    "This is equivalent to <a href=\"#eq-9\">eq (9)</a>. Note that the $K$ in the denominator is still equal to $V$, because the denominator acts as a normalization factor, as described <a href=\"#output_layer\">below</a>. However, the size of $K$ in the denominator can still be reduced to smaller size using <a href=\"#negsample\">negative sampling</a>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network Structure of Skip-Gram\n",
    "\n",
    "How is neural network used to minimize the cost functoin described in <a href=\"#eq-11\">eq (11)</a>? One needs to look into the structure of the Skip-Gram model to gain insights about their correlation. \n",
    "\n",
    "For illustration purpose, let's assume that the entire corpus is composed of the quote from the Game of Thrones, <i>\"The man who passes the sentence should swing the sword\"</i>, by Ned Stark. There are 10 words ($T = 10$), and 8 unique words ($V = 8$). \n",
    "\n",
    "Note that in real life, the corpus is much bigger than just one sentence. \n",
    "\n",
    "<blockquote class=\"quote\">\n",
    "  The man who passes the sentence should swing the sword.\n",
    "  <span>- Ned Stark</span>\n",
    "</blockquote>\n",
    "\n",
    "<div id=\"window_id\"></div>\n",
    "\n",
    "We will use <code>window=1</code>, and assume that <i>'passes'</i> is the current center word, making <i>'who'</i> and <i>'the'</i> context words. <code>window</code> is a hyper-parameter that can be empirically tuned. It typically has a range of $[1, 10]$.\n",
    "\n",
    "<div id=\"fig4\" class=\"row give-margin-inline-big-plot\">\n",
    "    <div class=\"col\"><img src=\"jupyter_images/quote_ned_stark.png\"></div>\n",
    "    <div class=\"col-12\"><p class=\"image-description\">Figure 4: Training Window</p></div>\n",
    "</div>\n",
    "\n",
    "<div id=\"size_3\"></div>\n",
    "For illustration purpose, a three-dimensional neural net will be constructed. In *gensim*, this can be implemented by setting <code>size=3</code>. This makes $N = 3$. Note that <code>size</code> is also a hyper-parameter that can be empirically tuned. In real life, a typical Word2Vec model has 200-600 neurons. \n",
    "\n",
    "<pre>\n",
    "    <code class=\"language-python\">\n",
    "        from gensim.models import Word2Vec\n",
    "\n",
    "        model = Word2Vec(corpus, size=3, window=1)\n",
    "    </code>\n",
    "</pre>\n",
    "\n",
    "This means that the input weight matrix ($W_{input}$) will have a size of $8 \\times 3$, and output weight matrix ($W_{output}^T$) will have a size of $3 \\times 8$. Recall that the corpus, <i>\"The man who passes the sentence should swing the sword\"</i>, has 8 unique vocabularies ($V = 8$). \n",
    "\n",
    "<div class=\"row\" id=\"fig5\">\n",
    "    <div class=\"col\"><img src=\"jupyter_images/word2vec_skip-gram.png\" style=\"margin: 0;\"></div>\n",
    "    <div class=\"col-12\"><p class=\"image-description\">Figure 5: Skip-Gram model structure</p></div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training: Forward Propagation\n",
    "\n",
    "The word embedding matrices ($W_{input}$, $W_{output}$) in Skip-Gram are optimized through forward and backward propagations. For each iteration of forward + backward propagations, the model learns to reduce prediction error by optimizing the weight matrix ($\\theta$), thus acquiring higher quality embedding matrices that better capture relationships among words. \n",
    "\n",
    "Forward propagation includes obtaining the probability distribution of words ($y_{pred}$ in <a href=\"#fig5\">figure 5</a>) given a center word, and backward propagation includes calculating the prediction error, and updating the weight (embedding) matrices to minimize the prediction error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=\"input_layer\"></div>\n",
    "\n",
    "### Input Layer ($x$)\n",
    "\n",
    "The input layer is a $V$-dim one-hot encoded vector. Every element in the vector is 0 except one element that corresponds to the center (input) word. Input vector is multiplied with the input weight matrix ($W_{input}$) of size $V \\times N$, and yields a hidden (projection) layer ($h$) of $N$-dim vector. Because the input layer is one-hot encoded, it makes the input weight matrix ($W_{input}$) to behave like a *look-up table* for the center word. Assuming epoch number of 1 (<code>iter=1</code> in *gensim* Word2Vec implementation) and stochastic gradient descent, the input vector is injected into the network $T$ times for every word in the corpus and makes $T$ updates to the weight matrix ($\\theta$) to learn from the training samples. Derivation of the stochasitc update equations are explained <a href=\"#backward\">below</a>.\n",
    "\n",
    "<div class=\"row\">\n",
    "    <div class=\"col\"><img src=\"jupyter_images/one-hot-vector.png\"></div>\n",
    "    <div class=\"col-12\"><p class=\"image-description\">Figure 6: One-hot encoded input vector and parameter update</p></div>\n",
    "</div>\n",
    "\n",
    "<div id=\"stochastic\"></div>\n",
    "<div class=\"alert alert-info\">\n",
    "    <h4>Notes: Stochastic Gradient Descent</h4>\n",
    "    <p>The goal of any machine learning model is to find the optimal values of a weight matrix ($\\theta$) to minimize prediction error. A general update equation for weight matrix looks like the following:</p>\n",
    "    <p><center style=\"font-size: 1rem; margin-top: 20px;\">$\\theta^{(new)}=\\theta^{(old)}-\\eta\\cdot\\nabla_{J(\\theta)}$</center></p>\n",
    "    <p>$\\eta$ is learning rate, $\\nabla_{J(\\theta)}$ is gradient for the weight matrix, and $J(\\theta)$ is the cost function that has different forms for each model. The cost function for the Skip-Gram model proposed in the <a href=\"https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf\">Word2Vec original paper</a> has the following equation:</p>\n",
    "    <p><center style=\"font-size: 1rem; margin-top: 20px;\">$$J(\\theta) = -\\frac{1}{T} \\sum^T_{t=1} \\sum_{-c\\leq j \\leq c,j\\neq 0} \\log p(w_{t+j} \\mid w_t ; \\theta)$$</center></p>\n",
    "    <p>Here, what gives us headache is the expression, $\\frac{1}{T} \\sum^T_{t=1}$, because $T$ can be larger than billions or more in many NLP applications. It is basically telling us that billions of iterations need to be computed to make just one update to the weight matrix ($\\theta$). In order to mitigate this computational burden, the author of the paper states that Stochastic Gradient Descent (SGD) was used for parameter optimization. SGD removes the expression, $\\frac{1}{T} \\sum^T_{t=1}$, from the cost function and performs parameter update for each training example, $w^{(t)}$:</p>\n",
    "    <p><center style=\"font-size: 1rem; margin-top: 20px;\">$$J(\\theta;w^{(t)}) = -\\sum_{-c\\leq j \\leq c,j\\neq 0} \\log p(w_{t+j} \\mid w_t ; \\theta)$$</center></p>\n",
    "    <p>Then, the new parameter update equation for SGD becomes:</p>\n",
    "    <p><center style=\"font-size: 1rem; margin-top: 20px;\">$\\theta^{(new)}=\\theta^{(old)}-\\eta\\cdot\\nabla_{J(\\theta;w^{(t)})}$</center></p>\n",
    "    <p>The original vanilla graident descent makes $1$ parameter update for $T$ training samples, but the new update equation using SGD makes $T$ parameter update for $T$ training samples. However, this comes at the price of higher fluctuation (or variance) in minimizing prediction error.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=\"weight_matrix\"></div>\n",
    "\n",
    "### Input and Output Weight Matrix  ($W_{input}$, $W_{output}$)\n",
    "\n",
    "Why does Skip-Gram model attempt to predict context words given a center word? How does predicting context words help with quantifying words and representing them in a vector space? In fact, the ultimate goal of the model is not to predict context words, but to construct the word embedding matrices ($W_{input}$, $W_{output}$) that best caputure relationship among words in a vector space. Skip-Gram achieves this by using a neural net — it optimizes the weight (embedding) matrices by adjusting the weight matrix to minimize the <a href=\"#predict_error\">prediction error</a> ($y_{pred} - y_{true}$). This will make more sense once you understand how the embedding matrix behaves like a *look-up table*.\n",
    "\n",
    "Each row in a word-embedding matrix is a word-vector for each word. Consider the following word-embedding matrix, $W_{input}$.\n",
    "\n",
    "<div class=\"row give-margin-inline-plot mobile_responsive_plot_full_width\">\n",
    "    <div class=\"col\"><img src=\"jupyter_images/embedding_matrix_input.png\" style=\"height: 300px;\"></div>\n",
    "    <div class=\"col-12\"><p class=\"image-description\">Figure 7: Word-embedding matrix, $W_{input}$</p></div>\n",
    "</div>\n",
    "\n",
    "The words of our interest are *\"passes\"* and *\"should\"*. *\"passes\"* has a word vector of $[0.1 \\quad 0.2 \\quad 0.7]$ and *\"should\"* has $[-2 \\quad 0.2 \\quad 0.8]$. Since we set the size of the weight matrix to be <code>size=3</code> <a href=\"#size_3\">above</a>, the matrix is three-dimensional, and can be visualized in a 3D vector space:\n",
    "\n",
    "<div class=\"row full_screen_margin_md mobile_responsive_plot_full_width\">\n",
    "    <div class=\"col\"><img src=\"jupyter_images/word2vec_3d.png\" style=\"height: 300px;\"></div>\n",
    "    <div class=\"col-12\"><p class=\"image-description\">Figure 8: 3D visualization of word vectors in embedding matrix</p></div>\n",
    "</div>\n",
    "\n",
    "Optimizing the embedding (weight) matrices ($\\theta$) results in representing words in a high quality vector space, and the model will be able to capture meaningful relationships among words.\n",
    "\n",
    "<div id=\"theta_in_cost\"></div>\n",
    "<div class=\"alert alert-info\">\n",
    "    <h4>Notes: $\\theta$ in cost function</h4>\n",
    "    <p>There are two weight matrices that need to be optimized in Skip-Gram model: $W_{input}$ and $W_{output}$. Often times in neural net, the weights are expressed as $\\theta$. In Skip-Gram, $\\theta$ is a concatenation of input and output weight matrices — $[W_{input} \\quad W_{output}]$.</p>\n",
    "    <div style=\"font-size: 1rem; margin-top: 20px;\">$$ \\theta = [W_{input} \\quad W_{output}] = \\left[ \\begin{array}{l} u_{the} \\\\ u_{passes} \\\\ \\vdots \\\\ u_{who} \\\\  v_{the} \\\\ v_{passes} \\\\ \\vdots \\\\ v_{who} \\end{array} \\right] \\in \\mathbb{R}^{2NV}$$</div>\n",
    "    <p>$\\theta$ has a size of $2V \\times N$, where $V$ is the number of unique vocab in a corpus, and $N$ is the dimension of word vectors in the embedding matrices. $2$ is multipled to $V$ because there are two weight matrices, $W_{input}$ and $W_{output}$. $u$ is a word vector from $W_{input}$ and $v$ is a word vector from $W_{output}$. Each word vectors are $N$-dim row vectors from input and output embedding matrices.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=\"hidden_layer\"></div>\n",
    "\n",
    "### Hidden (Projection) Layer ($h$)\n",
    "\n",
    "Skip-Gram uses a neural net with one hidden layer. In the context of natural language processing, hidden layer is often referred to as a *projection* layer, because $h$ is essentially an 1D vector projected by the one-hot encoded input vector.\n",
    "\n",
    "<div class=\"row full_screen_margin mobile_responsive_plot_full_width\">\n",
    "    <div class=\"col\"><img src=\"jupyter_images/skip-gram_lookup.png\"></div>\n",
    "    <div class=\"col-12\"><p class=\"image-description\">Figure 9: Computing projection layer</p></div>\n",
    "</div>\n",
    "\n",
    "$h$ is obtained by multiplying the input word embedding matrix with the $V$-dim input vector.\n",
    "\n",
    "<div style=\"font-size: 1rem;\" id=\"eq-12\">$$h = W_{input}^T \\cdot x  \\in \\mathbb{R}^{N} \\tag{12}$$</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=\"output_layer\"></div>\n",
    "\n",
    "### Softmax Output Layer ($y_{pred}$)\n",
    "\n",
    "The output layer is a $V$-dim probability distribution of all unique words in the corpus, given a center word. In statistics, the conditional probability of $A$ given $B$ is denoted as $p(A|B)$. In Skip-Gram, we use the notation, $p(w_{context}| w_{center})$, to denote the conditional probability of observing a context word given a center word. It is obtained by using the softmax function,\n",
    "\n",
    "<div id=\"eq-13\" style=\"font-size: 1rem;\">$$ p(w_{context}|w_{center}) = \\frac{exp(W_{output_{(context)}} \\cdot h)}{\\sum^V_{i=1}exp(W_{output_{(i)}} \\cdot h)} \\in \\mathbb{R}^{1} \\tag{13} $$</div>\n",
    "\n",
    "where $W_{output_{(i)}}$ is the $i$-th row vector of size $1 \\times N$ from the output embedding matrix, $W_{output_{context}}$ is also a row vector of size $1 \\times N$ from the output embedding matrix corresponding to the context word. $V$ is the size of unique vocab in the corpus, and $h$ is the hidden (projection) layer of size ($N \\times 1$). The output is an $1 \\times 1$ scalar value of probability of range $[0, 1)$.\n",
    "\n",
    "This probability is computed $V$ times to obtain a conditional probability distribution of observing each unique vocabs in the corpus, given a center word. \n",
    "\n",
    "<div style=\"font-size: 1rem;\" id=\"eq-14\">$$ \\left[ \\begin{array}{c} p(w_{1}|w_{center}) \\\\ p(w_{2}|w_{center}) \\\\ p(w_{3}|w_{center}) \\\\ \\vdots \\\\ p(w_{V}|w_{center}) \\end{array} \\right] = \\frac{exp(W_{output} \\cdot h)}{\\sum^V_{i=1}exp(W_{output_{(i)}} \\cdot h)} \\in \\mathbb{R}^{V}\\tag{14} $$</div>\n",
    "\n",
    "$W_{output}$ in the denominator of eq 13 has size $V \\times N$. Multiplying $W_{output}$ with $h$ of size $N \\times 1$ will yield a dot product vector of size $V \\times 1$. This dot product vector goes through the softmax function:\n",
    "\n",
    "<div class=\"row full_screen_margin_md mobile_responsive_plot_full_width\">\n",
    "    <div class=\"col\"><img src=\"jupyter_images/softmax_function.png\"></div>\n",
    "    <div class=\"col-12\"><p class=\"image-description\">Figure 10: softmax function transformation</p></div>\n",
    "</div>\n",
    "\n",
    "The exponentiation ensures that the transformed values are positive, and the normalization factor in the denominator ensures that the values have a range of $[0, 1)$. The result is a conditional probability distribution of observing each unique vocabs in the corpus, given a center word. \n",
    "\n",
    "<div id=\"negsample\" class=\"alert alert-info\">\n",
    "    <h4>Notes: Negative Sampling</h4>\n",
    "    <p>Softmax function in Skip-Gram has the following equation:</p>\n",
    "    <p><div style=\"font-size: 1rem; margin-top: 20px;\">$$ P = \\frac{exp(W_{output} \\cdot h)}{\\sum^V_{i=1}exp(W_{output_{(i)}} \\cdot h)} \\in \\mathbb{R}^{V}$$</div></p>\n",
    "    <p>There is an issue with softmax in Skip-Gram — it is computationally very expensive, as it requires scanning through the entire output embedding matrix ($W_{output}$) to compute the probability distribution of all $V$ words, where $V$ can be millions or more. Furtheremore, the normalization factor in the denominator also requires $V$ iterations. When implemented in codes, the normalization factor is computed only once and cached as a Python variable, making the alogrithm complexity = $O(V+V)\\approx O(V)$.</p>\n",
    "\n",
    "<p>Due to this computational inefficiency, <b>softmax is not used in most implementaions of Skip-Gram</b>. Instead we use an alternative called negative sampling with sigmoid function, which rephrases the problem into a set of independent binary classification task of algorithm complexity = $O(K+1)$, where $K$ typically has a range of $[5,20]$. Then, the new probability distribution is defined as:</p>\n",
    "    <p><div style=\"font-size: 1rem; margin-top: 20px;\">$$ P = \\frac{1}{1+exp(-(\\{c_{pos}\\} \\cup W_{neg}) \\cdot h)}\n",
    "\\in \\mathbb{R}^{K+1}$$</div></p>\n",
    "    <p>$K=20$ is used for small corpus, and $K=5$ is used for big corpus. Negative sampling is much cheaper than vanilla Skip-Gram with softmax, because $K$ is between 5 ~ 20, whereas $V$ can be millions. Moreover, no extra iterations are necessary to compute the normalization factor in the denominator, because sigmoid function is a binary regression classifier. The algorithm complexity of the probability distribution of vanilla Skip-Gram is $O(V)$, whereas negative sampling's is $O(K+1)$. This shows why negative sampling saves a significant amount of computational cost per iteration.</p>\n",
    "    <p>In <i>gensim</i>, negative sampling is applied by default with <code>Word2Vec(negative=5, ns_exponent=0.75)</code>, where <code>negative</code> is the number of $K$-negative samples, and <code>ns_exponent</code> is a hyperparameter related to negative sampling, of range $(0, 1)$. The details of the methodology behind negative sampling deserves another fully devoted post, and as such, covered in a <a href=\"https://aegis4048.github.io/optimize_computational_efficiency_of_skip-gram_with_negative_sampling\" target=\"_blank\">different post</a>.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=\"backward\"></div>\n",
    "\n",
    "## Training: Backward Propagation\n",
    "\n",
    "Backward propagation involves computing prediction errors, and updating the weight matrix ($\\theta$) to optimize vector representation of words. Assuming <a href=\"#stochastic\">stochastic gradient descent</a>, we have the following general update equations for the weight matrix ($\\theta$):\n",
    "\n",
    "<div style=\"font-size: 1rem;\">$$ \\theta_{new}=\\theta_{old}-\\eta\\cdot\\nabla_{J(\\theta;w^{(t)}}) \\tag{15} $$</div>\n",
    "\n",
    "$\\eta$ is learning rate, $\\nabla_{J(\\theta;w^{(t)})}$ is gradient for the weight matrix, and $J(\\theta;w^{(t)})$ is the cost function defined in <a href=\"#eq-6\">eq (6)</a>. Since the $\\theta$ is a concatenation of input and output weight matrices ($[W_{input} \\quad W_{output}]$) as described <a href=\"#theta_in_cost\">above</a>, there are two update equations for each embedding matrix:\n",
    "\n",
    "<div style=\"font-size: 1rem;\" id=\"eq-16\">$$ W_{input}^{(new)}=W_{input}^{(old)}- \\eta \\cdot \\frac{\\partial J}{\\partial W_{input}} \\tag{16} $$</div>\n",
    "<div style=\"font-size: 1rem;\" id=\"eq-17\">$$ W_{output}^{(new)}=W_{output}^{(old)}- \\eta \\cdot \\frac{\\partial J}{\\partial W_{output}} \\tag{17} $$</div>\n",
    "\n",
    "Mathematically, it can be shown that the gradients of $W_{input}$ $W_{output}$ have the following forms:\n",
    "\n",
    "<div style=\"font-size: 1rem;\" id=\"eq-18\">$$ \\frac{\\partial J}{\\partial W_{input}}  = x \\cdot (W_{output}^T \\sum^C_{c=1} e_c) \\tag{18}$$</div>\n",
    "<div style=\"font-size: 1rem;\" id=\"eq-19\">$$ \\frac{\\partial J}{\\partial W_{output}} = h \\cdot \\sum^C_{c=1} e_c \\tag{19}$$</div>\n",
    "\n",
    "The gradients can be substitued into <a href=\"#eq-16\">eq (16)</a> and <a href=\"#eq-17\">eq (17)</a>:\n",
    "\n",
    "<div style=\"font-size: 1rem;\" id=\"eq-20\">$$ W_{input}^{(new)}=W_{input}^{(old)}- \\eta \\cdot x \\cdot (W_{output}^T \\sum^C_{c=1} e_c)  \\tag{20} $$</div>\n",
    "<div style=\"font-size: 1rem;\" id=\"eq-21\">$$ W_{output}^{(new)}=W_{output}^{(old)}- \\eta \\cdot h \\cdot \\sum^C_{c=1} e_c \\tag{21} $$</div>\n",
    "\n",
    "$W_{input}$ is <a href=\"#weight_matrix\">input weight matrix</a>, $W_{output}$ is <a href=\"#weight_matrix\">output weight matrix</a>, $x$ is one-hot encoded <a href=\"#input_layer\">input layer</a>, $C$ is <a href=\"#window_size\">window size</a>, and $e_{c}$ is <a href=\"#predict_error\">prediction error</a> for $c$-th context word in the window. Note that $h$ (hidden layer) is equivalent to $W_{input}^T x$.\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "    <h4>Notes: Applying softmax</h4>\n",
    "    <p>Although <a href=\"#eq-21\">eq (21)</a> does not explicitly show it, softmax function is applied in the prediction error ($e_c$). Prediction error is the difference between the predicted and true probability ($y_{pred} - y_{true}$) as illustrated <a href=\"#forward_softmax\">below</a>. The predicted probability $y_{pred}$ is computed using softmax function using <a href=\"#eq-13\">eq (13)</a>.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=\"predict_error\"></div>\n",
    "\n",
    "### Prediction Error ($y_{pred} - y_{true}$)\n",
    "\n",
    "Skip-Gram model optimizes the weight matrix ($\\theta$) to reduce the prediction error. Prediction error is the difference between the probability distribution of words computed from the <a href=\"#output_layer\">softmax output layer</a> ($y_{pred}$) and the true probability distribution ($y_{true}$) of the $c$-th context word. Just like the input layer, $y_{true}$ is one-hot encoded vector, in which only one element in the vector that corresponds to the $c$-th context word is $1$, and the rest is all $0$. \n",
    "\n",
    "<div class=\"row give-margin-inline-plot\">\n",
    "    <div class=\"col\"><img src=\"jupyter_images/error_window.png\" style=\"height: 400px;\"></div>\n",
    "    <div class=\"col-12\"><p class=\"image-description\">Figure 11: Prediction error window</p></div>\n",
    "</div>\n",
    "\n",
    "The figure has a window size of $2$, so two prediction errors were computed. Recall from the above notes about the <a href=\"#window_size\">window size</a> that the original softmax regression classifier (<a href=\"#eq-10\">eq (10)</a>) has $K$ labels to classify, in which $K = V$ in NLP applications because there are $V$ words to classify. Employing window size transforms <a href=\"#eq-10\">eq (10)</a> into <a href=\"#eq-11\">eq (11)</a> and significantly reduces the algorithm complexity because the model only needs to compute prediction errors for $[1, 10]$ neighboring words, instead of computing all $V$-prediction errors for all vocabs that can be millions or more.\n",
    "\n",
    "Then, prediction errors for all $C$ context words are summed up to compute weight gradients to the update weight matrices.\n",
    "\n",
    "<div class=\"row full_screen_margin mobile_responsive_plot_full_width\">\n",
    "    <div class=\"col\"><img src=\"jupyter_images/prediction_error_sum.png\" style=\"height: 230px;\"></div>\n",
    "    <div class=\"col-12\"><p class=\"image-description\">Figure 12: Sum of prediction errors</p></div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numerical Demonstration\n",
    "\n",
    "For the ease of illustration, screenshots from Excel will be used to demonstrate the concept of updating weight matrices through forward and backward propagations.\n",
    "\n",
    "**Forward Propagation: Computing hidden (projection) layer**\n",
    "\n",
    "Center word is *\"passes\"*. Window size is <code>size=1</code>, making *\"the\"* and *\"who\"* context words. Hidden layer ($h$) is <i>looked up</i> from the input weight matrix. It is computed with <a href=\"#eq-12\">eq (12)</a>.\n",
    "\n",
    "<div class=\"row\">\n",
    "    <div class=\"col\"><img src=\"jupyter_images/forward_1.png\"></div>\n",
    "    <div class=\"col-12\"><p class=\"image-description\">Figure 13: Computing hidden (projection) layer</p></div>\n",
    "</div>\n",
    "\n",
    "<div id=\"forward_softmax\"></div>\n",
    "\n",
    "**Forward Propagation: Softmax output layer**\n",
    "\n",
    "Output layer is a probability distribution of all words, given a center word. It is computed with <a href=\"#eq-14\">eq (14)</a>. Note that all context windows share the same output layer ($y_{pred}$). Only the errors ($e_c$) are different.\n",
    "\n",
    "<div class=\"row\">\n",
    "    <div class=\"col\"><img src=\"jupyter_images/forward_2.png\"></div>\n",
    "    <div class=\"col-12\"><p class=\"image-description\">Figure 14: Softmax output layer</p></div>\n",
    "</div>\n",
    "\n",
    "**Backward Propagation: Sum of Prediction Errors**\n",
    "\n",
    "$C$ different prediction errors are computed, then summed up. In this case, since we set <code>window=1</code> <a href=\"#window_id\">above</a>, only two errors are computed.\n",
    "\n",
    "<div class=\"row\">\n",
    "    <div class=\"col\"><img src=\"jupyter_images/backward_1.png\"></div>\n",
    "    <div class=\"col-12\"><p class=\"image-description\">Figure 15: Prediction errors of context words</p></div>\n",
    "</div>\n",
    "\n",
    "**Backward Propagation: Computing $\\nabla W_{input}$**\n",
    "\n",
    "Gradients of input weight matrix ($\\frac{\\partial J}{\\partial W_{input}}$) are computed using <a href=\"#eq-18\">eq (18)</a>. Note that multiplying $W_{output}^T \\sum^C_{c=1} e_c$ with the one-hot-encoded input vector ($x$) makes the neural net to <u>update only one word vector</u> that corresponds to the input (center) word.\n",
    "\n",
    "<div class=\"row\">\n",
    "    <div class=\"col\"><img src=\"jupyter_images/backward_2.png\"></div>\n",
    "    <div class=\"col-12\"><p class=\"image-description\">Figure 16: Computing input weight matrix gradient $\\nabla W_{input}$</p></div>\n",
    "</div>\n",
    "\n",
    "**Backward Propagation: Computing $\\nabla W_{output}$**\n",
    "\n",
    "Gradients of output weight matrix ($\\frac{\\partial J}{\\partial W_{output}}$) are computed using <a href=\"#eq-19\">eq (19)</a>. Unlike the input weight matrix ($W_{input}$), all word vectors in the output weight matrix ($W_{output}$) are updated.\n",
    "\n",
    "<div class=\"row\">\n",
    "    <div class=\"col\"><img src=\"jupyter_images/backward_3.png\"></div>\n",
    "    <div class=\"col-12\"><p class=\"image-description\">Figure 17: Computing output weight matrix gradient $\\nabla W_{output}$</p></div>\n",
    "</div>\n",
    "\n",
    "<div id=\"weight_update\"></div>\n",
    "\n",
    "**Backward Propagation: Updating Weight matrices**\n",
    "\n",
    "Input and output weight matrices ($[W_{input} \\quad W_{output}]$) are updated using <a href=\"#eq-20\">eq (20)</a> and <a href=\"#eq-21\">eq (21)</a>. \n",
    "\n",
    "<div class=\"row\">\n",
    "    <div class=\"col\"><img src=\"jupyter_images/backward_4.png\"></div>\n",
    "    <div class=\"col-12\"><p class=\"image-description\">Figure 18: Updating $W_{input}$</p></div>\n",
    "</div>\n",
    "\n",
    "<div class=\"row\">\n",
    "    <div class=\"col\"><img src=\"jupyter_images/backward_5.png\"></div>\n",
    "    <div class=\"col-12\"><p class=\"image-description\">Figure 19: Updating $W_{output}$</p></div>\n",
    "</div>\n",
    "\n",
    "Note that for each iteration in the learning process, all weights in $W_{output}$ are updated, but only one row vector that corresponds to the center word is updated in $W_{input}$. When the model finishes updating both of the weight matrices, then one iteration is completed. The model then moves to the next iteration with the next center word. However, remember that this uses <a href=\"#eq-8\">eq (8)</a> as the cost function and assumes <a href=\"#stochastic\">stochastic gradient descent</a>. This means that one update is made for each training example. If <a href=\"#eq-9\">eq (9)</a> is used as a cost function instead (which is almost never the case), then one update is made for all $T$ training examples in the corpus."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
