{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0. Data set description\n",
    "    - Rock data: continuous variable\n",
    "    - Housing data: categorical variable\n",
    "1. Multiple Linear Regression\n",
    "    - Continuous\n",
    "    - Categorical\n",
    "2. Performance evalulation\n",
    "    - R^2\n",
    "    - Adjusted R^2\n",
    "3. Regression coefficient interpretation\n",
    "    - No collinearity\n",
    "        * (Cover independent features - hypothesis testing)\n",
    "    - Multicollinearity\n",
    "        * Unstable model parameters\n",
    "        * Detection\n",
    "        * Remedies\n",
    "            - Leave as it is\n",
    "            - PCA\n",
    "            - Drop one of the variables\n",
    "            - obtain more data\n",
    "            - feature selection\n",
    "4. Feature selection & ranking\n",
    "    - Forward selection\n",
    "    - Random forest feature importance\n",
    "    - Recursive feature elimnation?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We seldom encounter real-life problems that can be modeled by a single feature. In the other words, you can't model them by visualizing the data in a simple 2D plane. If "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hacker's Guide: Flowchart\n",
    "\n",
    "Many real-life problems are multivariate. Simple linear regression with only one feature is limited. It violates the assumption of linear regression - Independence of ??? \n",
    "\n",
    "1. Bivariate Linear regression\n",
    "    * Pythonic Tip: aa\n",
    "2. Multivariate linear regression. \n",
    "    There is a linear 2D plane. \n",
    "    * Pythonic Tip: aa\n",
    "3. Feature Ranking\n",
    "    Observations\n",
    "    \n",
    "    \n",
    "y = ax + b \n",
    "\n",
    "becomes\n",
    "\n",
    "y = a1x1 + a2x2 + a3x3 + b \n",
    "\n",
    "The multiple linear regression explains the relationship between one continuous dependent variable (y) and two or more independent variables (x1, x2, x3… etc).\n",
    "\n",
    "best fit line to represent the effects of your predictors and the dependant variable, and does not include the effects of one predictor on another.\n",
    "\n",
    "Linear regression assumes the features to be independent.\n",
    "\n",
    "## Why use multiple linear regression?\n",
    "\n",
    "Adding radio to the model leads to a substantial improvement in R-squared. This implies that a model that uses TV and radio expenditures to predict sales is substantially better than one that uses only TV advertising.\n",
    "\n",
    "## Does multi-colinearity affect prediction?\n",
    "\n",
    "Multicollinearity makes it hard to assess the relative importance of independent variables, but it does not affect the usefulness of the regression equation for prediction.\n",
    "\n",
    "There are usually two aims by using linear regression: identification of the predictor effect and prediction. For prediction purpose, multicoloinerity has no “bad” effect on it. You don’t need use PCA or ridge to make corrections.\n",
    "\n",
    "But for identification, the multicollinearity must be considered, hence you can use PCA or ridge or other approach\n",
    "\n",
    "Principal component analysis and ridge regression are popular used. \n",
    "\n",
    "Linear dependence between features induces the multicollinearity problem and leads to instability of the model and redundancy of the feature set.\n",
    "\n",
    "\n",
    "**Multi-collinearity makes the model parameters to be unreliable, individually**\n",
    "\n",
    " In the presence of multicollinearity, common methods of regression analysis, such as least squares, build unstable models of excessive complexity\n",
    "\n",
    "### Multi-colinearity\n",
    "\n",
    "one can be linearly predicted from the others with a substantial degree of accuracy\n",
    "\n",
    "Try seeing what happens if you use independent subsets of your data for estimation and apply those estimates to the whole data set. Theoretically you should obtain somewhat higher variance from the smaller datasets used for estimation, but the expectation of the coefficient values should be the same. Naturally, the observed coefficient values will vary, but look at how much they vary.\n",
    "\n",
    "In this situation the coefficient estimates of the multiple regression may change erratically in response to small changes in the model or the data. \n",
    "\n",
    " In the presence of multicollinearity, common methods of regression analysis, such as least squares, build unstable models of excessive complexity\n",
    "\n",
    "Multicollinearity is a strong correlation between features that affect the target vector simultaneously\n",
    "\n",
    "Independent variable is no longer independent. \n",
    "\n",
    "Consider PCA\n",
    "\n",
    "When the t-tests for each of the individual slopes are non-significant, but the overall F-test is significant. This is because multicollinearity causes some variables to seem useless, so lowering the t-stat, but has no effect on the F-statistics which takes an overall view.\n",
    "\n",
    "In regression, multicollinearity refers to the extent to which independent variables are correlated. Multicollinearity exists when:\n",
    "\n",
    "One independent variable is correlated with another independent variable.\n",
    "One independent variable is correlated with a linear combination of two or more independent variables.\n",
    "\n",
    "When one independent variable is highly correlated with another independent variable (or with a combination of two or more other independent variables), the marginal contribution of that independent variable is influenced by other independent variables. As a result:\n",
    "Estimates for regression coefficients can be unreliable.\n",
    "Tests of significance for regression coefficients can be misleading.\n",
    "\n",
    "This means that the analysis of regression coefficients should be preceded by an analysis of multicollinearity.\n",
    "\n",
    "If you only want to predict the value of a dependent variable, you may not have to worry about multicollinearity. Multiple regression can produce a regression equation that will work for you, even when independent variables are highly correlated.\n",
    "\n",
    "https://stattrek.com/multiple-regression/multicollinearity.aspx\n",
    "\n",
    "https://towardsdatascience.com/super-simple-machine-learning-by-me-multiple-linear-regression-part-1-447800e8b624\n",
    "\n",
    "https://www.kaggle.com/mashimo/features-selection-for-multiple-linear-regression\n",
    "\n",
    "### Performance Evaluation\n",
    "\n",
    "In case of more than 3D:\n",
    "\n",
    "They look at the magnitude of coefficients, and they test the statistical significance of coefficients.\n",
    "\n",
    "R^2\n",
    "\n",
    "If the coefficient for a particular variable is significantly greater than zero, researchers judge that the variable contributes to the predictive ability of the regression equation. In this way, it is possible to distinguish variables that are more useful for prediction from those that are less useful. **This approach works only when multi-collinearity is negligible.**\n",
    "\n",
    "One thing to note is that R-squared will always increase when more variables are added to the model, even if those variables are only weakly associated with the response.\n",
    "Therefore an adjusted R-squared is provided, which is R-squared adjusted by the number of predictors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression analysis\n",
    "\n",
    "A collinear system will have large standard errors, which makes the individual variables nonsignificant.\n",
    "\n",
    "keep in mind that large VIF or Condition Index only indicates presence of multicollinearity; but multicollinearity (MC) does not necessarily indicate a problem. MC only means the standard error will be inflated\n",
    "\n",
    "So even with the presence of MC the regression can still be precise if the \"inflated\" SE is still small enough."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Removal\n",
    "\n",
    "When you center the independent variables, it’s very convenient because you can interpret the regression coefficients in the usual way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Ranking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These methods are most often used for prediction, but looking at the feature importances can give you a sense of which of your variables have the most effect in these models. You can use that information to engineer new features, drop out features that look like noise, or just to inform you as you continue building models.\n",
    "\n",
    "Prior to inspecting the feature importances, it is important to check that the model predictive performance is high enough. Indeed there would be little interest of inspecting the important features of a non-predictive model.\n",
    "\n",
    "R^2 and adjusted R^2\n",
    "\n",
    "Could try LASSO regression, but LASSO does not work well in the presence of high multi-collinearity [here](https://stats.stackexchange.com/questions/25611/how-to-deal-with-multicollinearity-when-performing-variable-selection)\n",
    "\n",
    "I'm not actually dead-set on using linear regression: the only thing I want is to be able to understand which of the 9 variables is truly driving the variation in the Score variable. Preferably, this would be some method that takes the strong potential for collinearity in these 9 variables into account.\n",
    " \n",
    "For example in Ecology it is very common to calculate a correlation matrix between all the independent variables and remove one of them, when the correlation is bigger than 0.7.\n",
    " \n",
    "\"Feature Importance\" means which predictors drives the target variable the most. \n",
    "\n",
    "Training a model that accurately predicts outcomes is great, but most of the time you don't just need predictions, you want to be able to interpret your model. For example, if you build a model of house prices, knowing which features are most predictive of price tells us which features people are willing to pay for\n",
    "\n",
    "The intuition behind permutation importance is that if a feature is not useful for predicting an outcome, then altering or permuting its values will not result in a significant reduction in a model’s performance. This technique is commonly used in random forests\n",
    "\n",
    "**Random Forest Feature importance**\n",
    "\n",
    " - Random forest does not require normalization\n",
    "\n",
    "There is actually a second way of computing feature importance often called “Gini importance“. In the case of a random forest regressor, the importance is assessed by how much a feature decreases the weighted variance in a tree (in the case of classification it’s not variance but Gini impurity)\n",
    "\n",
    " In fact, the RF importance technique we'll introduce here (permutation importance) is applicable to any model, though few machine learning practitioners seem to realize this. Permutation importance is a common, reasonably efficient, and very reliable technique. It directly measures variable importance by observing the effect on model accuracy of randomly shuffling each predictor variable. This technique is broadly-applicable because it doesn't rely on internal model parameters, such as linear regression coefficients (which are really just poor proxies for feature importance).\n",
    " \n",
    " In scikit-learn, we implement the importance as described in [1] (often cited, but unfortunately rarely read...). It is sometimes called \"gini importance\" or \"mean decrease impurity\" and is defined as the total decrease in node impurity (weighted by the probability of reaching that node (which is approximated by the proportion of samples reaching that node)) averaged over all trees of the ensemble.\n",
    "\n",
    "In the literature or in some other packages, you can also find feature importances implemented as the \"mean decrease accuracy\". Basically, the idea is to measure the decrease in accuracy on OOB data when you randomly permute the values for that feature. If the decrease is low, then the feature is not important, and vice-versa.\n",
    "\n",
    "The mean decrease in impurity importance of a feature is computed by measuring how effective the feature is at reducing uncertainty (classifiers) or variance (regressors) when creating decision trees within RFs\n",
    "\n",
    "https://explained.ai/rf-importance/\n",
    "\n",
    "This inflates the importance of continuous & high-cardinality categorical variables\n",
    "\n",
    "The advantage of Random Forests, of course, is that they provide OOB samples by construction so users don't have to extract their own validation set and pass it to the feature importance function.\n",
    "\n",
    "**Permuation feature importance**\n",
    "\n",
    "Does not require normalization\n",
    "\n",
    "\n",
    "\n",
    "## Shapley Value\n",
    "\n",
    "The Shapley value, coined by Shapley (1953)41, is a method for assigning payouts to players depending on their contribution to the total payout. Players cooperate in a coalition and receive a certain profit from this cooperation.\n",
    " \n",
    "## Recursive VIF elimination\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
