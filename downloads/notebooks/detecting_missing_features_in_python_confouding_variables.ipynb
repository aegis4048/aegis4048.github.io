{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://statisticsbyjim.com/regression/confounding-variables-bias/\n",
    "\n",
    "If a residual is correlated to a feature, it is a sign of missing collinear features. We know which independent variable correlates with confounding variables\n",
    "\n",
    "You saw one method of detecting omitted variable bias in this post. If you include different combinations of independent variables in the model, and you see the coefficients changing, you’re watching omitted variable bias in action!\n",
    "\n",
    "**Multicollinearity**\n",
    "\n",
    "It’s important to note a tradeoff that might occur between precision and bias. As you include the formerly omitted variables, you lessen the bias, but the multicollinearity can potentially reduce the precision of the estimates.\n",
    "\n",
    "Bias exists if the residuals have an overall positive or negative mean\n",
    "\n",
    "**Constant term**\n",
    "\n",
    "A portion of the estimation process for the y-intercept is based on the exclusion of relevant variables from the regression model. When you leave relevant variables out, this can produce bias in the model. Bias exists if the residuals have an overall positive or negative mean. In other words, the model tends to make predictions that are systematically too high or too low. The constant term prevents this overall bias by forcing the residual mean to equal zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Semi-partial correlation\n",
    "\n",
    "https://ourcodingclub.github.io/2017/03/15/mixed-models.html#second\n",
    "\n",
    "But we are not interested in quantifying test scores for each specific mountain range: we just want to know whether body length affects test scores and we want to simply control for the variation coming from mountain ranges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=\"Model instability with feature selection\"></div>\n",
    "\n",
    "### 3.3. Model instability with feature selection\n",
    "\n",
    "In this section, I show that the values of regression coefficients fluctuate with different choices of features. We will use real-life data: rock properties and natural gas production. Please refer to <i>Section 0: Sample data description</i> of my <a href=\"https://aegis4048.github.io/mutiple_linear_regression_and_visualization_in_python#0.-Sample-data-description\" target=\"_blank\">previous post</a> for more information of this dataset. \n",
    "\n",
    "<a href=\"#fig-7\">Figure (7)</a> shows the relative importance of the individual features relative to the response variable, using permutation feature ranking. We see that <i>Por</i> and <i>Brittle</i> are the most important features.\n",
    "\n",
    "<div id=\"fig-7\" class=\"row full_screen_margin_md mobile_responsive_plot_full_width\" style=\"margin-top: 15px;\">\n",
    "    <div class=\"col\"><img src=\"jupyter_images/multiple_linear_permutation_feature_importance.png\"></div>\n",
    "    <div class=\"col-12\"><p class=\"image-description\">Figure 7: Permutation feature ranking</p></div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"solution_panel closed\">\n",
    "    <div class=\"solution_title\">\n",
    "        <p class=\"solution_title_string\">Source Code For Figure (7)</p>\n",
    "        <ul class=\"nav navbar-right panel_toolbox\">\n",
    "            <li><a class=\"collapse-link\"><i class=\"fa fa-chevron-down\"></i></a></li>\n",
    "        </ul>\n",
    "    <div class=\"clearfix\"></div>\n",
    "    </div>\n",
    "    <div class=\"solution_content\">\n",
    "        <pre>\n",
    "            <code class=\"language-python\">\n",
    "                import rfpimp\n",
    "                import pandas as pd\n",
    "                import numpy as np\n",
    "                from sklearn.ensemble import RandomForestRegressor\n",
    "                from sklearn.model_selection import train_test_split\n",
    "\n",
    "                ######################################## Data preparation #########################################\n",
    "                \n",
    "                # data source: https://github.com/GeostatsGuy/GeoDataSets/blob/master/unconv_MV_v5.csv\n",
    "                file = 'https://aegis4048.github.io/downloads/notebooks/sample_data/unconv_MV_v5.csv'\n",
    "                df = pd.read_csv(file)\n",
    "                features = ['Por', 'Perm', 'AI', 'Brittle', 'TOC', 'VR', 'Prod']\n",
    "\n",
    "                ######################################## Train/test split #########################################\n",
    "\n",
    "                df_train, df_test = train_test_split(df, test_size=0.20)\n",
    "                df_train = df_train[features]\n",
    "                df_test = df_test[features]\n",
    "\n",
    "                X_train, y_train = df_train.drop('Prod',axis=1), df_train['Prod']\n",
    "                X_test, y_test = df_test.drop('Prod',axis=1), df_test['Prod']\n",
    "\n",
    "                ################################################ Train #############################################\n",
    "\n",
    "                rf = RandomForestRegressor(n_estimators=100, n_jobs=-1)\n",
    "                rf.fit(X_train, y_train)\n",
    "\n",
    "                ############################### Permutation feature importance #####################################\n",
    "\n",
    "                imp = rfpimp.importances(rf, X_test, y_test)\n",
    "\n",
    "                ############################################## Plot ################################################\n",
    "\n",
    "                fig, ax = plt.subplots(figsize=(6, 3))\n",
    "\n",
    "                ax.barh(imp.index, imp['Importance'], height=0.8, facecolor='grey', alpha=0.8, edgecolor='k')\n",
    "                ax.set_xlabel('Importance score')\n",
    "                ax.set_title('Permutation feature importance')\n",
    "                ax.text(0.8, 0.15, 'aegis4048.github.io', fontsize=12, ha='center', va='center',\n",
    "                        transform=ax.transAxes, color='grey', alpha=0.5)\n",
    "                plt.gca().invert_yaxis()\n",
    "\n",
    "                fig.tight_layout()\n",
    "            </code>\n",
    "        </pre>\n",
    "    </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the result of feature ranking, we train a linear model with two features: <code>features = ['Por', 'Brittle']</code>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import linear_model\n",
    "\n",
    "# data source: https://github.com/GeostatsGuy/GeoDataSets/blob/master/unconv_MV_v5.csv\n",
    "file = 'https://aegis4048.github.io/downloads/notebooks/sample_data/unconv_MV_v5.csv'\n",
    "df = pd.read_csv(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features                :  ['Por', 'Brittle']\n",
      "Regression Coefficients :  [320.39, 31.38]\n",
      "R-squared               :  0.93\n",
      "Y-intercept             :  -2003.01\n"
     ]
    }
   ],
   "source": [
    "features = ['Por', 'Brittle']\n",
    "target = 'Prod'\n",
    "\n",
    "X = df[features].values.reshape(-1, len(features))\n",
    "y = df[target]\n",
    "\n",
    "ols = linear_model.LinearRegression()\n",
    "model = ols.fit(X, y)\n",
    "\n",
    "print('Features                :  %s' % features)\n",
    "print('Regression Coefficients : ', [round(item, 2) for item in model.coef_])\n",
    "print('R-squared               :  %.2f' % model.score(X, y))\n",
    "print('Y-intercept             :  %.2f' % model.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"margin-top: -20px\"></div>\n",
    "\n",
    "The trained model explains 93% variability (<code>R-squared</code>) of the data with two features. Let's say that we are not satisfied with the R-squared value, and that we want a more powerful predictive model. We proceed to train a new linear model with six features: <code>features = ['Por', 'Brittle', 'Perm', 'TOC', 'AI', 'VR']</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features                :  ['Por', 'Brittle', 'Perm', 'TOC', 'AI', 'VR']\n",
      "Regression Coefficients :  [230.3, 25.0, 116.23, -77.44, -363.74, 783.19]\n",
      "R-squared               :  0.96\n",
      "Y-intercept             :  -1230.26\n"
     ]
    }
   ],
   "source": [
    "features = ['Por', 'Brittle', 'Perm', 'TOC', 'AI', 'VR']\n",
    "target = 'Prod'\n",
    "\n",
    "X = df[features].values.reshape(-1, len(features))\n",
    "y = df[target]\n",
    "\n",
    "ols = linear_model.LinearRegression()\n",
    "model = ols.fit(X, y)\n",
    "\n",
    "print('Features                :  %s' % features)\n",
    "print('Regression Coefficients : ', [round(item, 2) for item in model.coef_])\n",
    "print('R-squared               :  %.2f' % model.score(X, y))\n",
    "print('Y-intercept             :  %.2f' % model.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"margin-top: -20px\"></div>\n",
    "\n",
    "Indeed, the addition of more features increased the <code>R-squared</code> value from 93% to 96%. However, we observe significant changes in the values of the regression coefficients. For <code>Por</code>, the coefficient jumped from 320.4 $\\longrightarrow$ 230.3. For <code>Brittle</code>, the coefficient jumped from 31.4 $\\longrightarrow$ 25.0. I simulated more different combinations of features, and showed the results in <a href=\"#fig-8\">figure (8)</a>. Even the <code>R-squared</code> value is over 93%, the regression coefficient value for <code>Por</code> seems to always fluctuate.\n",
    "\n",
    "<div id=\"fig-8\" class=\"row full_screen_margin_md mobile_responsive_plot_full_width\" style=\"margin-top: 15px;\">\n",
    "    <div class=\"col\"><img src=\"jupyter_images/multiple_linear_model_instability.png\"></div>\n",
    "    <div class=\"col-12\"><p class=\"image-description\">Figure 8: Unstable regression coefficients due to multicollinearity</p></div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"solution_panel closed\">\n",
    "    <div class=\"solution_title\">\n",
    "        <p class=\"solution_title_string\">Source Code For Figure (8)</p>\n",
    "        <ul class=\"nav navbar-right panel_toolbox\">\n",
    "            <li><a class=\"collapse-link\"><i class=\"fa fa-chevron-down\"></i></a></li>\n",
    "        </ul>\n",
    "    <div class=\"clearfix\"></div>\n",
    "    </div>\n",
    "    <div class=\"solution_content\">\n",
    "        <pre>\n",
    "            <code class=\"language-python\">\n",
    "                import pandas as pd\n",
    "                import numpy as np\n",
    "                from sklearn import linear_model\n",
    "                import matplotlib.pyplot as plt\n",
    "                \n",
    "                # data source: https://github.com/GeostatsGuy/GeoDataSets/blob/master/unconv_MV_v5.csv\n",
    "                file = 'https://aegis4048.github.io/downloads/notebooks/sample_data/unconv_MV_v5.csv'\n",
    "                df = pd.read_csv(file)\n",
    "\n",
    "                ########################################################################################\n",
    "\n",
    "                features = ['Por', 'Brittle', 'Perm', 'TOC', 'AI', 'VR']\n",
    "                target = 'Prod'\n",
    "\n",
    "                X = df[features].values.reshape(-1, len(features))\n",
    "                y = df[target].values\n",
    "\n",
    "                ols = linear_model.LinearRegression()\n",
    "                model = ols.fit(X, y)\n",
    "\n",
    "                print('Features                :  %s' % features)\n",
    "                print('Regression Coefficients : ', [round(item, 2) for item in model.coef_])\n",
    "                print('R-squared               :  %.2f' % model.score(X, y))\n",
    "                print('Y-intercept             :  %.2f' % model.intercept_)\n",
    "                print('')\n",
    "\n",
    "                ########################################################################################\n",
    "\n",
    "                features = ['Por', 'Brittle', 'Perm', 'TOC', 'VR']\n",
    "                target = 'Prod'\n",
    "\n",
    "                X = df[features].values.reshape(-1, len(features))\n",
    "                y = df[target].values\n",
    "\n",
    "                ols = linear_model.LinearRegression()\n",
    "                model = ols.fit(X, y)\n",
    "\n",
    "                print('Features                :  %s' % features)\n",
    "                print('Regression Coefficients : ', [round(item, 2) for item in model.coef_])\n",
    "                print('R-squared               :  %.2f' % model.score(X, y))\n",
    "                print('Y-intercept             :  %.2f' % model.intercept_)\n",
    "                print('')\n",
    "\n",
    "                ########################################################################################\n",
    "\n",
    "                features = ['Por', 'Brittle', 'Perm', 'TOC', 'AI']\n",
    "                target = 'Prod'\n",
    "\n",
    "                X = df[features].values.reshape(-1, len(features))\n",
    "                y = df[target].values\n",
    "\n",
    "                ols = linear_model.LinearRegression()\n",
    "                model = ols.fit(X, y)\n",
    "\n",
    "                print('Features                :  %s' % features)\n",
    "                print('Regression Coefficients : ', [round(item, 2) for item in model.coef_])\n",
    "                print('R-squared               :  %.2f' % model.score(X, y))\n",
    "                print('Y-intercept             :  %.2f' % model.intercept_)\n",
    "                print('')\n",
    "\n",
    "                ########################################################################################\n",
    "\n",
    "                features = ['Por', 'Brittle', 'Perm', 'TOC']\n",
    "                target = 'Prod'\n",
    "\n",
    "                X = df[features].values.reshape(-1, len(features))\n",
    "                y = df[target].values\n",
    "\n",
    "                ols = linear_model.LinearRegression()\n",
    "                model = ols.fit(X, y)\n",
    "\n",
    "                print('Features                :  %s' % features)\n",
    "                print('Regression Coefficients : ', [round(item, 2) for item in model.coef_])\n",
    "                print('R-squared               :  %.2f' % model.score(X, y))\n",
    "                print('Y-intercept             :  %.2f' % model.intercept_)\n",
    "                print('')\n",
    "\n",
    "                ########################################################################################\n",
    "\n",
    "                features = ['Por', 'Brittle', 'Perm', 'AI']\n",
    "                target = 'Prod'\n",
    "\n",
    "                X = df[features].values.reshape(-1, len(features))\n",
    "                y = df[target].values\n",
    "\n",
    "                ols = linear_model.LinearRegression()\n",
    "                model = ols.fit(X, y)\n",
    "\n",
    "                print('Features                :  %s' % features)\n",
    "                print('Regression Coefficients : ', [round(item, 2) for item in model.coef_])\n",
    "                print('R-squared               :  %.2f' % model.score(X, y))\n",
    "                print('Y-intercept             :  %.2f' % model.intercept_)\n",
    "                print('')\n",
    "\n",
    "                ########################################################################################\n",
    "\n",
    "                features = ['Por', 'Brittle', 'Perm', 'VR']\n",
    "                target = 'Prod'\n",
    "\n",
    "                X = df[features].values.reshape(-1, len(features))\n",
    "                y = df[target].values\n",
    "\n",
    "                ols = linear_model.LinearRegression()\n",
    "                model = ols.fit(X, y)\n",
    "\n",
    "                print('Features                :  %s' % features)\n",
    "                print('Regression Coefficients : ', [round(item, 2) for item in model.coef_])\n",
    "                print('R-squared               :  %.2f' % model.score(X, y))\n",
    "                print('Y-intercept             :  %.2f' % model.intercept_)\n",
    "                print('')\n",
    "\n",
    "                ########################################################################################\n",
    "\n",
    "                features = ['Por', 'Brittle', 'TOC', 'VR']\n",
    "                target = 'Prod'\n",
    "\n",
    "                X = df[features].values.reshape(-1, len(features))\n",
    "                y = df[target].values\n",
    "\n",
    "                ols = linear_model.LinearRegression()\n",
    "                model = ols.fit(X, y)\n",
    "\n",
    "                print('Features                :  %s' % features)\n",
    "                print('Regression Coefficients : ', [round(item, 2) for item in model.coef_])\n",
    "                print('R-squared               :  %.2f' % model.score(X, y))\n",
    "                print('Y-intercept             :  %.2f' % model.intercept_)\n",
    "                print('')\n",
    "\n",
    "                ########################################################################################\n",
    "\n",
    "                features = ['Por', 'Brittle', 'TOC']\n",
    "                target = 'Prod'\n",
    "\n",
    "                X = df[features].values.reshape(-1, len(features))\n",
    "                y = df[target].values\n",
    "\n",
    "                ols = linear_model.LinearRegression()\n",
    "                model = ols.fit(X, y)\n",
    "\n",
    "                print('Features                :  %s' % features)\n",
    "                print('Regression Coefficients : ', [round(item, 2) for item in model.coef_])\n",
    "                print('R-squared               :  %.2f' % model.score(X, y))\n",
    "                print('Y-intercept             :  %.2f' % model.intercept_)\n",
    "                print('')\n",
    "\n",
    "                ########################################################################################\n",
    "\n",
    "                features = ['Por', 'Brittle', 'VR']\n",
    "                target = 'Prod'\n",
    "\n",
    "                X = df[features].values.reshape(-1, len(features))\n",
    "                y = df[target].values\n",
    "\n",
    "                ols = linear_model.LinearRegression()\n",
    "                model = ols.fit(X, y)\n",
    "\n",
    "                print('Features                :  %s' % features)\n",
    "                print('Regression Coefficients : ', [round(item, 2) for item in model.coef_])\n",
    "                print('R-squared               :  %.2f' % model.score(X, y))\n",
    "                print('Y-intercept             :  %.2f' % model.intercept_)\n",
    "                print('')\n",
    "\n",
    "                ########################################################################################\n",
    "\n",
    "                features = ['Por', 'Brittle', 'AI']\n",
    "                target = 'Prod'\n",
    "\n",
    "                X = df[features].values.reshape(-1, len(features))\n",
    "                y = df[target].values\n",
    "\n",
    "                ols = linear_model.LinearRegression()\n",
    "                model = ols.fit(X, y)\n",
    "\n",
    "                print('Features                :  %s' % features)\n",
    "                print('Regression Coefficients : ', [round(item, 2) for item in model.coef_])\n",
    "                print('R-squared               :  %.2f' % model.score(X, y))\n",
    "                print('Y-intercept             :  %.2f' % model.intercept_)\n",
    "                print('')\n",
    "\n",
    "                ########################################################################################\n",
    "\n",
    "                features = ['Por', 'Brittle']\n",
    "                target = 'Prod'\n",
    "\n",
    "                X = df[features].values.reshape(-1, len(features))\n",
    "                y = df[target].values\n",
    "\n",
    "                ols = linear_model.LinearRegression()\n",
    "                model = ols.fit(X, y)\n",
    "\n",
    "                print('Features                :  %s' % features)\n",
    "                print('Regression Coefficients : ', [round(item, 2) for item in model.coef_])\n",
    "                print('R-squared               :  %.2f' % model.score(X, y))\n",
    "                print('Y-intercept             :  %.2f' % model.intercept_)\n",
    "            </code>\n",
    "        </pre>\n",
    "    </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe instability in regression coefficients with feature selection, because this particular data set shows high multicollinearity, as shown by the variance inflation factors (VIF) of the features. I discuss VIF in more detail in <i>Section 4.1: VIF</i> <a href=\"#VIF\">below</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
