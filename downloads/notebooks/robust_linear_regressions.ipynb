{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c847a4ed",
   "metadata": {},
   "source": [
    "<div id=\"toc_container\" class=\"display-none\">\n",
    "    <p class=\"toc_title\"><i class=\"fas fa-list\"></i>Contents</p>\n",
    "    <ul class=\"toc_list\">\n",
    "        <li><a href=\"#1.-Why-is-OLS-vulnerable-to-outliers?\"><span class=\"toc_label\">1.</span>\n",
    "            Why is OLS vulnerable to outliers?</a></li>\n",
    "        <ul>\n",
    "            <li><a href=\"#1.1.-Convergence-to-mean\"><span class=\"toc_label\">1.1.</span>\n",
    "                Convergence to mean</a></li>\n",
    "            <li><a href=\"#1.2.-Squared-term-magnifies-impact-of-outliers\"><span class=\"toc_label\">1.2.</span>\n",
    "                Squared term magnifies impact of outliers</a></li>\n",
    "            <li><a href=\"#1.3.-Understanding-the-L-norms-in-cartesian-space\"><span class=\"toc_label\">1.3.</span>\n",
    "                Understanding the L-norms in cartesian space</a></li>\n",
    "        </ul>\n",
    "        <li><a href=\"#2.-Robust-Models\"><span class=\"toc_label\">2.</span>\n",
    "            Robust Models</a></li>\n",
    "        <ul>\n",
    "            <li><a href=\"#2.1.-RANSAC-regressor\"><span class=\"toc_label\">2.1.</span>\n",
    "                RANSAC regressor</a></li>\n",
    "                 <ul>\n",
    "                    <li><a href=\"#2.1.1.-Visual-demonstrations\"><span class=\"toc_label\">2.1.1.</span>\n",
    "                        Visual demonstrations</a></li>\n",
    "                    <li><a href=\"#2.1.2.-Median-Absolute-Deviation-(MAD)-threshold\"><span class=\"toc_label\">2.1.2.</span>\n",
    "                        Median Absolute Deviation (MAD) threshold</a></li>\n",
    "                    <li><a href=\"#2.1.3.-Required-iteration-calculation\"><span class=\"toc_label\">2.1.3.</span>\n",
    "                        Required iteration calculation</a></li>\n",
    "                    <li><a href=\"#2.1.4.-More-detailed-visual-demonstrations\"><span class=\"toc_label\">2.1.4.</span>\n",
    "                        More detailed visual demonstrations</a></li>\n",
    "                    <li><a href=\"#2.1.5.-RANSAC-code-snippets\"><span class=\"toc_label\">2.1.5.</span>\n",
    "                        RANSAC code snippets</a></li>\n",
    "                </ul>\n",
    "            <li><a href=\"#2.2.-Huber-regressor\"><span class=\"toc_label\">2.2.</span>\n",
    "                Huber regressor</a></li>\n",
    "                 <ul>\n",
    "                    <li><a href=\"#2.2.1.-Huber-loss-function\"><span class=\"toc_label\">2.2.1.</span>\n",
    "                        Huber loss function</a></li>\n",
    "                    <li><a href=\"#2.2.2.-Motivation\"><span class=\"toc_label\">2.2.2.</span>\n",
    "                        Motivation</a></li>\n",
    "                    <li><a href=\"#2.2.3.-Parameter-tuning-($\\delta$)\"><span class=\"toc_label\">2.2.3.</span>\n",
    "                        Parameter tuning</a></li>\n",
    "                    <li><a href=\"#2.2.4.-Huber-code-snippets\"><span class=\"toc_label\">2.2.4.</span>\n",
    "                        Huber code snippets</a></li>\n",
    "                </ul>\n",
    "            <li><a href=\"#2.3.-Theil-Sen-regressor\"><span class=\"toc_label\">2.3.</span>\n",
    "                Theil-Sen regressor</a></li>\n",
    "                 <ul>\n",
    "                    <li><a href=\"#2.3.1.-Sample-size-and-model-robustness\"><span class=\"toc_label\">2.3.1.</span>\n",
    "                        Sample size and model robustness</a></li>\n",
    "                    <li><a href=\"#2.3.2.-Spatial-median\"><span class=\"toc_label\">2.3.2.</span>\n",
    "                        Spatial median</a></li>\n",
    "                    <li><a href=\"#2.3.3.-Why-use-median-instead-of-mean-with-outliers?\"><span class=\"toc_label\">2.3.3.</span>\n",
    "                        Why use median instead of mean with outliers?</a></li>\n",
    "                     <ul>\n",
    "                        <li><a href=\"#2.3.3.1.-Effect-of-the-squared-term\"><span class=\"toc_label\">2.3.3.1.</span>\n",
    "                            Effect of the squared term</a></li>\n",
    "                        <li><a href=\"#2.3.3.2.-Measure-of-central-tendency\"><span class=\"toc_label\">2.3.3.2.</span>\n",
    "                            Measure of central tendency</a></li>\n",
    "                    </ul>\n",
    "                    <li><a href=\"#2.3.4.-Theil-Sen-code-snippets\"><span class=\"toc_label\">2.3.4.</span>\n",
    "                        Theil-Sen code snippets</a></li>\n",
    "                </ul>\n",
    "            <li><a href=\"#2.4.-Summary\"><span class=\"toc_label\">2.4.</span>\n",
    "                Summary</a></li>\n",
    "        </ul>\n",
    "        <li><a href=\"#3.-Extension-to-3D+-multivariate-linear-regressions\"><span class=\"toc_label\">3.</span>\n",
    "            Extension to 3D+ multivariate linear regressions</a></li>\n",
    "        <ul><li><a href=\"#3.1.-Visual-demonstrations\"><span class=\"toc_label\">3.1.</span>\n",
    "            Visual demonstrations</a></li>\n",
    "        </ul>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f28189e",
   "metadata": {},
   "source": [
    "In regression analysis, understanding and mitigating the impact of outliers is crucial for accurate model predictions. Common approaches like Ordinary Least Squares (OLS) often fall short when outliers are present, leading to skewed results. This discussion will explore how various regression techniques, including OLS, RANSAC, Huber, and Theil-Sen regressors, each handle outliers differently. By examining these methods through both theoretical insights and practical demonstrations using Python, I aim to highlight their unique responses to outlier influences, thereby guiding the selection of the most appropriate regression model for datasets with varying outlier characteristics.\n",
    "\n",
    "<div id=\"fig-1\" class=\"row full_screen_margin_90 mobile_responsive_plot_full_width\" style=\"\">\n",
    "<div class=\"col\"><img src=\"jupyter_images/robust linear regression comprisons.png\"></div>\n",
    "</div>\n",
    "<div class=\"col-12 fig-title\"><p class=\"image-description\"><strong>Figure 1:</strong> In scenarios involving outliers, the RANSAC regressor is often the preferred choice as it identifies and excludes outliers before fitting the model. Other robust regressors, like Huber and Theil-Sen, aim to <i>dampen</i> the impact of outliers rather than exclude them. This approach is beneficial if the so-called 'outliers' are actually integral parts of the dataset, needing consideration rather than exclusion. In terms of robustness to outliers, the hierarchy typically follows: RANSAC > Theil-Sen > Huber > OLS. Note that the points labeled with the 'x' markers are outliers detected by the RANSAC model.</p></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "480b7f71",
   "metadata": {},
   "source": [
    "<div class=\"solution_panel closed\">\n",
    "    <div class=\"solution_title\">\n",
    "        <p class=\"solution_title_string\">Source Code For Figure (1)</p>\n",
    "        <ul class=\"nav navbar-right panel_toolbox\">\n",
    "            <li><a class=\"collapse-link\"><i class=\"fa fa-chevron-down\"></i></a></li>\n",
    "        </ul>\n",
    "    <div class=\"clearfix\"></div>\n",
    "    </div>\n",
    "    <div class=\"solution_content\">\n",
    "        <pre>\n",
    "            <code class=\"language-python\">\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import linear_model\n",
    "from sklearn.linear_model import RANSACRegressor, HuberRegressor, TheilSenRegressor\n",
    "\n",
    "###################################### data ######################################\n",
    "\n",
    "X = np.array([ 0.15, -0.34, 0.32, 0.43, -0.4, -0.04, -0.51, 0.3, 0.47, 0.12, 0.08, 0.04, -0.08,\n",
    "              -0.23, 0.08, -0.03, 0.03, 0.04, 0.01, 0.06, 0.03, 0., -0.04, -0.18, -0.19, -0.06,\n",
    "              -0.26, -0.16, 0.13, 0.09, 0.03, -0.03, 0.04, 0.14, -0.01, 0.4, -0.06, 0.15, 0.08,\n",
    "              0.05, -0.15, -0.09, -0.15, -0.11, -0.07, -0.19, -0.06, 0.17, 0.23, 0.18]).reshape(-1, 1)\n",
    "y = np.array([17.44, 25.46, 18.61, 26.07, 24.96, -1.22, 26.45, 26.5, 20.57, 3.08, 35.9 , 32.47,\n",
    "              20.84, 13.37, 42.44, 27.23, 35.65, 29.51, 31.28, 41.34, 32.19, 33.67, 25.64, 9.3,\n",
    "              14.63, 25.1, 4.69, 14.42, 47.53, 33.82, 32.2 , 24.81, 32.64, 45.11, 26.76, 68.01,\n",
    "              23.39, 43.49, 37.88, 36.01, 16.32, 19.77, 16.34, 19.57, 29.28, 16.62, 24.39, 43.77,\n",
    "              50.46, 47.09])\n",
    "\n",
    "################################### Model fits ###################################\n",
    "\n",
    "# Ordinary Least Squares\n",
    "ols = linear_model.LinearRegression().fit(X, y)\n",
    "y_pred_ols = ols.predict(X)\n",
    "coefs_ols = ols.coef_\n",
    "intercept_ols = ols.intercept_\n",
    "\n",
    "# RANSAC\n",
    "ransac = RANSACRegressor(random_state=1).fit(X, y)\n",
    "y_pred_ransac = ransac.predict(X)\n",
    "coefs_ransac = ransac.estimator_.coef_\n",
    "intercept_ransac = ransac.estimator_.intercept_\n",
    "\n",
    "# Huber\n",
    "huber = HuberRegressor().fit(X, y)\n",
    "y_pred_huber = huber.predict(X)\n",
    "coefs_huber = huber.coef_\n",
    "intercept_huber = huber.intercept_\n",
    "\n",
    "# TheilSen\n",
    "TS = TheilSenRegressor().fit(X, y)\n",
    "y_pred_TS = TS.predict(X)\n",
    "coefs_TS = TS.coef_\n",
    "intercept_TS = TS.intercept_\n",
    "\n",
    "########################## Outliers detected - RANSAC ############################\n",
    "\n",
    "X_outlier_ransac = X[~ransac.inlier_mask_ ]\n",
    "y_outlier_ransac = y[~ransac.inlier_mask_ ]\n",
    "\n",
    "#################################### Plotting ####################################\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 4.5))\n",
    "\n",
    "ax.scatter(X_outlier_ransac, y_outlier_ransac, s=100, c='#ff7f0e', marker='x')\n",
    "ax.scatter(X, y, s=100, fc='grey', lw=1, edgecolors='k', alpha=0.3)\n",
    "ax.plot(X, y_pred_ols, label='OLS')\n",
    "ax.plot(X, y_pred_ransac, label='RANSAC')\n",
    "ax.plot(X, y_pred_huber, label='Huber')\n",
    "ax.plot(X, y_pred_TS, label='TheilSen')\n",
    "\n",
    "ax.legend(fontsize=12, ncol=4)\n",
    "\n",
    "ax.grid(axis='y', linestyle='--', color='#acacac', alpha=0.5)\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "\n",
    "ax.set_xlabel('X', fontsize=13)\n",
    "ax.set_ylabel('y', fontsize=13)\n",
    "ax.text(0.99, 0.1, 'aegis4048.github.io', fontsize=12, ha='right', va='center',\n",
    "    transform=ax.transAxes, color='grey', alpha=0.5)\n",
    "\n",
    "def setbold(txt):\n",
    "    return ' '.join([r\"$\\bf{\" + item + \"}$\" for item in txt.split(' ')])\n",
    "\n",
    "bold_txt = setbold('Robust Linear Regression')\n",
    "plain_txt = r', performance comprisons of OLS vs. robust models'\n",
    "fig.suptitle(bold_txt + plain_txt, verticalalignment='top', x=0, horizontalalignment='left', fontsize=13, y=0.96)\n",
    "yloc = 0.88\n",
    "ax.annotate('', xy=(0.01, yloc + 0.01), xycoords='figure fraction', xytext=(1.02, yloc + 0.01),\n",
    "            arrowprops=dict(arrowstyle=\"-\", color='k', lw=0.7))\n",
    "\n",
    "fig.tight_layout()\n",
    "            </code>\n",
    "        </pre>\n",
    "    </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4f7d92e",
   "metadata": {},
   "source": [
    "## 1. Why is OLS vulnerable to outliers?\n",
    "\n",
    "A simple 2D Ordinary Least Squares (OLS) solves for arguments (slope $m$ and intercept $b$) that minimize the following objective (loss) function:\n",
    "\n",
    "<div id=\"eq-1\" style=\"font-size: 1rem;\">\n",
    "$$L_{2}^{2} = \\underset{m, b}{\\text{argmin}} \\sum_{i=1}^n [y_i - (mx_i + b)]^2 \\tag{1}$$\n",
    "</div>\n",
    "\n",
    "Intuitively the objective function is finding the best slope and intercept that will minimize the **squared** prediction error (residuals) of your model prediction $\\hat{y} = mX + b$ against the observation $y$. <a href=\"#eq-1\" class=\"internal-link\">Eq-1</a> is often referred to as *$L_{2}$-Norm squared*. It has some interesting properties that make it vulnerable to the outliers:\n",
    "\n",
    "<div><hr></div>\n",
    "\n",
    "\n",
    "<div id=\"The equation converges to mean, and inherits the statistical properties of mean\"></div>\n",
    "\n",
    "### 1.1. Convergence to mean\n",
    "\n",
    "The equation has mathematical properties that make it converge to mean. While I'm not gonna bore the readers explaining the mathematical proof (I don't understand them anyway), I can show you in Python that it indeed converges to the mean. Consider the following code snippet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4200e96c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L2-squared minimized: [14.5]\n",
      "Mean of x: [14.5]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "# Implemenation of eq-1 above\n",
    "def L2_norm_squared(point, _x):\n",
    "    return np.sum((_x - point[0])**2)\n",
    "\n",
    "x = np.array([3, 9, 21, 25])\n",
    "x0 = 0   # initial guess, this can be anything\n",
    "\n",
    "L2 = minimize(L2_norm_squared, x0, args=(x), method='Nelder-Mead')\n",
    "\n",
    "print(\"L2-squared minimized:\", L2.x)\n",
    "print(\"Mean of x:\", [np.mean(x)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a92d5c9",
   "metadata": {},
   "source": [
    "<div style='margin-top: -15px;'></div>\n",
    "\n",
    "Observe that the parameter <code>14.5</code>, which minimizes the objective function, is also the sample mean. This is because the objective function converges towards the mean. However, the problem with the mean is that it's not the best measure of central tendency in the presence of outliers that induce skewness. Consider the below figure:\n",
    "\n",
    "<div id=\"fig-2\" class=\"row full_screen_margin_60 mobile_responsive_plot_full_width\" style=\"\">\n",
    "<div class=\"col\"><img src=\"jupyter_images/mean - median - mode - outliers.png\"></div>\n",
    "</div>\n",
    "<div class=\"col-12 fig-title\"><p class=\"image-description\"><strong>Figure 2:</strong> Three measures of central tendency in presence of outliers: mode, median and mean (image adpated from <a href=\"https://www.researchgate.net/publication/10633674_Comparison_of_mode_estimation_methods_and_application_in_molecular_clock_analysis\" target=\"_blank\">here).</a></p></div>\n",
    "\n",
    "In an ideal normal distribution without outliers, the mode, median, and mean coincide. However, outliers skew the distribution, pulling the mean away from where most data points lie. The median, less affected by outliers, provides a better measure of central tendency in such cases, leading to the preference for median-based estimators like the*$L_{1}$-Norm* absolute loss over squared loss. <a href=\"#fig-18\" class=\"internal-link\">Figures 18 and 19</a> below shows visual demonstrations of superior performance of median-based estimators in presence of outliers. \n",
    "\n",
    "<div id=\"Because the residuals are squared, large outliers disproportionately affects model fit\"></div>\n",
    "\n",
    "### 1.2. Squared term magnifies impact of outliers\n",
    "\n",
    "The OLS loss function aims to minimize the squared residuals, which escalates the influence of outliers because the <u>loss increases exponentially with larger residuals</u>. This can lead to a poor fit. Less severe loss functions, like the regular *$L_{2}$-Norm* (square root of sum of squared residuals):\n",
    "\n",
    "<div id=\"eq-2\" style=\"font-size: 1rem;\">\n",
    "$$L_2 = \\underset{m, b}{\\text{argmin}} \\sqrt{\\sum_{i=1}^n [y_i - (mx_i + b)]^2} \\tag{2}$$\n",
    "</div>\n",
    "\n",
    "or *$L_{1}$-Norm* (absolute residuals): \n",
    "\n",
    "<div id=\"eq-3\" style=\"font-size: 1rem;\">\n",
    "$$L_1 = \\underset{m, b}{\\text{argmin}} \\sum_{i=1}^n |y_i - (mx_i + b)| \\tag{3}$$\n",
    "</div>\n",
    "\n",
    "can be more resilient to outliers, dampening their effects. <a href=\"#eq-2\" class=\"internal-link\">Eq-2</a> has the squared term, but the effect of squaring is mitigated by taking the squared root, making it more robust than *$L_{2}$-Norm squared* . <a href=\"#eq-3\" class=\"internal-link\">Eq-3</a> on the other hand, has no squared term at all, making it the most robust choice among the three.\n",
    "\n",
    "<div><hr></div>\n",
    "\n",
    "<div id=\"Understanding the L-norms in cartesian space\"></div>\n",
    "\n",
    "### 1.3. Understanding the L-norms in cartesian space\n",
    "\n",
    "Interesting observations arise when the norms are adapted in a Cartesian space, and these can be used to illustrate why the OLS is vulnerable to outliers. Consider four data points in a 2D Cartesian space along the x-axis and y-axis. Minimizing the *$L_{2}$-Norm squared* (<a href=\"#eq-1\" class=\"internal-link\">eq-1</a>, which is the objective function of the OLS) for the distance from these four points identifies the centroid (center of mass) coordinate that minimizes the sum of the squares of distances to each point. This point, having the smallest average distance to each of the four points, can be found through a simple formula: its x and y coordinates are the averages of the x's and y's of the four points, respectively. This is possible because the *$L_{2}$-Norm squared* has the mathematical property of converging to the mean, as demonstrated <a href=\"#The equation converges to mean, and inherits the statistical properties of mean\" class=\"internal-link\">above.</a>\n",
    "\n",
    "On the other hand, minimizing the *$L_{2}$-Norm* <a href=\"#eq-2\" class=\"internal-link\">eq-2</a> identifies a point that minimizes the sum of <a href=\"https://en.wikipedia.org/wiki/Euclidean_distance\" target=\"_blank\">Euclidean distances</a> to each point, known as the <a href=\"https://en.wikipedia.org/wiki/Geometric_median\" target=\"_blank\">spatial (geometric) median.</a> Note that the *$L_{2}$-Norm* essentially generalizes the Pythagorean theorem for n-dimensional spaces.\n",
    "\n",
    "(Minimizing the *$L_{1}$-Norm* absolute loss <a href=\"#eq-3\" class=\"internal-link\">eq-3</a> reduces the <a href=\"https://en.wikipedia.org/wiki/Taxicab_geometry\" target=\"_blank\">Manhattan distances</a> from all points, a process without any specific Cartesian property associated with it)\n",
    "\n",
    "<a href=\"#fig-3\" class=\"internal-link\">Figure 3</a> below demonstrates how outliers affect the mean (centroid, *$L_{2}$-Norm squared*) compared to the median (spatial median, *$L_{2}$-Norm*). When an outlier with an extreme y-value is introduced, it shifts the mean upwards more significantly than the median. This difference arises because the median's objective function computes the square root of squared residuals, thereby reducing the impact of squared errors and assigning less weight to this outlier than the mean-based estimator. Given that such outliers are typically undesirable in regression model fitting, methods that either mitigate (like <a href=\"#Theil-Sen regressor\" class=\"internal-link\">Theil-Sen</a> and <a href=\"#Huber regressor\" class=\"internal-link\">Huber</a> regressors) or eliminate (like the <a href=\"#RANSAC regressor\" class=\"internal-link\">RANSAC</a> regressor) the influence of these outliers are preferred.\n",
    "\n",
    "\n",
    "<div id=\"fig-3\" class=\"row\" style=\"\">\n",
    "<div class=\"col\"><img src=\"jupyter_images/mean vs median - effects of outliers.png\"></div>\n",
    "<div class=\"col-12\"><p class=\"image-description\"><strong>Figure 3:</strong> The graph illustrates the robustness of median-based estimators compared to mean-based estimators in the presence of outliers. The lef plot shows a dataset without outliers. The mean (green plus) represents the centroid (a point that minimizes the average distances) of the four points and the median (orange star) shows the spatial median (a point that minimizes the sum of Euclidean distances). On the right, the introduction of an outlier (marked with a red 'x') significantly shifts the mean upwards, whereas the median remains relatively stable, highlighting its robustness. This contrast is due to the squared error in the mean calculation magnifying the effect of outliers, whereas the median—equivalent to the square root of the squared error—dampens their impact, thus remaining more stable in their presence.</p></div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c2789ff",
   "metadata": {},
   "source": [
    "<div class=\"solution_panel closed\">\n",
    "    <div class=\"solution_title\">\n",
    "        <p class=\"solution_title_string\">Source Code For Figure (3)</p>\n",
    "        <ul class=\"nav navbar-right panel_toolbox\">\n",
    "            <li><a class=\"collapse-link\"><i class=\"fa fa-chevron-down\"></i></a></li>\n",
    "        </ul>\n",
    "    <div class=\"clearfix\"></div>\n",
    "    </div>\n",
    "    <div class=\"solution_content\">\n",
    "        <pre>\n",
    "            <code class=\"language-python\">\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "x = [3, 9, 21, 25]\n",
    "y = [12, 35, 16, 28]\n",
    "\n",
    "x_outlier = [3, 9, 15, 21, 25]\n",
    "y_outlier =[12, 35, 100, 16, 28]\n",
    "\n",
    "ys = [y, y_outlier]\n",
    "xs = [x, x_outlier]\n",
    "\n",
    "# L2 loss - euclidean distance\n",
    "def L2_objective_func(point, _x, _y):\n",
    "    return np.sum(np.sqrt((point[0] - _x)**2 + (point[1] - _y)**2))\n",
    "\n",
    "# L2 squared loss\n",
    "def L2_squared_objective_func(point, _x, _y):\n",
    "    return np.sum((_x - point[0])**2 + (_y - point[1])**2)\n",
    "\n",
    "s = 150\n",
    "init_guess = [0, 0]\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(9, 4))\n",
    "for i, (ax, x, y) in enumerate(zip(axes, xs, ys)):\n",
    "    \n",
    "    # calculates L2 euclidean loss. This results in a spatial median\n",
    "    result_L2 = minimize(L2_objective_func, init_guess, args=(x, y), method='Nelder-Mead')\n",
    "\n",
    "    # calculatse L2 squared loss. This results in a centroid (center of mass)\n",
    "    result_L2_2 = minimize(L2_squared_objective_func, init_guess, args=(x, y), method='Nelder-Mead')\n",
    "    \n",
    "    # Proves that minimizing L2 squared loss results in mean\n",
    "    assert np.allclose(result_L2_2.x, [np.mean(x), np.mean(y)], atol=1e-4)\n",
    "\n",
    "    ax.scatter(x, y, s=s, edgecolor='blue', fc=(0, 0, 1, 0.05))\n",
    "    _s1 = ax.scatter(result_L2.x[0], result_L2.x[1], s=s, \n",
    "                     label=r'Median:  $\\underset{x, y}{\\mathrm{argmin}} \\sum^{n}_{i=1}\\sqrt{(x_{i} - \\hat{x}_{i})^{2} + (y_{i} - \\hat{y}_{i})^2}$', marker='*')\n",
    "    _s2 = ax.scatter(result_L2_2.x[0], result_L2_2.x[1], s=s, \n",
    "                     label=r'Mean:  $\\underset{x, y}{\\mathrm{argmin}} \\sum^{n}_{i=1}[(x_{i} - \\hat{x}_{i})^{2} + (y_{i} - \\hat{y}_{i})^2]$', marker='+', lw=3)\n",
    "    \n",
    "    xmax = 30\n",
    "    ymax = 110\n",
    "    ax.set_xlim(0 - 0.05 * xmax, xmax)\n",
    "    ax.set_ylim(0 - 0.05 * ymax, ymax)\n",
    "\n",
    "    ax.grid(axis='both', linestyle='--', color='#acacac', alpha=0.5)\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    \n",
    "    if i == 0:\n",
    "        x_increment = 1.2\n",
    "    else:\n",
    "        x_increment = 1.5\n",
    "    \n",
    "    ax.text(result_L2_2.x[0] - x_increment, result_L2_2.x[1] + 6, '(%d, %d)' % (result_L2_2.x[0], result_L2_2.x[1]), color=_s2.get_facecolor()[0], ha='center')\n",
    "    ax.text(result_L2.x[0] + x_increment, result_L2.x[1] + 6, '(%d, %d)' % (result_L2.x[0], result_L2.x[1]), color=_s1.get_facecolor()[0], ha='center')\n",
    "    ax.text(0.98, 0.08, 'aegis4048.github.io', fontsize=10, ha='right', transform=ax.transAxes, color='grey', alpha=0.5)\n",
    "    \n",
    "    ax.text(result_L2_2.x[0] - x_increment, result_L2_2.x[1] + 14, 'Mean', color=_s2.get_facecolor()[0], ha='center')\n",
    "    ax.text(result_L2.x[0] + x_increment, result_L2.x[1] + 14, 'Median', color=_s1.get_facecolor()[0], ha='center')\n",
    "\n",
    "    ax.set_ylabel('Y')\n",
    "    ax.set_xlabel('X')\n",
    "    \n",
    "axes[1].scatter(x[2], y_outlier[2], s=100, marker='x')\n",
    "axes[1].text(x[2], y_outlier[2] - 10, 'outlier', color='red', ha='center')\n",
    "\n",
    "\n",
    "handles, labels = [], []\n",
    "for ax in axes:\n",
    "    for h, l in zip(*ax.get_legend_handles_labels()):\n",
    "        handles.append(h)\n",
    "        labels.append(l)\n",
    "    break\n",
    "fig.legend(handles, labels, fontsize=10.5, ncol=2, loc='lower center', bbox_to_anchor=(0.5, -0.14), frameon=True)\n",
    "\n",
    "def setbold(txt):\n",
    "    return ' '.join([r\"$\\bf{\" + item + \"}$\" for item in txt.split(' ')])\n",
    "\n",
    "bold_txt = setbold('Mean vs Median, ')\n",
    "plain_txt = r'effect of outliers on median- vs mean-based estimators'\n",
    "fig.suptitle(bold_txt + plain_txt, verticalalignment='top', x=0, horizontalalignment='left', fontsize=12, y=0.96)\n",
    "yloc = 1.035\n",
    "ax.annotate('', xy=(0.01, yloc + 0.01), xycoords='figure fraction', xytext=(1.02, yloc + 0.01),\n",
    "            arrowprops=dict(arrowstyle=\"-\", color='k', lw=0.7))\n",
    "\n",
    "fig.tight_layout()\n",
    "            </code>\n",
    "        </pre>\n",
    "    </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e806b9e",
   "metadata": {},
   "source": [
    "## 2. Robust Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8e2ebc8",
   "metadata": {},
   "source": [
    "<div id=\"RANSAC regressor\"></div>\n",
    "\n",
    "### 2.1. RANSAC regressor\n",
    "\n",
    "**RAN**dom **SA**mple **C**onsensus (RANSAC) is an iterative algorithm that separates inliers vs. outliers, and fits a regression model only using the separated inliers. \n",
    "\n",
    "<div><hr></div>\n",
    "<div id=\"RANSAC Steps\"></div>\n",
    "\n",
    "*(Note that these steps are based on <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.RANSACRegressor.html\" target=_blank>sklearn's implementation of RANSAC regressor.</a>)*\n",
    "\n",
    "<div class=\"ordered-list\">\n",
    "    <h2>Steps</h2>\n",
    "    <ol>\n",
    "        <li>Start the 1st iteration. Randomly select $m$ (<code>min_samples</code>) data points from the whole data set. $m$ = number of features + 1. Therefore, for 2D linear regression, $m=2$. No duplicates.</li>\n",
    "        <li>Fit a linear regression model on the selected $m$ data points. The objective function being minimized is the <i>$L_{1}$-Norm</i> absolute loss <a href=\"#eq-3\" class=\"internal-link\">eq-3</a> (<code>loss='absolute_error</code> by default. Alternatively you could try <code>loss='squared_error'</code>). When $m=2$, the slope generalizes to a simple $(y_{j}−y_{i})/(x_{j}−x_{i})$.</li>\n",
    "        <li>Classify a point as an inlier if it falls within a certain threshold value (<code>residual_threshold</code>) from the fitted regression line. By default, the threshold is the <a href=\"#Advanced: Mean Absolute Deviation (MAD) threshold\" class=\"internal-link\">Median Absolute Deviation (MAD)</a> (auto-computed) of the whole data. Repeat this for every data point in the original data set. </li>\n",
    "        <li>Compute the value of $N$ (<a href=\"#Advanced: Required iteration ($k$) calculation\" class=\"internal-link\">required number of iterations</a>). </li>\n",
    "        <li>Start the 2nd iteration. Repeat steps 1-4.</li>\n",
    "        <li>If the newly fitted model from the 2nd iteration contains more inliers, replace the previous model with the new one. Otherwise keep the old model from the 1st iteration. If both the old and new models have the same number of inliers, pick the one with a better $R^2$ score.</li>\n",
    "        <li>Compare the old and new $N$ values, and pick a smaller one: <code>min(old_N, new_N)</code></li>\n",
    "        <li>Repeat steps 1-7 until the $N$-th iteration. $N$ can be indirectly controlled by tweaking <code>stop_probability</code> and <code>min_samples</code> arguments. </li>\n",
    "        <li>Iterations are finished. Fit a new regression model using only the final inliers. Return the fitted model.</li>\n",
    "    </ol>\n",
    "</div>\n",
    "\n",
    "<div id=\"Visual demonstrations\"></div>\n",
    "\n",
    "#### 2.1.1. Visual demonstrations\n",
    "\n",
    "<div class=\"row\" style=\"margin-top: 0px;\" id=\"fig-4\">\n",
    "<div class=\"col\"><img src=\"jupyter_images/ransac_1.png\"></div>\n",
    "</div>\n",
    "\n",
    "<div class=\"row\" style=\"margin-top: 0px;\">\n",
    "<div class=\"col\"><img src=\"jupyter_images/ransac_2.png\"></div>\n",
    "</div>\n",
    "\n",
    "<div class=\"row\" style=\"margin-top: 0px;\">\n",
    "<div class=\"col\"><img src=\"jupyter_images/ransac_3.png\"></div>\n",
    "</div>\n",
    "<div class=\"col-12\"><p class=\"image-description\"><strong>Figure 4:</strong> Simplified visual demonstration of RANSAC algorithm $m=2$ (<code>min_samples=2</code>).</p></div>\n",
    "</div>\n",
    "\n",
    "<div><hr></div>\n",
    "\n",
    "<div id=\"Advanced: Mean Absolute Deviation (MAD) threshold\"></div>\n",
    "\n",
    "#### 2.1.2. Median Absolute Deviation (MAD) threshold\n",
    "\n",
    "The default threshold for RANSAC employs the Median Absolute Deviation (MAD), a value calculated once before the iterative process begins. Just as how the standard deviation serves as a measure of spread around the mean, MAD represents variability around the median. This choice is motivated by the robustness of the median over the mean as a measure of central tendency as explained <a href=\"#The equation converges to mean, and inherits the statistical properties of mean\" class=\"internal-link\">above</a>. \n",
    "\n",
    "MAD can be computed with the following codes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8e710df6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.5"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "X = np.array([3,  6, 9, 10, 12, 15, 17, 18, 20, 24, 25, 26, 27]).reshape(-1, 1)\n",
    "y = np.array([8, 12, 15.5, 13.5, 17, 20, 18, 24, 24.5, 8, 6, 9, 7])\n",
    "\n",
    "MAD = np.median(np.abs(y - np.median(y)))\n",
    "MAD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "997d21b7",
   "metadata": {},
   "source": [
    "<div id=\"fig-5\" class=\"row\" style=\"margin-top: -15px;\">\n",
    "<div class=\"col\"><img src=\"jupyter_images/MAD-gumbel.png\"></div>\n",
    "<div class=\"col-12\"><p class=\"image-description\"><strong>Figure 5:</strong> Median Absolute Deviation (MAD) is a measure centered around the median, akin to how the standard deviation ($\\pm\\sigma$) around the mean in a normal distribution encompasses 68.2% of the population. Importantly, the median provides a more accurate representation of central tendency in the presence of skewness or outliers, compared to the mean.</p></div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95fed802",
   "metadata": {},
   "source": [
    "<div class=\"solution_panel closed\">\n",
    "    <div class=\"solution_title\">\n",
    "        <p class=\"solution_title_string\">Source Code For Figure (5)</p>\n",
    "        <ul class=\"nav navbar-right panel_toolbox\">\n",
    "            <li><a class=\"collapse-link\"><i class=\"fa fa-chevron-down\"></i></a></li>\n",
    "        </ul>\n",
    "        <div class=\"clearfix\"></div>\n",
    "    </div>\n",
    "    <div class=\"solution_content\">\n",
    "        <pre>\n",
    "            <code class=\"language-python\">\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import gumbel_r, iqr\n",
    "\n",
    "\n",
    "data = gumbel_r.rvs(size=100000, random_state=1)\n",
    "x = np.linspace(min(data), max(data), 100000)\n",
    "pdf = gumbel_r.pdf(x)\n",
    "\n",
    "M = np.median(data)\n",
    "MAD = np.median(np.abs(data - M))\n",
    "Mean = np.mean(data)\n",
    "\n",
    "perc_left = gumbel_r.cdf(M - MAD)\n",
    "perc_right = 1 - gumbel_r.cdf(M + MAD)\n",
    "perc_center = 1 - perc_left - perc_right\n",
    "\n",
    "def y_finder(y_arr, x_arr, x_val, idx_increment=0):\n",
    "    idx = np.argmin(np.abs(np.array(x_arr) - x_val))\n",
    "    return y_arr[idx + idx_increment]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 3))\n",
    "\n",
    "ax.plot(x, pdf, 'k-', lw=2, zorder=99)\n",
    "\n",
    "ax.fill_between(x, pdf, where=((x >= M - MAD) & (x <= M + MAD)), color='skyblue', alpha=0.3)\n",
    "ax.vlines(x=M - MAD, ymax=y_finder(pdf, x, M - MAD), ymin=0, ls='--')\n",
    "ax.vlines(x=M + MAD, ymax=y_finder(pdf, x, M + MAD), ymin=0, ls='--')\n",
    "ax.vlines(x=M, ymax=y_finder(pdf, x, M), ymin=0, color='r', ls='--')\n",
    "\n",
    "ax.text(M - MAD - 0.1, 0.01, '%.1f%s' % (perc_left * 100, r'%'), ha='right', va='bottom')\n",
    "ax.text(M + MAD + 0.1, 0.01, '%.1f%s' % (perc_right * 100, r'%'), ha='left', va='bottom')\n",
    "ax.text(M, 0.15, '%.1f%s' % (perc_center * 100, r'%'), ha='center', va='center', fontsize=20)\n",
    "ax.text(M - MAD - 0.1, y_finder(pdf, x, M - MAD), 'Median-MAD', ha='right', va='center', color='#1f77b4', fontsize=13)\n",
    "ax.text(M + MAD + 0.1, y_finder(pdf, x, M + MAD), 'Median+MAD', ha='left', va='center', color='#1f77b4', fontsize=13)\n",
    "ax.text(M + 0.1, y_finder(pdf, x, M), 'Median', ha='left', va='center', color='r', fontsize=13)\n",
    "ax.text(0.01, 0.15, 'aegis4048.github.io', fontsize=10, ha='left', transform=ax.transAxes, color='grey', alpha=0.5)\n",
    "\n",
    "ax.set_ylim(0, None)\n",
    "ax.set_xlim(-2.5, 6)\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "\n",
    "def setbold(txt):\n",
    "    return ' '.join([r\"$\\bf{\" + item + \"}$\" for item in txt.split(' ')])\n",
    "\n",
    "bold_txt = setbold('Mean Absolute Deviation (MAD), ')\n",
    "plain_txt = r'measure of central tendency using median; Gumbel distribution'\n",
    "fig.suptitle(bold_txt + plain_txt, verticalalignment='top', x=0, horizontalalignment='left', fontsize=11, y=0.95)\n",
    "yloc = 0.85\n",
    "ax.annotate('', xy=(0.01, yloc + 0.01), xycoords='figure fraction', xytext=(1.02, yloc + 0.01),\n",
    "            arrowprops=dict(arrowstyle=\"-\", color='k', lw=0.7))\n",
    "\n",
    "fig.tight_layout()     \n",
    "            </code>\n",
    "        </pre>\n",
    "    </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01fc239b",
   "metadata": {},
   "source": [
    "<div id=\"Advanced: Required iteration ($k$) calculation\" style=\"margin-top: -15px;\"></div>\n",
    "\n",
    "#### 2.1.3. Required iteration calculation\n",
    "\n",
    "Another important component of the RANSAC is the required iteration ($N$).\n",
    "\n",
    "<div id=\"eq-4\" style=\"font-size: 1rem;\">\n",
    "$$ N >= \\frac{log(1-p)}{log(1-e^m)} \\tag{4}$$\n",
    "</div>\n",
    "\n",
    "<div class=\"eq-terms\">\n",
    "    <div class=\"row eq-terms-where\">where</div>\n",
    "    <div class=\"row\">\n",
    "        <div class=\"col-2\">$N$</div>\n",
    "        <div class=\"col-1 max-width-3\">:</div>\n",
    "        <div class=\"col-9\">required number of iterations.</div>\n",
    "    </div>\n",
    "    <div class=\"row\">\n",
    "        <div class=\"col-2\">$p$</div>\n",
    "        <div class=\"col-1 max-width-3\">:</div>\n",
    "        <div class=\"col-9\">probability (confidence) that one outlier-free sample is generated. (default=0.99). </div>\n",
    "    </div>    \n",
    "    <div class=\"row\">\n",
    "        <div class=\"col-2\">$e$</div>\n",
    "        <div class=\"col-1 max-width-3\">:</div>\n",
    "        <div class=\"col-9\">inlier ratio (# of inliers / # of entire data points). This may change every iteration.</div>\n",
    "    </div>\n",
    "    <div class=\"row\">\n",
    "        <div class=\"col-2\">$m$</div>\n",
    "        <div class=\"col-1 max-width-3\">:</div>\n",
    "        <div class=\"col-9\">minimum number of samples chosen randomly from origianl data (default=2).</div>\n",
    "    </div>    \n",
    "</div>\n",
    "\n",
    "$N$ is rounded up to the nearest integer. Note that you can *indirectly* control the value of $N$ by changing <code>stop_probability</code> and <code>min_samples</code> in the sklearn package. It can be computed with the following codes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "354d94d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11.0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "p = 0.99       # sklearn default\n",
    "m = 2          # sklearn default for 2D linear regression\n",
    "w = 0.6        # this may change every iteration\n",
    "\n",
    "N = abs(float(np.ceil(np.log(1 - p) / np.log(1 - w ** m))))\n",
    "N"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bc1544d",
   "metadata": {},
   "source": [
    "<div><hr></div>\n",
    "\n",
    "<div id=\"More detailed visual demonstrations\"></div>\n",
    "\n",
    "#### 2.1.4. More detailed visual demonstrations\n",
    "\n",
    "<div class=\"row\" style=\"margin-top: 15px;\" id=\"fig-6\">\n",
    "<div class=\"col\"><img src=\"jupyter_images/ransac_advanced_1st.png\"></div>\n",
    "    <div class=\"col-12\"><p class=\"image-description\"><strong>Figure 6:</strong> First iteration in RANSAC. Left plot shows randomly selected 2 data points (blue dots) for regression. The right plot displays residuals, with the dashed red line indicating MAD. Points with residuals exceeding MAD are marked as outliers. Initially, $N$ is set to 100 <code>max_trials=100</code>. After the first iteration, $N$ is computed to be 20 using <a href=\"#eq-4\" class=\"internal-link\">eq-4</a>. Since 20 is smaller than 100, $N$ is updated to 20.  A total of 6 inliers are found in this round.</p></div>\n",
    "</div>\n",
    "\n",
    "<div class=\"row\" style=\"margin-top: 15px;\" id=\"fig-7\">\n",
    "<div class=\"col\"><img src=\"jupyter_images/ransac_advanced_2nd.png\"></div>\n",
    "<div class=\"col-12\"><p class=\"image-description\"><strong>Figure 7:</strong> Second iteration. The new model has more inliers (9) then the previous model (6). Model is updated. The inlier ratio ($w$ in <a href=\"#eq-4\" class=\"internal-link\">eq-4</a>) changes. $N$ is computed to be 8 in this iteration. Since 8 is smaller than 20 from the previous iteration, $N$ is updated to 8.</p></div>\n",
    "</div>\n",
    "\n",
    "<div class=\"row\" style=\"margin-top: 15px;\" id=\"fig-8\">\n",
    "<div class=\"col\"><img src=\"jupyter_images/ransac_advanced_3rd.png\"></div>\n",
    "<div class=\"col-12\"><p class=\"image-description\"><strong>Figure 8:</strong> Third iteration. Although the number of inliers equals the previous iteration, a visual inspection of the left plot reveals that the current model is incorrect. To decide whether an update is necessary, we compare the $R^2$ scores. Since the previous model has a higher $R^2$ (0.92) than the current one (0.62), no model update is performed. This process continues until the $N$-th (8th) iteration, assuming that $N$ remains constant. Once all iterations are completed, the final model is fitted using the selected inliers.</p></div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08ce3046",
   "metadata": {},
   "source": [
    "<div class=\"solution_panel closed\">\n",
    "    <div class=\"solution_title\">\n",
    "        <p class=\"solution_title_string\">Source Code For Figures (6-8)</p>\n",
    "        <ul class=\"nav navbar-right panel_toolbox\">\n",
    "            <li><a class=\"collapse-link\"><i class=\"fa fa-chevron-down\"></i></a></li>\n",
    "        </ul>\n",
    "    <div class=\"clearfix\"></div>\n",
    "    </div>\n",
    "    <div class=\"solution_content\">\n",
    "        <pre>\n",
    "            <code class=\"language-python\">\n",
    "            \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import linear_model\n",
    "from sklearn.linear_model import RANSACRegressor\n",
    "\n",
    "X = np.array([3,  6, 9, 10, 12, 15, 17, 18, 20, 24, 25, 26, 27]).reshape(-1, 1)\n",
    "y = np.array([8, 12, 15.5, 13.5, 17, 20, 18, 24, 24.5, 8, 6, 9, 7])\n",
    "MAD = np.median(np.abs(y - np.median(y)))\n",
    "\n",
    "N_PREV = 100\n",
    "\n",
    "def fit_and_plot(X, y, MAD, random_state, selected_idx, stage, update_status):\n",
    "    \n",
    "    global N_PREV\n",
    "\n",
    "    X_selected = X[selected_idx]\n",
    "    y_selected = y[selected_idx]\n",
    "\n",
    "    ols = linear_model.LinearRegression()\n",
    "    model = ols.fit(X_selected, y_selected)\n",
    "    y_pred = model.predict(X)\n",
    "    residuals = np.abs(y - y_pred)\n",
    "\n",
    "    ransac = RANSACRegressor(max_trials=1, random_state=random_state).fit(X, y) # deal with rs=11\n",
    "    y_pred_ransac = ransac.predict(X)\n",
    "    num_inliers = np.sum(ransac.inlier_mask_)\n",
    "    R2 = ransac.score(X[ransac.inlier_mask_], y[ransac.inlier_mask_])\n",
    "\n",
    "    p = 0.99\n",
    "    m = 2\n",
    "    w = num_inliers / len(y)\n",
    "    N = abs(float(np.ceil(np.ceil(np.log(1 - p) / np.log(1 - w ** m)))))\n",
    "\n",
    "    ####################################################################################\n",
    "\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n",
    "\n",
    "    axes[0].scatter(X, y, s=100, color='#70ad47', label='Inliers')\n",
    "    axes[0].scatter(X[~ransac.inlier_mask_], y[~ransac.inlier_mask_], s=100, color='red', label='Outliers')\n",
    "    axes[0].scatter(X_selected, y_selected, s=100, color='#4472c4', label='Random')\n",
    "\n",
    "    axes[0].axline(xy1=(X_selected[0][0], y_selected[0]), xy2=(X_selected[-1][0], y_selected[-1]), color='#7030a0', label='Regression on Random')\n",
    "    axes[0].legend(loc='upper left', ncol=2, fontsize=9)\n",
    "    axes[0].set_title('RANSAC: %s iteration' % stage, fontsize=10)\n",
    "\n",
    "    scatter_dict = {'s':100, 'fc':'none', 'linewidth':1.5}\n",
    "    axes[1].scatter(X[ransac.inlier_mask_], residuals[ransac.inlier_mask_], edgecolors='k', label='Inliers', **scatter_dict)\n",
    "    axes[1].scatter(X[~ransac.inlier_mask_], residuals[~ransac.inlier_mask_], edgecolors='r', label='Outliers', **scatter_dict)\n",
    "    axes[1].scatter(X[selected_idx], residuals[selected_idx], edgecolors='b', label='Random', **scatter_dict)\n",
    "    \n",
    "    axes[1].axhline(y=MAD, color='r', linestyle='--')\n",
    "    axes[1].legend(loc='upper left', ncol=3, fontsize=9)\n",
    "    axes[1].text(-0.5, MAD + 0.5, 'Threshold (MAD) = 5.50', fontsize=9, ha='left', va='bottom', color='r', alpha=1)\n",
    "    axes[1].text(0, 25, '# of Inliers = %d' % num_inliers, ha='left', weight='bold', fontsize=12)\n",
    "    axes[1].text(0, 22.75, update_status, ha='left', fontsize=10)\n",
    "    axes[1].text(0, 20.5, 'min($N_{old}$, $N_{new}$)= min(%d, %d)= %d' % (N_PREV, N, N), ha='left', fontsize=10)\n",
    "    axes[1].text(0, 18.25, '$R^2$ = %.2f' % round(R2, 2), ha='left', fontsize=10)\n",
    "    axes[1].set_title('Residuals of regression', fontsize=10)\n",
    "\n",
    "    xmax = 30\n",
    "    ymax = 30\n",
    "    for ax in axes:\n",
    "        ax.spines.top.set_visible(False)\n",
    "        ax.spines.right.set_visible(False)\n",
    "        ax.set_xlim(0 - 0.05 * xmax, xmax + 0.05 * xmax)\n",
    "        ax.set_ylim(0 - 0.05 * ymax, ymax + 0.05 * ymax)\n",
    "        \n",
    "    N_PREV = N\n",
    "    \n",
    "fit_and_plot(X, y, MAD, random_state=3, selected_idx=[4, 6], stage='1st', update_status='model updated (initial)')\n",
    "fit_and_plot(X, y, MAD, random_state=11, selected_idx=[0, 6], stage='2nd', update_status='model updated')\n",
    "fit_and_plot(X, y, MAD, random_state=29, selected_idx=[-9, -1], stage='3rd', update_status='model NOT updated')\n",
    "            </code>\n",
    "        </pre>\n",
    "    </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b735d247",
   "metadata": {},
   "source": [
    "<div id=\"RANSAC code snippets\" style=\"margin-top: -15px;\"></div>\n",
    "\n",
    "#### 2.1.5. RANSAC code snippets\n",
    "\n",
    "For quick copy-paste, replace <code>X</code> and <code>y</code> with your own data. Make sure to reshape your <code>X</code> so that it is a 2D <code>numpy.ndarray</code> object with shape like <code>(13, 1)</code>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ec263423",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RANSAC -----------------------------------\n",
      "\n",
      "Coefficients         : [92.50723098]\n",
      "Intercept            : 30.331391269710938\n",
      "# of inliers         : 40\n",
      "Fraction of inliers  : 0.8\n",
      "\n",
      "------------------------------------------\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import RANSACRegressor\n",
    "from sklearn import linear_model\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# sample data\n",
    "X = np.array([ 0.15, -0.34, 0.32, 0.43, -0.4, -0.04, -0.51, 0.3, 0.47, 0.12, 0.08, 0.04, -0.08, \n",
    "              -0.23, 0.08, -0.03, 0.03, 0.04, 0.01, 0.06, 0.03, 0., -0.04, -0.18, -0.19, -0.06, \n",
    "              -0.26, -0.16, 0.13, 0.09, 0.03, -0.03, 0.04, 0.14, -0.01, 0.4, -0.06, 0.15, 0.08, \n",
    "              0.05, -0.15, -0.09, -0.15, -0.11, -0.07, -0.19, -0.06, 0.17, 0.23, 0.18]).reshape(-1, 1)\n",
    "y = np.array([17.44, 25.46, 18.61, 26.07, 24.96, -1.22, 26.45, 26.5, 20.57, 3.08, 35.9 , 32.47, \n",
    "              20.84, 13.37, 42.44, 27.23, 35.65, 29.51, 31.28, 41.34, 32.19, 33.67, 25.64, 9.3, \n",
    "              14.63, 25.1, 4.69, 14.42, 47.53, 33.82, 32.2 , 24.81, 32.64, 45.11, 26.76, 68.01, \n",
    "              23.39, 43.49, 37.88, 36.01, 16.32, 19.77, 16.34, 19.57, 29.28, 16.62, 24.39, 43.77, \n",
    "              50.46, 47.09])\n",
    "\n",
    "# fit and predict: RANSAC\n",
    "ransac = RANSACRegressor(random_state=1).fit(X, y)      \n",
    "y_pred_ransac = ransac.predict(X)\n",
    "\n",
    "# retrieve the fitted parameters\n",
    "coefs = ransac.estimator_.coef_              # RANSAC fits many regression models. The \".estimator_\" attribute\n",
    "intercept = ransac.estimator_.intercept_     # has the final model fitted. \n",
    "\n",
    "# seperate into inliers vs. outliers\n",
    "X_inlier = X[ransac.inlier_mask_ ]\n",
    "y_inlier = y[ransac.inlier_mask_ ]\n",
    "X_outlier = X[~ransac.inlier_mask_ ]\n",
    "y_outlier = y[~ransac.inlier_mask_ ]\n",
    "\n",
    "print(\"RANSAC -----------------------------------\\n\")\n",
    "print(\"Coefficients         :\", coefs)\n",
    "print(\"Intercept            :\", intercept)\n",
    "print(\"# of inliers         :\", sum(ransac.inlier_mask_))\n",
    "print(\"Fraction of inliers  :\", sum(ransac.inlier_mask_) / len(y))\n",
    "print(\"\\n------------------------------------------\")\n",
    "\n",
    "# fit and predict: Ordinary Least Squares\n",
    "ols = linear_model.LinearRegression().fit(X, y)\n",
    "y_pred_ols = ols.predict(X)\n",
    "\n",
    "# plot\n",
    "plt.title('RANSAC Regressor', fontsize=14)\n",
    "plt.scatter(X_inlier, y_inlier, s=100, c='green', label='Inliers')\n",
    "plt.scatter(X_outlier, y_outlier, s=100, c='red', label='Outliers')\n",
    "plt.plot(X, y_pred_ols, label='OLS')\n",
    "plt.plot(X, y_pred_ransac, label='RANSAC')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "214eaeed",
   "metadata": {},
   "source": [
    "<div class=\"highlights red-theme\" id=\"warning-RANSAC\">\n",
    "    <div class=\"highlights-title red-theme\">WARNING!</div>\n",
    "    <div class=\"highlights-content red-theme\">RANSAC relies on random sampling, which means that with a larger dataset (exceeding 50 points), your regression outcomes can differ due to this inherent randomness. This approach offers the advantage of flexibility for iterative adjustments (as observed when running the same code multiple times without a fixed random seed) when turning the model. However, it also leads to variability in outcomes, which can be problematic for replicating results or sharing code. To achieve consistent results when you are done tuning and trying to report, it's advisable to set a random seed, as in: <code>model = RANSACRegressor(random_state=3)</code>.\n",
    "    </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dd773ad",
   "metadata": {},
   "source": [
    "<div id=\"Huber regressor\"></div>\n",
    "\n",
    "### 2.2. Huber regressor\n",
    "\n",
    "The Huber regressor doesn't eliminate outliers but mitigates their effect, making it a preferred choice when you have reason to believe that so-called outliers may have relevance in the regression analysis. In contrast to other robust regression techniques like RANSAC or Theil-Sen, which involve substantial structural modifications in their algorithms compared to default linear regression, such as subsampling and fitting regressions on these subsets, the Huber regressor differes only in its choice of loss function while retaining the fundamental structure of default linear regression.\n",
    "\n",
    "<div><hr></div>\n",
    "\n",
    "<div id=\"Huber loss function\"></div>\n",
    "\n",
    "#### 2.2.1. Huber loss function\n",
    "\n",
    "The original Huber loss is a piece-wise function shown in <a href=\"#eq-5\" class=\"internal-link\">eq-5</a>. This is the most generic search result you will see if you google Huber regressor. Note that the term, $y - \\hat{y}$, are often referred to as residuals (prediction error).\n",
    "\n",
    "<div id=\"eq-5\" style=\"font-size: 1rem;\">\n",
    "$$ L_{\\delta} (y - \\hat{y})=   \\left\\{\n",
    "    \\begin{array}{ll}\n",
    "          \\frac{1}{2}(y - \\hat{y})^{2} & \\text{for } |y - \\hat{y}| \\leq \\delta \\text{ (inlier)},  \\\\\n",
    "          \\delta \\cdot (|y - \\hat{y}| - \\frac{1}{2}\\delta) &  \\text{otherwise (outlier)}.  \\\\\n",
    "    \\end{array} \n",
    "    \\right.  \n",
    "    \\tag{5}$$\n",
    "</div>\n",
    "\n",
    "<div class=\"eq-terms\">\n",
    "    <div class=\"row eq-terms-where\">where</div>\n",
    "    <div class=\"row\">\n",
    "        <div class=\"col-2\">$y$</div>\n",
    "        <div class=\"col-1 max-width-3\">:</div>\n",
    "        <div class=\"col-9\">observation, original data point</div>\n",
    "    </div>\n",
    "    <div class=\"row\">\n",
    "        <div class=\"col-2\">$\\hat{y}$</div>\n",
    "        <div class=\"col-1 max-width-3\">:</div>\n",
    "        <div class=\"col-9\">prediction</div>\n",
    "    </div>\n",
    "    <div class=\"row\">\n",
    "        <div class=\"col-2\">$\\delta$</div>\n",
    "        <div class=\"col-1 max-width-3\">:</div>\n",
    "        <div class=\"col-9\">absolute parameter that controls the number of outliers</div>\n",
    "    </div>    \n",
    "    <div class=\"row\">\n",
    "        <div class=\"col-2\">$L$</div>\n",
    "        <div class=\"col-1 max-width-3\">:</div>\n",
    "        <div class=\"col-9\">loss function</div>\n",
    "    </div>    \n",
    "</div>\n",
    "\n",
    "HOWEVER, a challenge with the equation <a href=\"#eq-5\" class=\"internal-link\">eq-5</a> is that the $\\delta$ parameter is in absolute scale. For instance, if 95% of your residuals fall within the range of (45, 60), setting $\\delta=1.35$ equates to a 2.25% to 3.00% residual tolerance, which works well. But when dealing with a different dataset where the residuals range from (4355, 13205), applying the same $\\delta=1.35$ results in an impractical ~0.001% residual tolerance. Therefore, a unique $\\delta$ value would be needed for each dataset's residual range. To address this, scikit-learn modifies the original Huber loss by using the scale parameter $\\sigma$, allowing for a consistent threshold parameter that operates on a <i>relative</i> scale instead of an absolute one.\n",
    "\n",
    "The loss function that <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.HuberRegressor.html#sklearn.linear_model.HuberRegressor\" target=_blank>HuberRegressor</a> minimizes is given by:\n",
    "\n",
    "<div id=\"eq-6\" style=\"font-size: 1rem;\">\n",
    "$$\n",
    "\\begin{align}\n",
    "     \\underset{w, \\sigma}{\\text{argmin}}\\left[\\sum^{n}_{i=1}\\left(\\sigma + L_{\\epsilon}\\left(\\frac{X_{i}w - y_{i}}{\\sigma}\\right)\\sigma\\right) + \\alpha ||w||_{2}^{2}\\right]\n",
    "     \\tag{6}\n",
    "\\end{align}\n",
    "$$</div>\n",
    "\n",
    "where\n",
    "\n",
    "<div id=\"eq-7\" style=\"font-size: 1rem;\">\n",
    "$$ L_{\\epsilon}(z)=   \\left\\{\n",
    "    \\begin{array}{ll}\n",
    "          z^{2} & \\text{for } z \\leq \\epsilon \\text{ (inlier)},  \\\\\n",
    "          2\\epsilon |z| - \\epsilon^{2} &  \\text{otherwise (outlier)}.  \\\\\n",
    "    \\end{array} \n",
    "    \\right.  \n",
    "    \\tag{7}$$\n",
    "</div>\n",
    "\n",
    "<div class=\"eq-terms\">\n",
    "    <div class=\"row eq-terms-where\">where</div>\n",
    "    <div class=\"row\">\n",
    "        <div class=\"col-2\">$w$</div>\n",
    "        <div class=\"col-1 max-width-3\">:</div>\n",
    "        <div class=\"col-9\">linear regression arguments. For 2D liear regression, it's (slope, intercept)</div>\n",
    "    </div>\n",
    "    <div class=\"row\">\n",
    "        <div class=\"col-2\">$\\sigma$</div>\n",
    "        <div class=\"col-1 max-width-3\">:</div>\n",
    "        <div class=\"col-9\">scaling parameter. This allows the residual threshold to be on a relative scale so it can work for all residual ranges.</div>\n",
    "    </div>\n",
    "    <div class=\"row\">\n",
    "        <div class=\"col-2\">$i$</div>\n",
    "        <div class=\"col-1 max-width-3\">:</div>\n",
    "        <div class=\"col-9\">index of a data point</div>\n",
    "    </div>    \n",
    "    <div class=\"row\">\n",
    "        <div class=\"col-2\">$n$</div>\n",
    "        <div class=\"col-1 max-width-3\">:</div>\n",
    "        <div class=\"col-9\">number of samples.</div>\n",
    "    </div>\n",
    "    <div class=\"row\">\n",
    "        <div class=\"col-2\">$L$</div>\n",
    "        <div class=\"col-1 max-width-3\">:</div>\n",
    "        <div class=\"col-9\">Huber loss function</div>\n",
    "    </div>    \n",
    "    <div class=\"row\">\n",
    "        <div class=\"col-2\">$\\epsilon$</div>\n",
    "        <div class=\"col-1 max-width-3\">:</div>\n",
    "        <div class=\"col-9\">relative <code>epsilon</code> parameter that controls the number of outliers.</div>\n",
    "    </div>\n",
    "    <div class=\"row\">\n",
    "        <div class=\"col-2\">$X$</div>\n",
    "        <div class=\"col-1 max-width-3\">:</div>\n",
    "        <div class=\"col-9\">independent variable. Note that $X_{i}w$ is equivalent to prediction $\\hat{y}_{i}$</div>\n",
    "    </div> \n",
    "    <div class=\"row\">\n",
    "        <div class=\"col-2\">$y$</div>\n",
    "        <div class=\"col-1 max-width-3\">:</div>\n",
    "        <div class=\"col-9\">observation, original data point</div>\n",
    "    </div>        \n",
    "    <div class=\"row\">\n",
    "        <div class=\"col-2\">$\\alpha$</div>\n",
    "        <div class=\"col-1 max-width-3\">:</div>\n",
    "        <div class=\"col-9\">learning rate</div>\n",
    "    </div>    \n",
    "    <div class=\"row\">\n",
    "        <div class=\"col-2\">$z$</div>\n",
    "        <div class=\"col-1 max-width-3\">:</div>\n",
    "        <div class=\"col-9\">residual divided by $\\sigma$</div>\n",
    "    </div>        \n",
    "</div>\n",
    "\n",
    "The loss function maybe a bit challenging to interpret for beginners. <a href=\"#fig-9\" class=\"internal-link\">Figure 9</a> below shows how the equation breaks down. Recall that $L_{\\epsilon}$ is a piece-wise function that applies different formula depending on whether the data point is identified as an outlier or not by the $\\epsilon$ parameter, which is set to be 1.35 (recommended) by scikit learn.\n",
    "\n",
    "<div class=\"row\" style=\"\" id=\"fig-9\">\n",
    "<div class=\"col\"><img src=\"jupyter_images/huber explained.png\"></div>\n",
    "<div class=\"col-12\"><p class=\"image-description\"><strong>Figure 9:</strong> Explanation of <a href=\"#eq-9\" class=\"internal-link\">eq-9</a>.</p></div>\n",
    "</div>\n",
    "\n",
    "<div><hr></div>\n",
    "\n",
    "<div id=\"Motivation\"></div>\n",
    "\n",
    "#### 2.2.2. Motivation\n",
    "\n",
    "The two most commonly used loss functions are squared loss, $L(y - \\hat{y}) = (y - \\hat{y})^2$ and absolute loss, $L(y - \\hat{y}) = |y - \\hat{y}|$. While the squared loss is more accurate, it has the disadvantage that it has the tendency to be dominated by outliers. This susceptibility arises because squared residuals magnify the effect of outliers, as the residuals are squared, leading to an exponential increase in loss as residuals grow. In contrast, absolute loss grows linearly with residuals, rendering it more robust to outliers.\n",
    "\n",
    "The Huber regressor offers a compelling compromise, leveraging the strengths of squared and absolute loss functions while mitigating their weaknesses. It employs squared loss for small-residual data points (inliers) and absolute loss for large-residual data points (outliers). The distinction between inliers and outliers is governed by the $\\delta$ parameter, set to 1.35 by default in scikit-learn.\n",
    "\n",
    "<div id=\"Parameter tuning\"></div>\n",
    "\n",
    "#### 2.2.3. Parameter tuning ($\\delta$)\n",
    "\n",
    "The $\\delta$ parameter controls the residual threshold for determining whether squared loss or Huber loss is applied. As shown in <a href=\"#fig-10\" class=\"internal-link\">Figure 10</a>, smaller $\\delta$ values lead to a more robust approach by applying Huber loss to a greater number of data points, emphasizing absolute loss. Conversely, larger $\\delta$ values result in squared loss being more prevalent, making the method more susceptible to outliers. Notably, for very large $\\delta$ values, Huber loss converges to squared loss (susceptible to outliers), as described in <a href=\"#eq-8\" class=\"internal-link\">eq-8</a>:\n",
    "\n",
    "\n",
    "<div id=\"eq-8\" style=\"font-size: 1rem;\">\n",
    "$$ \\lim_{\\delta\\to \\infty}: \\text{Huber Loss} \\approx \\text{Squared Loss} \\tag{8}$$\n",
    "</div>\n",
    "\n",
    "Note that in the <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.HuberRegressor.html\" target=_blank>scikit-learn implementation</a>, the residuals are divided by the scale parameter sigma <code>|(y - Xw - c) / sigma|</code> to ensure that one does not need to rescale epsilon to achieve the same robustness. The default value of $\\delta$ in scikit-learn is 1.35.\n",
    "\n",
    "<div class=\"row\" style=\"margin-top: 15px;\" id=\"fig-10\">\n",
    "<div class=\"col\"><img src=\"jupyter_images/huber loss effect of delta.png\"></div>\n",
    "<div class=\"col-12\"><p class=\"image-description\"><strong>Figure 10:</strong> $\\delta$ controls the residual thresholds used to determine whether to compute squared loss or Huber loss. Small value of $\\delta$ (left plot) allows Huber loss to be applied for wider residual ranges of data, making it more robust to outliers. Observe that the blue line (Huber Loss) in the left plot applies to greater residual ranges with smaller $\\delta$(=1.35).</p></div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1825d6a0",
   "metadata": {},
   "source": [
    "<div class=\"solution_panel closed\">\n",
    "    <div class=\"solution_title\">\n",
    "        <p class=\"solution_title_string\">Source Code For Figure (10)</p>\n",
    "        <ul class=\"nav navbar-right panel_toolbox\">\n",
    "            <li><a class=\"collapse-link\"><i class=\"fa fa-chevron-down\"></i></a></li>\n",
    "        </ul>\n",
    "    <div class=\"clearfix\"></div>\n",
    "    </div>\n",
    "    <div class=\"solution_content\">\n",
    "        <pre>\n",
    "            <code class=\"language-python\">\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "error_range = np.linspace(-3.3, 3.3, 100)\n",
    "theta_values = [1.35, 2.35]\n",
    "\n",
    "huber_color = 'blue'\n",
    "squared_color = 'green'\n",
    "fig, axes = plt.subplots(1, len(theta_values), figsize=(9, 4.5))\n",
    "\n",
    "for i, theta in enumerate(theta_values):\n",
    "    huber_loss = np.where(np.abs(error_range) <= theta, 0.5 * error_range ** 2, theta * (np.abs(error_range) - 0.5 * theta))\n",
    "    squared_error_loss = 0.5 * error_range ** 2\n",
    "\n",
    "    ax = axes[i]\n",
    "\n",
    "    # Set the range for which the line styles and alphas will change\n",
    "    x_range = (-theta, theta)\n",
    "\n",
    "    # Plot Huber Loss line\n",
    "    huber_loss_segment = np.where((error_range >= x_range[0]) & (error_range <= x_range[1]), huber_loss, np.nan)\n",
    "    ax.plot(error_range, huber_loss_segment, linewidth=2, zorder=3, alpha=0.3, linestyle='dashed', color=huber_color)\n",
    "    huber_loss_segment = np.where((error_range < x_range[0]) | (error_range > x_range[1]), huber_loss, np.nan)\n",
    "    ax.plot(error_range, huber_loss_segment, label='Huber Loss', linewidth=2, zorder=3, alpha=1, linestyle='-', color=huber_color)\n",
    "\n",
    "    # Plot Squared Loss line\n",
    "    squared_loss_segment = np.where((error_range >= x_range[0]) & (error_range <= x_range[1]), squared_error_loss, np.nan)\n",
    "    ax.plot(error_range, squared_loss_segment, label='Squared Loss', linewidth=2, zorder=3, alpha=1, linestyle='-', color=squared_color)\n",
    "    squared_loss_segment = np.where((error_range < x_range[0]) | (error_range > x_range[1]), squared_error_loss, np.nan)\n",
    "    ax.plot(error_range, squared_loss_segment, linewidth=2, zorder=3, alpha=0.3, linestyle='dashed', color=squared_color)\n",
    "    \n",
    "    # Fill the area between the axvlines\n",
    "    fill_alpha = 0.03\n",
    "    ax.axvspan(-5, -theta, alpha=fill_alpha, color=huber_color, zorder=-8, label='Outliers')\n",
    "    ax.axvspan(theta, 5, alpha=fill_alpha, color=huber_color, zorder=-9)\n",
    "    ax.axvspan(-theta, theta, alpha=fill_alpha, color=squared_color, zorder=-9, label='Inliers')\n",
    "    \n",
    "    ax.set_xlabel('Residuals ($y - \\\\hat{y}$)', fontsize=13)\n",
    "    ax.set_ylabel('Loss', fontsize=13)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    lg = ax.legend(loc='upper center', ncol=2)\n",
    "    for i, lh in enumerate(lg.legendHandles):\n",
    "        if i > 1:\n",
    "            lh.set_alpha(0.4)\n",
    "    \n",
    "    ax.axvline(x=theta, color='r', linestyle=\"dotted\", alpha=0.7)\n",
    "    ax.axvline(x=-theta, color='r', linestyle=\"dotted\", alpha=0.7)\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.set_xlim(-3.3, 3.3)\n",
    "\n",
    "    ax.text(0, 3.5, '$\\delta = %.2f$' % theta,\n",
    "            fontsize=13, ha='center', va='top', color='r', alpha=0.7, rotation=0)\n",
    "    ax.text(0.05, 0.1, 'aegis4048.github.io', fontsize=10, ha='left', va='center',\n",
    "        transform=ax.transAxes, color='grey', alpha=0.5)\n",
    "    \n",
    "    \n",
    "axes[0].text(3, 0.1, 'more robust', ha='right', fontsize=13, bbox=dict( facecolor='white'))\n",
    "axes[1].text(3, 0.1, 'less robust', ha='right', fontsize=13, bbox=dict( facecolor='white'))\n",
    "\n",
    "def setbold(txt):\n",
    "    return ' '.join([r\"$\\bf{\" + item + \"}$\" for item in txt.split(' ')])   \n",
    "\n",
    "bold_txt = setbold('Huber Loss, ')\n",
    "plain_txt = r'effect of $\\delta$-parameter on inlier vs. outlier detection'\n",
    "fig.suptitle(bold_txt + plain_txt, verticalalignment='top', x=0, horizontalalignment='left', fontsize=11)\n",
    "yloc = 0.9\n",
    "ax.annotate('', xy=(0.01, yloc + 0.01), xycoords='figure fraction', xytext=(1.02, yloc + 0.01),\n",
    "            arrowprops=dict(arrowstyle=\"-\", color='k', lw=0.7))\n",
    "\n",
    "fig.tight_layout()\n",
    "            </code>\n",
    "        </pre>\n",
    "    </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e14ee3a8",
   "metadata": {},
   "source": [
    "<div id=\"Huber code snippets\" style=\"margin-top: -15px\"></div>\n",
    "\n",
    "#### 2.2.4. Huber code snippets\n",
    "\n",
    "For quick copy-paste, replace <code>X</code> and <code>y</code> with your own data. Make sure to reshape your <code>X</code> so that it is a 2D <code>numpy.ndarray</code> object with shape like <code>(13, 1)</code>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "690def86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Huber -----------------------------------\n",
      "\n",
      "Coefficients         : [50.32948547]\n",
      "Intercept            : 29.03041669256188\n",
      "# of inliers         : 31\n",
      "Fraction of inliers  : 0.62\n",
      "\n",
      "------------------------------------------\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import HuberRegressor\n",
    "from sklearn import linear_model\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# sample data\n",
    "X = np.array([ 0.15, -0.34, 0.32, 0.43, -0.4, -0.04, -0.51, 0.3, 0.47, 0.12, 0.08, 0.04, -0.08, \n",
    "              -0.23, 0.08, -0.03, 0.03, 0.04, 0.01, 0.06, 0.03, 0., -0.04, -0.18, -0.19, -0.06, \n",
    "              -0.26, -0.16, 0.13, 0.09, 0.03, -0.03, 0.04, 0.14, -0.01, 0.4, -0.06, 0.15, 0.08, \n",
    "              0.05, -0.15, -0.09, -0.15, -0.11, -0.07, -0.19, -0.06, 0.17, 0.23, 0.18]).reshape(-1, 1)\n",
    "y = np.array([17.44, 25.46, 18.61, 26.07, 24.96, -1.22, 26.45, 26.5, 20.57, 3.08, 35.9 , 32.47, \n",
    "              20.84, 13.37, 42.44, 27.23, 35.65, 29.51, 31.28, 41.34, 32.19, 33.67, 25.64, 9.3, \n",
    "              14.63, 25.1, 4.69, 14.42, 47.53, 33.82, 32.2 , 24.81, 32.64, 45.11, 26.76, 68.01, \n",
    "              23.39, 43.49, 37.88, 36.01, 16.32, 19.77, 16.34, 19.57, 29.28, 16.62, 24.39, 43.77, \n",
    "              50.46, 47.09])\n",
    "\n",
    "# fit and predict: Huber \n",
    "huber = HuberRegressor(epsilon=1.35).fit(X, y)   # 1.35 is the default. You can try different epsilon values.\n",
    "y_pred_huber = huber.predict(X)\n",
    "\n",
    "# retrieve the fitted parameters\n",
    "coefs = huber.coef_\n",
    "intercept = huber.intercept_\n",
    "\n",
    "# seperate into inliers vs. outliers\n",
    "X_inlier = X[~huber.outliers_]\n",
    "y_inlier = y[~huber.outliers_]\n",
    "X_outlier = X[huber.outliers_]\n",
    "y_outlier = y[huber.outliers_]\n",
    "\n",
    "print(\"Huber -----------------------------------\\n\")\n",
    "print(\"Coefficients         :\", coefs)\n",
    "print(\"Intercept            :\", intercept)\n",
    "print(\"# of inliers         :\", sum(~huber.outliers_))\n",
    "print(\"Fraction of inliers  :\", sum(~huber.outliers_) / len(y))\n",
    "print(\"\\n------------------------------------------\")\n",
    "\n",
    "# fit and predict: Ordinary Least Squares\n",
    "ols = linear_model.LinearRegression().fit(X, y)\n",
    "y_pred_ols = ols.predict(X)\n",
    "\n",
    "# plot\n",
    "plt.title('Huber Regressor', fontsize=14)\n",
    "plt.scatter(X_inlier, y_inlier, s=100, c='green', label='Inliers')\n",
    "plt.scatter(X_outlier, y_outlier, s=100, c='red', label='Outliers')\n",
    "plt.plot(X, y_pred_ols, label='OLS')\n",
    "plt.plot(X, y_pred_huber, label='Huber')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d6003e5",
   "metadata": {},
   "source": [
    "<div class=\"highlights red-theme\" id=\"huber-warning\">\n",
    "    <div class=\"highlights-title red-theme\">WARNING!</div>\n",
    "    <div class=\"highlights-content red-theme\">Despite the application of the robust Huber regressor, the regression fit in the plot above (Figure ?) is still notably influenced by the presence of outliers, albeit to a lesser extent. This influence persists because, instead of completely excluding outliers during the fitting process, Huber aims to attenuate their impact by employing absolute loss rather than squared loss. Therefore, unless there is a compelling reason to retain and account for the so-called \"outliers\" within your data, using the Huber regressor is not typically recommended.\n",
    "    </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40fb8f64",
   "metadata": {},
   "source": [
    "<div id=\"Theil-Sen regressor\"></div>\n",
    "\n",
    "### 2.3. Theil-Sen regressor\n",
    "\n",
    "The Theil-Sen Regressor calculates the slope by examining all possible combinations of subsets from the dataset and taking the median of these slopes. This approach is based on the <i>\"$n$ choose $k$\"</i> $\\binom{n}{k}$ method, where $n$ is the number of data points and $k$ is the subset size. For example, with $k=2$ (standard for 2D linear regression) and $n=50$, the algorithm computes $\\binom{50}{2} = 1225$ combinations, each with a subset size of $k$. The median of the slopes from these subsets is used as the final slope for the regression line. This method can be adapted for different values of $k$, leading to a varying number of combinations.\n",
    "\n",
    "It is important to note that the TheilSen regressor does not explicitly identify outliers from inliers, unlike RANSAC or Huber. \n",
    "\n",
    "*(Note that these steps are based on <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.TheilSenRegressor.html#sklearn.linear_model.TheilSenRegressor\" target=_blank>sklearn's implementation of TheilSen regressor.</a>)*\n",
    "\n",
    "<div class=\"ordered-list\">\n",
    "    <h2>Steps</h2>\n",
    "    <ol>\n",
    "        <li>Consider an example dataset of size $n=6$: $A(1, 3)$, $B(2,2)$, $C(3,6)$, $D(4,5)$, $E(5,7)$, $F(6,5)$\n",
    "<div class=\"row full_screen_margin_80 mobile_responsive_plot_full_width\" style=\"\" id=\"fig-11\">\n",
    "<div class=\"col\"><img src=\"jupyter_images/TS_initial_data.png\"></div>\n",
    "<div class=\"col-12\"><p class=\"image-description\"><strong>Figure 11:</strong> Initial dataset of size $n=6$.</p></div>\n",
    "</div>\n",
    "                <div class=\"solution_panel closed\" style=\"margin-top: 20px;\">\n",
    "                <div class=\"solution_title\">\n",
    "                    <p class=\"solution_title_string\">Source Code For Figure (11)</p>\n",
    "                    <ul class=\"nav navbar-right panel_toolbox\">\n",
    "                        <li><a class=\"collapse-link\"><i class=\"fa fa-chevron-down\"></i></a></li>\n",
    "                    </ul>\n",
    "                <div class=\"clearfix\"></div>\n",
    "                </div>\n",
    "                <div class=\"solution_content\">\n",
    "                    <pre>\n",
    "                        <code class=\"language-python\">\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "##################################### sample data #####################################\n",
    "\n",
    "X = np.array([1, 2, 3, 4, 5, 6]).reshape(-1, 1)\n",
    "y = np.array([3, 2, 6, 5, 7, 5])\n",
    "\n",
    "###################################### plotting #######################################\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 4.5))\n",
    "\n",
    "ax.scatter(X, y, label='Original Data')\n",
    "\n",
    "points = ['A','B','C','D','E','F']\n",
    "for x_, y_, point in zip(X, y, points):\n",
    "    ax.text(x_, y_ + 1, '%s(%d,%d)' % (point, x_, y_), ha='left')\n",
    "    \n",
    "ax.legend(loc='upper left')\n",
    "ax.grid(axis='y', linestyle='--', color='#acacac', alpha=0.5)\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "\n",
    "ymax = 15\n",
    "ax.set_ylim(0 - 0.05 * ymax, ymax)  \n",
    "\n",
    "ax.set_xlabel('X', fontsize=13)\n",
    "ax.set_ylabel('y', fontsize=13)\n",
    "ax.text(0.99, 0.1, 'aegis4048.github.io', fontsize=10, ha='right', va='center',\n",
    "    transform=ax.transAxes, color='grey', alpha=0.5)\n",
    "\n",
    "def setbold(txt):\n",
    "    return ' '.join([r\"$\\bf{\" + item + \"}$\" for item in txt.split(' ')])\n",
    "\n",
    "bold_txt = setbold('Initial Data')\n",
    "plain_txt = r', $n=6$'\n",
    "fig.suptitle(bold_txt + plain_txt, verticalalignment='top', x=0, horizontalalignment='left', fontsize=13, y=0.96)\n",
    "yloc = 0.88\n",
    "ax.annotate('', xy=(0.01, yloc + 0.01), xycoords='figure fraction', xytext=(1.02, yloc + 0.01),\n",
    "            arrowprops=dict(arrowstyle=\"-\", color='k', lw=0.7))\n",
    "\n",
    "fig.tight_layout()\n",
    "                        </code>\n",
    "                    </pre>\n",
    "                </div>\n",
    "            </div>\n",
    "        </li>\n",
    "        <li>Let $k=2$ (default for 2D linear regression). $\\binom{6}{2}=15$ samples are generated.</li>\n",
    "        <li>Consider the first sample: $A(1, 3)$ and $B(2,2)$. The slope can be obtained with a simple $(y_{j}−y_{i})/(x_{j}−x_{i})$. Obtain intercept from the slope.\n",
    "<div class=\"row full_screen_margin_80 mobile_responsive_plot_full_width\" style=\"\" id=\"fig-12\">\n",
    "<div class=\"col\"><img src=\"jupyter_images/TS - first sample slope calculation.png\"></div>\n",
    "<div class=\"col-12\"><p class=\"image-description\"><strong>Figure 12:</strong> Linear regression on the first sample, composed of points $A$ and $B$.</p></div>\n",
    "</div>\n",
    "                <div class=\"solution_panel closed\" style=\"margin-top: 20px;\">\n",
    "                <div class=\"solution_title\">\n",
    "                    <p class=\"solution_title_string\">Source Code For Figure (12)</p>\n",
    "                    <ul class=\"nav navbar-right panel_toolbox\">\n",
    "                        <li><a class=\"collapse-link\"><i class=\"fa fa-chevron-down\"></i></a></li>\n",
    "                    </ul>\n",
    "                <div class=\"clearfix\"></div>\n",
    "                </div>\n",
    "                <div class=\"solution_content\">\n",
    "                    <pre>\n",
    "                        <code class=\"language-python\">\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "##################################### sample data #####################################\n",
    "\n",
    "X = np.array([1, 2, 3, 4, 5, 6]).reshape(-1, 1)\n",
    "y = np.array([3, 2, 6, 5, 7, 5])\n",
    "\n",
    "########################## slope and intercept calc# ##################################\n",
    "\n",
    "slope = (y[0] - y[1]) / (X[0] - X[1])\n",
    "intercept = y[0] - slope * X[0]\n",
    "y_pred = X * slope + intercept\n",
    "\n",
    "###################################### plotting #######################################\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 4.5))\n",
    "\n",
    "ax.scatter(X, y, label='Original Data')\n",
    "ax.scatter(X[:2], y[:2], label='Current sample')\n",
    "ax.plot(X, y_pred, label='Linear model for (A, B):  y = %.1fx + %.1f' % (slope, intercept), color='#ff7f0e', ls='-.')\n",
    "\n",
    "points = ['A','B','C','D','E','F']\n",
    "for x_, y_, point in zip(X, y, points):\n",
    "    ax.text(x_, y_ + 1, '%s(%d,%d)' % (point, x_, y_), ha='left')\n",
    "\n",
    "ax.legend(loc='upper left')\n",
    "ax.grid(axis='y', linestyle='--', color='#acacac', alpha=0.5)\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "\n",
    "ymax = 15\n",
    "ax.set_ylim(0 - 0.05 * ymax, ymax)\n",
    "\n",
    "ax.set_xlabel('X', fontsize=13)\n",
    "ax.set_ylabel('y', fontsize=13)\n",
    "ax.text(0.99, 0.1, 'aegis4048.github.io', fontsize=10, ha='right', va='center',\n",
    "    transform=ax.transAxes, color='grey', alpha=0.5)\n",
    "\n",
    "def setbold(txt):\n",
    "    return ' '.join([r\"$\\bf{\" + item + \"}$\" for item in txt.split(' ')])\n",
    "\n",
    "bold_txt = setbold('First Sample Slope Calculation')\n",
    "plain_txt = r', $k=2$ ($A$ and $B$)'\n",
    "fig.suptitle(bold_txt + plain_txt, verticalalignment='top', x=0, horizontalalignment='left', fontsize=13, y=0.96)\n",
    "yloc = 0.88\n",
    "ax.annotate('', xy=(0.01, yloc + 0.01), xycoords='figure fraction', xytext=(1.02, yloc + 0.01),\n",
    "            arrowprops=dict(arrowstyle=\"-\", color='k', lw=0.7))\n",
    "\n",
    "fig.tight_layout()\n",
    "                        </code>\n",
    "                    </pre>\n",
    "                </div>\n",
    "            </div>        \n",
    "        </li>\n",
    "        <li class=\"no-number\"><strong>(Optional 3.A).</strong> When more than two data points $k>2$ are present, the least-squares method, or OLS <a class=\"internal-link\" href=\"#eq-1\">eq-1</a>, is used for parameter fitting. This method is necessary as the simple slope formula $(y_{j}−y_{i})/(x_{j}−x_{i})$ is only valid for two data points. As the number of data points $k$ approaches the total number of observations $n$, the Theil-Sen regressor converges towards OLS results, which is not robust. \n",
    "<div class=\"row full_screen_margin_80 mobile_responsive_plot_full_width\" style=\"\" id=\"fig-13\">\n",
    "<div class=\"col\"><img src=\"jupyter_images/TS - first sample slope k=3.png\"></div>\n",
    "<div class=\"col-12\"><p class=\"image-description\"><strong>Figure 13:</strong> Because there are $k=3$ points, least-squares (OLS) method is necessary to fit a slope and an intercept. Note that the model robustness decreases as $k$ increases.</p></div>\n",
    "</div>\n",
    "                <div class=\"solution_panel closed\" style=\"margin-top: 20px;\">\n",
    "                <div class=\"solution_title\">\n",
    "                    <p class=\"solution_title_string\">Source Code For Figure (13)</p>\n",
    "                    <ul class=\"nav navbar-right panel_toolbox\">\n",
    "                        <li><a class=\"collapse-link\"><i class=\"fa fa-chevron-down\"></i></a></li>\n",
    "                    </ul>\n",
    "                <div class=\"clearfix\"></div>\n",
    "                </div>\n",
    "                <div class=\"solution_content\">\n",
    "                    <pre>\n",
    "                        <code class=\"language-python\">\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import linear_model\n",
    "\n",
    "##################################### sample data #####################################\n",
    "\n",
    "X = np.array([1, 2, 3, 4, 5, 6]).reshape(-1, 1)\n",
    "y = np.array([3, 2, 6, 5, 7, 5])\n",
    "\n",
    "########################## OLS on the current sample ##################################\n",
    "\n",
    "# Ordinary Least Squares\n",
    "ols = linear_model.LinearRegression().fit(X[:3], y[:3])\n",
    "y_pred_ols = ols.predict(X)\n",
    "coefs_ols = ols.coef_\n",
    "intercept_ols = ols.intercept_\n",
    "\n",
    "###################################### plotting #######################################\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 4.5))\n",
    "\n",
    "ax.scatter(X, y, label='Original Data')\n",
    "ax.scatter(X[:3], y[:3], label='Current sample')\n",
    "ax.plot(X, y_pred_ols, label='OLS (A, B, C):  y = %.1fx + %.1f' % (coefs_ols[0], intercept_ols), color='#ff7f0e', ls='-.')\n",
    "\n",
    "points = ['A','B','C','D','E','F']\n",
    "for x_, y_, point in zip(X, y, points):\n",
    "    ax.text(x_, y_ + 1, '%s(%d,%d)' % (point, x_, y_), ha='left')\n",
    "\n",
    "ax.legend(loc='upper left')\n",
    "ax.grid(axis='y', linestyle='--', color='#acacac', alpha=0.5)\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "\n",
    "ymax = 15\n",
    "ax.set_ylim(0 - 0.05 * ymax, ymax)\n",
    "\n",
    "ax.set_xlabel('X', fontsize=13)\n",
    "ax.set_ylabel('y', fontsize=13)\n",
    "ax.text(0.99, 0.1, 'aegis4048.github.io', fontsize=10, ha='right', va='center',\n",
    "    transform=ax.transAxes, color='grey', alpha=0.5)\n",
    "\n",
    "def setbold(txt):\n",
    "    return ' '.join([r\"$\\bf{\" + item + \"}$\" for item in txt.split(' ')])\n",
    "\n",
    "bold_txt = setbold('First Sample Slope Calculation')\n",
    "plain_txt = r', $k=3$ ($A$, $B$ and $C$)'\n",
    "fig.suptitle(bold_txt + plain_txt, verticalalignment='top', x=0, horizontalalignment='left', fontsize=13, y=0.96)\n",
    "yloc = 0.88\n",
    "ax.annotate('', xy=(0.01, yloc + 0.01), xycoords='figure fraction', xytext=(1.02, yloc + 0.01),\n",
    "            arrowprops=dict(arrowstyle=\"-\", color='k', lw=0.7))\n",
    "\n",
    "fig.tight_layout()\n",
    "                        </code>\n",
    "                    </pre>\n",
    "                </div>\n",
    "            </div>            \n",
    "        </li>\n",
    "        <li class=\"no-number\"><strong>4.</strong> Repeat step 3 for all 15 samples. A 2x15 matrix is obtained for 15 slopes and 15 intercepts. This is equivalent to fitting 15 straight lines.\n",
    "<div class=\"row full_screen_margin_80 mobile_responsive_plot_full_width\" style=\"\" id=\"fig-14\">\n",
    "<div class=\"col\"><img src=\"jupyter_images/TS - 15 samples fit.png\"></div>\n",
    "<div class=\"col-12\"><p class=\"image-description\"><strong>Figure 14:</strong> Regression fitted on all of the 15 samples, each of size $k=2$. Observe that all straight lines pass through $k=2$ points.</p></div>\n",
    "</div>\n",
    "                <div class=\"solution_panel closed\" style=\"margin-top: 20px;\">\n",
    "                <div class=\"solution_title\">\n",
    "                    <p class=\"solution_title_string\">Source Code For Figure (14)</p>\n",
    "                    <ul class=\"nav navbar-right panel_toolbox\">\n",
    "                        <li><a class=\"collapse-link\"><i class=\"fa fa-chevron-down\"></i></a></li>\n",
    "                    </ul>\n",
    "                <div class=\"clearfix\"></div>\n",
    "                </div>\n",
    "                <div class=\"solution_content\">\n",
    "                    <pre>\n",
    "                        <code class=\"language-python\">\n",
    "from itertools import combinations\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression, TheilSenRegressor\n",
    "from scipy.optimize import minimize\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "##################################### sample data #####################################\n",
    "\n",
    "X = np.array([1, 2, 3, 4, 5, 6]).reshape(-1, 1)\n",
    "y = np.array([3, 2, 6, 5, 7, 5])\n",
    "\n",
    "############################# parameter optimization ##################################\n",
    "\n",
    "n_samples, n_features = X.shape\n",
    "n_subsamples = 2\n",
    "fit_intercept = True\n",
    "\n",
    "# \"n choose k\" -> n Combination k number of samples. \n",
    "indices = np.array(list(combinations(range(n_samples), n_subsamples)))\n",
    "\n",
    "parameters = []\n",
    "for subset in indices:\n",
    "    X_subset = X[subset]\n",
    "    y_subset = y[subset]\n",
    "    model = LinearRegression(fit_intercept=fit_intercept)\n",
    "    model.fit(X_subset, y_subset)\n",
    "    parameters.append([model.intercept_, model.coef_[0]])\n",
    "parameters = np.vstack(parameters)\n",
    "\n",
    "slopes = parameters[:, 1]\n",
    "intercepts = parameters[:, 0]\n",
    "\n",
    "###################################### plotting #######################################\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 4.5))\n",
    "\n",
    "ax.scatter(X, y, label='Original Data')\n",
    "\n",
    "for slope, intercept in zip(slopes, intercepts):\n",
    "    y_pred_sample = slope * X + intercept\n",
    "    ax.plot(X, y_pred_sample, zorder=-99, color='silver')\n",
    "ax.plot(X, y_pred_sample, zorder=-99, color='lightgrey', label='Sample fit, $k=2$')\n",
    "\n",
    "points = ['A','B','C','D','E','F']\n",
    "for x_, y_, point in zip(X, y, points):\n",
    "    ax.text(x_, y_ + 1, '%s(%d,%d)' % (point, x_, y_), ha='left')\n",
    "\n",
    "ymax = 15\n",
    "ax.set_ylim(0 - 0.05 * ymax, ymax)    \n",
    "\n",
    "ax.legend(loc='upper left')\n",
    "ax.grid(axis='y', linestyle='--', color='#acacac', alpha=0.5)\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "\n",
    "ax.set_xlabel('X', fontsize=13)\n",
    "ax.set_ylabel('y', fontsize=13)\n",
    "ax.text(0.99, 0.1, 'aegis4048.github.io', fontsize=10, ha='right', va='center',\n",
    "    transform=ax.transAxes, color='grey', alpha=0.5)\n",
    "\n",
    "\n",
    "def setbold(txt):\n",
    "    return ' '.join([r\"$\\bf{\" + item + \"}$\" for item in txt.split(' ')])\n",
    "\n",
    "bold_txt = setbold('Fitting 6C2 = 15 Samples')\n",
    "plain_txt = r', 15 regression models fitted'\n",
    "fig.suptitle(bold_txt + plain_txt, verticalalignment='top', x=0, horizontalalignment='left', fontsize=13, y=0.96)\n",
    "yloc = 0.88\n",
    "ax.annotate('', xy=(0.01, yloc + 0.01), xycoords='figure fraction', xytext=(1.02, yloc + 0.01),\n",
    "            arrowprops=dict(arrowstyle=\"-\", color='k', lw=0.7))\n",
    "\n",
    "fig.tight_layout()                        \n",
    "                        </code>\n",
    "                    </pre>\n",
    "                </div>\n",
    "            </div>            \n",
    "        </li>\n",
    "        <li class=\"no-number\"><strong>5.</strong> A spatial median of the 15 slopes and 15 intercepts is computed. Note that the spatial median is not the simple median of each slopes and and intercepts independently. \n",
    "<div class=\"row full_screen_margin_80 mobile_responsive_plot_full_width\" style=\"\" id=\"fig-15\">\n",
    "<div class=\"col\"><img src=\"jupyter_images/spatial median of slopes and intercepts.png\"></div>\n",
    "<div class=\"col-12\"><p class=\"image-description\"><strong>Figure 15:</strong> This plot displays regression parameters of a 2D linear model, with intercepts casted on the x-axis and slopes casted on the y-axis. For a model with three parameters, such visualization would extend into 3D space. The spatial median is determined to be slope = 0.67 and interecept = 2.33.</p></div>\n",
    "</div>\n",
    "                <div class=\"solution_panel closed\" style=\"margin-top: 20px;\">\n",
    "                <div class=\"solution_title\">\n",
    "                    <p class=\"solution_title_string\">Source Code For Figure (15)</p>\n",
    "                    <ul class=\"nav navbar-right panel_toolbox\">\n",
    "                        <li><a class=\"collapse-link\"><i class=\"fa fa-chevron-down\"></i></a></li>\n",
    "                    </ul>\n",
    "                <div class=\"clearfix\"></div>\n",
    "                </div>\n",
    "                <div class=\"solution_content\">\n",
    "                    <pre>\n",
    "                        <code class=\"language-python\">     \n",
    "from itertools import combinations\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression, TheilSenRegressor\n",
    "from scipy.optimize import minimize\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "##################################### sample data #####################################\n",
    "\n",
    "X = np.array([1, 2, 3, 4, 5, 6]).reshape(-1, 1)\n",
    "y = np.array([3, 2, 6, 5, 7, 5])\n",
    "\n",
    "############################# parameter optimization ##################################\n",
    "\n",
    "n_samples, n_features = X.shape\n",
    "n_subsamples = 2\n",
    "fit_intercept = True\n",
    "\n",
    "# \"n choose k\" -> n Combination k number of samples. \n",
    "indices = np.array(list(combinations(range(n_samples), n_subsamples)))\n",
    "\n",
    "parameters = []\n",
    "for subset in indices:\n",
    "    X_subset = X[subset]\n",
    "    y_subset = y[subset]\n",
    "    model = LinearRegression(fit_intercept=fit_intercept)\n",
    "    model.fit(X_subset, y_subset)\n",
    "    parameters.append([model.intercept_, model.coef_[0]])\n",
    "parameters = np.vstack(parameters)\n",
    "\n",
    "########################## spatial median approximation ###############################\n",
    "\n",
    "# L2 loss - euclidean distance\n",
    "def L2_objective_func(point, _x, _y):\n",
    "    return np.sum(np.sqrt((point[0] - _x)**2 + (point[1] - _y)**2))\n",
    "\n",
    "intercepts = parameters[:, 0] \n",
    "slopes = parameters[:, 1]\n",
    "\n",
    "init_guess = [np.mean(intercepts), np.mean(slopes)]  # starting with mean is a good guess to reduce computational cost\n",
    "result_L2 = minimize(L2_objective_func, init_guess, args=(intercepts, slopes), method='Nelder-Mead')\n",
    "\n",
    "################################### result validation #################################\n",
    "\n",
    "# Checks that the implemented codes here agree with the sklearn implementation\n",
    "TS = TheilSenRegressor().fit(X, y)\n",
    "y_pred_TS = TS.predict(X)\n",
    "\n",
    "np.testing.assert_almost_equal(result_L2.x[1], TS.coef_[0], decimal=2)\n",
    "np.testing.assert_almost_equal(result_L2.x[0], TS.intercept_, decimal=2)\n",
    "\n",
    "###################################### plotting #######################################\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(7, 4))\n",
    "\n",
    "ax.scatter(intercepts, slopes, s=150, edgecolor='blue', fc=(0, 0, 1, 0.05))\n",
    "_s1 = ax.scatter(result_L2.x[0], result_L2.x[1], s=400, marker='*',\n",
    "                 label=r'Median:  $\\underset{x, y}{\\mathrm{argmin}} \\sum^{n}_{i=1}\\sqrt{(x_{i} - \\hat{x}_{i})^{2} + (y_{i} - \\hat{y}_{i})^2}$')\n",
    "\n",
    "ax.grid(axis='both', linestyle='--', color='#acacac', alpha=0.5)\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "\n",
    "ax.set_xlabel('Intercepts')\n",
    "ax.set_ylabel('Slopes')\n",
    "\n",
    "ax.text(result_L2.x[0] + 0.5, result_L2.x[1] + 0.15, '(%.2f, %.2f)' % (result_L2.x[0], result_L2.x[1]), color=_s1.get_facecolor()[0], ha='left')\n",
    "ax.text(result_L2.x[0] + 0.5, result_L2.x[1] + 0.5, 'Spatial Median', color=_s1.get_facecolor()[0], ha='left')\n",
    "ax.text(0.98, 0.08, 'aegis4048.github.io', fontsize=10, ha='right', transform=ax.transAxes, color='grey', alpha=0.5)\n",
    "\n",
    "def setbold(txt):\n",
    "    return ' '.join([r\"$\\bf{\" + item + \"}$\" for item in txt.split(' ')])\n",
    "\n",
    "bold_txt = setbold('Spatial Median of Slopes and Intercepts, ')\n",
    "plain_txt = r'obtained by minimizing the $L_{2}$ norm.'\n",
    "fig.suptitle(bold_txt + plain_txt, verticalalignment='top', x=0, horizontalalignment='left', fontsize=11, y=0.95)\n",
    "yloc = 0.87\n",
    "ax.annotate('', xy=(0.01, yloc + 0.01), xycoords='figure fraction', xytext=(1.02, yloc + 0.01),\n",
    "            arrowprops=dict(arrowstyle=\"-\", color='k', lw=0.7))\n",
    "\n",
    "\n",
    "fig.tight_layout()                    \n",
    "                        </code>\n",
    "                    </pre>\n",
    "                </div>\n",
    "            </div>               \n",
    "        </li>\n",
    "        <li class=\"no-number\"><strong>6.</strong> Return the final model.\n",
    "<div class=\"row full_screen_margin_80 mobile_responsive_plot_full_width\" style=\"\" id=\"fig-16\">\n",
    "<div class=\"col\"><img src=\"jupyter_images/TS - final slope and intercept.png\"></div>\n",
    "<div class=\"col-12\"><p class=\"image-description\"><strong>Figure 16:</strong> Straight line drawn from the spatial median of (intercepts, slopes) from step 5. </p></div>\n",
    "</div>\n",
    "                <div class=\"solution_panel closed\" style=\"margin-top: 20px;\">\n",
    "                <div class=\"solution_title\">\n",
    "                    <p class=\"solution_title_string\">Source Code For Figure (16)</p>\n",
    "                    <ul class=\"nav navbar-right panel_toolbox\">\n",
    "                        <li><a class=\"collapse-link\"><i class=\"fa fa-chevron-down\"></i></a></li>\n",
    "                    </ul>\n",
    "                <div class=\"clearfix\"></div>\n",
    "                </div>\n",
    "                <div class=\"solution_content\">\n",
    "                    <pre>\n",
    "                        <code class=\"language-python\">     \n",
    "from itertools import combinations\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression, TheilSenRegressor\n",
    "from scipy.optimize import minimize\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "##################################### sample data #####################################\n",
    "\n",
    "X = np.array([1, 2, 3, 4, 5, 6]).reshape(-1, 1)\n",
    "y = np.array([3, 2, 6, 5, 7, 5])\n",
    "\n",
    "############################# parameter optimization ##################################\n",
    "\n",
    "n_samples, n_features = X.shape\n",
    "n_subsamples = 2\n",
    "fit_intercept = True\n",
    "\n",
    "# \"n choose k\" -> n Combination k number of samples.\n",
    "indices = np.array(list(combinations(range(n_samples), n_subsamples)))\n",
    "\n",
    "parameters = []\n",
    "for subset in indices:\n",
    "    X_subset = X[subset]\n",
    "    y_subset = y[subset]\n",
    "    model = LinearRegression(fit_intercept=fit_intercept)\n",
    "    model.fit(X_subset, y_subset)\n",
    "    parameters.append([model.intercept_, model.coef_[0]])\n",
    "parameters = np.vstack(parameters)\n",
    "\n",
    "########################## spatial median approximation ###############################\n",
    "\n",
    "# L2 loss - euclidean distance\n",
    "def L2_objective_func(point, _x, _y):\n",
    "    return np.sum(np.sqrt((point[0] - _x)**2 + (point[1] - _y)**2))\n",
    "\n",
    "intercepts = parameters[:, 0]\n",
    "slopes = parameters[:, 1]\n",
    "\n",
    "init_guess = [np.mean(intercepts), np.mean(slopes)]  # starting with mean is a good guess to reduce computational cost\n",
    "result_L2 = minimize(L2_objective_func, init_guess, args=(intercepts, slopes), method='Nelder-Mead')\n",
    "\n",
    "final_slope = result_L2.x[1]\n",
    "final_intercept = result_L2.x[0]\n",
    "y_pred = X * final_slope + final_intercept\n",
    "\n",
    "################################### result validation #################################\n",
    "\n",
    "# Checks that the implemented codes here agree with the sklearn implementation\n",
    "TS = TheilSenRegressor().fit(X, y)\n",
    "y_pred_TS = TS.predict(X)\n",
    "\n",
    "np.testing.assert_almost_equal(result_L2.x[1], TS.coef_[0], decimal=2)\n",
    "np.testing.assert_almost_equal(result_L2.x[0], TS.intercept_, decimal=2)\n",
    "\n",
    "###################################### plotting #######################################\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 4.5))\n",
    "\n",
    "ax.scatter(X, y, label='Original Data')\n",
    "ax.plot(X, y_pred, label='Final TheilSen model:  y = %.1fx + %.1f' % (final_slope, final_intercept), color='#ff7f0e', ls='-.')\n",
    "\n",
    "points = ['A','B','C','D','E','F']\n",
    "for x_, y_, point in zip(X, y, points):\n",
    "    ax.text(x_, y_ + 1, '%s(%d,%d)' % (point, x_, y_), ha='left')\n",
    "\n",
    "ax.legend(loc='upper left')\n",
    "ax.grid(axis='y', linestyle='--', color='#acacac', alpha=0.5)\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "\n",
    "ymax = 15\n",
    "ax.set_ylim(0 - 0.05 * ymax, ymax)\n",
    "\n",
    "ax.set_xlabel('X', fontsize=13)\n",
    "ax.set_ylabel('y', fontsize=13)\n",
    "ax.text(0.99, 0.1, 'aegis4048.github.io', fontsize=10, ha='right', va='center',\n",
    "    transform=ax.transAxes, color='grey', alpha=0.5)\n",
    "\n",
    "def setbold(txt):\n",
    "    return ' '.join([r\"$\\bf{\" + item + \"}$\" for item in txt.split(' ')])\n",
    "\n",
    "bold_txt = setbold('Final Slope and Intercept')\n",
    "plain_txt = r', TheilSen regressor for $n=6$ and $k=2$'\n",
    "fig.suptitle(bold_txt + plain_txt, verticalalignment='top', x=0, horizontalalignment='left', fontsize=13, y=0.96)\n",
    "yloc = 0.88\n",
    "ax.annotate('', xy=(0.01, yloc + 0.01), xycoords='figure fraction', xytext=(1.02, yloc + 0.01),\n",
    "            arrowprops=dict(arrowstyle=\"-\", color='k', lw=0.7))\n",
    "\n",
    "fig.tight_layout()           \n",
    "                        </code>\n",
    "                    </pre>\n",
    "                </div>\n",
    "            </div>                       \n",
    "        </li>\n",
    "    </ol>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "624b9a00",
   "metadata": {},
   "source": [
    "<div id=\"Advanced: Impact of adjusting $k$ on the model robustness\" style=\"margin-top: -15px\"></div>\n",
    "\n",
    "#### 2.3.1. Sample size and model robustness\n",
    "\n",
    "When a sample size $k> 2$, indicating more than two data points in each sample, the Theil-Sen regressor employs the OLS method for each sample generated. This method uses the *$L_2$-norm squared* <a href=\"#eq-1\" class=\"internal-link\">eq-1</a> as its objective function, which, while effective, is sensitive to outliers because it places greater emphasis on larger residuals due to the squared term. As $k$ approaches $n$, the robustness of the Theil-Sen regressor decreases. This decrease in robustness occurs because at $k=n$, the model is essentially fitting an OLS model to the entire dataset as one single sample ($\\binom{n}{k} = 1$). In a 3D regression context, $k$ is at leat 3, factoring in the number of features plus 1 if <code>fit_intercept=True</code> (default). The sklearn package sets $k$ to its minimum value by default to maximize robustness. <u>Adjusting $k$ is not recommended</u> unless there's a clear understanding of how it affects the model's robustness."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "709bfcb3",
   "metadata": {},
   "source": [
    "<div id=\"Advanced: Spatial median\" style=\"\"></div>\n",
    "\n",
    "#### 2.3.2. Spatial median\n",
    "\n",
    "The spatial median is the point minimizing the sum of Euclidean distances from all points in a given space. This concept extends the 2D Pythagorean theorem to n-dimensional space and is mathematically defined as the point minimizing the *$L_2$-norm* <a href=\"#eq-2\" class=\"internal-link\">eq-2</a>. For clarity, consider a scenario with five oil and gas wells and a central refinery. To minimize pipeline construction costs, the refinery should be located at the spatial median, thereby reducing the total length of required pipelines. It's crucial to note that the spatial median is not simply the median of x-axis and y-axis values considered separately, but a combined evaluation of all dimension, as highlighted in  <a href=\"#eq-17\" class=\"internal-link\">Figure 17</a>.\n",
    "\n",
    "<div class=\"row\" style=\"\" id=\"fig-17\">\n",
    "<div class=\"col\"><img src=\"jupyter_images/separate medians vs spatial medians.png\"></div>\n",
    "<div class=\"col-12\"><p class=\"image-description\"><strong>Figure 17:</strong> Taking the median of the x-axis and y-axis separately yields different results from the spatial median.</p></div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d47596e8",
   "metadata": {},
   "source": [
    "<div class=\"solution_panel closed\">\n",
    "    <div class=\"solution_title\">\n",
    "        <p class=\"solution_title_string\">Source Code For Figure (17)</p>\n",
    "        <ul class=\"nav navbar-right panel_toolbox\">\n",
    "            <li><a class=\"collapse-link\"><i class=\"fa fa-chevron-down\"></i></a></li>\n",
    "        </ul>\n",
    "    <div class=\"clearfix\"></div>\n",
    "    </div>\n",
    "    <div class=\"solution_content\">\n",
    "        <pre>\n",
    "            <code class=\"language-python\">\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "x = [3, 9, 21, 25]\n",
    "y = [12, 35, 16, 28]\n",
    "\n",
    "x_outlier = [3, 9, 15, 21, 25]\n",
    "y_outlier =[12, 35, 100, 16, 28]\n",
    "\n",
    "ys = [y, y_outlier]\n",
    "xs = [x, x_outlier]\n",
    "\n",
    "# L2 loss - euclidean distance\n",
    "def L2_objective_func(point, _x, _y):\n",
    "    return np.sum(np.sqrt((point[0] - _x)**2 + (point[1] - _y)**2))\n",
    "\n",
    "s = 150\n",
    "init_guess = [0, 0]\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(9, 4))\n",
    "for i, (ax, x, y) in enumerate(zip(axes, xs, ys)):\n",
    "\n",
    "    # calculates L2 euclidean loss. This results in a spatial median\n",
    "    result_L2 = minimize(L2_objective_func, init_guess, args=(x, y), method='Nelder-Mead')\n",
    "\n",
    "    ax.scatter(x, y, s=s, edgecolor='blue', fc=(0, 0, 1, 0.05))\n",
    "    _s1 = ax.scatter(result_L2.x[0], result_L2.x[1], s=s,\n",
    "                     label=r'Spatial Median', marker='*')\n",
    "    \n",
    "    _s2 = ax.scatter(np.median(x), np.median(y), s=s,\n",
    "                     label=r'Separate Medians', marker='+', lw=3)\n",
    "\n",
    "    xmax = 30\n",
    "    ymax = 110\n",
    "    ax.set_xlim(0 - 0.05 * xmax, xmax)\n",
    "    ax.set_ylim(0 - 0.05 * ymax, ymax)\n",
    "\n",
    "    ax.grid(axis='both', linestyle='--', color='#acacac', alpha=0.5)\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "\n",
    "    ax.text(np.median(x), np.median(y) + 6, '(%d, %d)' % (np.median(x), np.median(y)), color=_s2.get_facecolor()[0], ha='right')\n",
    "    ax.text(result_L2.x[0], result_L2.x[1] + 6, '(%d, %d)' % (result_L2.x[0], result_L2.x[1]), color=_s1.get_facecolor()[0], ha='left')\n",
    "    ax.text(0.98, 0.08, 'aegis4048.github.io', fontsize=10, ha='right', transform=ax.transAxes, color='grey', alpha=0.5)\n",
    "\n",
    "    ax.text(np.median(x), np.median(y) + 14, 'Separate Medians', color=_s2.get_facecolor()[0], ha='right')\n",
    "    ax.text(result_L2.x[0], result_L2.x[1] + 14, 'Spatial Median', color=_s1.get_facecolor()[0], ha='left')\n",
    "\n",
    "    ax.set_ylabel('Y')\n",
    "    ax.set_xlabel('X')\n",
    "\n",
    "axes[1].scatter(x[2], y_outlier[2], s=100, marker='x')\n",
    "axes[1].text(x[2], y_outlier[2] - 10, 'outlier', color='red', ha='center')\n",
    "\n",
    "def setbold(txt):\n",
    "    return ' '.join([r\"$\\bf{\" + item + \"}$\" for item in txt.split(' ')])\n",
    "\n",
    "bold_txt = setbold('Separate Medians vs Spatial Medians, ')\n",
    "plain_txt = r''\n",
    "fig.suptitle(bold_txt + plain_txt, verticalalignment='top', x=0, horizontalalignment='left', fontsize=12, y=0.96)\n",
    "yloc = 0.88\n",
    "ax.annotate('', xy=(0.01, yloc + 0.01), xycoords='figure fraction', xytext=(1.02, yloc + 0.01),\n",
    "            arrowprops=dict(arrowstyle=\"-\", color='k', lw=0.7))\n",
    "\n",
    "fig.tight_layout()\n",
    "            </code>\n",
    "        </pre>\n",
    "    </div>\n",
    "</div>\n",
    "\n",
    "A convenient library for calculating spatial median is <a href=\"https://github.com/daleroberts/hdmedians\" target=\"_blank\">hdmedians</a>. This library features a Cython implementation that enables faster computation. An example usage is as the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "93383c5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([17.06557376, 22.22950821])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import hdmedians as hd\n",
    "\n",
    "x = np.array([3, 9, 21, 25], dtype=float)    # dtype=float is necessary to avoid TypeError due to conflicts with Cython\n",
    "y = np.array([12, 35, 16, 28], dtype=float)\n",
    "data = np.array([x, y])\n",
    "\n",
    "np.array(hd.geomedian(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "083b99e5",
   "metadata": {},
   "source": [
    "<div id=\"Advanced: Why use median instead of mean for outliers?\" style=\"\"></div>\n",
    "\n",
    "#### 2.3.3. Why use median instead of mean with outliers?\n",
    "\n",
    "In <a href=\"#fig-15\" class=\"internal-link\">Figure 15</a>, where 15 slopes and intercepts were fitted, the TheilSen regressor uses the spatial median of these parameters rather than their mean. This choice is informed by the median's superior resilience to outliers, which can be understood deeper from two perspectives:\n",
    "\n",
    "<div><hr></div>\n",
    "\n",
    "<div id=\"1. Effect of the squared term\"></div>\n",
    "\n",
    "##### 2.3.3.1. Effect of the squared term\n",
    "\n",
    "The squared term in the mean calculation, stemming from the least-squares method (which converges to the mean as previously demonstrated <a href=\"#The equation converges to mean, and inherits the statistical properties of mean\" class=\"internal-link\">above</a>), amplifies the influence of extreme data points. In a 2D or n-dimensional space, the mean, or centroid, is the point that minimizes the squared sum of distances from each point, effectively minimizing the *$L_2$-norm squared* <a href=\"#eq-1\" class=\"internal-link\">eq-1</a>. This minimization is akin to reducing the average distances from all points. Due to the squaring of distances, outliers significantly impact the mean, as illustrated in <a href=\"#fig-3\" class=\"internal-link\">Figure 3</a>.\n",
    "\n",
    "In contrast, the spatial median employs the *$L_2$-norm* <a href=\"#eq-2\" class=\"internal-link\">eq-2</a>. While both equations incorporate a squared term, the spatial median involves taking the square root of the sum of these squared distances, which lessens the impact of extreme values. Note that for 1D array ($k=1$, not applicable to 2D regression), the spatial median is equivalent to 1D median because <a href=\"#eq-2\" class=\"internal-link\">eq-2</a> equates to <a href=\"#eq-1\" class=\"internal-link\">eq-1</a>.\n",
    "\n",
    "\n",
    "<div id=\"2. Measure of central tendency\"></div>\n",
    "\n",
    "##### 2.3.3.2. Measure of central tendency\n",
    "\n",
    "Median is a better measure of central tendency than mean in presence of outliers. For simple illustration, consider an array $x = [1, 2, 3, 4, 5]$, where both the mean and median are 3. Introducing an outlier to form $x^{'} = [1,2,3,4,5, 5000]$ shifts the median only slightly to 3.5, while the mean soars to 836. This is not a good representation of the point in which most data points cluster.\n",
    "\n",
    "For a detailed demonstration, consider a Theil-Sen regression with the same dataset as in the previous RANSAC example <a href=\"#RANSAC code snippets\" class=\"internal-link\">above</a>, featuring $n=50$ points. The dataset, generated from the model $y = 92x + 30$ with some outliers, leads to $\\binom{50}{2} = 1225$ unique samples, each yielding a slope and an intercept. The distribution of these fitted parameters is depicted in <a href=\"#fig-18\" class=\"internal-link\">Figure 18</a>.\n",
    "\n",
    "The distribution of slopes (left plot) exhibits rightward skewness. The median slope, calculated as 74.6, aligns more closely with the true model parameter (92) used to generate the data. This aligns with the theory that the median is less influenced by outliers than the mean, providing a better representation of central tendency, as also illustrated in <a href=\"#fig-2\" class=\"internal-link\">Figure 2</a> above.\n",
    "\n",
    "<div class=\"row\" style=\"\" id=\"fig-18\">\n",
    "<div class=\"col\"><img src=\"jupyter_images/distribution of slopes and intercepts.png\"></div>\n",
    "<div class=\"col-12\"><p class=\"image-description\"><strong>Figure 18:</strong> Displays the distribution of fitted parameters from the 1225 samples. The model's true parameters are defined by $y=92x + 30$. The left plot, representing the slope distribution, highlights the median as a more accurate representation of the central clustering of data points, underscoring the median's superiority over the mean in outlier-affected scenarios. Conversely, in the intercept distribution (right plot), the mean and median are nearly identical, a result of the intercepts' normal (symmetric) distribution. It's crucial to recognize that the medians depicted here are one-dimensional, used solely for illustrative purposes, and do not represent spatial medians.</p></div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f492aeb8",
   "metadata": {},
   "source": [
    "<div class=\"solution_panel closed\">\n",
    "    <div class=\"solution_title\">\n",
    "        <p class=\"solution_title_string\">Source Code For Figure (18)</p>\n",
    "        <ul class=\"nav navbar-right panel_toolbox\">\n",
    "            <li><a class=\"collapse-link\"><i class=\"fa fa-chevron-down\"></i></a></li>\n",
    "        </ul>\n",
    "    <div class=\"clearfix\"></div>\n",
    "    </div>\n",
    "    <div class=\"solution_content\">\n",
    "        <pre>\n",
    "            <code class=\"language-python\">\n",
    "from itertools import combinations\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "\n",
    "###################################### data ######################################\n",
    "\n",
    "X = np.array([ 0.15, -0.34, 0.32, 0.43, -0.4, -0.04, -0.51, 0.3, 0.47, 0.12, 0.08, 0.04, -0.08,\n",
    "              -0.23, 0.08, -0.03, 0.03, 0.04, 0.01, 0.06, 0.03, 0., -0.04, -0.18, -0.19, -0.06,\n",
    "              -0.26, -0.16, 0.13, 0.09, 0.03, -0.03, 0.04, 0.14, -0.01, 0.4, -0.06, 0.15, 0.08,\n",
    "              0.05, -0.15, -0.09, -0.15, -0.11, -0.07, -0.19, -0.06, 0.17, 0.23, 0.18]).reshape(-1, 1)\n",
    "y = np.array([17.44, 25.46, 18.61, 26.07, 24.96, -1.22, 26.45, 26.5, 20.57, 3.08, 35.9 , 32.47,\n",
    "              20.84, 13.37, 42.44, 27.23, 35.65, 29.51, 31.28, 41.34, 32.19, 33.67, 25.64, 9.3,\n",
    "              14.63, 25.1, 4.69, 14.42, 47.53, 33.82, 32.2 , 24.81, 32.64, 45.11, 26.76, 68.01,\n",
    "              23.39, 43.49, 37.88, 36.01, 16.32, 19.77, 16.34, 19.57, 29.28, 16.62, 24.39, 43.77,\n",
    "              50.46, 47.09])\n",
    "\n",
    "############################# parameter optimization ##################################\n",
    "\n",
    "n_samples, n_features = X.shape\n",
    "n_subsamples = 2\n",
    "fit_intercept = True\n",
    "\n",
    "# \"n choose k\" -> n Combination k number of samples.\n",
    "indices = np.array(list(combinations(range(n_samples), n_subsamples)))\n",
    "\n",
    "parameters = []\n",
    "for subset in indices:\n",
    "    X_subset = X[subset]\n",
    "    y_subset = y[subset]\n",
    "    model = LinearRegression(fit_intercept=fit_intercept)\n",
    "    model.fit(X_subset, y_subset)\n",
    "    parameters.append([model.intercept_, model.coef_[0]])\n",
    "parameters = np.vstack(parameters)\n",
    "\n",
    "intercepts = parameters[:, 0]\n",
    "slopes = parameters[:, 1]\n",
    "\n",
    "###################################### plotting ######################################\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n",
    "\n",
    "items = [slopes, intercepts]\n",
    "nbins = [400, 400]\n",
    "\n",
    "for ax, item, nbin in zip(axes, items, nbins):\n",
    "\n",
    "    mean = np.mean(item)\n",
    "    median = np.median(item)\n",
    "\n",
    "    ax.hist(item, bins=nbin, histtype='stepfilled', edgecolor='k', alpha=0.4, color='grey',)\n",
    "    ax.axvline(x=median, color='r', alpha=0.7, label='Median=%.1f' % median)\n",
    "    ax.axvline(x=mean, color='k', alpha=0.7, label='Mean=%.1f' % mean)\n",
    "\n",
    "    ax.legend(loc='upper left', ncol=1)\n",
    "    ax.grid(axis='y', linestyle='--', color='#acacac', alpha=0.5)\n",
    "    ax.set_ylim(0, 340)\n",
    "\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "\n",
    "    ax.set_ylabel(\"Occurrences\", fontsize=12)\n",
    "    ax.text(0.99, 0.1, 'aegis4048.github.io', fontsize=10, ha='right', va='center',\n",
    "        transform=ax.transAxes, color='grey', alpha=0.5)\n",
    "\n",
    "axes[0].set_xlim(-275, 300)\n",
    "axes[1].set_xlim(-15, 65)\n",
    "\n",
    "axes[0].set_xlabel(\"Slope range\", fontsize=12)\n",
    "axes[1].set_xlabel(\"Intercept range\", fontsize=12)\n",
    "\n",
    "def setbold(txt):\n",
    "    return ' '.join([r\"$\\bf{\" + item + \"}$\" for item in txt.split(' ')])\n",
    "\n",
    "bold_txt = setbold('Distribution of Slopes & Intercepts, ')\n",
    "plain_txt = r'comparison of median vs mean, 1225 samples'\n",
    "fig.suptitle(bold_txt + plain_txt, verticalalignment='top', x=0, horizontalalignment='left', fontsize=14, y=0.98)\n",
    "yloc = 0.88\n",
    "ax.annotate('', xy=(0.01, yloc + 0.01), xycoords='figure fraction', xytext=(1.02, yloc + 0.01),\n",
    "            arrowprops=dict(arrowstyle=\"-\", color='k', lw=0.7))\n",
    "\n",
    "fig.tight_layout()\n",
    "            </code>\n",
    "        </pre>\n",
    "    </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63cb92ac",
   "metadata": {},
   "source": [
    "<div style=\"margin-top: -15px;\"></div>\n",
    "\n",
    "<a href=\"#fig-19\" class=\"internal-link\">Figure 19</a> compares the performance of mean-based and median-based parameter estimators. The median-based estimator $y=74.6x + 29.5$ is observed to more closely approximate the true population model $y=92x + 30$ than other estimators.\n",
    "\n",
    "However, it's noteworthy that the spatial median TheilSen regressor does not align perfectly with the true population parameters as RANSAC does, as shown <a href=\"#RANSAC code snippets\" class=\"internal-link\">above</a>. This difference arises because RANSAC identifies and excludes outliers before optimizing parameters, whereas TheilSen attempts to lessen the impact of outliers by taking the square root of the squared sum of residuals. Therefore, if outlier exclusion is preferable to mitigation, RANSAC tends to outperform TheilSen.\n",
    "\n",
    "<div class=\"row\" style=\"\" id=\"fig-19\">\n",
    "<div class=\"col\"><img src=\"jupyter_images/mean vs median regression comparison.png\"></div>\n",
    "<div class=\"col-12\"><p class=\"image-description\"><strong>Figure 19:</strong> From the original 50 data points, 1225 samples of size $k=2$ are generated, and 1225 lines (light grey) are fitted. The median slope and intercept model (orange line) demonstrates greater robustness to outliers compared to the mean-based (blue) and OLS models (green).</p></div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "916c6204",
   "metadata": {},
   "source": [
    "<div class=\"solution_panel closed\">\n",
    "    <div class=\"solution_title\">\n",
    "        <p class=\"solution_title_string\">Source Code For Figure (19)</p>\n",
    "        <ul class=\"nav navbar-right panel_toolbox\">\n",
    "            <li><a class=\"collapse-link\"><i class=\"fa fa-chevron-down\"></i></a></li>\n",
    "        </ul>\n",
    "    <div class=\"clearfix\"></div>\n",
    "    </div>\n",
    "    <div class=\"solution_content\">\n",
    "        <pre>\n",
    "            <code class=\"language-python\">\n",
    "from itertools import combinations\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression, TheilSenRegressor\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "\n",
    "###################################### data ######################################\n",
    "\n",
    "X = np.array([ 0.15, -0.34, 0.32, 0.43, -0.4, -0.04, -0.51, 0.3, 0.47, 0.12, 0.08, 0.04, -0.08,\n",
    "              -0.23, 0.08, -0.03, 0.03, 0.04, 0.01, 0.06, 0.03, 0., -0.04, -0.18, -0.19, -0.06,\n",
    "              -0.26, -0.16, 0.13, 0.09, 0.03, -0.03, 0.04, 0.14, -0.01, 0.4, -0.06, 0.15, 0.08,\n",
    "              0.05, -0.15, -0.09, -0.15, -0.11, -0.07, -0.19, -0.06, 0.17, 0.23, 0.18]).reshape(-1, 1)\n",
    "y = np.array([17.44, 25.46, 18.61, 26.07, 24.96, -1.22, 26.45, 26.5, 20.57, 3.08, 35.9 , 32.47,\n",
    "              20.84, 13.37, 42.44, 27.23, 35.65, 29.51, 31.28, 41.34, 32.19, 33.67, 25.64, 9.3,\n",
    "              14.63, 25.1, 4.69, 14.42, 47.53, 33.82, 32.2 , 24.81, 32.64, 45.11, 26.76, 68.01,\n",
    "              23.39, 43.49, 37.88, 36.01, 16.32, 19.77, 16.34, 19.57, 29.28, 16.62, 24.39, 43.77,\n",
    "              50.46, 47.09])\n",
    "\n",
    "############################# parameter optimization ##################################\n",
    "\n",
    "n_samples, n_features = X.shape\n",
    "n_subsamples = 2\n",
    "fit_intercept = True\n",
    "\n",
    "# \"n choose k\" -> n Combination k number of samples.\n",
    "indices = np.array(list(combinations(range(n_samples), n_subsamples)))\n",
    "\n",
    "parameters = []\n",
    "for subset in indices:\n",
    "    X_subset = X[subset]\n",
    "    y_subset = y[subset]\n",
    "    model = LinearRegression(fit_intercept=fit_intercept)\n",
    "    model.fit(X_subset, y_subset)\n",
    "    parameters.append([model.intercept_, model.coef_[0]])\n",
    "parameters = np.vstack(parameters)\n",
    "\n",
    "intercepts = parameters[:, 0]\n",
    "slopes = parameters[:, 1]\n",
    "\n",
    "########################## spatial median approximation ###############################\n",
    "\n",
    "# L2 loss - euclidean distance\n",
    "def L2_objective_func(point, _x, _y):\n",
    "    return np.sum(np.sqrt((point[0] - _x)**2 + (point[1] - _y)**2))\n",
    "\n",
    "init_guess = [np.mean(intercepts), np.mean(slopes)]  # starting with mean is a good guess to reduce computational cost\n",
    "result_L2 = minimize(L2_objective_func, init_guess, args=(intercepts, slopes), method='Nelder-Mead')\n",
    "\n",
    "slope_spatial_median = result_L2.x[1]\n",
    "intercept_spatial_median = result_L2.x[0]\n",
    "y_pred_spatial_median = X * slope_spatial_median + intercept_spatial_median\n",
    "\n",
    "################################### result validation #################################\n",
    "\n",
    "# Checks that the implemented codes here agree with the sklearn implementation\n",
    "TS = TheilSenRegressor().fit(X, y)\n",
    "y_pred_TS = TS.predict(X)\n",
    "\n",
    "np.testing.assert_almost_equal(slope_spatial_median, TS.coef_[0], decimal=2)\n",
    "np.testing.assert_almost_equal(intercept_spatial_median, TS.intercept_, decimal=2)\n",
    "\n",
    "########################################## OLS #######################################\n",
    "\n",
    "ols = LinearRegression().fit(X, y)\n",
    "y_pred_ols = ols.predict(X)\n",
    "\n",
    "##################################### means ##########################################\n",
    "\n",
    "slope_mean = np.mean(slopes)\n",
    "intercept_mean = np.mean(intercepts)\n",
    "y_pred_mean = slope_mean * X + intercept_mean\n",
    "\n",
    "###################################### plotting ######################################\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 4.5))\n",
    "\n",
    "ax.scatter(X, y)\n",
    "ax.plot(X, y_pred_mean, label='TS Mean     : y = %.1fx + %.1f' % (slope_mean, intercept_mean))\n",
    "ax.plot(X, y_pred_spatial_median, label='TS Median  : y = %.1fx + %.1f' % (slope_spatial_median, intercept_spatial_median))\n",
    "ax.plot(X, y_pred_ols, label='OLS            : y = %.1fx + %.1f' % (ols.coef_[0], ols.intercept_), alpha=0.5)\n",
    " \n",
    "for slope, intercept in zip(slopes, intercepts):\n",
    "    y_pred_sample = slope * X + intercept\n",
    "    ax.plot(X, y_pred_sample, alpha=0.03, zorder=-99, color='silver')\n",
    "ax.plot(X, y_pred_sample, alpha=0.01, zorder=-99, color='lightgrey', label='Random sample fit, $k=2$')\n",
    "\n",
    "lg = ax.legend(loc='upper left', ncol=1)\n",
    "for i, lh in enumerate(lg.legendHandles):\n",
    "    lh.set_alpha(1)\n",
    "\n",
    "ax.grid(axis='y', linestyle='--', color='#acacac', alpha=0.5)\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.set_ylim(-12, 72)\n",
    "\n",
    "ax.set_xlabel('X', fontsize=13)\n",
    "ax.set_ylabel('y', fontsize=13)\n",
    "ax.text(0.99, 0.1, 'aegis4048.github.io', fontsize=10, ha='right', va='center',\n",
    "    transform=ax.transAxes, color='grey', alpha=0.5)\n",
    "ax.text(0.02, 0.62, 'True Solution: y = 92x + 30', fontsize=12, ha='left', va='center',\n",
    "    transform=ax.transAxes, alpha=1, color='r')\n",
    "\n",
    "def setbold(txt):\n",
    "    return ' '.join([r\"$\\bf{\" + item + \"}$\" for item in txt.split(' ')])\n",
    "\n",
    "bold_txt = setbold('TheilSen Regression')\n",
    "plain_txt = r', robustness of median to the outliers'\n",
    "fig.suptitle(bold_txt + plain_txt, verticalalignment='top', x=0, horizontalalignment='left', fontsize=13, y=0.96)\n",
    "yloc = 0.88\n",
    "ax.annotate('', xy=(0.01, yloc + 0.01), xycoords='figure fraction', xytext=(1.02, yloc + 0.01),\n",
    "            arrowprops=dict(arrowstyle=\"-\", color='k', lw=0.7))\n",
    "\n",
    "fig.tight_layout()\n",
    "            </code>\n",
    "        </pre>\n",
    "    </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6972ae07",
   "metadata": {},
   "source": [
    "<div id=\"Advanced: Why use median instead of mean for outliers?\" style=\"\"></div>\n",
    "\n",
    "#### 2.3.4. Theil-Sen code snippets\n",
    "\n",
    "For quick copy-paste, replace <code>X</code> and <code>y</code> with your own data. Make sure to reshape your <code>X</code> so that it is a 2D <code>numpy.ndarray</code> object with shape like <code>(13, 1)</code>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6e13b478",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TheilSen -----------------------------------\n",
      "\n",
      "Coefficients         : [73.65998569]\n",
      "Intercept            : 29.378865864952733\n",
      "\n",
      "------------------------------------------\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import TheilSenRegressor\n",
    "from sklearn import linear_model\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# sample data\n",
    "X = np.array([ 0.15, -0.34, 0.32, 0.43, -0.4, -0.04, -0.51, 0.3, 0.47, 0.12, 0.08, 0.04, -0.08, \n",
    "              -0.23, 0.08, -0.03, 0.03, 0.04, 0.01, 0.06, 0.03, 0., -0.04, -0.18, -0.19, -0.06, \n",
    "              -0.26, -0.16, 0.13, 0.09, 0.03, -0.03, 0.04, 0.14, -0.01, 0.4, -0.06, 0.15, 0.08, \n",
    "              0.05, -0.15, -0.09, -0.15, -0.11, -0.07, -0.19, -0.06, 0.17, 0.23, 0.18]).reshape(-1, 1)\n",
    "y = np.array([17.44, 25.46, 18.61, 26.07, 24.96, -1.22, 26.45, 26.5, 20.57, 3.08, 35.9 , 32.47, \n",
    "              20.84, 13.37, 42.44, 27.23, 35.65, 29.51, 31.28, 41.34, 32.19, 33.67, 25.64, 9.3, \n",
    "              14.63, 25.1, 4.69, 14.42, 47.53, 33.82, 32.2 , 24.81, 32.64, 45.11, 26.76, 68.01, \n",
    "              23.39, 43.49, 37.88, 36.01, 16.32, 19.77, 16.34, 19.57, 29.28, 16.62, 24.39, 43.77, \n",
    "              50.46, 47.09])\n",
    "\n",
    "# fit and predict: Huber \n",
    "TS = TheilSenRegressor(n_subsamples=2).fit(X, y)   # 2 by default for 2D linear regression. \n",
    "                                                   # Increasing it will decrease robustness (not recommended)\n",
    "y_pred_TS = TS.predict(X)\n",
    "\n",
    "# retrieve the fitted parameters\n",
    "coefs = TS.coef_\n",
    "intercept = TS.intercept_\n",
    "\n",
    "# TheilSen does not explicitly identify outliers from inliers.\n",
    "\n",
    "print(\"TheilSen -----------------------------------\\n\")\n",
    "print(\"Coefficients         :\", coefs)\n",
    "print(\"Intercept            :\", intercept)\n",
    "print(\"\\n------------------------------------------\")\n",
    "\n",
    "# fit and predict: Ordinary Least Squares\n",
    "ols = linear_model.LinearRegression().fit(X, y)\n",
    "y_pred_ols = ols.predict(X)\n",
    "\n",
    "# plot\n",
    "plt.scatter(X, y, s=100, c='green', label='Data')\n",
    "plt.title('TheilSen Regressor', fontsize=14)\n",
    "plt.plot(X, y_pred_ols, label='OLS')\n",
    "plt.plot(X, y_pred_TS, label='TheilSen')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ff02fcf",
   "metadata": {},
   "source": [
    "### 2.4. Summary\n",
    "\n",
    "<div class=\"highlights\" id=\"key1\">\n",
    "    <div class=\"highlights-title\">1. RANSAC regressor</div>\n",
    "    <div class=\"highlights-content\"><a href=\"#RANSAC regressor\" class=\"internal-link\">Quick Scroll:</a> Identifies and excludes outliers prior to fitting the final model. The outliers are defined as the points with residuals that exceed the Median Absolute Deviation (MAD). It repeats random sampling and fitting a test model until enough inliers are detected.</div>\n",
    "</div>\n",
    "\n",
    "<div class=\"highlights\" id=\"key2\">\n",
    "    <div class=\"highlights-title\">2. Huber regressor</div>\n",
    "    <div class=\"highlights-content\"><a href=\"#Huber regressor\" class=\"internal-link\">Quick Scroll:</a> Detects outliers that exceed a certain threshold, and selectively apply different loss functions for outliers vs. inliers.</div>\n",
    "</div>\n",
    "\n",
    "<div class=\"highlights\" id=\"key3\">\n",
    "    <div class=\"highlights-title\">3. Theil-Sen regressor</div>\n",
    "    <div class=\"highlights-content\"><a href=\"#Theil-Sen regressor\" class=\"internal-link\">Quick Scroll:</a> Fits OLS on all possible combinations of points (each of size 2 by default) and returns their spatial median slope and intercept as the final parameters.</div>\n",
    "</div>\n",
    "\n",
    "In the order of robustness:  RANSAC > Theil-Sen > Huber.\n",
    "\n",
    "RANSAC shows the best robustness because it identifies and excludes outliers prior to fitting the final model, whereas the other two attempts to *dampen* the effect of outliers instead of excluding them. When in doubt, use RANSAC. However, when reporting a result obtained with RANSAC, make sure to fix the random seed <code>RANSACRegressor(random_state=3)</code>, as explained <a href=\"#warning-RANSAC\" class=\"internal-link\">above</a>."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c32fb37",
   "metadata": {},
   "source": [
    "## 3. Extension to 3D+ multivariate linear regressions\n",
    "\n",
    "All three of the robust models are applicable to 3D or more n-dimensional regressions. Below is the simple code snippet for multivariate linear regression with 2 features and an intercept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "390e224d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OLS --------------------------------------\n",
      "Coefficients         : [43.07  1.73]\n",
      "Intercept            : 1.41\n",
      "\n",
      "RANSAC -----------------------------------\n",
      "Coefficients         : [42.62  1.11]\n",
      "Intercept            : 2.08\n",
      "\n",
      "Huber ------------------------------------\n",
      "Coefficients         : [43.13  2.07]\n",
      "Intercept            : 1.23\n",
      "\n",
      "TheilSen ---------------------------------\n",
      "Coefficients         : [42.15  2.95]\n",
      "Intercept            : 0.71\n",
      "------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.linear_model import LinearRegression, RANSACRegressor, HuberRegressor, TheilSenRegressor\n",
    "\n",
    "# generate sample data\n",
    "X, y, coef = datasets.make_regression(\n",
    "    n_samples=50, n_features=2, n_informative=1, noise=10, coef=True, random_state=0,\n",
    ")\n",
    "\n",
    "# Ordinary Least Squares\n",
    "ols = linear_model.LinearRegression().fit(X, y)\n",
    "y_pred_ols = ols.predict(X)\n",
    "\n",
    "# RANSAC\n",
    "ransac = RANSACRegressor(random_state=1).fit(X, y)\n",
    "y_pred_ransac = ransac.predict(X)\n",
    "\n",
    "# Huber\n",
    "huber = HuberRegressor().fit(X, y)\n",
    "y_pred_huber = huber.predict(X)\n",
    "\n",
    "# TheilSen\n",
    "TS = TheilSenRegressor().fit(X, y)\n",
    "y_pred_TS = TS.predict(X)\n",
    "\n",
    "print(\"OLS --------------------------------------\")\n",
    "print(\"Coefficients         :\", np.round(ols.coef_, 2))\n",
    "print(\"Intercept            :\", np.round(ols.intercept_, 2))\n",
    "print()\n",
    "print(\"RANSAC -----------------------------------\")\n",
    "print(\"Coefficients         :\", np.round(ransac.estimator_.coef_, 2))\n",
    "print(\"Intercept            :\", np.round(ransac.estimator_.intercept_, 2))\n",
    "print()\n",
    "print(\"Huber ------------------------------------\")\n",
    "print(\"Coefficients         :\", np.round(huber.coef_, 2))\n",
    "print(\"Intercept            :\", np.round(huber.intercept_, 2))\n",
    "print()\n",
    "print(\"TheilSen ---------------------------------\")\n",
    "print(\"Coefficients         :\", np.round(TS.coef_, 2))\n",
    "print(\"Intercept            :\", np.round(TS.intercept_, 2))\n",
    "print(\"------------------------------------------\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6e6879b",
   "metadata": {},
   "source": [
    "### 3.1. Visual demonstrations\n",
    "\n",
    "(*Note that for models with more than 3D features, including intercept, they can't be visualized but the idea of robust regression still extends beyond 3D Cartesian space.*)\n",
    "\n",
    "For demonstrations, a 3D sample data set of 200 points is generated from the true model: $y = 200 x_{1} + 28x_{2} + 300$. Random Gaussian noise is added to the $y$ values to emulate the randomness and variability present in real-life data. Currently, the dataset is free of outliers. Fitting an OLS regression results in <a href=\"#fig-20\" class=\"internal-link\">Figure 20</a>:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86d71783",
   "metadata": {},
   "source": [
    "<div class=\"row\" style=\"\" id=\"fig-20\">\n",
    "<div class=\"col\"><img src=\"jupyter_images/OLS_3d_free.png\"></div>\n",
    "<div class=\"col-12\"><p class=\"image-description\"><strong>Figure 20:</strong> This illustration presents the 3D linear model alongside the dataset within a 3D Cartesian space. When an OLS model is applied to this dataset, it closely approximates the true model parameters, particularly in scenarios devoid of outliers. The middle plot highlights how the data points are closely aligned with the 3D model fit (illustrated as a blue plane).</p></div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56f82c4e",
   "metadata": {},
   "source": [
    "<div class=\"solution_panel closed\">\n",
    "    <div class=\"solution_title\">\n",
    "        <p class=\"solution_title_string\">Source Code For Figure (20)</p>\n",
    "        <ul class=\"nav navbar-right panel_toolbox\">\n",
    "            <li><a class=\"collapse-link\"><i class=\"fa fa-chevron-down\"></i></a></li>\n",
    "        </ul>\n",
    "    <div class=\"clearfix\"></div>\n",
    "    </div>\n",
    "    <div class=\"solution_content\">\n",
    "        <pre>\n",
    "            <code class=\"language-python\">\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import linear_model\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn.linear_model import (LinearRegression, HuberRegressor,\n",
    "                              \tRANSACRegressor, TheilSenRegressor)\n",
    "\n",
    "###################################### Sample Data Generation ###################################\n",
    " \n",
    "np.random.seed(0)   # for reproducibility\n",
    "\n",
    "num_samples = 200  \n",
    "\n",
    "# Porosity\n",
    "x1_min, x1_max = 5, 15\n",
    "mean_x1 = (x1_min + x1_max) / 2  \n",
    "std_dev_x1 = (x1_max - x1_min) / 4 \n",
    "x1 = np.random.normal(mean_x1, std_dev_x1, num_samples)\n",
    "\n",
    "# Brittleness\n",
    "x2_min, x2_max = 20, 85\n",
    "mean_x2 = (x2_min + x2_max) / 2 \n",
    "std_dev_x2 = (x2_max - x2_min) / 4  \n",
    "x2 = np.random.normal(mean_x2, std_dev_x2, num_samples)  \n",
    "\n",
    "# Reshape X for comptibility with regression models\n",
    "X = np.vstack((x1, x2)).T\n",
    "\n",
    "# True model\n",
    "Y = 200* x1 + 28 * x2 + 300\n",
    "\n",
    "# Add Gaussian noise to Y\n",
    "noise = np.random.normal(0, 200, num_samples)  # Mean = 0, Standard deviation = 50\n",
    "Y = Y + noise\n",
    "\n",
    "######################## Prepare model data point for visualization ###############################\n",
    "\n",
    "x = X[:, 0]\n",
    "y = X[:, 1]\n",
    "z = Y\n",
    "\n",
    "x_pred = np.linspace(0, 25, 30)   # range of porosity values\n",
    "y_pred = np.linspace(0, 100, 30)  # range of brittleness values\n",
    "xx_pred, yy_pred = np.meshgrid(x_pred, y_pred)\n",
    "model_viz = np.array([xx_pred.flatten(), yy_pred.flatten()]).T\n",
    "\n",
    "################################################ Train #############################################\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(X, Y)\n",
    "predicted = model.predict(model_viz)\n",
    "\n",
    "############################################## Plot ################################################\n",
    "\n",
    "plt.style.use('default')\n",
    "\n",
    "fig = plt.figure(figsize=(12, 4))\n",
    "\n",
    "ax1 = fig.add_subplot(131, projection='3d')\n",
    "ax2 = fig.add_subplot(132, projection='3d')\n",
    "ax3 = fig.add_subplot(133, projection='3d')\n",
    "\n",
    "axes = [ax1, ax2, ax3]\n",
    "\n",
    "for ax in axes:\n",
    "    ax.plot(x, y, z, color='k', zorder=15, linestyle='none', marker='o', alpha=0.5)\n",
    "    ax.scatter(xx_pred.flatten(), yy_pred.flatten(), predicted, facecolor=(0,0,0,0), s=20, edgecolor='#70b3f0')\n",
    "    ax.set_xlabel(r'$X_{1}$', fontsize=12)\n",
    "    ax.set_ylabel(r'$X_{2}$', fontsize=12)\n",
    "    ax.set_zlabel('Y', fontsize=12)\n",
    "    ax.locator_params(nbins=4, axis='x')\n",
    "    ax.locator_params(nbins=5, axis='x')\n",
    "    ax.set_xlim(0, 25)\n",
    "    ax.set_ylim(100, 0)\n",
    "    ax.set_zlim(0, 8000)\n",
    "    \n",
    "\n",
    "ax1.text2D(0.2, 0.32, 'aegis4048.github.io', fontsize=13, ha='center', va='center',\n",
    "           transform=ax1.transAxes, color='grey', alpha=0.5)\n",
    "ax2.text2D(0.5, 0.32, 'aegis4048.github.io', fontsize=13, ha='center', va='center',\n",
    "           transform=ax2.transAxes, color='grey', alpha=0.5)\n",
    "ax3.text2D(0.85, 0.85, 'aegis4048.github.io', fontsize=13, ha='center', va='center',\n",
    "           transform=ax3.transAxes, color='grey', alpha=0.5)\n",
    "\n",
    "ax1.view_init(elev=30, azim=50)\n",
    "ax2.view_init(elev=2, azim=60)\n",
    "ax3.view_init(elev=60, azim=165)\n",
    "\n",
    "fig.tight_layout()\n",
    "\n",
    "def setbold(txt):\n",
    "    return ' '.join([r\"$\\bf{\" + item + \"}$\" for item in txt.split(' ')])\n",
    "\n",
    "bold_txt = setbold(' OLS 3D Linear Regression (without outliers), ')\n",
    "plain_txt = r'fitted model:   $y = $ %.1f$x_{1}$ + %.1f$x_{2}$ + %.1f' % (model.coef_[0], model.coef_[1], model.intercept_)\n",
    "fig.suptitle(bold_txt + plain_txt, verticalalignment='top', x=0, horizontalalignment='left', fontsize=16, y=1.03)\n",
    "yloc = 0.95\n",
    "ax.annotate('', xy=(0.01, yloc + 0.01), xycoords='figure fraction', xytext=(1.02, yloc + 0.01),\n",
    "            arrowprops=dict(arrowstyle=\"-\", color='k', lw=0.7))\n",
    "\n",
    "fig.text(0.48, 0.8, 'true model:   $y = $ 200$x_{1}$ + 28$x_{2}$ + 300', ha='center', va='center', fontsize=16, color='grey');\n",
    "            </code>\n",
    "        </pre>\n",
    "    </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a7a15d9",
   "metadata": {},
   "source": [
    "Now we will introduce 20 outlier points, constituting 10% of the original dataset, and reapply the OLS regression, as depicted in <a href=\"#fig-21\" class=\"internal-link\">Figure 21</a>. It is noticeable that the OLS parameters fitted in this scenario exhibit a more significant deviation from the population parameters compared to the initial fit that lacked outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9911af1d",
   "metadata": {},
   "source": [
    "<div class=\"row\" style=\"\" id=\"fig-21\">\n",
    "<div class=\"col\"><img src=\"jupyter_images/OLS_3d.png\"></div>\n",
    "<div class=\"col-12\"><p class=\"image-description\"><strong>Figure 21:</strong> This figure highlights outlier points in red. The introduction of these outliers has caused a noticeable shift in the fitted blue plane, which is now skewed towards these outliers, affecting the accuracy of the model fit. The left plot distinctly shows the misalignment of the model plane with the main dataset. For a comparative analysis, refer to the visual and numerical parameters in <a href=\"#fig-20\" class=\"internal-link\">Figure 20</a>.</p></div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b39b1270",
   "metadata": {},
   "source": [
    "<div class=\"solution_panel closed\">\n",
    "    <div class=\"solution_title\">\n",
    "        <p class=\"solution_title_string\">Source Code For Figure (21)</p>\n",
    "        <ul class=\"nav navbar-right panel_toolbox\">\n",
    "            <li><a class=\"collapse-link\"><i class=\"fa fa-chevron-down\"></i></a></li>\n",
    "        </ul>\n",
    "    <div class=\"clearfix\"></div>\n",
    "    </div>\n",
    "    <div class=\"solution_content\">\n",
    "        <pre>\n",
    "            <code class=\"language-python\">\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import linear_model\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "###################################### Sample Data Generation ###################################\n",
    " \n",
    "np.random.seed(0)   # for reproducibility\n",
    "\n",
    "num_samples = 200  \n",
    "\n",
    "# Porosity\n",
    "x1_min, x1_max = 5, 15\n",
    "mean_x1 = (x1_min + x1_max) / 2  \n",
    "std_dev_x1 = (x1_max - x1_min) / 4 \n",
    "x1 = np.random.normal(mean_x1, std_dev_x1, num_samples)\n",
    "\n",
    "# Brittleness\n",
    "x2_min, x2_max = 20, 85\n",
    "mean_x2 = (x2_min + x2_max) / 2 \n",
    "std_dev_x2 = (x2_max - x2_min) / 4  \n",
    "x2 = np.random.normal(mean_x2, std_dev_x2, num_samples)  \n",
    "\n",
    "# Reshape X for comptibility with regression models\n",
    "X = np.vstack((x1, x2)).T\n",
    "\n",
    "# True model\n",
    "Y = 200* x1 + 28 * x2 + 300\n",
    "\n",
    "# Add Gaussian noise to Y\n",
    "noise = np.random.normal(0, 200, num_samples)  # Mean = 0, Standard deviation = 50\n",
    "Y_noisy = Y + noise\n",
    "\n",
    "# Add outliers\n",
    "num_outliers = int(0.1 * num_samples)                    # define 5% of data to be outliers\n",
    "outlier_x1 = np.random.uniform(10, 12, num_outliers)     # outlier between range 10 ~ 12\n",
    "outlier_x2 = np.random.uniform(0, 5, num_outliers)       # outlier between range 0 ~ 5\n",
    "outlier_Y = np.random.uniform(6500, 7000, num_outliers)  # outlier between range 6500 ~ 7000\n",
    "X_outliers = np.vstack((outlier_x1, outlier_x2)).T\n",
    "\n",
    "# Append outliers to the original data\n",
    "X = np.vstack((X, X_outliers))\n",
    "Y = np.append(Y_noisy, outlier_Y)\n",
    "\n",
    "######################## Prepare model data point for visualization ###############################\n",
    "\n",
    "x = X[:, 0]\n",
    "y = X[:, 1]\n",
    "z = Y\n",
    "\n",
    "x_pred = np.linspace(0, 25, 30)   # range of porosity values\n",
    "y_pred = np.linspace(0, 100, 30)  # range of brittleness values\n",
    "xx_pred, yy_pred = np.meshgrid(x_pred, y_pred)\n",
    "model_viz = np.array([xx_pred.flatten(), yy_pred.flatten()]).T\n",
    "\n",
    "################################################ Train #############################################\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(X, Y)\n",
    "predicted = model.predict(model_viz)\n",
    "\n",
    "############################################## Plot ################################################\n",
    "\n",
    "plt.style.use('default')\n",
    "\n",
    "fig = plt.figure(figsize=(12, 4))\n",
    "\n",
    "ax1 = fig.add_subplot(131, projection='3d')\n",
    "ax2 = fig.add_subplot(132, projection='3d')\n",
    "ax3 = fig.add_subplot(133, projection='3d')\n",
    "\n",
    "axes = [ax1, ax2, ax3]\n",
    "\n",
    "for ax in axes:\n",
    "    ax.plot(x, y, z, color='k', zorder=15, linestyle='none', marker='o', alpha=0.5)\n",
    "    ax.plot(X_outliers[:, 0], X_outliers[:, 1], outlier_Y, color='r', zorder=15, linestyle='none', marker='x', alpha=1, markersize=10)\n",
    "    ax.scatter(xx_pred.flatten(), yy_pred.flatten(), predicted, facecolor=(0,0,0,0), s=20, edgecolor='#70b3f0')\n",
    "    ax.set_xlabel(r'$X_{1}$', fontsize=12)\n",
    "    ax.set_ylabel(r'$X_{2}$', fontsize=12)\n",
    "    ax.set_zlabel('Y', fontsize=12)\n",
    "    ax.locator_params(nbins=4, axis='x')\n",
    "    ax.locator_params(nbins=5, axis='x')\n",
    "    ax.set_xlim(0, 25)\n",
    "    ax.set_ylim(100, 0)\n",
    "    ax.set_zlim(0, 8000)\n",
    "    \n",
    "\n",
    "ax1.text2D(0.2, 0.32, 'aegis4048.github.io', fontsize=13, ha='center', va='center',\n",
    "           transform=ax1.transAxes, color='grey', alpha=0.5)\n",
    "ax2.text2D(0.5, 0.32, 'aegis4048.github.io', fontsize=13, ha='center', va='center',\n",
    "           transform=ax2.transAxes, color='grey', alpha=0.5)\n",
    "ax3.text2D(0.85, 0.85, 'aegis4048.github.io', fontsize=13, ha='center', va='center',\n",
    "           transform=ax3.transAxes, color='grey', alpha=0.5)\n",
    "\n",
    "ax1.view_init(elev=30, azim=50)\n",
    "ax2.view_init(elev=2, azim=60)\n",
    "ax3.view_init(elev=60, azim=165)\n",
    "\n",
    "fig.tight_layout()\n",
    "\n",
    "def setbold(txt):\n",
    "    return ' '.join([r\"$\\bf{\" + item + \"}$\" for item in txt.split(' ')])\n",
    "\n",
    "bold_txt = setbold(' OLS 3D Linear Regression (with outliers), ')\n",
    "plain_txt = r'fitted model:   $y = $ %.1f$x_{1}$ + %.1f$x_{2}$ + %.1f' % (model.coef_[0], model.coef_[1], model.intercept_)\n",
    "fig.suptitle(bold_txt + plain_txt, verticalalignment='top', x=0, horizontalalignment='left', fontsize=16, y=1.03)\n",
    "yloc = 0.95\n",
    "ax.annotate('', xy=(0.01, yloc + 0.01), xycoords='figure fraction', xytext=(1.02, yloc + 0.01),\n",
    "            arrowprops=dict(arrowstyle=\"-\", color='k', lw=0.7))\n",
    "\n",
    "fig.text(0.48, 0.8, 'true model:   $y = $ 200$x_{1}$ + 28$x_{2}$ + 300', ha='center', va='center', fontsize=16, color='grey');\n",
    "            </code>\n",
    "        </pre>\n",
    "    </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4481db57",
   "metadata": {},
   "source": [
    "The below three figures (<a href=\"#fig-22\" class=\"internal-link\">Figure 22</a>, <a href=\"#fig-23\" class=\"internal-link\">Figure 23</a> and <a href=\"#fig-24\" class=\"internal-link\">Figure 24</a>) presents 3D model fits and visualizations for the three robust models: RANSAC, Huber, and TheilSen.\n",
    "\n",
    "<div class=\"row\" style=\"\" id=\"fig-22\">\n",
    "<div class=\"col\"><img src=\"jupyter_images/ransac_3d.png\"></div>\n",
    "<div class=\"col-12\"><p class=\"image-description\"><strong>Figure 22:</strong> RANSAC robust linear regression on 3D dataset with outliers.</p></div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ef85ff1",
   "metadata": {},
   "source": [
    "<div class=\"solution_panel closed\">\n",
    "    <div class=\"solution_title\">\n",
    "        <p class=\"solution_title_string\">Source Code For Figure (22)</p>\n",
    "        <ul class=\"nav navbar-right panel_toolbox\">\n",
    "            <li><a class=\"collapse-link\"><i class=\"fa fa-chevron-down\"></i></a></li>\n",
    "        </ul>\n",
    "    <div class=\"clearfix\"></div>\n",
    "    </div>\n",
    "    <div class=\"solution_content\">\n",
    "        <pre>\n",
    "            <code class=\"language-python\">\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import linear_model\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn.linear_model import RANSACRegressor\n",
    "\n",
    "###################################### Sample Data Generation ###################################\n",
    " \n",
    "np.random.seed(0)   # for reproducibility\n",
    "\n",
    "num_samples = 200  \n",
    "\n",
    "# Porosity\n",
    "x1_min, x1_max = 5, 15\n",
    "mean_x1 = (x1_min + x1_max) / 2  \n",
    "std_dev_x1 = (x1_max - x1_min) / 4 \n",
    "x1 = np.random.normal(mean_x1, std_dev_x1, num_samples)\n",
    "\n",
    "# Brittleness\n",
    "x2_min, x2_max = 20, 85\n",
    "mean_x2 = (x2_min + x2_max) / 2 \n",
    "std_dev_x2 = (x2_max - x2_min) / 4  \n",
    "x2 = np.random.normal(mean_x2, std_dev_x2, num_samples)  \n",
    "\n",
    "# Reshape X for comptibility with regression models\n",
    "X = np.vstack((x1, x2)).T\n",
    "\n",
    "# True model\n",
    "Y = 200* x1 + 28 * x2 + 300\n",
    "\n",
    "# Add Gaussian noise to Y\n",
    "noise = np.random.normal(0, 200, num_samples)  # Mean = 0, Standard deviation = 50\n",
    "Y_noisy = Y + noise\n",
    "\n",
    "# Add outliers\n",
    "num_outliers = int(0.1 * num_samples)                    # define 5% of data to be outliers\n",
    "outlier_x1 = np.random.uniform(10, 12, num_outliers)     # outlier between range 10 ~ 12\n",
    "outlier_x2 = np.random.uniform(0, 5, num_outliers)       # outlier between range 0 ~ 5\n",
    "outlier_Y = np.random.uniform(6500, 7000, num_outliers)  # outlier between range 6500 ~ 7000\n",
    "X_outliers = np.vstack((outlier_x1, outlier_x2)).T\n",
    "\n",
    "# Append outliers to the original data\n",
    "X = np.vstack((X, X_outliers))\n",
    "Y = np.append(Y_noisy, outlier_Y)\n",
    "\n",
    "######################## Prepare model data point for visualization ###############################\n",
    "\n",
    "x = X[:, 0]\n",
    "y = X[:, 1]\n",
    "z = Y\n",
    "\n",
    "x_pred = np.linspace(0, 25, 30)   # range of porosity values\n",
    "y_pred = np.linspace(0, 100, 30)  # range of brittleness values\n",
    "xx_pred, yy_pred = np.meshgrid(x_pred, y_pred)\n",
    "model_viz = np.array([xx_pred.flatten(), yy_pred.flatten()]).T\n",
    "\n",
    "################################################ Train #############################################\n",
    "\n",
    "model = RANSACRegressor()\n",
    "model.fit(X, Y)\n",
    "predicted = model.predict(model_viz)\n",
    "\n",
    "############################################## Plot ################################################\n",
    "\n",
    "plt.style.use('default')\n",
    "\n",
    "fig = plt.figure(figsize=(12, 4))\n",
    "\n",
    "ax1 = fig.add_subplot(131, projection='3d')\n",
    "ax2 = fig.add_subplot(132, projection='3d')\n",
    "ax3 = fig.add_subplot(133, projection='3d')\n",
    "\n",
    "axes = [ax1, ax2, ax3]\n",
    "\n",
    "for ax in axes:\n",
    "    ax.plot(x, y, z, color='k', zorder=15, linestyle='none', marker='o', alpha=0.5)\n",
    "    ax.plot(X_outliers[:, 0], X_outliers[:, 1], outlier_Y, color='r', zorder=15, linestyle='none', marker='x', alpha=1, markersize=10)\n",
    "    ax.scatter(xx_pred.flatten(), yy_pred.flatten(), predicted, facecolor=(0,0,0,0), s=20, edgecolor='#70b3f0')\n",
    "    ax.set_xlabel(r'$X_{1}$', fontsize=12)\n",
    "    ax.set_ylabel(r'$X_{2}$', fontsize=12)\n",
    "    ax.set_zlabel('Y', fontsize=12)\n",
    "    ax.locator_params(nbins=4, axis='x')\n",
    "    ax.locator_params(nbins=5, axis='x')\n",
    "    ax.set_xlim(0, 25)\n",
    "    ax.set_ylim(100, 0)\n",
    "    ax.set_zlim(0, 8000)\n",
    "    \n",
    "\n",
    "ax1.text2D(0.2, 0.32, 'aegis4048.github.io', fontsize=13, ha='center', va='center',\n",
    "           transform=ax1.transAxes, color='grey', alpha=0.5)\n",
    "ax2.text2D(0.5, 0.32, 'aegis4048.github.io', fontsize=13, ha='center', va='center',\n",
    "           transform=ax2.transAxes, color='grey', alpha=0.5)\n",
    "ax3.text2D(0.85, 0.85, 'aegis4048.github.io', fontsize=13, ha='center', va='center',\n",
    "           transform=ax3.transAxes, color='grey', alpha=0.5)\n",
    "\n",
    "ax1.view_init(elev=30, azim=50)\n",
    "ax2.view_init(elev=2, azim=60)\n",
    "ax3.view_init(elev=60, azim=165)\n",
    "\n",
    "fig.tight_layout()\n",
    "\n",
    "def setbold(txt):\n",
    "    return ' '.join([r\"$\\bf{\" + item + \"}$\" for item in txt.split(' ')])\n",
    "\n",
    "bold_txt = setbold(' RANSAC 3D Linear Regression, ')\n",
    "plain_txt = r'fitted model:   $y = $ %.1f$x_{1}$ + %.1f$x_{2}$ + %.1f' % (\n",
    "    model.estimator_.coef_[0], model.estimator_.coef_[1], model.estimator_.intercept_\n",
    ")\n",
    "fig.suptitle(bold_txt + plain_txt, verticalalignment='top', x=0, horizontalalignment='left', fontsize=16, y=1.03)\n",
    "yloc = 0.95\n",
    "ax.annotate('', xy=(0.01, yloc + 0.01), xycoords='figure fraction', xytext=(1.02, yloc + 0.01),\n",
    "            arrowprops=dict(arrowstyle=\"-\", color='k', lw=0.7))\n",
    "\n",
    "fig.text(0.48, 0.8, 'true model:   $y = $ 200$x_{1}$ + 28$x_{2}$ + 300', ha='center', va='center', fontsize=16, color='grey');\n",
    "            </code>\n",
    "        </pre>\n",
    "    </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ef2ea0a",
   "metadata": {},
   "source": [
    "<div class=\"row\" style=\"\" id=\"fig-23\">\n",
    "<div class=\"col\"><img src=\"jupyter_images/huber_3d.png\"></div>\n",
    "<div class=\"col-12\"><p class=\"image-description\"><strong>Figure 23:</strong> Huber robust linear regression on 3D dataset with outliers.</p></div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ff8fe90",
   "metadata": {},
   "source": [
    "<div class=\"solution_panel closed\">\n",
    "    <div class=\"solution_title\">\n",
    "        <p class=\"solution_title_string\">Source Code For Figure (23)</p>\n",
    "        <ul class=\"nav navbar-right panel_toolbox\">\n",
    "            <li><a class=\"collapse-link\"><i class=\"fa fa-chevron-down\"></i></a></li>\n",
    "        </ul>\n",
    "    <div class=\"clearfix\"></div>\n",
    "    </div>\n",
    "    <div class=\"solution_content\">\n",
    "        <pre>\n",
    "            <code class=\"language-python\">\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import linear_model\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn.linear_model import HuberRegressor\n",
    "\n",
    "###################################### Sample Data Generation ###################################\n",
    " \n",
    "np.random.seed(0)   # for reproducibility\n",
    "\n",
    "num_samples = 200  \n",
    "\n",
    "# Porosity\n",
    "x1_min, x1_max = 5, 15\n",
    "mean_x1 = (x1_min + x1_max) / 2  \n",
    "std_dev_x1 = (x1_max - x1_min) / 4 \n",
    "x1 = np.random.normal(mean_x1, std_dev_x1, num_samples)\n",
    "\n",
    "# Brittleness\n",
    "x2_min, x2_max = 20, 85\n",
    "mean_x2 = (x2_min + x2_max) / 2 \n",
    "std_dev_x2 = (x2_max - x2_min) / 4  \n",
    "x2 = np.random.normal(mean_x2, std_dev_x2, num_samples)  \n",
    "\n",
    "# Reshape X for comptibility with regression models\n",
    "X = np.vstack((x1, x2)).T\n",
    "\n",
    "# True model\n",
    "Y = 200* x1 + 28 * x2 + 300\n",
    "\n",
    "# Add Gaussian noise to Y\n",
    "noise = np.random.normal(0, 200, num_samples)  # Mean = 0, Standard deviation = 50\n",
    "Y_noisy = Y + noise\n",
    "\n",
    "# Add outliers\n",
    "num_outliers = int(0.1 * num_samples)                    # define 5% of data to be outliers\n",
    "outlier_x1 = np.random.uniform(10, 12, num_outliers)     # outlier between range 10 ~ 12\n",
    "outlier_x2 = np.random.uniform(0, 5, num_outliers)       # outlier between range 0 ~ 5\n",
    "outlier_Y = np.random.uniform(6500, 7000, num_outliers)  # outlier between range 6500 ~ 7000\n",
    "X_outliers = np.vstack((outlier_x1, outlier_x2)).T\n",
    "\n",
    "# Append outliers to the original data\n",
    "X = np.vstack((X, X_outliers))\n",
    "Y = np.append(Y_noisy, outlier_Y)\n",
    "\n",
    "######################## Prepare model data point for visualization ###############################\n",
    "\n",
    "x = X[:, 0]\n",
    "y = X[:, 1]\n",
    "z = Y\n",
    "\n",
    "x_pred = np.linspace(0, 25, 30)   # range of porosity values\n",
    "y_pred = np.linspace(0, 100, 30)  # range of brittleness values\n",
    "xx_pred, yy_pred = np.meshgrid(x_pred, y_pred)\n",
    "model_viz = np.array([xx_pred.flatten(), yy_pred.flatten()]).T\n",
    "\n",
    "################################################ Train #############################################\n",
    "\n",
    "model = HuberRegressor()\n",
    "model.fit(X, Y)\n",
    "predicted = model.predict(model_viz)\n",
    "\n",
    "############################################## Plot ################################################\n",
    "\n",
    "plt.style.use('default')\n",
    "\n",
    "fig = plt.figure(figsize=(12, 4))\n",
    "\n",
    "ax1 = fig.add_subplot(131, projection='3d')\n",
    "ax2 = fig.add_subplot(132, projection='3d')\n",
    "ax3 = fig.add_subplot(133, projection='3d')\n",
    "\n",
    "axes = [ax1, ax2, ax3]\n",
    "\n",
    "for ax in axes:\n",
    "    ax.plot(x, y, z, color='k', zorder=15, linestyle='none', marker='o', alpha=0.5)\n",
    "    ax.plot(X_outliers[:, 0], X_outliers[:, 1], outlier_Y, color='r', zorder=15, linestyle='none', marker='x', alpha=1, markersize=10)\n",
    "    ax.scatter(xx_pred.flatten(), yy_pred.flatten(), predicted, facecolor=(0,0,0,0), s=20, edgecolor='#70b3f0')\n",
    "    ax.set_xlabel(r'$X_{1}$', fontsize=12)\n",
    "    ax.set_ylabel(r'$X_{2}$', fontsize=12)\n",
    "    ax.set_zlabel('Y', fontsize=12)\n",
    "    ax.locator_params(nbins=4, axis='x')\n",
    "    ax.locator_params(nbins=5, axis='x')\n",
    "    ax.set_xlim(0, 25)\n",
    "    ax.set_ylim(100, 0)\n",
    "    ax.set_zlim(0, 8000)\n",
    "    \n",
    "\n",
    "ax1.text2D(0.2, 0.32, 'aegis4048.github.io', fontsize=13, ha='center', va='center',\n",
    "           transform=ax1.transAxes, color='grey', alpha=0.5)\n",
    "ax2.text2D(0.5, 0.32, 'aegis4048.github.io', fontsize=13, ha='center', va='center',\n",
    "           transform=ax2.transAxes, color='grey', alpha=0.5)\n",
    "ax3.text2D(0.85, 0.85, 'aegis4048.github.io', fontsize=13, ha='center', va='center',\n",
    "           transform=ax3.transAxes, color='grey', alpha=0.5)\n",
    "\n",
    "ax1.view_init(elev=30, azim=50)\n",
    "ax2.view_init(elev=2, azim=60)\n",
    "ax3.view_init(elev=60, azim=165)\n",
    "\n",
    "fig.tight_layout()\n",
    "\n",
    "def setbold(txt):\n",
    "    return ' '.join([r\"$\\bf{\" + item + \"}$\" for item in txt.split(' ')])\n",
    "\n",
    "bold_txt = setbold(' Huber 3D Linear Regression, ')\n",
    "plain_txt = r'fitted model:   $y = $ %.1f$x_{1}$ + %.1f$x_{2}$ + %.1f' % (model.coef_[0], model.coef_[1], model.intercept_)\n",
    "fig.suptitle(bold_txt + plain_txt, verticalalignment='top', x=0, horizontalalignment='left', fontsize=16, y=1.03)\n",
    "yloc = 0.95\n",
    "ax.annotate('', xy=(0.01, yloc + 0.01), xycoords='figure fraction', xytext=(1.02, yloc + 0.01),\n",
    "            arrowprops=dict(arrowstyle=\"-\", color='k', lw=0.7))\n",
    "\n",
    "fig.text(0.48, 0.8, 'true model:   $y = $ 200$x_{1}$ + 28$x_{2}$ + 300', ha='center', va='center', fontsize=16, color='grey');            </code>\n",
    "        </pre>\n",
    "    </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbfeeb94",
   "metadata": {},
   "source": [
    "<div class=\"row\" style=\"\" id=\"fig-24\">\n",
    "<div class=\"col\"><img src=\"jupyter_images/TS_3d.png\"></div>\n",
    "<div class=\"col-12\"><p class=\"image-description\"><strong>Figure 24:</strong> TheilSen robust linear regression on 3D dataset with outliers.</p></div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5a225e9",
   "metadata": {},
   "source": [
    "<div class=\"solution_panel closed\">\n",
    "    <div class=\"solution_title\">\n",
    "        <p class=\"solution_title_string\">Source Code For Figure (24)</p>\n",
    "        <ul class=\"nav navbar-right panel_toolbox\">\n",
    "            <li><a class=\"collapse-link\"><i class=\"fa fa-chevron-down\"></i></a></li>\n",
    "        </ul>\n",
    "    <div class=\"clearfix\"></div>\n",
    "    </div>\n",
    "    <div class=\"solution_content\">\n",
    "        <pre>\n",
    "            <code class=\"language-python\">\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import linear_model\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn.linear_model import TheilSenRegressor\n",
    "\n",
    "###################################### Sample Data Generation ###################################\n",
    " \n",
    "np.random.seed(0)   # for reproducibility\n",
    "\n",
    "num_samples = 200  \n",
    "\n",
    "# Porosity\n",
    "x1_min, x1_max = 5, 15\n",
    "mean_x1 = (x1_min + x1_max) / 2  \n",
    "std_dev_x1 = (x1_max - x1_min) / 4 \n",
    "x1 = np.random.normal(mean_x1, std_dev_x1, num_samples)\n",
    "\n",
    "# Brittleness\n",
    "x2_min, x2_max = 20, 85\n",
    "mean_x2 = (x2_min + x2_max) / 2 \n",
    "std_dev_x2 = (x2_max - x2_min) / 4  \n",
    "x2 = np.random.normal(mean_x2, std_dev_x2, num_samples)  \n",
    "\n",
    "# Reshape X for comptibility with regression models\n",
    "X = np.vstack((x1, x2)).T\n",
    "\n",
    "# True model\n",
    "Y = 200* x1 + 28 * x2 + 300\n",
    "\n",
    "# Add Gaussian noise to Y\n",
    "noise = np.random.normal(0, 200, num_samples)  # Mean = 0, Standard deviation = 50\n",
    "Y_noisy = Y + noise\n",
    "\n",
    "# Add outliers\n",
    "num_outliers = int(0.1 * num_samples)                    # define 5% of data to be outliers\n",
    "outlier_x1 = np.random.uniform(10, 12, num_outliers)     # outlier between range 10 ~ 12\n",
    "outlier_x2 = np.random.uniform(0, 5, num_outliers)       # outlier between range 0 ~ 5\n",
    "outlier_Y = np.random.uniform(6500, 7000, num_outliers)  # outlier between range 6500 ~ 7000\n",
    "X_outliers = np.vstack((outlier_x1, outlier_x2)).T\n",
    "\n",
    "# Append outliers to the original data\n",
    "X = np.vstack((X, X_outliers))\n",
    "Y = np.append(Y_noisy, outlier_Y)\n",
    "\n",
    "######################## Prepare model data point for visualization ###############################\n",
    "\n",
    "x = X[:, 0]\n",
    "y = X[:, 1]\n",
    "z = Y\n",
    "\n",
    "x_pred = np.linspace(0, 25, 30)   # range of porosity values\n",
    "y_pred = np.linspace(0, 100, 30)  # range of brittleness values\n",
    "xx_pred, yy_pred = np.meshgrid(x_pred, y_pred)\n",
    "model_viz = np.array([xx_pred.flatten(), yy_pred.flatten()]).T\n",
    "\n",
    "################################################ Train #############################################\n",
    "\n",
    "model = TheilSenRegressor()\n",
    "model.fit(X, Y)\n",
    "predicted = model.predict(model_viz)\n",
    "\n",
    "############################################## Plot ################################################\n",
    "\n",
    "plt.style.use('default')\n",
    "\n",
    "fig = plt.figure(figsize=(12, 4))\n",
    "\n",
    "ax1 = fig.add_subplot(131, projection='3d')\n",
    "ax2 = fig.add_subplot(132, projection='3d')\n",
    "ax3 = fig.add_subplot(133, projection='3d')\n",
    "\n",
    "axes = [ax1, ax2, ax3]\n",
    "\n",
    "for ax in axes:\n",
    "    ax.plot(x, y, z, color='k', zorder=15, linestyle='none', marker='o', alpha=0.5)\n",
    "    ax.plot(X_outliers[:, 0], X_outliers[:, 1], outlier_Y, color='r', zorder=15, linestyle='none', marker='x', alpha=1, markersize=10)\n",
    "    ax.scatter(xx_pred.flatten(), yy_pred.flatten(), predicted, facecolor=(0,0,0,0), s=20, edgecolor='#70b3f0')\n",
    "    ax.set_xlabel(r'$X_{1}$', fontsize=12)\n",
    "    ax.set_ylabel(r'$X_{2}$', fontsize=12)\n",
    "    ax.set_zlabel('Y', fontsize=12)\n",
    "    ax.locator_params(nbins=4, axis='x')\n",
    "    ax.locator_params(nbins=5, axis='x')\n",
    "    ax.set_xlim(0, 25)\n",
    "    ax.set_ylim(100, 0)\n",
    "    ax.set_zlim(0, 8000)\n",
    "    \n",
    "\n",
    "ax1.text2D(0.2, 0.32, 'aegis4048.github.io', fontsize=13, ha='center', va='center',\n",
    "           transform=ax1.transAxes, color='grey', alpha=0.5)\n",
    "ax2.text2D(0.5, 0.32, 'aegis4048.github.io', fontsize=13, ha='center', va='center',\n",
    "           transform=ax2.transAxes, color='grey', alpha=0.5)\n",
    "ax3.text2D(0.85, 0.85, 'aegis4048.github.io', fontsize=13, ha='center', va='center',\n",
    "           transform=ax3.transAxes, color='grey', alpha=0.5)\n",
    "\n",
    "ax1.view_init(elev=30, azim=50)\n",
    "ax2.view_init(elev=2, azim=60)\n",
    "ax3.view_init(elev=60, azim=165)\n",
    "\n",
    "fig.tight_layout()\n",
    "\n",
    "def setbold(txt):\n",
    "    return ' '.join([r\"$\\bf{\" + item + \"}$\" for item in txt.split(' ')])\n",
    "\n",
    "bold_txt = setbold(' TheilSen 3D Linear Regression, ')\n",
    "plain_txt = r'fitted model:   $y = $ %.1f$x_{1}$ + %.1f$x_{2}$ + %.1f' % (model.coef_[0], model.coef_[1], model.intercept_)\n",
    "fig.suptitle(bold_txt + plain_txt, verticalalignment='top', x=0, horizontalalignment='left', fontsize=16, y=1.03)\n",
    "yloc = 0.95\n",
    "ax.annotate('', xy=(0.01, yloc + 0.01), xycoords='figure fraction', xytext=(1.02, yloc + 0.01),\n",
    "            arrowprops=dict(arrowstyle=\"-\", color='k', lw=0.7))\n",
    "\n",
    "fig.text(0.48, 0.8, 'true model:   $y = $ 200$x_{1}$ + 28$x_{2}$ + 300', ha='center', va='center', fontsize=16, color='grey');\n",
    "            </code>\n",
    "        </pre>\n",
    "    </div>\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
