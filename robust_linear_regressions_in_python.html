<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">

            <meta name="google-site-verification" content="ZsWFnpirKDgtbmwb1YRymDnSfvnUrpzCbf6LD1F_4TY" />

            <meta name="msvalidate.01" content="8FF1B025212A47B5B27CC47163A042F0" />

            <meta name="author" content="ERIC KIM" />


            <meta name="description" content="        Illustrative guidance to outlier-robust linear regression modeling
" />

                <meta property="og:type" content="article" />
            <meta name="twitter:card" content="summary"/>

        <meta name="keywords" content="linear regression, robust linear regression, outliers, visualization, machine learning, statistics, TheilSen regression, RANSAC regression, Huber regression, OLS, L1 norm, L2 norm, L2 norm squared, spatial median, mean, median, central tendency, robustness, euclidean distance, absolute loss, squared loss, least-squares, Machine Learning, "/>

        <link rel="canonical" href="https://aegis4048.github.io/robust_linear_regressions_in_python">
    <meta property="og:title" content="Robust Linear Regressions In Python | Pythonic Excursions"/>
    <meta property="og:url" content="https://aegis4048.github.io/robust_linear_regressions_in_python" />
    <meta property="og:description" content="Illustrative guidance to outlier-robust linear regression modeling" />
    <meta property="og:site_name" content="Pythonic Excursions" />
    <meta property="og:article:author" content="ERIC KIM" />
        <meta property="og:article:published_time" content="2023-11-15T00:00:00-08:00" />
    <meta name="twitter:title" content="Robust Linear Regressions In Python | Pythonic Excursions">
    <meta name="twitter:description" content="Illustrative guidance to outlier-robust linear regression modeling">
        <meta property="og:image" content="https://aegis4048.github.io/images/featured_images/robust_linear_regression.png" />
        <meta name="twitter:image" content="https://aegis4048.github.io/images/featured_images/robust_linear_regression.png" >


        <title>    Robust Linear Regressions In Python  | Pythonic Excursions
</title>

                <link rel="stylesheet" type="text/css" href="https://aegis4048.github.io/theme/libs/bootstrap-4.2.1/dist/css/bootstrap.min.css">
                <link rel="stylesheet" type="text/css" href="https://aegis4048.github.io/theme/libs/fontawesome-free-5.2.0-web/css/all.min.css">
            <link rel="stylesheet" type="text/css" href="https://aegis4048.github.io/theme/css/custom.css" media="screen">
            <link rel="stylesheet" type="text/css" href="https://aegis4048.github.io/theme/css/ipynb.css" media="screen">

            <style>
                #progressBar::-webkit-progress-value {
                    background-color: #24292e;
                }
                #progressBar::-moz-progress-bar {
                    background-color: #24292e;
                }
            </style>

        <link href="https://aegis4048.github.io/theme/libs/prism.css" rel="stylesheet" />
<script type="text/x-mathjax-config">

MathJax.Hub.Config({
tex2jax: { inlineMath: [["$","$"],["\\(","\\)"]] },
"HTML-CSS": {
  linebreaks: { automatic: true, width: "container" }
}
});

</script>
<script type="text/javascript"
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>

        <link rel="shortcut icon" href="https://aegis4048.github.io/theme/img/favicon.ico" type="image/x-icon" />
        <link rel="icon" href="https://aegis4048.github.io/theme/img/apple-touch-icon-152x152.png" type="image/png" />
        <link rel="apple-touch-icon" href="https://aegis4048.github.io/theme/img/apple-touch-icon.png"  type="image/png" />
        <link rel="apple-touch-icon" sizes="57x57" href="https://aegis4048.github.io/theme/img/apple-touch-icon-57x57.png" type="image/png" />
        <link rel="apple-touch-icon" sizes="72x72" href="https://aegis4048.github.io/theme/img/apple-touch-icon-72x72.png" type="image/png" />
        <link rel="apple-touch-icon" sizes="76x76" href="https://aegis4048.github.io/theme/img/apple-touch-icon-76x76.png" type="image/png" />
        <link rel="apple-touch-icon" sizes="114x114" href="https://aegis4048.github.io/theme/img/apple-touch-icon-114x114.png" type="image/png" />
        <link rel="apple-touch-icon" sizes="120x120" href="https://aegis4048.github.io/theme/img/apple-touch-icon-120x120.png" type="image/png" />
        <link rel="apple-touch-icon" sizes="144x144" href="https://aegis4048.github.io/theme/img/apple-touch-icon-144x144.png" type="image/png" />
        <link rel="apple-touch-icon" sizes="152x152" href="https://aegis4048.github.io/theme/img/apple-touch-icon-152x152.png" type="image/png" />
        <link href="https://aegis4048.github.io/feeds/all.atom.xml" type="application/atom+xml" rel="alternate" title="Pythonic Excursions - Full Atom Feed" />

<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-133310548-1"></script>
<script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-133310548-1');
</script>

    </head>
    <body>
<progress id="progressBar" max="19827" class="flat">
    <div class="progress-container">
        <span class="progress-bar"></span>
    </div>
</progress>        <div class="banner-wrapper row" style="background-color: #24292e;">
            <div class="banner">
                <nav id="navbar" class="navbar navbar-expand-md navbar-light bg-light container">
                    <div class="container navbar-title">
                        <a href="/"><img id="banner-logo" src="https://aegis4048.github.io/theme/img/logo_with_subtitle.svg" style="height: 40px; margin: 6px 0;"></a>
                        <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarSupportedContent" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
                            <span class="navbar-toggler-icon"></span>
                        </button>
                    </div>
                <div class="collapse navbar-collapse justify-content-end" id="navbarSupportedContent" style="margin-bottom: 6.5px;margin-top: 6.5px">
                    <ul class="navbar-nav">
                        <li class="nav-item active">
                            <a class="nav-link rem_08" href="https://aegis4048.github.io/about.html">About</a>
                        </li>
                    </ul>
                    <ul class="navbar-nav">
                        <li id="second-item" class="nav-item">
                            <a class="nav-link rem_08" href="https://aegis4048.github.io/archives.html">Archive</a>
                        </li>
                     </ul>
                    <ul class="navbar-nav">
                    <form id="search-form" class="form-inline my-2 my-lg-0 justify-content-center" action="https://aegis4048.github.io/search.html">
                        <div id="third-item" class="search-box-div row" style="margin-top:-3.5px;">
                            <input id="tipue_search_input" class="form-control mr-md-2 rem_08 col-9" type="text" name="q" pattern=".{3,}" title="At least 3 characters" required="" aria-label="Search"/>
                            <button id="search-btn" class="btn btn-search btn-outline-success btn-circle rem_08 col-3" type="submit" for="tipue_search_input" style="margin-left: 3px;">Search</button>
                        </div>
                    </form>
                    </ul>
                </div>
                </nav>
            </div>
        </div>
        <div id="wrap">
<div></div>
<div id="article-container" class="row">
    <article id="article" class="col-12 col-xl-9">
        <header class="pop-over">
            <h1>Robust Linear Regressions In Python</h1>
            <div class="row justify-content-between no-margin">
                <h4 class="article-category">Category > <a class="article-category-link" href="https://aegis4048.github.io/archives.html">Machine Learning</a></h4>
                <span class="article-date">Nov 15, 2023</span>
            </div>
            <div class="meta meta-tag no-margin no-border">
                <div>
                        <a href="https://aegis4048.github.io/tag/linear-regression.html" class="tag">linear regression</a>
                        <a href="https://aegis4048.github.io/tag/robust-linear-regression.html" class="tag">robust linear regression</a>
                        <a href="https://aegis4048.github.io/tag/outliers.html" class="tag">outliers</a>
                        <a href="https://aegis4048.github.io/tag/visualization.html" class="tag">visualization</a>
                        <a href="https://aegis4048.github.io/tag/machine-learning.html" class="tag">machine learning</a>
                        <a href="https://aegis4048.github.io/tag/statistics.html" class="tag">statistics</a>
                        <a href="https://aegis4048.github.io/tag/theilsen-regression.html" class="tag">TheilSen regression</a>
                        <a href="https://aegis4048.github.io/tag/ransac-regression.html" class="tag">RANSAC regression</a>
                        <a href="https://aegis4048.github.io/tag/huber-regression.html" class="tag">Huber regression</a>
                        <a href="https://aegis4048.github.io/tag/ols.html" class="tag">OLS</a>
                        <a href="https://aegis4048.github.io/tag/l1-norm.html" class="tag">L1 norm</a>
                        <a href="https://aegis4048.github.io/tag/l2-norm.html" class="tag">L2 norm</a>
                        <a href="https://aegis4048.github.io/tag/l2-norm-squared.html" class="tag">L2 norm squared</a>
                        <a href="https://aegis4048.github.io/tag/spatial-median.html" class="tag">spatial median</a>
                        <a href="https://aegis4048.github.io/tag/mean.html" class="tag">mean</a>
                        <a href="https://aegis4048.github.io/tag/median.html" class="tag">median</a>
                        <a href="https://aegis4048.github.io/tag/central-tendency.html" class="tag">central tendency</a>
                        <a href="https://aegis4048.github.io/tag/robustness.html" class="tag">robustness</a>
                        <a href="https://aegis4048.github.io/tag/euclidean-distance.html" class="tag">euclidean distance</a>
                        <a href="https://aegis4048.github.io/tag/absolute-loss.html" class="tag">absolute loss</a>
                        <a href="https://aegis4048.github.io/tag/squared-loss.html" class="tag">squared loss</a>
                        <a href="https://aegis4048.github.io/tag/least-squares.html" class="tag">least-squares</a>
                </div>
            </div>
            <section>
    <div id="share-links" class="row justify-content-end mt-3" style="align-items: center">
        <div class="share-post-intro mr-2">Share This Post :</div>
        <div class="social-share-btns-container">
            <div class="social-share-btns">
                <a class="share-btn share-btn-twitter" href="https://twitter.com/home?status=https://aegis4048.github.io/robust_linear_regressions_in_python" rel="nofollow" target="_blank">
                    <i class="fab fa-twitter"></i>
                </a>
                <a class="share-btn share-btn-facebook" href="http://www.facebook.com/sharer/sharer.php?u=https://aegis4048.github.io/robust_linear_regressions_in_python" rel="nofollow" target="_blank">
                    <i class="fab fa-facebook-f"></i>
                </a>
                <a class="share-btn share-btn-linkedin" href="https://www.linkedin.com/shareArticle?mini=true&url=https://aegis4048.github.io/robust_linear_regressions_in_python" rel="nofollow" target="_blank">
                    <i class="fab fa-linkedin-in"></i>
                </a>
            </div>
        </div>
    </div>
</section>

        </header>
        <div class="article_content">
            
            <style type="text/css">/*!
*
* IPython notebook
*
*/
/* CSS font colors for translated ANSI escape sequences */
/* The color values are a mix of
   http://www.xcolors.net/dl/baskerville-ivorylight and
   http://www.xcolors.net/dl/euphrasia */
.ansi-black-fg {
  color: #3E424D;
}
.ansi-black-bg {
  background-color: #3E424D;
}
.ansi-black-intense-fg {
  color: #282C36;
}
.ansi-black-intense-bg {
  background-color: #282C36;
}
.ansi-red-fg {
  color: #E75C58;
}
.ansi-red-bg {
  background-color: #E75C58;
}
.ansi-red-intense-fg {
  color: #B22B31;
}
.ansi-red-intense-bg {
  background-color: #B22B31;
}
.ansi-green-fg {
  color: #00A250;
}
.ansi-green-bg {
  background-color: #00A250;
}
.ansi-green-intense-fg {
  color: #007427;
}
.ansi-green-intense-bg {
  background-color: #007427;
}
.ansi-yellow-fg {
  color: #DDB62B;
}
.ansi-yellow-bg {
  background-color: #DDB62B;
}
.ansi-yellow-intense-fg {
  color: #B27D12;
}
.ansi-yellow-intense-bg {
  background-color: #B27D12;
}
.ansi-blue-fg {
  color: #208FFB;
}
.ansi-blue-bg {
  background-color: #208FFB;
}
.ansi-blue-intense-fg {
  color: #0065CA;
}
.ansi-blue-intense-bg {
  background-color: #0065CA;
}
.ansi-magenta-fg {
  color: #D160C4;
}
.ansi-magenta-bg {
  background-color: #D160C4;
}
.ansi-magenta-intense-fg {
  color: #A03196;
}
.ansi-magenta-intense-bg {
  background-color: #A03196;
}
.ansi-cyan-fg {
  color: #60C6C8;
}
.ansi-cyan-bg {
  background-color: #60C6C8;
}
.ansi-cyan-intense-fg {
  color: #258F8F;
}
.ansi-cyan-intense-bg {
  background-color: #258F8F;
}
.ansi-white-fg {
  color: #C5C1B4;
}
.ansi-white-bg {
  background-color: #C5C1B4;
}
.ansi-white-intense-fg {
  color: #A1A6B2;
}
.ansi-white-intense-bg {
  background-color: #A1A6B2;
}
.ansi-default-inverse-fg {
  color: #FFFFFF;
}
.ansi-default-inverse-bg {
  background-color: #000000;
}
.ansi-bold {
  font-weight: bold;
}
.ansi-underline {
  text-decoration: underline;
}
/* The following styles are deprecated an will be removed in a future version */
.ansibold {
  font-weight: bold;
}
.ansi-inverse {
  outline: 0.5px dotted;
}
/* use dark versions for foreground, to improve visibility */
.ansiblack {
  color: black;
}
.ansired {
  color: darkred;
}
.ansigreen {
  color: darkgreen;
}
.ansiyellow {
  color: #c4a000;
}
.ansiblue {
  color: darkblue;
}
.ansipurple {
  color: darkviolet;
}
.ansicyan {
  color: steelblue;
}
.ansigray {
  color: gray;
}
/* and light for background, for the same reason */
.ansibgblack {
  background-color: black;
}
.ansibgred {
  background-color: red;
}
.ansibggreen {
  background-color: green;
}
.ansibgyellow {
  background-color: yellow;
}
.ansibgblue {
  background-color: blue;
}
.ansibgpurple {
  background-color: magenta;
}
.ansibgcyan {
  background-color: cyan;
}
.ansibggray {
  background-color: gray;
}
div.cell {
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: vertical;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: vertical;
  -moz-box-align: stretch;
  display: box;
  box-orient: vertical;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: column;
  align-items: stretch;
  border-radius: 2px;
  box-sizing: border-box;
  -moz-box-sizing: border-box;
  -webkit-box-sizing: border-box;
  border-width: 1px;
  border-style: solid;
  border-color: transparent;
  width: 100%;
  padding: 5px;
  /* This acts as a spacer between cells, that is outside the border */
  margin: 0px;
  outline: none;
  position: relative;
  overflow: visible;
}
div.cell:before {
  position: absolute;
  display: block;
  top: -1px;
  left: -1px;
  width: 5px;
  height: calc(100% +  2px);
  content: '';
  background: transparent;
}
div.cell.jupyter-soft-selected {
  border-left-color: #E3F2FD;
  border-left-width: 1px;
  padding-left: 5px;
  border-right-color: #E3F2FD;
  border-right-width: 1px;
  background: #E3F2FD;
}
@media print {
  div.cell.jupyter-soft-selected {
    border-color: transparent;
  }
}
div.cell.selected,
div.cell.selected.jupyter-soft-selected {
  border-color: #ababab;
}
div.cell.selected:before,
div.cell.selected.jupyter-soft-selected:before {
  position: absolute;
  display: block;
  top: -1px;
  left: -1px;
  width: 5px;
  height: calc(100% +  2px);
  content: '';
  background: #42A5F5;
}
@media print {
  div.cell.selected,
  div.cell.selected.jupyter-soft-selected {
    border-color: transparent;
  }
}
.edit_mode div.cell.selected {
  border-color: #66BB6A;
}
.edit_mode div.cell.selected:before {
  position: absolute;
  display: block;
  top: -1px;
  left: -1px;
  width: 5px;
  height: calc(100% +  2px);
  content: '';
  background: #66BB6A;
}
@media print {
  .edit_mode div.cell.selected {
    border-color: transparent;
  }
}
.prompt {
  /* This needs to be wide enough for 3 digit prompt numbers: In[100]: */
  min-width: 14ex;
  /* This padding is tuned to match the padding on the CodeMirror editor. */
  padding: 0.4em;
  margin: 0px;
  font-family: monospace;
  text-align: right;
  /* This has to match that of the the CodeMirror class line-height below */
  line-height: 1.21429em;
  /* Don't highlight prompt number selection */
  -webkit-touch-callout: none;
  -webkit-user-select: none;
  -khtml-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
  /* Use default cursor */
  cursor: default;
}
@media (max-width: 540px) {
  .prompt {
    text-align: left;
  }
}
div.inner_cell {
  min-width: 0;
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: vertical;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: vertical;
  -moz-box-align: stretch;
  display: box;
  box-orient: vertical;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: column;
  align-items: stretch;
  /* Old browsers */
  -webkit-box-flex: 1;
  -moz-box-flex: 1;
  box-flex: 1;
  /* Modern browsers */
  flex: 1;
}
/* input_area and input_prompt must match in top border and margin for alignment */
div.input_area {
  border: 1px solid #cfcfcf;
  border-radius: 2px;
  background: #f7f7f7;
  line-height: 1.21429em;
}
/* This is needed so that empty prompt areas can collapse to zero height when there
   is no content in the output_subarea and the prompt. The main purpose of this is
   to make sure that empty JavaScript output_subareas have no height. */
div.prompt:empty {
  padding-top: 0;
  padding-bottom: 0;
}
div.unrecognized_cell {
  padding: 5px 5px 5px 0px;
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: horizontal;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: horizontal;
  -moz-box-align: stretch;
  display: box;
  box-orient: horizontal;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: row;
  align-items: stretch;
}
div.unrecognized_cell .inner_cell {
  border-radius: 2px;
  padding: 5px;
  font-weight: bold;
  color: red;
  border: 1px solid #cfcfcf;
  background: #eaeaea;
}
div.unrecognized_cell .inner_cell a {
  color: inherit;
  text-decoration: none;
}
div.unrecognized_cell .inner_cell a:hover {
  color: inherit;
  text-decoration: none;
}
@media (max-width: 540px) {
  div.unrecognized_cell > div.prompt {
    display: none;
  }
}
div.code_cell {
  /* avoid page breaking on code cells when printing */
}
@media print {
  div.code_cell {
    page-break-inside: avoid;
  }
}
/* any special styling for code cells that are currently running goes here */
div.input {
  page-break-inside: avoid;
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: horizontal;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: horizontal;
  -moz-box-align: stretch;
  display: box;
  box-orient: horizontal;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: row;
  align-items: stretch;
}
@media (max-width: 540px) {
  div.input {
    /* Old browsers */
    display: -webkit-box;
    -webkit-box-orient: vertical;
    -webkit-box-align: stretch;
    display: -moz-box;
    -moz-box-orient: vertical;
    -moz-box-align: stretch;
    display: box;
    box-orient: vertical;
    box-align: stretch;
    /* Modern browsers */
    display: flex;
    flex-direction: column;
    align-items: stretch;
  }
}
/* input_area and input_prompt must match in top border and margin for alignment */
div.input_prompt {
  color: #303F9F;
  border-top: 1px solid transparent;
}
div.input_area > div.highlight {
  margin: 0.4em;
  border: none;
  padding: 0px;
  background-color: transparent;
}
div.input_area > div.highlight > pre {
  margin: 0px;
  border: none;
  padding: 0px;
  background-color: transparent;
}
/* The following gets added to the <head> if it is detected that the user has a
 * monospace font with inconsistent normal/bold/italic height.  See
 * notebookmain.js.  Such fonts will have keywords vertically offset with
 * respect to the rest of the text.  The user should select a better font.
 * See: https://github.com/ipython/ipython/issues/1503
 *
 * .CodeMirror span {
 *      vertical-align: bottom;
 * }
 */
.CodeMirror {
  line-height: 1.21429em;
  /* Changed from 1em to our global default */
  font-size: 14px;
  height: auto;
  /* Changed to auto to autogrow */
  background: none;
  /* Changed from white to allow our bg to show through */
}
.CodeMirror-scroll {
  /*  The CodeMirror docs are a bit fuzzy on if overflow-y should be hidden or visible.*/
  /*  We have found that if it is visible, vertical scrollbars appear with font size changes.*/
  overflow-y: hidden;
  overflow-x: auto;
}
.CodeMirror-lines {
  /* In CM2, this used to be 0.4em, but in CM3 it went to 4px. We need the em value because */
  /* we have set a different line-height and want this to scale with that. */
  /* Note that this should set vertical padding only, since CodeMirror assumes
       that horizontal padding will be set on CodeMirror pre */
  padding: 0.4em 0;
}
.CodeMirror-linenumber {
  padding: 0 8px 0 4px;
}
.CodeMirror-gutters {
  border-bottom-left-radius: 2px;
  border-top-left-radius: 2px;
}
.CodeMirror pre {
  /* In CM3 this went to 4px from 0 in CM2. This sets horizontal padding only,
    use .CodeMirror-lines for vertical */
  padding: 0 0.4em;
  border: 0;
  border-radius: 0;
}
.CodeMirror-cursor {
  border-left: 1.4px solid black;
}
@media screen and (min-width: 2138px) and (max-width: 4319px) {
  .CodeMirror-cursor {
    border-left: 2px solid black;
  }
}
@media screen and (min-width: 4320px) {
  .CodeMirror-cursor {
    border-left: 4px solid black;
  }
}
/*

Original style from softwaremaniacs.org (c) Ivan Sagalaev <Maniac@SoftwareManiacs.Org>
Adapted from GitHub theme

*/
.highlight-base {
  color: #000;
}
.highlight-variable {
  color: #000;
}
.highlight-variable-2 {
  color: #1a1a1a;
}
.highlight-variable-3 {
  color: #333333;
}
.highlight-string {
  color: #BA2121;
}
.highlight-comment {
  color: #408080;
  font-style: italic;
}
.highlight-number {
  color: #080;
}
.highlight-atom {
  color: #88F;
}
.highlight-keyword {
  color: #008000;
  font-weight: bold;
}
.highlight-builtin {
  color: #008000;
}
.highlight-error {
  color: #f00;
}
.highlight-operator {
  color: #AA22FF;
  font-weight: bold;
}
.highlight-meta {
  color: #AA22FF;
}
/* previously not defined, copying from default codemirror */
.highlight-def {
  color: #00f;
}
.highlight-string-2 {
  color: #f50;
}
.highlight-qualifier {
  color: #555;
}
.highlight-bracket {
  color: #997;
}
.highlight-tag {
  color: #170;
}
.highlight-attribute {
  color: #00c;
}
.highlight-header {
  color: blue;
}
.highlight-quote {
  color: #090;
}
.highlight-link {
  color: #00c;
}
/* apply the same style to codemirror */
.cm-s-ipython span.cm-keyword {
  color: #008000;
  font-weight: bold;
}
.cm-s-ipython span.cm-atom {
  color: #88F;
}
.cm-s-ipython span.cm-number {
  color: #080;
}
.cm-s-ipython span.cm-def {
  color: #00f;
}
.cm-s-ipython span.cm-variable {
  color: #000;
}
.cm-s-ipython span.cm-operator {
  color: #AA22FF;
  font-weight: bold;
}
.cm-s-ipython span.cm-variable-2 {
  color: #1a1a1a;
}
.cm-s-ipython span.cm-variable-3 {
  color: #333333;
}
.cm-s-ipython span.cm-comment {
  color: #408080;
  font-style: italic;
}
.cm-s-ipython span.cm-string {
  color: #BA2121;
}
.cm-s-ipython span.cm-string-2 {
  color: #f50;
}
.cm-s-ipython span.cm-meta {
  color: #AA22FF;
}
.cm-s-ipython span.cm-qualifier {
  color: #555;
}
.cm-s-ipython span.cm-builtin {
  color: #008000;
}
.cm-s-ipython span.cm-bracket {
  color: #997;
}
.cm-s-ipython span.cm-tag {
  color: #170;
}
.cm-s-ipython span.cm-attribute {
  color: #00c;
}
.cm-s-ipython span.cm-header {
  color: blue;
}
.cm-s-ipython span.cm-quote {
  color: #090;
}
.cm-s-ipython span.cm-link {
  color: #00c;
}
.cm-s-ipython span.cm-error {
  color: #f00;
}
.cm-s-ipython span.cm-tab {
  background: url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAADAAAAAMCAYAAAAkuj5RAAAAAXNSR0IArs4c6QAAAGFJREFUSMft1LsRQFAQheHPowAKoACx3IgEKtaEHujDjORSgWTH/ZOdnZOcM/sgk/kFFWY0qV8foQwS4MKBCS3qR6ixBJvElOobYAtivseIE120FaowJPN75GMu8j/LfMwNjh4HUpwg4LUAAAAASUVORK5CYII=);
  background-position: right;
  background-repeat: no-repeat;
}
div.output_wrapper {
  /* this position must be relative to enable descendents to be absolute within it */
  position: relative;
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: vertical;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: vertical;
  -moz-box-align: stretch;
  display: box;
  box-orient: vertical;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: column;
  align-items: stretch;
  z-index: 1;
}
/* class for the output area when it should be height-limited */
div.output_scroll {
  /* ideally, this would be max-height, but FF barfs all over that */
  height: 24em;
  /* FF needs this *and the wrapper* to specify full width, or it will shrinkwrap */
  width: 100%;
  overflow: auto;
  border-radius: 2px;
  -webkit-box-shadow: inset 0 2px 8px rgba(0, 0, 0, 0.8);
  box-shadow: inset 0 2px 8px rgba(0, 0, 0, 0.8);
  display: block;
}
/* output div while it is collapsed */
div.output_collapsed {
  margin: 0px;
  padding: 0px;
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: vertical;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: vertical;
  -moz-box-align: stretch;
  display: box;
  box-orient: vertical;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: column;
  align-items: stretch;
}
div.out_prompt_overlay {
  height: 100%;
  padding: 0px 0.4em;
  position: absolute;
  border-radius: 2px;
}
div.out_prompt_overlay:hover {
  /* use inner shadow to get border that is computed the same on WebKit/FF */
  -webkit-box-shadow: inset 0 0 1px #000;
  box-shadow: inset 0 0 1px #000;
  background: rgba(240, 240, 240, 0.5);
}
div.output_prompt {
  color: #D84315;
}
/* This class is the outer container of all output sections. */
div.output_area {
  padding: 0px;
  page-break-inside: avoid;
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: horizontal;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: horizontal;
  -moz-box-align: stretch;
  display: box;
  box-orient: horizontal;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: row;
  align-items: stretch;
}
div.output_area .MathJax_Display {
  text-align: left !important;
}
div.output_area 
div.output_area 
div.output_area img,
div.output_area svg {
  max-width: 100%;
  height: auto;
}
div.output_area img.unconfined,
div.output_area svg.unconfined {
  max-width: none;
}
div.output_area .mglyph > img {
  max-width: none;
}
/* This is needed to protect the pre formating from global settings such
   as that of bootstrap */
.output {
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: vertical;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: vertical;
  -moz-box-align: stretch;
  display: box;
  box-orient: vertical;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: column;
  align-items: stretch;
}
@media (max-width: 540px) {
  div.output_area {
    /* Old browsers */
    display: -webkit-box;
    -webkit-box-orient: vertical;
    -webkit-box-align: stretch;
    display: -moz-box;
    -moz-box-orient: vertical;
    -moz-box-align: stretch;
    display: box;
    box-orient: vertical;
    box-align: stretch;
    /* Modern browsers */
    display: flex;
    flex-direction: column;
    align-items: stretch;
  }
}
div.output_area pre {
  margin: 0;
  padding: 1px 0 1px 0;
  border: 0;
  vertical-align: baseline;
  color: black;
  background-color: transparent;
  border-radius: 0;
}
/* This class is for the output subarea inside the output_area and after
   the prompt div. */
div.output_subarea {
  overflow-x: auto;
  padding: 0.4em;
  /* Old browsers */
  -webkit-box-flex: 1;
  -moz-box-flex: 1;
  box-flex: 1;
  /* Modern browsers */
  flex: 1;
  max-width: calc(100% - 14ex);
}
div.output_scroll div.output_subarea {
  overflow-x: visible;
}
/* The rest of the output_* classes are for special styling of the different
   output types */
/* all text output has this class: */
div.output_text {
  text-align: left;
  color: #000;
  /* This has to match that of the the CodeMirror class line-height below */
  line-height: 1.21429em;
}
/* stdout/stderr are 'text' as well as 'stream', but execute_result/error are *not* streams */
div.output_stderr {
  background: #fdd;
  /* very light red background for stderr */
}
div.output_latex {
  text-align: left;
}
/* Empty output_javascript divs should have no height */
div.output_javascript:empty {
  padding: 0;
}
.js-error {
  color: darkred;
}
/* raw_input styles */
div.raw_input_container {
  line-height: 1.21429em;
  padding-top: 5px;
}
pre.raw_input_prompt {
  /* nothing needed here. */
}
input.raw_input {
  font-family: monospace;
  font-size: inherit;
  color: inherit;
  width: auto;
  /* make sure input baseline aligns with prompt */
  vertical-align: baseline;
  /* padding + margin = 0.5em between prompt and cursor */
  padding: 0em 0.25em;
  margin: 0em 0.25em;
}
input.raw_input:focus {
  box-shadow: none;
}
p.p-space {
  margin-bottom: 10px;
}
div.output_unrecognized {
  padding: 5px;
  font-weight: bold;
  color: red;
}
div.output_unrecognized a {
  color: inherit;
  text-decoration: none;
}
div.output_unrecognized a:hover {
  color: inherit;
  text-decoration: none;
}
.rendered_html {
  color: #000;
  /* any extras will just be numbers: */
}



.rendered_html :link {
  text-decoration: underline;
}
.rendered_html :visited {
  text-decoration: underline;
}






.rendered_html h1:first-child {
  margin-top: 0.538em;
}
.rendered_html h2:first-child {
  margin-top: 0.636em;
}
.rendered_html h3:first-child {
  margin-top: 0.777em;
}
.rendered_html h4:first-child {
  margin-top: 1em;
}
.rendered_html h5:first-child {
  margin-top: 1em;
}
.rendered_html h6:first-child {
  margin-top: 1em;
}
.rendered_html ul:not(.list-inline),
.rendered_html ol:not(.list-inline) {
  padding-left: 2em;
}








.rendered_html * + ul {
  margin-top: 1em;
}
.rendered_html * + ol {
  margin-top: 1em;
}





.rendered_html pre,




.rendered_html tr,
.rendered_html th,


.rendered_html tbody tr:nth-child(odd) {
  background: #f5f5f5;
}
.rendered_html tbody tr:hover {
  background: rgba(66, 165, 245, 0.2);
}
.rendered_html * + table {
  margin-top: 1em;
}

.rendered_html * + p {
  margin-top: 1em;
}

.rendered_html * + img {
  margin-top: 1em;
}
.rendered_html img,

.rendered_html img.unconfined,


.rendered_html * + .alert {
  margin-top: 1em;
}
[dir="rtl"] 
div.text_cell {
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: horizontal;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: horizontal;
  -moz-box-align: stretch;
  display: box;
  box-orient: horizontal;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: row;
  align-items: stretch;
}
@media (max-width: 540px) {
  div.text_cell > div.prompt {
    display: none;
  }
}
div.text_cell_render {
  /*font-family: "Helvetica Neue", Arial, Helvetica, Geneva, sans-serif;*/
  outline: none;
  resize: none;
  width: inherit;
  border-style: none;
  padding: 0.5em 0.5em 0.5em 0.4em;
  color: #000;
  box-sizing: border-box;
  -moz-box-sizing: border-box;
  -webkit-box-sizing: border-box;
}
a.anchor-link:link {
  text-decoration: none;
  padding: 0px 20px;
  visibility: hidden;
}
h1:hover .anchor-link,
h2:hover .anchor-link,
h3:hover .anchor-link,
h4:hover .anchor-link,
h5:hover .anchor-link,
h6:hover .anchor-link {
  visibility: visible;
}
.text_cell.rendered .input_area {
  display: none;
}
.text_cell.rendered 
.text_cell.rendered .rendered_html tr,
.text_cell.rendered .rendered_html th,
.text_cell.rendered 
.text_cell.unrendered .text_cell_render {
  display: none;
}
.text_cell .dropzone .input_area {
  border: 2px dashed #bababa;
  margin: -1px;
}
.cm-header-1,
.cm-header-2,
.cm-header-3,
.cm-header-4,
.cm-header-5,
.cm-header-6 {
  font-weight: bold;
  font-family: "Helvetica Neue", Helvetica, Arial, sans-serif;
}
.cm-header-1 {
  font-size: 185.7%;
}
.cm-header-2 {
  font-size: 157.1%;
}
.cm-header-3 {
  font-size: 128.6%;
}
.cm-header-4 {
  font-size: 110%;
}
.cm-header-5 {
  font-size: 100%;
  font-style: italic;
}
.cm-header-6 {
  font-size: 100%;
  font-style: italic;
}
</style>
<style type="text/css">pre { line-height: 125%; }
td.linenos .normal { color: inherit; background-color: transparent; padding-left: 5px; padding-right: 5px; }
span.linenos { color: inherit; background-color: transparent; padding-left: 5px; padding-right: 5px; }
td.linenos .special { color: #000000; background-color: #ffffc0; padding-left: 5px; padding-right: 5px; }
span.linenos.special { color: #000000; background-color: #ffffc0; padding-left: 5px; padding-right: 5px; }
.highlight .hll { background-color: #ffffcc }
.highlight { background: #f8f8f8; }
.highlight .c { color: #3D7B7B; font-style: italic } /* Comment */
.highlight .err { border: 1px solid #FF0000 } /* Error */
.highlight .k { color: #008000; font-weight: bold } /* Keyword */
.highlight .o { color: #666666 } /* Operator */
.highlight .ch { color: #3D7B7B; font-style: italic } /* Comment.Hashbang */
.highlight .cm { color: #3D7B7B; font-style: italic } /* Comment.Multiline */
.highlight .cp { color: #9C6500 } /* Comment.Preproc */
.highlight .cpf { color: #3D7B7B; font-style: italic } /* Comment.PreprocFile */
.highlight .c1 { color: #3D7B7B; font-style: italic } /* Comment.Single */
.highlight .cs { color: #3D7B7B; font-style: italic } /* Comment.Special */
.highlight .gd { color: #A00000 } /* Generic.Deleted */
.highlight .ge { font-style: italic } /* Generic.Emph */
.highlight .ges { font-weight: bold; font-style: italic } /* Generic.EmphStrong */
.highlight .gr { color: #E40000 } /* Generic.Error */
.highlight .gh { color: #000080; font-weight: bold } /* Generic.Heading */
.highlight .gi { color: #008400 } /* Generic.Inserted */
.highlight .go { color: #717171 } /* Generic.Output */
.highlight .gp { color: #000080; font-weight: bold } /* Generic.Prompt */
.highlight .gs { font-weight: bold } /* Generic.Strong */
.highlight .gu { color: #800080; font-weight: bold } /* Generic.Subheading */
.highlight .gt { color: #0044DD } /* Generic.Traceback */
.highlight .kc { color: #008000; font-weight: bold } /* Keyword.Constant */
.highlight .kd { color: #008000; font-weight: bold } /* Keyword.Declaration */
.highlight .kn { color: #008000; font-weight: bold } /* Keyword.Namespace */
.highlight .kp { color: #008000 } /* Keyword.Pseudo */
.highlight .kr { color: #008000; font-weight: bold } /* Keyword.Reserved */
.highlight .kt { color: #B00040 } /* Keyword.Type */
.highlight .m { color: #666666 } /* Literal.Number */
.highlight .s { color: #BA2121 } /* Literal.String */
.highlight .na { color: #687822 } /* Name.Attribute */
.highlight .nb { color: #008000 } /* Name.Builtin */
.highlight .nc { color: #0000FF; font-weight: bold } /* Name.Class */
.highlight .no { color: #880000 } /* Name.Constant */
.highlight .nd { color: #AA22FF } /* Name.Decorator */
.highlight .ni { color: #717171; font-weight: bold } /* Name.Entity */
.highlight .ne { color: #CB3F38; font-weight: bold } /* Name.Exception */
.highlight .nf { color: #0000FF } /* Name.Function */
.highlight .nl { color: #767600 } /* Name.Label */
.highlight .nn { color: #0000FF; font-weight: bold } /* Name.Namespace */
.highlight .nt { color: #008000; font-weight: bold } /* Name.Tag */
.highlight .nv { color: #19177C } /* Name.Variable */
.highlight .ow { color: #AA22FF; font-weight: bold } /* Operator.Word */
.highlight .w { color: #bbbbbb } /* Text.Whitespace */
.highlight .mb { color: #666666 } /* Literal.Number.Bin */
.highlight .mf { color: #666666 } /* Literal.Number.Float */
.highlight .mh { color: #666666 } /* Literal.Number.Hex */
.highlight .mi { color: #666666 } /* Literal.Number.Integer */
.highlight .mo { color: #666666 } /* Literal.Number.Oct */
.highlight .sa { color: #BA2121 } /* Literal.String.Affix */
.highlight .sb { color: #BA2121 } /* Literal.String.Backtick */
.highlight .sc { color: #BA2121 } /* Literal.String.Char */
.highlight .dl { color: #BA2121 } /* Literal.String.Delimiter */
.highlight .sd { color: #BA2121; font-style: italic } /* Literal.String.Doc */
.highlight .s2 { color: #BA2121 } /* Literal.String.Double */
.highlight .se { color: #AA5D1F; font-weight: bold } /* Literal.String.Escape */
.highlight .sh { color: #BA2121 } /* Literal.String.Heredoc */
.highlight .si { color: #A45A77; font-weight: bold } /* Literal.String.Interpol */
.highlight .sx { color: #008000 } /* Literal.String.Other */
.highlight .sr { color: #A45A77 } /* Literal.String.Regex */
.highlight .s1 { color: #BA2121 } /* Literal.String.Single */
.highlight .ss { color: #19177C } /* Literal.String.Symbol */
.highlight .bp { color: #008000 } /* Name.Builtin.Pseudo */
.highlight .fm { color: #0000FF } /* Name.Function.Magic */
.highlight .vc { color: #19177C } /* Name.Variable.Class */
.highlight .vg { color: #19177C } /* Name.Variable.Global */
.highlight .vi { color: #19177C } /* Name.Variable.Instance */
.highlight .vm { color: #19177C } /* Name.Variable.Magic */
.highlight .il { color: #666666 } /* Literal.Number.Integer.Long */</style><div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<div class="display-none" id="toc_container">
<p class="toc_title"><i class="fas fa-list"></i>Contents</p>
<ul class="toc_list">
<li><a href="#1.-Why-is-OLS-vulnerable-to-outliers?"><span class="toc_label">1.</span>
            Why is OLS vulnerable to outliers?</a></li>
<ul>
<li><a href="#1.1.-Convergence-to-mean"><span class="toc_label">1.1.</span>
                Convergence to mean</a></li>
<li><a href="#1.2.-Squared-term-magnifies-impact-of-outliers"><span class="toc_label">1.2.</span>
                Squared term magnifies impact of outliers</a></li>
<li><a href="#1.3.-Understanding-the-L-norms-in-cartesian-space"><span class="toc_label">1.3.</span>
                Understanding the L-norms in cartesian space</a></li>
</ul>
<li><a href="#2.-Robust-Models"><span class="toc_label">2.</span>
            Robust Models</a></li>
<ul>
<li><a href="#2.1.-RANSAC-regressor"><span class="toc_label">2.1.</span>
                RANSAC regressor</a></li>
<ul>
<li><a href="#2.1.1.-Visual-demonstrations"><span class="toc_label">2.1.1.</span>
                        Visual demonstrations</a></li>
<li><a href="#2.1.2.-Median-Absolute-Deviation-(MAD)-threshold"><span class="toc_label">2.1.2.</span>
                        Median Absolute Deviation (MAD) threshold</a></li>
<li><a href="#2.1.3.-Required-iteration-calculation"><span class="toc_label">2.1.3.</span>
                        Required iteration calculation</a></li>
<li><a href="#2.1.4.-More-detailed-visual-demonstrations"><span class="toc_label">2.1.4.</span>
                        More detailed visual demonstrations</a></li>
<li><a href="#2.1.5.-RANSAC-code-snippets"><span class="toc_label">2.1.5.</span>
                        RANSAC code snippets</a></li>
</ul>
<li><a href="#2.2.-Huber-regressor"><span class="toc_label">2.2.</span>
                Huber regressor</a></li>
<ul>
<li><a href="#2.2.1.-Huber-loss-function"><span class="toc_label">2.2.1.</span>
                        Huber loss function</a></li>
<li><a href="#2.2.2.-Motivation"><span class="toc_label">2.2.2.</span>
                        Motivation</a></li>
<li><a href="#2.2.3.-Parameter-tuning-($\delta$)"><span class="toc_label">2.2.3.</span>
                        Parameter tuning</a></li>
<li><a href="#2.2.4.-Huber-code-snippets"><span class="toc_label">2.2.4.</span>
                        Huber code snippets</a></li>
</ul>
<li><a href="#2.3.-Theil-Sen-regressor"><span class="toc_label">2.3.</span>
                Theil-Sen regressor</a></li>
<ul>
<li><a href="#2.3.1.-Sample-size-and-model-robustness"><span class="toc_label">2.3.1.</span>
                        Sample size and model robustness</a></li>
<li><a href="#2.3.2.-Spatial-median"><span class="toc_label">2.3.2.</span>
                        Spatial median</a></li>
<li><a href="#2.3.3.-Why-use-median-instead-of-mean-with-outliers?"><span class="toc_label">2.3.3.</span>
                        Why use median instead of mean with outliers?</a></li>
<ul>
<li><a href="#2.3.3.1.-Effect-of-the-squared-term"><span class="toc_label">2.3.3.1.</span>
                            Effect of the squared term</a></li>
<li><a href="#2.3.3.2.-Measure-of-central-tendency"><span class="toc_label">2.3.3.2.</span>
                            Measure of central tendency</a></li>
</ul>
<li><a href="#2.3.4.-Theil-Sen-code-snippets"><span class="toc_label">2.3.4.</span>
                        Theil-Sen code snippets</a></li>
</ul>
<li><a href="#2.4.-Summary"><span class="toc_label">2.4.</span>
                Summary</a></li>
</ul>
<li><a href="#3.-Extension-to-3D+-multivariate-linear-regressions"><span class="toc_label">3.</span>
            Extension to 3D+ multivariate linear regressions</a></li>
<ul><li><a href="#3.1.-Visual-demonstrations"><span class="toc_label">3.1.</span>
            Visual demonstrations</a></li>
</ul>
</ul>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>In regression analysis, understanding and mitigating the impact of outliers is crucial for accurate model predictions. Common approaches like Ordinary Least Squares (OLS) often fall short when outliers are present, leading to skewed results. This discussion will explore how various regression techniques, including OLS, RANSAC, Huber, and Theil-Sen regressors, each handle outliers differently. By examining these methods through both theoretical insights and practical demonstrations using Python, I aim to highlight their unique responses to outlier influences, thereby guiding the selection of the most appropriate regression model for datasets with varying outlier characteristics.</p>
<div class="row full_screen_margin_90 mobile_responsive_plot_full_width" id="fig-1" style="">
<div class="col"><img src="jupyter_images/robust linear regression comprisons.png"/></div>
</div>
<div class="col-12 fig-title"><p class="image-description"><strong>Figure 1:</strong> In scenarios involving outliers, the RANSAC regressor is often the preferred choice as it identifies and excludes outliers before fitting the model. Other robust regressors, like Huber and Theil-Sen, aim to <i>dampen</i> the impact of outliers rather than exclude them. This approach is beneficial if the so-called 'outliers' are actually integral parts of the dataset, needing consideration rather than exclusion. In terms of robustness to outliers, the hierarchy typically follows: RANSAC > Theil-Sen > Huber > OLS. Note that the points labeled with the 'x' markers are outliers detected by the RANSAC model.</p></div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<div class="solution_panel closed">
<div class="solution_title">
<p class="solution_title_string">Source Code For Figure (1)</p>
<ul class="nav navbar-right panel_toolbox">
<li><a class="collapse-link"><i class="fa fa-chevron-down"></i></a></li>
</ul>
<div class="clearfix"></div>
</div>
<div class="solution_content">
<pre>
            <code class="language-python">
import random
import numpy as np
import matplotlib.pyplot as plt

from sklearn import linear_model
from sklearn.linear_model import RANSACRegressor, HuberRegressor, TheilSenRegressor

###################################### data ######################################

X = np.array([ 0.15, -0.34, 0.32, 0.43, -0.4, -0.04, -0.51, 0.3, 0.47, 0.12, 0.08, 0.04, -0.08,
              -0.23, 0.08, -0.03, 0.03, 0.04, 0.01, 0.06, 0.03, 0., -0.04, -0.18, -0.19, -0.06,
              -0.26, -0.16, 0.13, 0.09, 0.03, -0.03, 0.04, 0.14, -0.01, 0.4, -0.06, 0.15, 0.08,
              0.05, -0.15, -0.09, -0.15, -0.11, -0.07, -0.19, -0.06, 0.17, 0.23, 0.18]).reshape(-1, 1)
y = np.array([17.44, 25.46, 18.61, 26.07, 24.96, -1.22, 26.45, 26.5, 20.57, 3.08, 35.9 , 32.47,
              20.84, 13.37, 42.44, 27.23, 35.65, 29.51, 31.28, 41.34, 32.19, 33.67, 25.64, 9.3,
              14.63, 25.1, 4.69, 14.42, 47.53, 33.82, 32.2 , 24.81, 32.64, 45.11, 26.76, 68.01,
              23.39, 43.49, 37.88, 36.01, 16.32, 19.77, 16.34, 19.57, 29.28, 16.62, 24.39, 43.77,
              50.46, 47.09])

################################### Model fits ###################################

# Ordinary Least Squares
ols = linear_model.LinearRegression().fit(X, y)
y_pred_ols = ols.predict(X)
coefs_ols = ols.coef_
intercept_ols = ols.intercept_

# RANSAC
ransac = RANSACRegressor(random_state=1).fit(X, y)
y_pred_ransac = ransac.predict(X)
coefs_ransac = ransac.estimator_.coef_
intercept_ransac = ransac.estimator_.intercept_

# Huber
huber = HuberRegressor().fit(X, y)
y_pred_huber = huber.predict(X)
coefs_huber = huber.coef_
intercept_huber = huber.intercept_

# TheilSen
TS = TheilSenRegressor().fit(X, y)
y_pred_TS = TS.predict(X)
coefs_TS = TS.coef_
intercept_TS = TS.intercept_

########################## Outliers detected - RANSAC ############################

X_outlier_ransac = X[~ransac.inlier_mask_ ]
y_outlier_ransac = y[~ransac.inlier_mask_ ]

#################################### Plotting ####################################

fig, ax = plt.subplots(figsize=(8, 4.5))

ax.scatter(X_outlier_ransac, y_outlier_ransac, s=100, c='#ff7f0e', marker='x')
ax.scatter(X, y, s=100, fc='grey', lw=1, edgecolors='k', alpha=0.3)
ax.plot(X, y_pred_ols, label='OLS')
ax.plot(X, y_pred_ransac, label='RANSAC')
ax.plot(X, y_pred_huber, label='Huber')
ax.plot(X, y_pred_TS, label='TheilSen')

ax.legend(fontsize=12, ncol=4)

ax.grid(axis='y', linestyle='--', color='#acacac', alpha=0.5)
ax.spines['top'].set_visible(False)
ax.spines['right'].set_visible(False)

ax.set_xlabel('X', fontsize=13)
ax.set_ylabel('y', fontsize=13)
ax.text(0.99, 0.1, 'aegis4048.github.io', fontsize=12, ha='right', va='center',
    transform=ax.transAxes, color='grey', alpha=0.5)

def setbold(txt):
    return ' '.join([r"$\bf{" + item + "}$" for item in txt.split(' ')])

bold_txt = setbold('Robust Linear Regression')
plain_txt = r', performance comprisons of OLS vs. robust models'
fig.suptitle(bold_txt + plain_txt, verticalalignment='top', x=0, horizontalalignment='left', fontsize=13, y=0.96)
yloc = 0.88
ax.annotate('', xy=(0.01, yloc + 0.01), xycoords='figure fraction', xytext=(1.02, yloc + 0.01),
            arrowprops=dict(arrowstyle="-", color='k', lw=0.7))

fig.tight_layout()
            </code>
        </pre>
</div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="1.-Why-is-OLS-vulnerable-to-outliers?">1. Why is OLS vulnerable to outliers?<a class="anchor-link" href="#1.-Why-is-OLS-vulnerable-to-outliers?">¶</a></h2><p>A simple 2D Ordinary Least Squares (OLS) solves for arguments (slope $m$ and intercept $b$) that minimize the following objective (loss) function:</p>
<div id="eq-1" style="font-size: 1rem;">
$$L_{2}^{2} = \underset{m, b}{\text{argmin}} \sum_{i=1}^n [y_i - (mx_i + b)]^2 \tag{1}$$
</div><p>Intuitively the objective function is finding the best slope and intercept that will minimize the <strong>squared</strong> prediction error (residuals) of your model prediction $\hat{y} = mX + b$ against the observation $y$. <a class="internal-link" href="#eq-1">Eq-1</a> is often referred to as <em>$L_{2}$-Norm squared</em>. It has some interesting properties that make it vulnerable to the outliers:</p>
<div><hr/></div><div id="The equation converges to mean, and inherits the statistical properties of mean"></div><h3 id="1.1.-Convergence-to-mean">1.1. Convergence to mean<a class="anchor-link" href="#1.1.-Convergence-to-mean">¶</a></h3><p>The equation has mathematical properties that make it converge to mean. While I'm not gonna bore the readers explaining the mathematical proof (I don't understand them anyway), I can show you in Python that it indeed converges to the mean. Consider the following code snippet:</p>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In [1]:</div>
<div class="inner_cell">
<div class="input_area">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">scipy.optimize</span> <span class="kn">import</span> <span class="n">minimize</span>

<span class="c1"># Implemenation of eq-1 above</span>
<span class="k">def</span> <span class="nf">L2_norm_squared</span><span class="p">(</span><span class="n">point</span><span class="p">,</span> <span class="n">_x</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">((</span><span class="n">_x</span> <span class="o">-</span> <span class="n">point</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">3</span><span class="p">,</span> <span class="mi">9</span><span class="p">,</span> <span class="mi">21</span><span class="p">,</span> <span class="mi">25</span><span class="p">])</span>
<span class="n">x0</span> <span class="o">=</span> <span class="mi">0</span>   <span class="c1"># initial guess, this can be anything</span>

<span class="n">L2</span> <span class="o">=</span> <span class="n">minimize</span><span class="p">(</span><span class="n">L2_norm_squared</span><span class="p">,</span> <span class="n">x0</span><span class="p">,</span> <span class="n">args</span><span class="o">=</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">method</span><span class="o">=</span><span class="s1">'Nelder-Mead'</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">"L2-squared minimized:"</span><span class="p">,</span> <span class="n">L2</span><span class="o">.</span><span class="n">x</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"Mean of x:"</span><span class="p">,</span> <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">x</span><span class="p">)])</span>
</pre></div>
</div>
</div>
</div>
<div class="output_wrapper">
<div class="output">
<div class="output_area">
<div class="prompt"></div>
<div class="output_subarea output_stream output_stdout output_text">
<pre>L2-squared minimized: [14.5]
Mean of x: [14.5]
</pre>
</div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<div style="margin-top: -15px;"></div><p>Observe that the parameter <code>14.5</code>, which minimizes the objective function, is also the sample mean. This is because the objective function converges towards the mean. However, the problem with the mean is that it's not the best measure of central tendency in the presence of outliers that induce skewness. Consider the below figure:</p>
<div class="row full_screen_margin_60 mobile_responsive_plot_full_width" id="fig-2" style="">
<div class="col"><img src="jupyter_images/mean - median - mode - outliers.png"/></div>
</div>
<div class="col-12 fig-title"><p class="image-description"><strong>Figure 2:</strong> Three measures of central tendency in presence of outliers: mode, median and mean (image adpated from <a href="https://www.researchgate.net/publication/10633674_Comparison_of_mode_estimation_methods_and_application_in_molecular_clock_analysis" target="_blank">here).</a></p></div><p>In an ideal normal distribution without outliers, the mode, median, and mean coincide. However, outliers skew the distribution, pulling the mean away from where most data points lie. The median, less affected by outliers, provides a better measure of central tendency in such cases, leading to the preference for median-based estimators like the<em>$L_{1}$-Norm</em> absolute loss over squared loss. <a class="internal-link" href="#fig-18">Figures 18 and 19</a> below shows visual demonstrations of superior performance of median-based estimators in presence of outliers.</p>
<div id="Because the residuals are squared, large outliers disproportionately affects model fit"></div><h3 id="1.2.-Squared-term-magnifies-impact-of-outliers">1.2. Squared term magnifies impact of outliers<a class="anchor-link" href="#1.2.-Squared-term-magnifies-impact-of-outliers">¶</a></h3><p>The OLS loss function aims to minimize the squared residuals, which escalates the influence of outliers because the <u>loss increases exponentially with larger residuals</u>. This can lead to a poor fit. Less severe loss functions, like the regular <em>$L_{2}$-Norm</em> (square root of sum of squared residuals):</p>
<div id="eq-2" style="font-size: 1rem;">
$$L_2 = \underset{m, b}{\text{argmin}} \sqrt{\sum_{i=1}^n [y_i - (mx_i + b)]^2} \tag{2}$$
</div><p>or <em>$L_{1}$-Norm</em> (absolute residuals):</p>
<div id="eq-3" style="font-size: 1rem;">
$$L_1 = \underset{m, b}{\text{argmin}} \sum_{i=1}^n |y_i - (mx_i + b)| \tag{3}$$
</div><p>can be more resilient to outliers, dampening their effects. <a class="internal-link" href="#eq-2">Eq-2</a> has the squared term, but the effect of squaring is mitigated by taking the squared root, making it more robust than <em>$L_{2}$-Norm squared</em> . <a class="internal-link" href="#eq-3">Eq-3</a> on the other hand, has no squared term at all, making it the most robust choice among the three.</p>
<div><hr/></div><div id="Understanding the L-norms in cartesian space"></div><h3 id="1.3.-Understanding-the-L-norms-in-cartesian-space">1.3. Understanding the L-norms in cartesian space<a class="anchor-link" href="#1.3.-Understanding-the-L-norms-in-cartesian-space">¶</a></h3><p>Interesting observations arise when the norms are adapted in a Cartesian space, and these can be used to illustrate why the OLS is vulnerable to outliers. Consider four data points in a 2D Cartesian space along the x-axis and y-axis. Minimizing the <em>$L_{2}$-Norm squared</em> (<a class="internal-link" href="#eq-1">eq-1</a>, which is the objective function of the OLS) for the distance from these four points identifies the centroid (center of mass) coordinate that minimizes the sum of the squares of distances to each point. This point, having the smallest average distance to each of the four points, can be found through a simple formula: its x and y coordinates are the averages of the x's and y's of the four points, respectively. This is possible because the <em>$L_{2}$-Norm squared</em> has the mathematical property of converging to the mean, as demonstrated <a class="internal-link" href="#The equation converges to mean, and inherits the statistical properties of mean">above.</a></p>
<p>On the other hand, minimizing the <em>$L_{2}$-Norm</em> <a class="internal-link" href="#eq-2">eq-2</a> identifies a point that minimizes the sum of <a href="https://en.wikipedia.org/wiki/Euclidean_distance" target="_blank">Euclidean distances</a> to each point, known as the <a href="https://en.wikipedia.org/wiki/Geometric_median" target="_blank">spatial (geometric) median.</a> Note that the <em>$L_{2}$-Norm</em> essentially generalizes the Pythagorean theorem for n-dimensional spaces.</p>
<p>(Minimizing the <em>$L_{1}$-Norm</em> absolute loss <a class="internal-link" href="#eq-3">eq-3</a> reduces the <a href="https://en.wikipedia.org/wiki/Taxicab_geometry" target="_blank">Manhattan distances</a> from all points, a process without any specific Cartesian property associated with it)</p>
<p><a class="internal-link" href="#fig-3">Figure 3</a> below demonstrates how outliers affect the mean (centroid, <em>$L_{2}$-Norm squared</em>) compared to the median (spatial median, <em>$L_{2}$-Norm</em>). When an outlier with an extreme y-value is introduced, it shifts the mean upwards more significantly than the median. This difference arises because the median's objective function computes the square root of squared residuals, thereby reducing the impact of squared errors and assigning less weight to this outlier than the mean-based estimator. Given that such outliers are typically undesirable in regression model fitting, methods that either mitigate (like <a class="internal-link" href="#Theil-Sen regressor">Theil-Sen</a> and <a class="internal-link" href="#Huber regressor">Huber</a> regressors) or eliminate (like the <a class="internal-link" href="#RANSAC regressor">RANSAC</a> regressor) the influence of these outliers are preferred.</p>
<div class="row" id="fig-3" style="">
<div class="col"><img src="jupyter_images/mean vs median - effects of outliers.png"/></div>
<div class="col-12"><p class="image-description"><strong>Figure 3:</strong> The graph illustrates the robustness of median-based estimators compared to mean-based estimators in the presence of outliers. The lef plot shows a dataset without outliers. The mean (green plus) represents the centroid (a point that minimizes the average distances) of the four points and the median (orange star) shows the spatial median (a point that minimizes the sum of Euclidean distances). On the right, the introduction of an outlier (marked with a red 'x') significantly shifts the mean upwards, whereas the median remains relatively stable, highlighting its robustness. This contrast is due to the squared error in the mean calculation magnifying the effect of outliers, whereas the median—equivalent to the square root of the squared error—dampens their impact, thus remaining more stable in their presence.</p></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<div class="solution_panel closed">
<div class="solution_title">
<p class="solution_title_string">Source Code For Figure (3)</p>
<ul class="nav navbar-right panel_toolbox">
<li><a class="collapse-link"><i class="fa fa-chevron-down"></i></a></li>
</ul>
<div class="clearfix"></div>
</div>
<div class="solution_content">
<pre>
            <code class="language-python">
import matplotlib.pyplot as plt
import numpy as np
from scipy.optimize import minimize

x = [3, 9, 21, 25]
y = [12, 35, 16, 28]

x_outlier = [3, 9, 15, 21, 25]
y_outlier =[12, 35, 100, 16, 28]

ys = [y, y_outlier]
xs = [x, x_outlier]

# L2 loss - euclidean distance
def L2_objective_func(point, _x, _y):
    return np.sum(np.sqrt((point[0] - _x)**2 + (point[1] - _y)**2))

# L2 squared loss
def L2_squared_objective_func(point, _x, _y):
    return np.sum((_x - point[0])**2 + (_y - point[1])**2)

s = 150
init_guess = [0, 0]

fig, axes = plt.subplots(1, 2, figsize=(9, 4))
for i, (ax, x, y) in enumerate(zip(axes, xs, ys)):

    # calculates L2 euclidean loss. This results in a spatial median
    result_L2 = minimize(L2_objective_func, init_guess, args=(x, y), method='Nelder-Mead')

    # calculatse L2 squared loss. This results in a centroid (center of mass)
    result_L2_2 = minimize(L2_squared_objective_func, init_guess, args=(x, y), method='Nelder-Mead')

    # Proves that minimizing L2 squared loss results in mean
    assert np.allclose(result_L2_2.x, [np.mean(x), np.mean(y)], atol=1e-4)

    ax.scatter(x, y, s=s, edgecolor='blue', fc=(0, 0, 1, 0.05))
    _s1 = ax.scatter(result_L2.x[0], result_L2.x[1], s=s, 
                     label=r'Median:  $\underset{x, y}{\mathrm{argmin}} \sum^{n}_{i=1}\sqrt{(x_{i} - \hat{x}_{i})^{2} + (y_{i} - \hat{y}_{i})^2}$', marker='*')
    _s2 = ax.scatter(result_L2_2.x[0], result_L2_2.x[1], s=s, 
                     label=r'Mean:  $\underset{x, y}{\mathrm{argmin}} \sum^{n}_{i=1}[(x_{i} - \hat{x}_{i})^{2} + (y_{i} - \hat{y}_{i})^2]$', marker='+', lw=3)

    xmax = 30
    ymax = 110
    ax.set_xlim(0 - 0.05 * xmax, xmax)
    ax.set_ylim(0 - 0.05 * ymax, ymax)

    ax.grid(axis='both', linestyle='--', color='#acacac', alpha=0.5)
    ax.spines['top'].set_visible(False)
    ax.spines['right'].set_visible(False)

    if i == 0:
        x_increment = 1.2
    else:
        x_increment = 1.5

    ax.text(result_L2_2.x[0] - x_increment, result_L2_2.x[1] + 6, '(%d, %d)' % (result_L2_2.x[0], result_L2_2.x[1]), color=_s2.get_facecolor()[0], ha='center')
    ax.text(result_L2.x[0] + x_increment, result_L2.x[1] + 6, '(%d, %d)' % (result_L2.x[0], result_L2.x[1]), color=_s1.get_facecolor()[0], ha='center')
    ax.text(0.98, 0.08, 'aegis4048.github.io', fontsize=10, ha='right', transform=ax.transAxes, color='grey', alpha=0.5)

    ax.text(result_L2_2.x[0] - x_increment, result_L2_2.x[1] + 14, 'Mean', color=_s2.get_facecolor()[0], ha='center')
    ax.text(result_L2.x[0] + x_increment, result_L2.x[1] + 14, 'Median', color=_s1.get_facecolor()[0], ha='center')

    ax.set_ylabel('Y')
    ax.set_xlabel('X')

axes[1].scatter(x[2], y_outlier[2], s=100, marker='x')
axes[1].text(x[2], y_outlier[2] - 10, 'outlier', color='red', ha='center')


handles, labels = [], []
for ax in axes:
    for h, l in zip(*ax.get_legend_handles_labels()):
        handles.append(h)
        labels.append(l)
    break
fig.legend(handles, labels, fontsize=10.5, ncol=2, loc='lower center', bbox_to_anchor=(0.5, -0.14), frameon=True)

def setbold(txt):
    return ' '.join([r"$\bf{" + item + "}$" for item in txt.split(' ')])

bold_txt = setbold('Mean vs Median, ')
plain_txt = r'effect of outliers on median- vs mean-based estimators'
fig.suptitle(bold_txt + plain_txt, verticalalignment='top', x=0, horizontalalignment='left', fontsize=12, y=0.96)
yloc = 1.035
ax.annotate('', xy=(0.01, yloc + 0.01), xycoords='figure fraction', xytext=(1.02, yloc + 0.01),
            arrowprops=dict(arrowstyle="-", color='k', lw=0.7))

fig.tight_layout()
            </code>
        </pre>
</div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="2.-Robust-Models">2. Robust Models<a class="anchor-link" href="#2.-Robust-Models">¶</a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<div id="RANSAC regressor"></div><h3 id="2.1.-RANSAC-regressor">2.1. RANSAC regressor<a class="anchor-link" href="#2.1.-RANSAC-regressor">¶</a></h3><p><strong>RAN</strong>dom <strong>SA</strong>mple <strong>C</strong>onsensus (RANSAC) is an iterative algorithm that separates inliers vs. outliers, and fits a regression model only using the separated inliers.</p>
<div><hr/></div>
<div id="RANSAC Steps"></div><p><em>(Note that these steps are based on <a href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.RANSACRegressor.html" target="_blank">sklearn's implementation of RANSAC regressor.</a>)</em></p>
<div class="ordered-list">
<h2>Steps</h2>
<ol>
<li>Start the 1st iteration. Randomly select $m$ (<code>min_samples</code>) data points from the whole data set. $m$ = number of features + 1. Therefore, for 2D linear regression, $m=2$. No duplicates.</li>
<li>Fit a linear regression model on the selected $m$ data points. The objective function being minimized is the <i>$L_{1}$-Norm</i> absolute loss <a class="internal-link" href="#eq-3">eq-3</a> (<code>loss='absolute_error</code> by default. Alternatively you could try <code>loss='squared_error'</code>). When $m=2$, the slope generalizes to a simple $(y_{j}−y_{i})/(x_{j}−x_{i})$.</li>
<li>Classify a point as an inlier if it falls within a certain threshold value (<code>residual_threshold</code>) from the fitted regression line. By default, the threshold is the <a class="internal-link" href="#Advanced: Mean Absolute Deviation (MAD) threshold">Median Absolute Deviation (MAD)</a> (auto-computed) of the whole data. Repeat this for every data point in the original data set. </li>
<li>Compute the value of $N$ (<a class="internal-link" href="#Advanced: Required iteration ($k$) calculation">required number of iterations</a>). </li>
<li>Start the 2nd iteration. Repeat steps 1-4.</li>
<li>If the newly fitted model from the 2nd iteration contains more inliers, replace the previous model with the new one. Otherwise keep the old model from the 1st iteration. If both the old and new models have the same number of inliers, pick the one with a better $R^2$ score.</li>
<li>Compare the old and new $N$ values, and pick a smaller one: <code>min(old_N, new_N)</code></li>
<li>Repeat steps 1-7 until the $N$-th iteration. $N$ can be indirectly controlled by tweaking <code>stop_probability</code> and <code>min_samples</code> arguments. </li>
<li>Iterations are finished. Fit a new regression model using only the final inliers. Return the fitted model.</li>
</ol>
</div><div id="Visual demonstrations"></div><h4 id="2.1.1.-Visual-demonstrations">2.1.1. Visual demonstrations<a class="anchor-link" href="#2.1.1.-Visual-demonstrations">¶</a></h4><div class="row" id="fig-4" style="margin-top: 0px;">
<div class="col"><img src="jupyter_images/ransac_1.png"/></div>
</div><div class="row" style="margin-top: 0px;">
<div class="col"><img src="jupyter_images/ransac_2.png"/></div>
</div><div class="row" style="margin-top: 0px;">
<div class="col"><img src="jupyter_images/ransac_3.png"/></div>
</div>
<div class="col-12"><p class="image-description"><strong>Figure 4:</strong> Simplified visual demonstration of RANSAC algorithm $m=2$ (<code>min_samples=2</code>).</p></div>
</div><div><hr/></div><div id="Advanced: Mean Absolute Deviation (MAD) threshold"></div><h4 id="2.1.2.-Median-Absolute-Deviation-(MAD)-threshold">2.1.2. Median Absolute Deviation (MAD) threshold<a class="anchor-link" href="#2.1.2.-Median-Absolute-Deviation-(MAD)-threshold">¶</a></h4><p>The default threshold for RANSAC employs the Median Absolute Deviation (MAD), a value calculated once before the iterative process begins. Just as how the standard deviation serves as a measure of spread around the mean, MAD represents variability around the median. This choice is motivated by the robustness of the median over the mean as a measure of central tendency as explained <a class="internal-link" href="#The equation converges to mean, and inherits the statistical properties of mean">above</a>.</p>
<p>MAD can be computed with the following codes:</p>
</div>
</div>

<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In [5]:</div>
<div class="inner_cell">
<div class="input_area">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">3</span><span class="p">,</span>  <span class="mi">6</span><span class="p">,</span> <span class="mi">9</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">12</span><span class="p">,</span> <span class="mi">15</span><span class="p">,</span> <span class="mi">17</span><span class="p">,</span> <span class="mi">18</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">24</span><span class="p">,</span> <span class="mi">25</span><span class="p">,</span> <span class="mi">26</span><span class="p">,</span> <span class="mi">27</span><span class="p">])</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">8</span><span class="p">,</span> <span class="mi">12</span><span class="p">,</span> <span class="mf">15.5</span><span class="p">,</span> <span class="mf">13.5</span><span class="p">,</span> <span class="mi">17</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">18</span><span class="p">,</span> <span class="mi">24</span><span class="p">,</span> <span class="mf">24.5</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">9</span><span class="p">,</span> <span class="mi">7</span><span class="p">])</span>

<span class="n">MAD</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">median</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">y</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">median</span><span class="p">(</span><span class="n">y</span><span class="p">)))</span>
<span class="n">MAD</span>
</pre></div>
</div>
</div>
</div>
<div class="output_wrapper">
<div class="output">
<div class="output_area">
<div class="prompt output_prompt">Out[5]:</div>
<div class="output_text output_subarea output_execute_result">
<pre>5.5</pre>
</div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<div class="row" id="fig-5" style="margin-top: -15px;">
<div class="col"><img src="jupyter_images/MAD-gumbel.png"/></div>
<div class="col-12"><p class="image-description"><strong>Figure 5:</strong> Median Absolute Deviation (MAD) is a measure centered around the median, akin to how the standard deviation ($\pm\sigma$) around the mean in a normal distribution encompasses 68.2% of the population. Importantly, the median provides a more accurate representation of central tendency in the presence of skewness or outliers, compared to the mean.</p></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<div class="solution_panel closed">
<div class="solution_title">
<p class="solution_title_string">Source Code For Figure (5)</p>
<ul class="nav navbar-right panel_toolbox">
<li><a class="collapse-link"><i class="fa fa-chevron-down"></i></a></li>
</ul>
<div class="clearfix"></div>
</div>
<div class="solution_content">
<pre>
            <code class="language-python">
import numpy as np
import matplotlib.pyplot as plt
from scipy.stats import gumbel_r, iqr


data = gumbel_r.rvs(size=100000, random_state=1)
x = np.linspace(min(data), max(data), 100000)
pdf = gumbel_r.pdf(x)

M = np.median(data)
MAD = np.median(np.abs(data - M))
Mean = np.mean(data)

perc_left = gumbel_r.cdf(M - MAD)
perc_right = 1 - gumbel_r.cdf(M + MAD)
perc_center = 1 - perc_left - perc_right

def y_finder(y_arr, x_arr, x_val, idx_increment=0):
    idx = np.argmin(np.abs(np.array(x_arr) - x_val))
    return y_arr[idx + idx_increment]

fig, ax = plt.subplots(figsize=(8, 3))

ax.plot(x, pdf, 'k-', lw=2, zorder=99)

ax.fill_between(x, pdf, where=((x >= M - MAD) & (x <= M + MAD)), color='skyblue', alpha=0.3)
ax.vlines(x=M - MAD, ymax=y_finder(pdf, x, M - MAD), ymin=0, ls='--')
ax.vlines(x=M + MAD, ymax=y_finder(pdf, x, M + MAD), ymin=0, ls='--')
ax.vlines(x=M, ymax=y_finder(pdf, x, M), ymin=0, color='r', ls='--')

ax.text(M - MAD - 0.1, 0.01, '%.1f%s' % (perc_left * 100, r'%'), ha='right', va='bottom')
ax.text(M + MAD + 0.1, 0.01, '%.1f%s' % (perc_right * 100, r'%'), ha='left', va='bottom')
ax.text(M, 0.15, '%.1f%s' % (perc_center * 100, r'%'), ha='center', va='center', fontsize=20)
ax.text(M - MAD - 0.1, y_finder(pdf, x, M - MAD), 'Median-MAD', ha='right', va='center', color='#1f77b4', fontsize=13)
ax.text(M + MAD + 0.1, y_finder(pdf, x, M + MAD), 'Median+MAD', ha='left', va='center', color='#1f77b4', fontsize=13)
ax.text(M + 0.1, y_finder(pdf, x, M), 'Median', ha='left', va='center', color='r', fontsize=13)
ax.text(0.01, 0.15, 'aegis4048.github.io', fontsize=10, ha='left', transform=ax.transAxes, color='grey', alpha=0.5)

ax.set_ylim(0, None)
ax.set_xlim(-2.5, 6)
ax.spines['top'].set_visible(False)
ax.spines['right'].set_visible(False)

def setbold(txt):
    return ' '.join([r"$\bf{" + item + "}$" for item in txt.split(' ')])

bold_txt = setbold('Mean Absolute Deviation (MAD), ')
plain_txt = r'measure of central tendency using median; Gumbel distribution'
fig.suptitle(bold_txt + plain_txt, verticalalignment='top', x=0, horizontalalignment='left', fontsize=11, y=0.95)
yloc = 0.85
ax.annotate('', xy=(0.01, yloc + 0.01), xycoords='figure fraction', xytext=(1.02, yloc + 0.01),
            arrowprops=dict(arrowstyle="-", color='k', lw=0.7))

fig.tight_layout()     
            </code>
        </pre>
</div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<div id="Advanced: Required iteration ($k$) calculation" style="margin-top: -15px;"></div><h4 id="2.1.3.-Required-iteration-calculation">2.1.3. Required iteration calculation<a class="anchor-link" href="#2.1.3.-Required-iteration-calculation">¶</a></h4><p>Another important component of the RANSAC is the required iteration ($N$).</p>
<div id="eq-4" style="font-size: 1rem;">
$$ N >= \frac{log(1-p)}{log(1-e^m)} \tag{4}$$
</div><div class="eq-terms">
<div class="row eq-terms-where">where</div>
<div class="row">
<div class="col-2">$N$</div>
<div class="col-1 max-width-3">:</div>
<div class="col-9">required number of iterations.</div>
</div>
<div class="row">
<div class="col-2">$p$</div>
<div class="col-1 max-width-3">:</div>
<div class="col-9">probability (confidence) that one outlier-free sample is generated. (default=0.99). </div>
</div>
<div class="row">
<div class="col-2">$e$</div>
<div class="col-1 max-width-3">:</div>
<div class="col-9">inlier ratio (# of inliers / # of entire data points). This may change every iteration.</div>
</div>
<div class="row">
<div class="col-2">$m$</div>
<div class="col-1 max-width-3">:</div>
<div class="col-9">minimum number of samples chosen randomly from origianl data (default=2).</div>
</div>
</div><p>$N$ is rounded up to the nearest integer. Note that you can <em>indirectly</em> control the value of $N$ by changing <code>stop_probability</code> and <code>min_samples</code> in the sklearn package. It can be computed with the following codes:</p>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In [6]:</div>
<div class="inner_cell">
<div class="input_area">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="n">p</span> <span class="o">=</span> <span class="mf">0.99</span>       <span class="c1"># sklearn default</span>
<span class="n">m</span> <span class="o">=</span> <span class="mi">2</span>          <span class="c1"># sklearn default for 2D linear regression</span>
<span class="n">w</span> <span class="o">=</span> <span class="mf">0.6</span>        <span class="c1"># this may change every iteration</span>

<span class="n">N</span> <span class="o">=</span> <span class="nb">abs</span><span class="p">(</span><span class="nb">float</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ceil</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">p</span><span class="p">)</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">w</span> <span class="o">**</span> <span class="n">m</span><span class="p">))))</span>
<span class="n">N</span>
</pre></div>
</div>
</div>
</div>
<div class="output_wrapper">
<div class="output">
<div class="output_area">
<div class="prompt output_prompt">Out[6]:</div>
<div class="output_text output_subarea output_execute_result">
<pre>11.0</pre>
</div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<div><hr/></div><div id="More detailed visual demonstrations"></div><h4 id="2.1.4.-More-detailed-visual-demonstrations">2.1.4. More detailed visual demonstrations<a class="anchor-link" href="#2.1.4.-More-detailed-visual-demonstrations">¶</a></h4><div class="row" id="fig-6" style="margin-top: 15px;">
<div class="col"><img src="jupyter_images/ransac_advanced_1st.png"/></div>
<div class="col-12"><p class="image-description"><strong>Figure 6:</strong> First iteration in RANSAC. Left plot shows randomly selected 2 data points (blue dots) for regression. The right plot displays residuals, with the dashed red line indicating MAD. Points with residuals exceeding MAD are marked as outliers. Initially, $N$ is set to 100 <code>max_trials=100</code>. After the first iteration, $N$ is computed to be 20 using <a class="internal-link" href="#eq-4">eq-4</a>. Since 20 is smaller than 100, $N$ is updated to 20.  A total of 6 inliers are found in this round.</p></div>
</div><div class="row" id="fig-7" style="margin-top: 15px;">
<div class="col"><img src="jupyter_images/ransac_advanced_2nd.png"/></div>
<div class="col-12"><p class="image-description"><strong>Figure 7:</strong> Second iteration. The new model has more inliers (9) then the previous model (6). Model is updated. The inlier ratio ($w$ in <a class="internal-link" href="#eq-4">eq-4</a>) changes. $N$ is computed to be 8 in this iteration. Since 8 is smaller than 20 from the previous iteration, $N$ is updated to 8.</p></div>
</div><div class="row" id="fig-8" style="margin-top: 15px;">
<div class="col"><img src="jupyter_images/ransac_advanced_3rd.png"/></div>
<div class="col-12"><p class="image-description"><strong>Figure 8:</strong> Third iteration. Although the number of inliers equals the previous iteration, a visual inspection of the left plot reveals that the current model is incorrect. To decide whether an update is necessary, we compare the $R^2$ scores. Since the previous model has a higher $R^2$ (0.92) than the current one (0.62), no model update is performed. This process continues until the $N$-th (8th) iteration, assuming that $N$ remains constant. Once all iterations are completed, the final model is fitted using the selected inliers.</p></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<div class="solution_panel closed">
<div class="solution_title">
<p class="solution_title_string">Source Code For Figures (6-8)</p>
<ul class="nav navbar-right panel_toolbox">
<li><a class="collapse-link"><i class="fa fa-chevron-down"></i></a></li>
</ul>
<div class="clearfix"></div>
</div>
<div class="solution_content">
<pre>
            <code class="language-python">

import numpy as np
import matplotlib.pyplot as plt
from sklearn import linear_model
from sklearn.linear_model import RANSACRegressor

X = np.array([3,  6, 9, 10, 12, 15, 17, 18, 20, 24, 25, 26, 27]).reshape(-1, 1)
y = np.array([8, 12, 15.5, 13.5, 17, 20, 18, 24, 24.5, 8, 6, 9, 7])
MAD = np.median(np.abs(y - np.median(y)))

N_PREV = 100

def fit_and_plot(X, y, MAD, random_state, selected_idx, stage, update_status):

    global N_PREV

    X_selected = X[selected_idx]
    y_selected = y[selected_idx]

    ols = linear_model.LinearRegression()
    model = ols.fit(X_selected, y_selected)
    y_pred = model.predict(X)
    residuals = np.abs(y - y_pred)

    ransac = RANSACRegressor(max_trials=1, random_state=random_state).fit(X, y) # deal with rs=11
    y_pred_ransac = ransac.predict(X)
    num_inliers = np.sum(ransac.inlier_mask_)
    R2 = ransac.score(X[ransac.inlier_mask_], y[ransac.inlier_mask_])

    p = 0.99
    m = 2
    w = num_inliers / len(y)
    N = abs(float(np.ceil(np.ceil(np.log(1 - p) / np.log(1 - w ** m)))))

    ####################################################################################

    fig, axes = plt.subplots(1, 2, figsize=(10, 4))

    axes[0].scatter(X, y, s=100, color='#70ad47', label='Inliers')
    axes[0].scatter(X[~ransac.inlier_mask_], y[~ransac.inlier_mask_], s=100, color='red', label='Outliers')
    axes[0].scatter(X_selected, y_selected, s=100, color='#4472c4', label='Random')

    axes[0].axline(xy1=(X_selected[0][0], y_selected[0]), xy2=(X_selected[-1][0], y_selected[-1]), color='#7030a0', label='Regression on Random')
    axes[0].legend(loc='upper left', ncol=2, fontsize=9)
    axes[0].set_title('RANSAC: %s iteration' % stage, fontsize=10)

    scatter_dict = {'s':100, 'fc':'none', 'linewidth':1.5}
    axes[1].scatter(X[ransac.inlier_mask_], residuals[ransac.inlier_mask_], edgecolors='k', label='Inliers', **scatter_dict)
    axes[1].scatter(X[~ransac.inlier_mask_], residuals[~ransac.inlier_mask_], edgecolors='r', label='Outliers', **scatter_dict)
    axes[1].scatter(X[selected_idx], residuals[selected_idx], edgecolors='b', label='Random', **scatter_dict)

    axes[1].axhline(y=MAD, color='r', linestyle='--')
    axes[1].legend(loc='upper left', ncol=3, fontsize=9)
    axes[1].text(-0.5, MAD + 0.5, 'Threshold (MAD) = 5.50', fontsize=9, ha='left', va='bottom', color='r', alpha=1)
    axes[1].text(0, 25, '# of Inliers = %d' % num_inliers, ha='left', weight='bold', fontsize=12)
    axes[1].text(0, 22.75, update_status, ha='left', fontsize=10)
    axes[1].text(0, 20.5, 'min($N_{old}$, $N_{new}$)= min(%d, %d)= %d' % (N_PREV, N, N), ha='left', fontsize=10)
    axes[1].text(0, 18.25, '$R^2$ = %.2f' % round(R2, 2), ha='left', fontsize=10)
    axes[1].set_title('Residuals of regression', fontsize=10)

    xmax = 30
    ymax = 30
    for ax in axes:
        ax.spines.top.set_visible(False)
        ax.spines.right.set_visible(False)
        ax.set_xlim(0 - 0.05 * xmax, xmax + 0.05 * xmax)
        ax.set_ylim(0 - 0.05 * ymax, ymax + 0.05 * ymax)

    N_PREV = N

fit_and_plot(X, y, MAD, random_state=3, selected_idx=[4, 6], stage='1st', update_status='model updated (initial)')
fit_and_plot(X, y, MAD, random_state=11, selected_idx=[0, 6], stage='2nd', update_status='model updated')
fit_and_plot(X, y, MAD, random_state=29, selected_idx=[-9, -1], stage='3rd', update_status='model NOT updated')
            </code>
        </pre>
</div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<div id="RANSAC code snippets" style="margin-top: -15px;"></div><h4 id="2.1.5.-RANSAC-code-snippets">2.1.5. RANSAC code snippets<a class="anchor-link" href="#2.1.5.-RANSAC-code-snippets">¶</a></h4><p>For quick copy-paste, replace <code>X</code> and <code>y</code> with your own data. Make sure to reshape your <code>X</code> so that it is a 2D <code>numpy.ndarray</code> object with shape like <code>(13, 1)</code>.</p>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In [7]:</div>
<div class="inner_cell">
<div class="input_area">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">RANSACRegressor</span>
<span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">linear_model</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="c1"># sample data</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span> <span class="mf">0.15</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.34</span><span class="p">,</span> <span class="mf">0.32</span><span class="p">,</span> <span class="mf">0.43</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.4</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.04</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.51</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">,</span> <span class="mf">0.47</span><span class="p">,</span> <span class="mf">0.12</span><span class="p">,</span> <span class="mf">0.08</span><span class="p">,</span> <span class="mf">0.04</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.08</span><span class="p">,</span> 
              <span class="o">-</span><span class="mf">0.23</span><span class="p">,</span> <span class="mf">0.08</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.03</span><span class="p">,</span> <span class="mf">0.03</span><span class="p">,</span> <span class="mf">0.04</span><span class="p">,</span> <span class="mf">0.01</span><span class="p">,</span> <span class="mf">0.06</span><span class="p">,</span> <span class="mf">0.03</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.04</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.18</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.19</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.06</span><span class="p">,</span> 
              <span class="o">-</span><span class="mf">0.26</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.16</span><span class="p">,</span> <span class="mf">0.13</span><span class="p">,</span> <span class="mf">0.09</span><span class="p">,</span> <span class="mf">0.03</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.03</span><span class="p">,</span> <span class="mf">0.04</span><span class="p">,</span> <span class="mf">0.14</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.01</span><span class="p">,</span> <span class="mf">0.4</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.06</span><span class="p">,</span> <span class="mf">0.15</span><span class="p">,</span> <span class="mf">0.08</span><span class="p">,</span> 
              <span class="mf">0.05</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.15</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.09</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.15</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.11</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.07</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.19</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.06</span><span class="p">,</span> <span class="mf">0.17</span><span class="p">,</span> <span class="mf">0.23</span><span class="p">,</span> <span class="mf">0.18</span><span class="p">])</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">17.44</span><span class="p">,</span> <span class="mf">25.46</span><span class="p">,</span> <span class="mf">18.61</span><span class="p">,</span> <span class="mf">26.07</span><span class="p">,</span> <span class="mf">24.96</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.22</span><span class="p">,</span> <span class="mf">26.45</span><span class="p">,</span> <span class="mf">26.5</span><span class="p">,</span> <span class="mf">20.57</span><span class="p">,</span> <span class="mf">3.08</span><span class="p">,</span> <span class="mf">35.9</span> <span class="p">,</span> <span class="mf">32.47</span><span class="p">,</span> 
              <span class="mf">20.84</span><span class="p">,</span> <span class="mf">13.37</span><span class="p">,</span> <span class="mf">42.44</span><span class="p">,</span> <span class="mf">27.23</span><span class="p">,</span> <span class="mf">35.65</span><span class="p">,</span> <span class="mf">29.51</span><span class="p">,</span> <span class="mf">31.28</span><span class="p">,</span> <span class="mf">41.34</span><span class="p">,</span> <span class="mf">32.19</span><span class="p">,</span> <span class="mf">33.67</span><span class="p">,</span> <span class="mf">25.64</span><span class="p">,</span> <span class="mf">9.3</span><span class="p">,</span> 
              <span class="mf">14.63</span><span class="p">,</span> <span class="mf">25.1</span><span class="p">,</span> <span class="mf">4.69</span><span class="p">,</span> <span class="mf">14.42</span><span class="p">,</span> <span class="mf">47.53</span><span class="p">,</span> <span class="mf">33.82</span><span class="p">,</span> <span class="mf">32.2</span> <span class="p">,</span> <span class="mf">24.81</span><span class="p">,</span> <span class="mf">32.64</span><span class="p">,</span> <span class="mf">45.11</span><span class="p">,</span> <span class="mf">26.76</span><span class="p">,</span> <span class="mf">68.01</span><span class="p">,</span> 
              <span class="mf">23.39</span><span class="p">,</span> <span class="mf">43.49</span><span class="p">,</span> <span class="mf">37.88</span><span class="p">,</span> <span class="mf">36.01</span><span class="p">,</span> <span class="mf">16.32</span><span class="p">,</span> <span class="mf">19.77</span><span class="p">,</span> <span class="mf">16.34</span><span class="p">,</span> <span class="mf">19.57</span><span class="p">,</span> <span class="mf">29.28</span><span class="p">,</span> <span class="mf">16.62</span><span class="p">,</span> <span class="mf">24.39</span><span class="p">,</span> <span class="mf">43.77</span><span class="p">,</span> 
              <span class="mf">50.46</span><span class="p">,</span> <span class="mf">47.09</span><span class="p">])</span>

<span class="c1"># fit and predict: RANSAC</span>
<span class="n">ransac</span> <span class="o">=</span> <span class="n">RANSACRegressor</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>      
<span class="n">y_pred_ransac</span> <span class="o">=</span> <span class="n">ransac</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

<span class="c1"># retrieve the fitted parameters</span>
<span class="n">coefs</span> <span class="o">=</span> <span class="n">ransac</span><span class="o">.</span><span class="n">estimator_</span><span class="o">.</span><span class="n">coef_</span>              <span class="c1"># RANSAC fits many regression models. The ".estimator_" attribute</span>
<span class="n">intercept</span> <span class="o">=</span> <span class="n">ransac</span><span class="o">.</span><span class="n">estimator_</span><span class="o">.</span><span class="n">intercept_</span>     <span class="c1"># has the final model fitted. </span>

<span class="c1"># seperate into inliers vs. outliers</span>
<span class="n">X_inlier</span> <span class="o">=</span> <span class="n">X</span><span class="p">[</span><span class="n">ransac</span><span class="o">.</span><span class="n">inlier_mask_</span> <span class="p">]</span>
<span class="n">y_inlier</span> <span class="o">=</span> <span class="n">y</span><span class="p">[</span><span class="n">ransac</span><span class="o">.</span><span class="n">inlier_mask_</span> <span class="p">]</span>
<span class="n">X_outlier</span> <span class="o">=</span> <span class="n">X</span><span class="p">[</span><span class="o">~</span><span class="n">ransac</span><span class="o">.</span><span class="n">inlier_mask_</span> <span class="p">]</span>
<span class="n">y_outlier</span> <span class="o">=</span> <span class="n">y</span><span class="p">[</span><span class="o">~</span><span class="n">ransac</span><span class="o">.</span><span class="n">inlier_mask_</span> <span class="p">]</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">"RANSAC -----------------------------------</span><span class="se">\n</span><span class="s2">"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"Coefficients         :"</span><span class="p">,</span> <span class="n">coefs</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"Intercept            :"</span><span class="p">,</span> <span class="n">intercept</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"# of inliers         :"</span><span class="p">,</span> <span class="nb">sum</span><span class="p">(</span><span class="n">ransac</span><span class="o">.</span><span class="n">inlier_mask_</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"Fraction of inliers  :"</span><span class="p">,</span> <span class="nb">sum</span><span class="p">(</span><span class="n">ransac</span><span class="o">.</span><span class="n">inlier_mask_</span><span class="p">)</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">y</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"</span><span class="se">\n</span><span class="s2">------------------------------------------"</span><span class="p">)</span>

<span class="c1"># fit and predict: Ordinary Least Squares</span>
<span class="n">ols</span> <span class="o">=</span> <span class="n">linear_model</span><span class="o">.</span><span class="n">LinearRegression</span><span class="p">()</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="n">y_pred_ols</span> <span class="o">=</span> <span class="n">ols</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

<span class="c1"># plot</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">'RANSAC Regressor'</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X_inlier</span><span class="p">,</span> <span class="n">y_inlier</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s1">'green'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">'Inliers'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X_outlier</span><span class="p">,</span> <span class="n">y_outlier</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s1">'red'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">'Outliers'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y_pred_ols</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">'OLS'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y_pred_ransac</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">'RANSAC'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">();</span>
</pre></div>
</div>
</div>
</div>
<div class="output_wrapper">
<div class="output">
<div class="output_area">
<div class="prompt"></div>
<div class="output_subarea output_stream output_stdout output_text">
<pre>RANSAC -----------------------------------

Coefficients         : [92.50723098]
Intercept            : 30.331391269710938
# of inliers         : 40
Fraction of inliers  : 0.8

------------------------------------------
</pre>
</div>
</div>
<div class="output_area">
<div class="prompt"></div>
<div class="output_png output_subarea">
<img src="data:image/png;base64,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
"/>
</div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<div class="highlights red-theme" id="warning-RANSAC">
<div class="highlights-title red-theme">WARNING!</div>
<div class="highlights-content red-theme">RANSAC relies on random sampling, which means that with a larger dataset (exceeding 50 points), your regression outcomes can differ due to this inherent randomness. This approach offers the advantage of flexibility for iterative adjustments (as observed when running the same code multiple times without a fixed random seed) when turning the model. However, it also leads to variability in outcomes, which can be problematic for replicating results or sharing code. To achieve consistent results when you are done tuning and trying to report, it's advisable to set a random seed, as in: <code>model = RANSACRegressor(random_state=3)</code>.
    </div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<div id="Huber regressor"></div><h3 id="2.2.-Huber-regressor">2.2. Huber regressor<a class="anchor-link" href="#2.2.-Huber-regressor">¶</a></h3><p>The Huber regressor doesn't eliminate outliers but mitigates their effect, making it a preferred choice when you have reason to believe that so-called outliers may have relevance in the regression analysis. In contrast to other robust regression techniques like RANSAC or Theil-Sen, which involve substantial structural modifications in their algorithms compared to default linear regression, such as subsampling and fitting regressions on these subsets, the Huber regressor differes only in its choice of loss function while retaining the fundamental structure of default linear regression.</p>
<div><hr/></div><div id="Huber loss function"></div><h4 id="2.2.1.-Huber-loss-function">2.2.1. Huber loss function<a class="anchor-link" href="#2.2.1.-Huber-loss-function">¶</a></h4><p>The original Huber loss is a piece-wise function shown in <a class="internal-link" href="#eq-5">eq-5</a>. This is the most generic search result you will see if you google Huber regressor. Note that the term, $y - \hat{y}$, are often referred to as residuals (prediction error).</p>
<div id="eq-5" style="font-size: 1rem;">
$$ L_{\delta} (y - \hat{y})=   \left\{
    \begin{array}{ll}
          \frac{1}{2}(y - \hat{y})^{2} & \text{for } |y - \hat{y}| \leq \delta \text{ (inlier)},  \\
          \delta \cdot (|y - \hat{y}| - \frac{1}{2}\delta) &  \text{otherwise (outlier)}.  \\
    \end{array} 
    \right.  
    \tag{5}$$
</div><div class="eq-terms">
<div class="row eq-terms-where">where</div>
<div class="row">
<div class="col-2">$y$</div>
<div class="col-1 max-width-3">:</div>
<div class="col-9">observation, original data point</div>
</div>
<div class="row">
<div class="col-2">$\hat{y}$</div>
<div class="col-1 max-width-3">:</div>
<div class="col-9">prediction</div>
</div>
<div class="row">
<div class="col-2">$\delta$</div>
<div class="col-1 max-width-3">:</div>
<div class="col-9">absolute parameter that controls the number of outliers</div>
</div>
<div class="row">
<div class="col-2">$L$</div>
<div class="col-1 max-width-3">:</div>
<div class="col-9">loss function</div>
</div>
</div><p>HOWEVER, a challenge with the equation <a class="internal-link" href="#eq-5">eq-5</a> is that the $\delta$ parameter is in absolute scale. For instance, if 95% of your residuals fall within the range of (45, 60), setting $\delta=1.35$ equates to a 2.25% to 3.00% residual tolerance, which works well. But when dealing with a different dataset where the residuals range from (4355, 13205), applying the same $\delta=1.35$ results in an impractical ~0.001% residual tolerance. Therefore, a unique $\delta$ value would be needed for each dataset's residual range. To address this, scikit-learn modifies the original Huber loss by using the scale parameter $\sigma$, allowing for a consistent threshold parameter that operates on a <i>relative</i> scale instead of an absolute one.</p>
<p>The loss function that <a href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.HuberRegressor.html#sklearn.linear_model.HuberRegressor" target="_blank">HuberRegressor</a> minimizes is given by:</p>
<div id="eq-6" style="font-size: 1rem;">
$$
\begin{align}
     \underset{w, \sigma}{\text{argmin}}\left[\sum^{n}_{i=1}\left(\sigma + L_{\epsilon}\left(\frac{X_{i}w - y_{i}}{\sigma}\right)\sigma\right) + \alpha ||w||_{2}^{2}\right]
     \tag{6}
\end{align}
$$</div><p>where</p>
<div id="eq-7" style="font-size: 1rem;">
$$ L_{\epsilon}(z)=   \left\{
    \begin{array}{ll}
          z^{2} & \text{for } z \leq \epsilon \text{ (inlier)},  \\
          2\epsilon |z| - \epsilon^{2} &  \text{otherwise (outlier)}.  \\
    \end{array} 
    \right.  
    \tag{7}$$
</div><div class="eq-terms">
<div class="row eq-terms-where">where</div>
<div class="row">
<div class="col-2">$w$</div>
<div class="col-1 max-width-3">:</div>
<div class="col-9">linear regression arguments. For 2D liear regression, it's (slope, intercept)</div>
</div>
<div class="row">
<div class="col-2">$\sigma$</div>
<div class="col-1 max-width-3">:</div>
<div class="col-9">scaling parameter. This allows the residual threshold to be on a relative scale so it can work for all residual ranges.</div>
</div>
<div class="row">
<div class="col-2">$i$</div>
<div class="col-1 max-width-3">:</div>
<div class="col-9">index of a data point</div>
</div>
<div class="row">
<div class="col-2">$n$</div>
<div class="col-1 max-width-3">:</div>
<div class="col-9">number of samples.</div>
</div>
<div class="row">
<div class="col-2">$L$</div>
<div class="col-1 max-width-3">:</div>
<div class="col-9">Huber loss function</div>
</div>
<div class="row">
<div class="col-2">$\epsilon$</div>
<div class="col-1 max-width-3">:</div>
<div class="col-9">relative <code>epsilon</code> parameter that controls the number of outliers.</div>
</div>
<div class="row">
<div class="col-2">$X$</div>
<div class="col-1 max-width-3">:</div>
<div class="col-9">independent variable. Note that $X_{i}w$ is equivalent to prediction $\hat{y}_{i}$</div>
</div>
<div class="row">
<div class="col-2">$y$</div>
<div class="col-1 max-width-3">:</div>
<div class="col-9">observation, original data point</div>
</div>
<div class="row">
<div class="col-2">$\alpha$</div>
<div class="col-1 max-width-3">:</div>
<div class="col-9">learning rate</div>
</div>
<div class="row">
<div class="col-2">$z$</div>
<div class="col-1 max-width-3">:</div>
<div class="col-9">residual divided by $\sigma$</div>
</div>
</div><p>The loss function maybe a bit challenging to interpret for beginners. <a class="internal-link" href="#fig-9">Figure 9</a> below shows how the equation breaks down. Recall that $L_{\epsilon}$ is a piece-wise function that applies different formula depending on whether the data point is identified as an outlier or not by the $\epsilon$ parameter, which is set to be 1.35 (recommended) by scikit learn.</p>
<div class="row" id="fig-9" style="">
<div class="col"><img src="jupyter_images/huber explained.png"/></div>
<div class="col-12"><p class="image-description"><strong>Figure 9:</strong> Explanation of <a class="internal-link" href="#eq-9">eq-9</a>.</p></div>
</div><div><hr/></div><div id="Motivation"></div><h4 id="2.2.2.-Motivation">2.2.2. Motivation<a class="anchor-link" href="#2.2.2.-Motivation">¶</a></h4><p>The two most commonly used loss functions are squared loss, $L(y - \hat{y}) = (y - \hat{y})^2$ and absolute loss, $L(y - \hat{y}) = |y - \hat{y}|$. While the squared loss is more accurate, it has the disadvantage that it has the tendency to be dominated by outliers. This susceptibility arises because squared residuals magnify the effect of outliers, as the residuals are squared, leading to an exponential increase in loss as residuals grow. In contrast, absolute loss grows linearly with residuals, rendering it more robust to outliers.</p>
<p>The Huber regressor offers a compelling compromise, leveraging the strengths of squared and absolute loss functions while mitigating their weaknesses. It employs squared loss for small-residual data points (inliers) and absolute loss for large-residual data points (outliers). The distinction between inliers and outliers is governed by the $\delta$ parameter, set to 1.35 by default in scikit-learn.</p>
<div id="Parameter tuning"></div><h4 id="2.2.3.-Parameter-tuning-($\delta$)">2.2.3. Parameter tuning ($\delta$)<a class="anchor-link" href="#2.2.3.-Parameter-tuning-($\delta$)">¶</a></h4><p>The $\delta$ parameter controls the residual threshold for determining whether squared loss or Huber loss is applied. As shown in <a class="internal-link" href="#fig-10">Figure 10</a>, smaller $\delta$ values lead to a more robust approach by applying Huber loss to a greater number of data points, emphasizing absolute loss. Conversely, larger $\delta$ values result in squared loss being more prevalent, making the method more susceptible to outliers. Notably, for very large $\delta$ values, Huber loss converges to squared loss (susceptible to outliers), as described in <a class="internal-link" href="#eq-8">eq-8</a>:</p>
<div id="eq-8" style="font-size: 1rem;">
$$ \lim_{\delta\to \infty}: \text{Huber Loss} \approx \text{Squared Loss} \tag{8}$$
</div><p>Note that in the <a href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.HuberRegressor.html" target="_blank">scikit-learn implementation</a>, the residuals are divided by the scale parameter sigma <code>|(y - Xw - c) / sigma|</code> to ensure that one does not need to rescale epsilon to achieve the same robustness. The default value of $\delta$ in scikit-learn is 1.35.</p>
<div class="row" id="fig-10" style="margin-top: 15px;">
<div class="col"><img src="jupyter_images/huber loss effect of delta.png"/></div>
<div class="col-12"><p class="image-description"><strong>Figure 10:</strong> $\delta$ controls the residual thresholds used to determine whether to compute squared loss or Huber loss. Small value of $\delta$ (left plot) allows Huber loss to be applied for wider residual ranges of data, making it more robust to outliers. Observe that the blue line (Huber Loss) in the left plot applies to greater residual ranges with smaller $\delta$(=1.35).</p></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<div class="solution_panel closed">
<div class="solution_title">
<p class="solution_title_string">Source Code For Figure (10)</p>
<ul class="nav navbar-right panel_toolbox">
<li><a class="collapse-link"><i class="fa fa-chevron-down"></i></a></li>
</ul>
<div class="clearfix"></div>
</div>
<div class="solution_content">
<pre>
            <code class="language-python">
import numpy as np
import matplotlib.pyplot as plt


error_range = np.linspace(-3.3, 3.3, 100)
theta_values = [1.35, 2.35]

huber_color = 'blue'
squared_color = 'green'
fig, axes = plt.subplots(1, len(theta_values), figsize=(9, 4.5))

for i, theta in enumerate(theta_values):
    huber_loss = np.where(np.abs(error_range) <= theta, 0.5 * error_range ** 2, theta * (np.abs(error_range) - 0.5 * theta))
    squared_error_loss = 0.5 * error_range ** 2

    ax = axes[i]

    # Set the range for which the line styles and alphas will change
    x_range = (-theta, theta)

    # Plot Huber Loss line
    huber_loss_segment = np.where((error_range >= x_range[0]) & (error_range <= x_range[1]), huber_loss, np.nan)
    ax.plot(error_range, huber_loss_segment, linewidth=2, zorder=3, alpha=0.3, linestyle='dashed', color=huber_color)
    huber_loss_segment = np.where((error_range < x_range[0]) | (error_range > x_range[1]), huber_loss, np.nan)
    ax.plot(error_range, huber_loss_segment, label='Huber Loss', linewidth=2, zorder=3, alpha=1, linestyle='-', color=huber_color)

    # Plot Squared Loss line
    squared_loss_segment = np.where((error_range >= x_range[0]) & (error_range <= x_range[1]), squared_error_loss, np.nan)
    ax.plot(error_range, squared_loss_segment, label='Squared Loss', linewidth=2, zorder=3, alpha=1, linestyle='-', color=squared_color)
    squared_loss_segment = np.where((error_range < x_range[0]) | (error_range > x_range[1]), squared_error_loss, np.nan)
    ax.plot(error_range, squared_loss_segment, linewidth=2, zorder=3, alpha=0.3, linestyle='dashed', color=squared_color)

    # Fill the area between the axvlines
    fill_alpha = 0.03
    ax.axvspan(-5, -theta, alpha=fill_alpha, color=huber_color, zorder=-8, label='Outliers')
    ax.axvspan(theta, 5, alpha=fill_alpha, color=huber_color, zorder=-9)
    ax.axvspan(-theta, theta, alpha=fill_alpha, color=squared_color, zorder=-9, label='Inliers')

    ax.set_xlabel('Residuals ($y - \\hat{y}$)', fontsize=13)
    ax.set_ylabel('Loss', fontsize=13)
    ax.grid(True, alpha=0.3)

    lg = ax.legend(loc='upper center', ncol=2)
    for i, lh in enumerate(lg.legendHandles):
        if i > 1:
            lh.set_alpha(0.4)

    ax.axvline(x=theta, color='r', linestyle="dotted", alpha=0.7)
    ax.axvline(x=-theta, color='r', linestyle="dotted", alpha=0.7)
    ax.spines['top'].set_visible(False)
    ax.spines['right'].set_visible(False)
    ax.set_xlim(-3.3, 3.3)

    ax.text(0, 3.5, '$\delta = %.2f$' % theta,
            fontsize=13, ha='center', va='top', color='r', alpha=0.7, rotation=0)
    ax.text(0.05, 0.1, 'aegis4048.github.io', fontsize=10, ha='left', va='center',
        transform=ax.transAxes, color='grey', alpha=0.5)


axes[0].text(3, 0.1, 'more robust', ha='right', fontsize=13, bbox=dict( facecolor='white'))
axes[1].text(3, 0.1, 'less robust', ha='right', fontsize=13, bbox=dict( facecolor='white'))

def setbold(txt):
    return ' '.join([r"$\bf{" + item + "}$" for item in txt.split(' ')])   

bold_txt = setbold('Huber Loss, ')
plain_txt = r'effect of $\delta$-parameter on inlier vs. outlier detection'
fig.suptitle(bold_txt + plain_txt, verticalalignment='top', x=0, horizontalalignment='left', fontsize=11)
yloc = 0.9
ax.annotate('', xy=(0.01, yloc + 0.01), xycoords='figure fraction', xytext=(1.02, yloc + 0.01),
            arrowprops=dict(arrowstyle="-", color='k', lw=0.7))

fig.tight_layout()
            </code>
        </pre>
</div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<div id="Huber code snippets" style="margin-top: -15px"></div><h4 id="2.2.4.-Huber-code-snippets">2.2.4. Huber code snippets<a class="anchor-link" href="#2.2.4.-Huber-code-snippets">¶</a></h4><p>For quick copy-paste, replace <code>X</code> and <code>y</code> with your own data. Make sure to reshape your <code>X</code> so that it is a 2D <code>numpy.ndarray</code> object with shape like <code>(13, 1)</code>.</p>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In [8]:</div>
<div class="inner_cell">
<div class="input_area">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">HuberRegressor</span>
<span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">linear_model</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="c1"># sample data</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span> <span class="mf">0.15</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.34</span><span class="p">,</span> <span class="mf">0.32</span><span class="p">,</span> <span class="mf">0.43</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.4</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.04</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.51</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">,</span> <span class="mf">0.47</span><span class="p">,</span> <span class="mf">0.12</span><span class="p">,</span> <span class="mf">0.08</span><span class="p">,</span> <span class="mf">0.04</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.08</span><span class="p">,</span> 
              <span class="o">-</span><span class="mf">0.23</span><span class="p">,</span> <span class="mf">0.08</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.03</span><span class="p">,</span> <span class="mf">0.03</span><span class="p">,</span> <span class="mf">0.04</span><span class="p">,</span> <span class="mf">0.01</span><span class="p">,</span> <span class="mf">0.06</span><span class="p">,</span> <span class="mf">0.03</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.04</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.18</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.19</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.06</span><span class="p">,</span> 
              <span class="o">-</span><span class="mf">0.26</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.16</span><span class="p">,</span> <span class="mf">0.13</span><span class="p">,</span> <span class="mf">0.09</span><span class="p">,</span> <span class="mf">0.03</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.03</span><span class="p">,</span> <span class="mf">0.04</span><span class="p">,</span> <span class="mf">0.14</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.01</span><span class="p">,</span> <span class="mf">0.4</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.06</span><span class="p">,</span> <span class="mf">0.15</span><span class="p">,</span> <span class="mf">0.08</span><span class="p">,</span> 
              <span class="mf">0.05</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.15</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.09</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.15</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.11</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.07</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.19</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.06</span><span class="p">,</span> <span class="mf">0.17</span><span class="p">,</span> <span class="mf">0.23</span><span class="p">,</span> <span class="mf">0.18</span><span class="p">])</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">17.44</span><span class="p">,</span> <span class="mf">25.46</span><span class="p">,</span> <span class="mf">18.61</span><span class="p">,</span> <span class="mf">26.07</span><span class="p">,</span> <span class="mf">24.96</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.22</span><span class="p">,</span> <span class="mf">26.45</span><span class="p">,</span> <span class="mf">26.5</span><span class="p">,</span> <span class="mf">20.57</span><span class="p">,</span> <span class="mf">3.08</span><span class="p">,</span> <span class="mf">35.9</span> <span class="p">,</span> <span class="mf">32.47</span><span class="p">,</span> 
              <span class="mf">20.84</span><span class="p">,</span> <span class="mf">13.37</span><span class="p">,</span> <span class="mf">42.44</span><span class="p">,</span> <span class="mf">27.23</span><span class="p">,</span> <span class="mf">35.65</span><span class="p">,</span> <span class="mf">29.51</span><span class="p">,</span> <span class="mf">31.28</span><span class="p">,</span> <span class="mf">41.34</span><span class="p">,</span> <span class="mf">32.19</span><span class="p">,</span> <span class="mf">33.67</span><span class="p">,</span> <span class="mf">25.64</span><span class="p">,</span> <span class="mf">9.3</span><span class="p">,</span> 
              <span class="mf">14.63</span><span class="p">,</span> <span class="mf">25.1</span><span class="p">,</span> <span class="mf">4.69</span><span class="p">,</span> <span class="mf">14.42</span><span class="p">,</span> <span class="mf">47.53</span><span class="p">,</span> <span class="mf">33.82</span><span class="p">,</span> <span class="mf">32.2</span> <span class="p">,</span> <span class="mf">24.81</span><span class="p">,</span> <span class="mf">32.64</span><span class="p">,</span> <span class="mf">45.11</span><span class="p">,</span> <span class="mf">26.76</span><span class="p">,</span> <span class="mf">68.01</span><span class="p">,</span> 
              <span class="mf">23.39</span><span class="p">,</span> <span class="mf">43.49</span><span class="p">,</span> <span class="mf">37.88</span><span class="p">,</span> <span class="mf">36.01</span><span class="p">,</span> <span class="mf">16.32</span><span class="p">,</span> <span class="mf">19.77</span><span class="p">,</span> <span class="mf">16.34</span><span class="p">,</span> <span class="mf">19.57</span><span class="p">,</span> <span class="mf">29.28</span><span class="p">,</span> <span class="mf">16.62</span><span class="p">,</span> <span class="mf">24.39</span><span class="p">,</span> <span class="mf">43.77</span><span class="p">,</span> 
              <span class="mf">50.46</span><span class="p">,</span> <span class="mf">47.09</span><span class="p">])</span>

<span class="c1"># fit and predict: Huber </span>
<span class="n">huber</span> <span class="o">=</span> <span class="n">HuberRegressor</span><span class="p">(</span><span class="n">epsilon</span><span class="o">=</span><span class="mf">1.35</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>   <span class="c1"># 1.35 is the default. You can try different epsilon values.</span>
<span class="n">y_pred_huber</span> <span class="o">=</span> <span class="n">huber</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

<span class="c1"># retrieve the fitted parameters</span>
<span class="n">coefs</span> <span class="o">=</span> <span class="n">huber</span><span class="o">.</span><span class="n">coef_</span>
<span class="n">intercept</span> <span class="o">=</span> <span class="n">huber</span><span class="o">.</span><span class="n">intercept_</span>

<span class="c1"># seperate into inliers vs. outliers</span>
<span class="n">X_inlier</span> <span class="o">=</span> <span class="n">X</span><span class="p">[</span><span class="o">~</span><span class="n">huber</span><span class="o">.</span><span class="n">outliers_</span><span class="p">]</span>
<span class="n">y_inlier</span> <span class="o">=</span> <span class="n">y</span><span class="p">[</span><span class="o">~</span><span class="n">huber</span><span class="o">.</span><span class="n">outliers_</span><span class="p">]</span>
<span class="n">X_outlier</span> <span class="o">=</span> <span class="n">X</span><span class="p">[</span><span class="n">huber</span><span class="o">.</span><span class="n">outliers_</span><span class="p">]</span>
<span class="n">y_outlier</span> <span class="o">=</span> <span class="n">y</span><span class="p">[</span><span class="n">huber</span><span class="o">.</span><span class="n">outliers_</span><span class="p">]</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">"Huber -----------------------------------</span><span class="se">\n</span><span class="s2">"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"Coefficients         :"</span><span class="p">,</span> <span class="n">coefs</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"Intercept            :"</span><span class="p">,</span> <span class="n">intercept</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"# of inliers         :"</span><span class="p">,</span> <span class="nb">sum</span><span class="p">(</span><span class="o">~</span><span class="n">huber</span><span class="o">.</span><span class="n">outliers_</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"Fraction of inliers  :"</span><span class="p">,</span> <span class="nb">sum</span><span class="p">(</span><span class="o">~</span><span class="n">huber</span><span class="o">.</span><span class="n">outliers_</span><span class="p">)</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">y</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"</span><span class="se">\n</span><span class="s2">------------------------------------------"</span><span class="p">)</span>

<span class="c1"># fit and predict: Ordinary Least Squares</span>
<span class="n">ols</span> <span class="o">=</span> <span class="n">linear_model</span><span class="o">.</span><span class="n">LinearRegression</span><span class="p">()</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="n">y_pred_ols</span> <span class="o">=</span> <span class="n">ols</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

<span class="c1"># plot</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">'Huber Regressor'</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X_inlier</span><span class="p">,</span> <span class="n">y_inlier</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s1">'green'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">'Inliers'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X_outlier</span><span class="p">,</span> <span class="n">y_outlier</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s1">'red'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">'Outliers'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y_pred_ols</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">'OLS'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y_pred_huber</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">'Huber'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">();</span>
</pre></div>
</div>
</div>
</div>
<div class="output_wrapper">
<div class="output">
<div class="output_area">
<div class="prompt"></div>
<div class="output_subarea output_stream output_stdout output_text">
<pre>Huber -----------------------------------

Coefficients         : [50.32948547]
Intercept            : 29.03041669256188
# of inliers         : 31
Fraction of inliers  : 0.62

------------------------------------------
</pre>
</div>
</div>
<div class="output_area">
<div class="prompt"></div>
<div class="output_png output_subarea">
<img src="data:image/png;base64,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
"/>
</div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<div class="highlights red-theme" id="huber-warning">
<div class="highlights-title red-theme">WARNING!</div>
<div class="highlights-content red-theme">Despite the application of the robust Huber regressor, the regression fit in the plot above (Figure ?) is still notably influenced by the presence of outliers, albeit to a lesser extent. This influence persists because, instead of completely excluding outliers during the fitting process, Huber aims to attenuate their impact by employing absolute loss rather than squared loss. Therefore, unless there is a compelling reason to retain and account for the so-called "outliers" within your data, using the Huber regressor is not typically recommended.
    </div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<div id="Theil-Sen regressor"></div><h3 id="2.3.-Theil-Sen-regressor">2.3. Theil-Sen regressor<a class="anchor-link" href="#2.3.-Theil-Sen-regressor">¶</a></h3><p>The Theil-Sen Regressor calculates the slope by examining all possible combinations of subsets from the dataset and taking the median of these slopes. This approach is based on the <i>"$n$ choose $k$"</i> $\binom{n}{k}$ method, where $n$ is the number of data points and $k$ is the subset size. For example, with $k=2$ (standard for 2D linear regression) and $n=50$, the algorithm computes $\binom{50}{2} = 1225$ combinations, each with a subset size of $k$. The median of the slopes from these subsets is used as the final slope for the regression line. This method can be adapted for different values of $k$, leading to a varying number of combinations.</p>
<p>It is important to note that the TheilSen regressor does not explicitly identify outliers from inliers, unlike RANSAC or Huber.</p>
<p><em>(Note that these steps are based on <a href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.TheilSenRegressor.html#sklearn.linear_model.TheilSenRegressor" target="_blank">sklearn's implementation of TheilSen regressor.</a>)</em></p>
<div class="ordered-list">
<h2>Steps</h2>
<ol>
<li>Consider an example dataset of size $n=6$: $A(1, 3)$, $B(2,2)$, $C(3,6)$, $D(4,5)$, $E(5,7)$, $F(6,5)$
<div class="row full_screen_margin_80 mobile_responsive_plot_full_width" id="fig-11" style="">
<div class="col"><img src="jupyter_images/TS_initial_data.png"/></div>
<div class="col-12"><p class="image-description"><strong>Figure 11:</strong> Initial dataset of size $n=6$.</p></div>
</div>
<div class="solution_panel closed" style="margin-top: 20px;">
<div class="solution_title">
<p class="solution_title_string">Source Code For Figure (11)</p>
<ul class="nav navbar-right panel_toolbox">
<li><a class="collapse-link"><i class="fa fa-chevron-down"></i></a></li>
</ul>
<div class="clearfix"></div>
</div>
<div class="solution_content">
<pre>
                        <code class="language-python">
import numpy as np
import matplotlib.pyplot as plt

##################################### sample data #####################################

X = np.array([1, 2, 3, 4, 5, 6]).reshape(-1, 1)
y = np.array([3, 2, 6, 5, 7, 5])

###################################### plotting #######################################

fig, ax = plt.subplots(figsize=(8, 4.5))

ax.scatter(X, y, label='Original Data')

points = ['A','B','C','D','E','F']
for x_, y_, point in zip(X, y, points):
    ax.text(x_, y_ + 1, '%s(%d,%d)' % (point, x_, y_), ha='left')

ax.legend(loc='upper left')
ax.grid(axis='y', linestyle='--', color='#acacac', alpha=0.5)
ax.spines['top'].set_visible(False)
ax.spines['right'].set_visible(False)

ymax = 15
ax.set_ylim(0 - 0.05 * ymax, ymax)  

ax.set_xlabel('X', fontsize=13)
ax.set_ylabel('y', fontsize=13)
ax.text(0.99, 0.1, 'aegis4048.github.io', fontsize=10, ha='right', va='center',
    transform=ax.transAxes, color='grey', alpha=0.5)

def setbold(txt):
    return ' '.join([r"$\bf{" + item + "}$" for item in txt.split(' ')])

bold_txt = setbold('Initial Data')
plain_txt = r', $n=6$'
fig.suptitle(bold_txt + plain_txt, verticalalignment='top', x=0, horizontalalignment='left', fontsize=13, y=0.96)
yloc = 0.88
ax.annotate('', xy=(0.01, yloc + 0.01), xycoords='figure fraction', xytext=(1.02, yloc + 0.01),
            arrowprops=dict(arrowstyle="-", color='k', lw=0.7))

fig.tight_layout()
                        </code>
                    </pre>
</div>
</div>
</li>
<li>Let $k=2$ (default for 2D linear regression). $\binom{6}{2}=15$ samples are generated.</li>
<li>Consider the first sample: $A(1, 3)$ and $B(2,2)$. The slope can be obtained with a simple $(y_{j}−y_{i})/(x_{j}−x_{i})$. Obtain intercept from the slope.
<div class="row full_screen_margin_80 mobile_responsive_plot_full_width" id="fig-12" style="">
<div class="col"><img src="jupyter_images/TS - first sample slope calculation.png"/></div>
<div class="col-12"><p class="image-description"><strong>Figure 12:</strong> Linear regression on the first sample, composed of points $A$ and $B$.</p></div>
</div>
<div class="solution_panel closed" style="margin-top: 20px;">
<div class="solution_title">
<p class="solution_title_string">Source Code For Figure (12)</p>
<ul class="nav navbar-right panel_toolbox">
<li><a class="collapse-link"><i class="fa fa-chevron-down"></i></a></li>
</ul>
<div class="clearfix"></div>
</div>
<div class="solution_content">
<pre>
                        <code class="language-python">
import numpy as np
import matplotlib.pyplot as plt

##################################### sample data #####################################

X = np.array([1, 2, 3, 4, 5, 6]).reshape(-1, 1)
y = np.array([3, 2, 6, 5, 7, 5])

########################## slope and intercept calc# ##################################

slope = (y[0] - y[1]) / (X[0] - X[1])
intercept = y[0] - slope * X[0]
y_pred = X * slope + intercept

###################################### plotting #######################################

fig, ax = plt.subplots(figsize=(8, 4.5))

ax.scatter(X, y, label='Original Data')
ax.scatter(X[:2], y[:2], label='Current sample')
ax.plot(X, y_pred, label='Linear model for (A, B):  y = %.1fx + %.1f' % (slope, intercept), color='#ff7f0e', ls='-.')

points = ['A','B','C','D','E','F']
for x_, y_, point in zip(X, y, points):
    ax.text(x_, y_ + 1, '%s(%d,%d)' % (point, x_, y_), ha='left')

ax.legend(loc='upper left')
ax.grid(axis='y', linestyle='--', color='#acacac', alpha=0.5)
ax.spines['top'].set_visible(False)
ax.spines['right'].set_visible(False)

ymax = 15
ax.set_ylim(0 - 0.05 * ymax, ymax)

ax.set_xlabel('X', fontsize=13)
ax.set_ylabel('y', fontsize=13)
ax.text(0.99, 0.1, 'aegis4048.github.io', fontsize=10, ha='right', va='center',
    transform=ax.transAxes, color='grey', alpha=0.5)

def setbold(txt):
    return ' '.join([r"$\bf{" + item + "}$" for item in txt.split(' ')])

bold_txt = setbold('First Sample Slope Calculation')
plain_txt = r', $k=2$ ($A$ and $B$)'
fig.suptitle(bold_txt + plain_txt, verticalalignment='top', x=0, horizontalalignment='left', fontsize=13, y=0.96)
yloc = 0.88
ax.annotate('', xy=(0.01, yloc + 0.01), xycoords='figure fraction', xytext=(1.02, yloc + 0.01),
            arrowprops=dict(arrowstyle="-", color='k', lw=0.7))

fig.tight_layout()
                        </code>
                    </pre>
</div>
</div>
</li>
<li class="no-number"><strong>(Optional 3.A).</strong> When more than two data points $k>2$ are present, the least-squares method, or OLS <a class="internal-link" href="#eq-1">eq-1</a>, is used for parameter fitting. This method is necessary as the simple slope formula $(y_{j}−y_{i})/(x_{j}−x_{i})$ is only valid for two data points. As the number of data points $k$ approaches the total number of observations $n$, the Theil-Sen regressor converges towards OLS results, which is not robust. 
<div class="row full_screen_margin_80 mobile_responsive_plot_full_width" id="fig-13" style="">
<div class="col"><img src="jupyter_images/TS - first sample slope k=3.png"/></div>
<div class="col-12"><p class="image-description"><strong>Figure 13:</strong> Because there are $k=3$ points, least-squares (OLS) method is necessary to fit a slope and an intercept. Note that the model robustness decreases as $k$ increases.</p></div>
</div>
<div class="solution_panel closed" style="margin-top: 20px;">
<div class="solution_title">
<p class="solution_title_string">Source Code For Figure (13)</p>
<ul class="nav navbar-right panel_toolbox">
<li><a class="collapse-link"><i class="fa fa-chevron-down"></i></a></li>
</ul>
<div class="clearfix"></div>
</div>
<div class="solution_content">
<pre>
                        <code class="language-python">
import numpy as np
import matplotlib.pyplot as plt
from sklearn import linear_model

##################################### sample data #####################################

X = np.array([1, 2, 3, 4, 5, 6]).reshape(-1, 1)
y = np.array([3, 2, 6, 5, 7, 5])

########################## OLS on the current sample ##################################

# Ordinary Least Squares
ols = linear_model.LinearRegression().fit(X[:3], y[:3])
y_pred_ols = ols.predict(X)
coefs_ols = ols.coef_
intercept_ols = ols.intercept_

###################################### plotting #######################################

fig, ax = plt.subplots(figsize=(8, 4.5))

ax.scatter(X, y, label='Original Data')
ax.scatter(X[:3], y[:3], label='Current sample')
ax.plot(X, y_pred_ols, label='OLS (A, B, C):  y = %.1fx + %.1f' % (coefs_ols[0], intercept_ols), color='#ff7f0e', ls='-.')

points = ['A','B','C','D','E','F']
for x_, y_, point in zip(X, y, points):
    ax.text(x_, y_ + 1, '%s(%d,%d)' % (point, x_, y_), ha='left')

ax.legend(loc='upper left')
ax.grid(axis='y', linestyle='--', color='#acacac', alpha=0.5)
ax.spines['top'].set_visible(False)
ax.spines['right'].set_visible(False)

ymax = 15
ax.set_ylim(0 - 0.05 * ymax, ymax)

ax.set_xlabel('X', fontsize=13)
ax.set_ylabel('y', fontsize=13)
ax.text(0.99, 0.1, 'aegis4048.github.io', fontsize=10, ha='right', va='center',
    transform=ax.transAxes, color='grey', alpha=0.5)

def setbold(txt):
    return ' '.join([r"$\bf{" + item + "}$" for item in txt.split(' ')])

bold_txt = setbold('First Sample Slope Calculation')
plain_txt = r', $k=3$ ($A$, $B$ and $C$)'
fig.suptitle(bold_txt + plain_txt, verticalalignment='top', x=0, horizontalalignment='left', fontsize=13, y=0.96)
yloc = 0.88
ax.annotate('', xy=(0.01, yloc + 0.01), xycoords='figure fraction', xytext=(1.02, yloc + 0.01),
            arrowprops=dict(arrowstyle="-", color='k', lw=0.7))

fig.tight_layout()
                        </code>
                    </pre>
</div>
</div>
</li>
<li class="no-number"><strong>4.</strong> Repeat step 3 for all 15 samples. A 2x15 matrix is obtained for 15 slopes and 15 intercepts. This is equivalent to fitting 15 straight lines.
<div class="row full_screen_margin_80 mobile_responsive_plot_full_width" id="fig-14" style="">
<div class="col"><img src="jupyter_images/TS - 15 samples fit.png"/></div>
<div class="col-12"><p class="image-description"><strong>Figure 14:</strong> Regression fitted on all of the 15 samples, each of size $k=2$. Observe that all straight lines pass through $k=2$ points.</p></div>
</div>
<div class="solution_panel closed" style="margin-top: 20px;">
<div class="solution_title">
<p class="solution_title_string">Source Code For Figure (14)</p>
<ul class="nav navbar-right panel_toolbox">
<li><a class="collapse-link"><i class="fa fa-chevron-down"></i></a></li>
</ul>
<div class="clearfix"></div>
</div>
<div class="solution_content">
<pre>
                        <code class="language-python">
from itertools import combinations
import numpy as np
from sklearn.linear_model import LinearRegression, TheilSenRegressor
from scipy.optimize import minimize
import matplotlib.pyplot as plt

##################################### sample data #####################################

X = np.array([1, 2, 3, 4, 5, 6]).reshape(-1, 1)
y = np.array([3, 2, 6, 5, 7, 5])

############################# parameter optimization ##################################

n_samples, n_features = X.shape
n_subsamples = 2
fit_intercept = True

# "n choose k" -> n Combination k number of samples. 
indices = np.array(list(combinations(range(n_samples), n_subsamples)))

parameters = []
for subset in indices:
    X_subset = X[subset]
    y_subset = y[subset]
    model = LinearRegression(fit_intercept=fit_intercept)
    model.fit(X_subset, y_subset)
    parameters.append([model.intercept_, model.coef_[0]])
parameters = np.vstack(parameters)

slopes = parameters[:, 1]
intercepts = parameters[:, 0]

###################################### plotting #######################################

fig, ax = plt.subplots(figsize=(8, 4.5))

ax.scatter(X, y, label='Original Data')

for slope, intercept in zip(slopes, intercepts):
    y_pred_sample = slope * X + intercept
    ax.plot(X, y_pred_sample, zorder=-99, color='silver')
ax.plot(X, y_pred_sample, zorder=-99, color='lightgrey', label='Sample fit, $k=2$')

points = ['A','B','C','D','E','F']
for x_, y_, point in zip(X, y, points):
    ax.text(x_, y_ + 1, '%s(%d,%d)' % (point, x_, y_), ha='left')

ymax = 15
ax.set_ylim(0 - 0.05 * ymax, ymax)    

ax.legend(loc='upper left')
ax.grid(axis='y', linestyle='--', color='#acacac', alpha=0.5)
ax.spines['top'].set_visible(False)
ax.spines['right'].set_visible(False)

ax.set_xlabel('X', fontsize=13)
ax.set_ylabel('y', fontsize=13)
ax.text(0.99, 0.1, 'aegis4048.github.io', fontsize=10, ha='right', va='center',
    transform=ax.transAxes, color='grey', alpha=0.5)


def setbold(txt):
    return ' '.join([r"$\bf{" + item + "}$" for item in txt.split(' ')])

bold_txt = setbold('Fitting 6C2 = 15 Samples')
plain_txt = r', 15 regression models fitted'
fig.suptitle(bold_txt + plain_txt, verticalalignment='top', x=0, horizontalalignment='left', fontsize=13, y=0.96)
yloc = 0.88
ax.annotate('', xy=(0.01, yloc + 0.01), xycoords='figure fraction', xytext=(1.02, yloc + 0.01),
            arrowprops=dict(arrowstyle="-", color='k', lw=0.7))

fig.tight_layout()                        
                        </code>
                    </pre>
</div>
</div>
</li>
<li class="no-number"><strong>5.</strong> A spatial median of the 15 slopes and 15 intercepts is computed. Note that the spatial median is not the simple median of each slopes and and intercepts independently. 
<div class="row full_screen_margin_80 mobile_responsive_plot_full_width" id="fig-15" style="">
<div class="col"><img src="jupyter_images/spatial median of slopes and intercepts.png"/></div>
<div class="col-12"><p class="image-description"><strong>Figure 15:</strong> This plot displays regression parameters of a 2D linear model, with intercepts casted on the x-axis and slopes casted on the y-axis. For a model with three parameters, such visualization would extend into 3D space. The spatial median is determined to be slope = 0.67 and interecept = 2.33.</p></div>
</div>
<div class="solution_panel closed" style="margin-top: 20px;">
<div class="solution_title">
<p class="solution_title_string">Source Code For Figure (15)</p>
<ul class="nav navbar-right panel_toolbox">
<li><a class="collapse-link"><i class="fa fa-chevron-down"></i></a></li>
</ul>
<div class="clearfix"></div>
</div>
<div class="solution_content">
<pre>
                        <code class="language-python">     
from itertools import combinations
import numpy as np
from sklearn.linear_model import LinearRegression, TheilSenRegressor
from scipy.optimize import minimize
import matplotlib.pyplot as plt

##################################### sample data #####################################

X = np.array([1, 2, 3, 4, 5, 6]).reshape(-1, 1)
y = np.array([3, 2, 6, 5, 7, 5])

############################# parameter optimization ##################################

n_samples, n_features = X.shape
n_subsamples = 2
fit_intercept = True

# "n choose k" -> n Combination k number of samples. 
indices = np.array(list(combinations(range(n_samples), n_subsamples)))

parameters = []
for subset in indices:
    X_subset = X[subset]
    y_subset = y[subset]
    model = LinearRegression(fit_intercept=fit_intercept)
    model.fit(X_subset, y_subset)
    parameters.append([model.intercept_, model.coef_[0]])
parameters = np.vstack(parameters)

########################## spatial median approximation ###############################

# L2 loss - euclidean distance
def L2_objective_func(point, _x, _y):
    return np.sum(np.sqrt((point[0] - _x)**2 + (point[1] - _y)**2))

intercepts = parameters[:, 0] 
slopes = parameters[:, 1]

init_guess = [np.mean(intercepts), np.mean(slopes)]  # starting with mean is a good guess to reduce computational cost
result_L2 = minimize(L2_objective_func, init_guess, args=(intercepts, slopes), method='Nelder-Mead')

################################### result validation #################################

# Checks that the implemented codes here agree with the sklearn implementation
TS = TheilSenRegressor().fit(X, y)
y_pred_TS = TS.predict(X)

np.testing.assert_almost_equal(result_L2.x[1], TS.coef_[0], decimal=2)
np.testing.assert_almost_equal(result_L2.x[0], TS.intercept_, decimal=2)

###################################### plotting #######################################

fig, ax = plt.subplots(figsize=(7, 4))

ax.scatter(intercepts, slopes, s=150, edgecolor='blue', fc=(0, 0, 1, 0.05))
_s1 = ax.scatter(result_L2.x[0], result_L2.x[1], s=400, marker='*',
                 label=r'Median:  $\underset{x, y}{\mathrm{argmin}} \sum^{n}_{i=1}\sqrt{(x_{i} - \hat{x}_{i})^{2} + (y_{i} - \hat{y}_{i})^2}$')

ax.grid(axis='both', linestyle='--', color='#acacac', alpha=0.5)
ax.spines['top'].set_visible(False)
ax.spines['right'].set_visible(False)

ax.set_xlabel('Intercepts')
ax.set_ylabel('Slopes')

ax.text(result_L2.x[0] + 0.5, result_L2.x[1] + 0.15, '(%.2f, %.2f)' % (result_L2.x[0], result_L2.x[1]), color=_s1.get_facecolor()[0], ha='left')
ax.text(result_L2.x[0] + 0.5, result_L2.x[1] + 0.5, 'Spatial Median', color=_s1.get_facecolor()[0], ha='left')
ax.text(0.98, 0.08, 'aegis4048.github.io', fontsize=10, ha='right', transform=ax.transAxes, color='grey', alpha=0.5)

def setbold(txt):
    return ' '.join([r"$\bf{" + item + "}$" for item in txt.split(' ')])

bold_txt = setbold('Spatial Median of Slopes and Intercepts, ')
plain_txt = r'obtained by minimizing the $L_{2}$ norm.'
fig.suptitle(bold_txt + plain_txt, verticalalignment='top', x=0, horizontalalignment='left', fontsize=11, y=0.95)
yloc = 0.87
ax.annotate('', xy=(0.01, yloc + 0.01), xycoords='figure fraction', xytext=(1.02, yloc + 0.01),
            arrowprops=dict(arrowstyle="-", color='k', lw=0.7))


fig.tight_layout()                    
                        </code>
                    </pre>
</div>
</div>
</li>
<li class="no-number"><strong>6.</strong> Return the final model.
<div class="row full_screen_margin_80 mobile_responsive_plot_full_width" id="fig-16" style="">
<div class="col"><img src="jupyter_images/TS - final slope and intercept.png"/></div>
<div class="col-12"><p class="image-description"><strong>Figure 16:</strong> Straight line drawn from the spatial median of (intercepts, slopes) from step 5. </p></div>
</div>
<div class="solution_panel closed" style="margin-top: 20px;">
<div class="solution_title">
<p class="solution_title_string">Source Code For Figure (16)</p>
<ul class="nav navbar-right panel_toolbox">
<li><a class="collapse-link"><i class="fa fa-chevron-down"></i></a></li>
</ul>
<div class="clearfix"></div>
</div>
<div class="solution_content">
<pre>
                        <code class="language-python">     
from itertools import combinations
import numpy as np
from sklearn.linear_model import LinearRegression, TheilSenRegressor
from scipy.optimize import minimize
import matplotlib.pyplot as plt

##################################### sample data #####################################

X = np.array([1, 2, 3, 4, 5, 6]).reshape(-1, 1)
y = np.array([3, 2, 6, 5, 7, 5])

############################# parameter optimization ##################################

n_samples, n_features = X.shape
n_subsamples = 2
fit_intercept = True

# "n choose k" -> n Combination k number of samples.
indices = np.array(list(combinations(range(n_samples), n_subsamples)))

parameters = []
for subset in indices:
    X_subset = X[subset]
    y_subset = y[subset]
    model = LinearRegression(fit_intercept=fit_intercept)
    model.fit(X_subset, y_subset)
    parameters.append([model.intercept_, model.coef_[0]])
parameters = np.vstack(parameters)

########################## spatial median approximation ###############################

# L2 loss - euclidean distance
def L2_objective_func(point, _x, _y):
    return np.sum(np.sqrt((point[0] - _x)**2 + (point[1] - _y)**2))

intercepts = parameters[:, 0]
slopes = parameters[:, 1]

init_guess = [np.mean(intercepts), np.mean(slopes)]  # starting with mean is a good guess to reduce computational cost
result_L2 = minimize(L2_objective_func, init_guess, args=(intercepts, slopes), method='Nelder-Mead')

final_slope = result_L2.x[1]
final_intercept = result_L2.x[0]
y_pred = X * final_slope + final_intercept

################################### result validation #################################

# Checks that the implemented codes here agree with the sklearn implementation
TS = TheilSenRegressor().fit(X, y)
y_pred_TS = TS.predict(X)

np.testing.assert_almost_equal(result_L2.x[1], TS.coef_[0], decimal=2)
np.testing.assert_almost_equal(result_L2.x[0], TS.intercept_, decimal=2)

###################################### plotting #######################################

fig, ax = plt.subplots(figsize=(8, 4.5))

ax.scatter(X, y, label='Original Data')
ax.plot(X, y_pred, label='Final TheilSen model:  y = %.1fx + %.1f' % (final_slope, final_intercept), color='#ff7f0e', ls='-.')

points = ['A','B','C','D','E','F']
for x_, y_, point in zip(X, y, points):
    ax.text(x_, y_ + 1, '%s(%d,%d)' % (point, x_, y_), ha='left')

ax.legend(loc='upper left')
ax.grid(axis='y', linestyle='--', color='#acacac', alpha=0.5)
ax.spines['top'].set_visible(False)
ax.spines['right'].set_visible(False)

ymax = 15
ax.set_ylim(0 - 0.05 * ymax, ymax)

ax.set_xlabel('X', fontsize=13)
ax.set_ylabel('y', fontsize=13)
ax.text(0.99, 0.1, 'aegis4048.github.io', fontsize=10, ha='right', va='center',
    transform=ax.transAxes, color='grey', alpha=0.5)

def setbold(txt):
    return ' '.join([r"$\bf{" + item + "}$" for item in txt.split(' ')])

bold_txt = setbold('Final Slope and Intercept')
plain_txt = r', TheilSen regressor for $n=6$ and $k=2$'
fig.suptitle(bold_txt + plain_txt, verticalalignment='top', x=0, horizontalalignment='left', fontsize=13, y=0.96)
yloc = 0.88
ax.annotate('', xy=(0.01, yloc + 0.01), xycoords='figure fraction', xytext=(1.02, yloc + 0.01),
            arrowprops=dict(arrowstyle="-", color='k', lw=0.7))

fig.tight_layout()           
                        </code>
                    </pre>
</div>
</div>
</li>
</ol>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<div id="Advanced: Impact of adjusting $k$ on the model robustness" style="margin-top: -15px"></div><h4 id="2.3.1.-Sample-size-and-model-robustness">2.3.1. Sample size and model robustness<a class="anchor-link" href="#2.3.1.-Sample-size-and-model-robustness">¶</a></h4><p>When a sample size $k> 2$, indicating more than two data points in each sample, the Theil-Sen regressor employs the OLS method for each sample generated. This method uses the <em>$L_2$-norm squared</em> <a class="internal-link" href="#eq-1">eq-1</a> as its objective function, which, while effective, is sensitive to outliers because it places greater emphasis on larger residuals due to the squared term. As $k$ approaches $n$, the robustness of the Theil-Sen regressor decreases. This decrease in robustness occurs because at $k=n$, the model is essentially fitting an OLS model to the entire dataset as one single sample ($\binom{n}{k} = 1$). In a 3D regression context, $k$ is at leat 3, factoring in the number of features plus 1 if <code>fit_intercept=True</code> (default). The sklearn package sets $k$ to its minimum value by default to maximize robustness. <u>Adjusting $k$ is not recommended</u> unless there's a clear understanding of how it affects the model's robustness.</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<div id="Advanced: Spatial median" style=""></div><h4 id="2.3.2.-Spatial-median">2.3.2. Spatial median<a class="anchor-link" href="#2.3.2.-Spatial-median">¶</a></h4><p>The spatial median is the point minimizing the sum of Euclidean distances from all points in a given space. This concept extends the 2D Pythagorean theorem to n-dimensional space and is mathematically defined as the point minimizing the <em>$L_2$-norm</em> <a class="internal-link" href="#eq-2">eq-2</a>. For clarity, consider a scenario with five oil and gas wells and a central refinery. To minimize pipeline construction costs, the refinery should be located at the spatial median, thereby reducing the total length of required pipelines. It's crucial to note that the spatial median is not simply the median of x-axis and y-axis values considered separately, but a combined evaluation of all dimension, as highlighted in  <a class="internal-link" href="#eq-17">Figure 17</a>.</p>
<div class="row" id="fig-17" style="">
<div class="col"><img src="jupyter_images/separate medians vs spatial medians.png"/></div>
<div class="col-12"><p class="image-description"><strong>Figure 17:</strong> Taking the median of the x-axis and y-axis separately yields different results from the spatial median.</p></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<div class="solution_panel closed">
<div class="solution_title">
<p class="solution_title_string">Source Code For Figure (17)</p>
<ul class="nav navbar-right panel_toolbox">
<li><a class="collapse-link"><i class="fa fa-chevron-down"></i></a></li>
</ul>
<div class="clearfix"></div>
</div>
<div class="solution_content">
<pre>
            <code class="language-python">
import matplotlib.pyplot as plt
import numpy as np
from scipy.optimize import minimize

x = [3, 9, 21, 25]
y = [12, 35, 16, 28]

x_outlier = [3, 9, 15, 21, 25]
y_outlier =[12, 35, 100, 16, 28]

ys = [y, y_outlier]
xs = [x, x_outlier]

# L2 loss - euclidean distance
def L2_objective_func(point, _x, _y):
    return np.sum(np.sqrt((point[0] - _x)**2 + (point[1] - _y)**2))

s = 150
init_guess = [0, 0]

fig, axes = plt.subplots(1, 2, figsize=(9, 4))
for i, (ax, x, y) in enumerate(zip(axes, xs, ys)):

    # calculates L2 euclidean loss. This results in a spatial median
    result_L2 = minimize(L2_objective_func, init_guess, args=(x, y), method='Nelder-Mead')

    ax.scatter(x, y, s=s, edgecolor='blue', fc=(0, 0, 1, 0.05))
    _s1 = ax.scatter(result_L2.x[0], result_L2.x[1], s=s,
                     label=r'Spatial Median', marker='*')

    _s2 = ax.scatter(np.median(x), np.median(y), s=s,
                     label=r'Separate Medians', marker='+', lw=3)

    xmax = 30
    ymax = 110
    ax.set_xlim(0 - 0.05 * xmax, xmax)
    ax.set_ylim(0 - 0.05 * ymax, ymax)

    ax.grid(axis='both', linestyle='--', color='#acacac', alpha=0.5)
    ax.spines['top'].set_visible(False)
    ax.spines['right'].set_visible(False)

    ax.text(np.median(x), np.median(y) + 6, '(%d, %d)' % (np.median(x), np.median(y)), color=_s2.get_facecolor()[0], ha='right')
    ax.text(result_L2.x[0], result_L2.x[1] + 6, '(%d, %d)' % (result_L2.x[0], result_L2.x[1]), color=_s1.get_facecolor()[0], ha='left')
    ax.text(0.98, 0.08, 'aegis4048.github.io', fontsize=10, ha='right', transform=ax.transAxes, color='grey', alpha=0.5)

    ax.text(np.median(x), np.median(y) + 14, 'Separate Medians', color=_s2.get_facecolor()[0], ha='right')
    ax.text(result_L2.x[0], result_L2.x[1] + 14, 'Spatial Median', color=_s1.get_facecolor()[0], ha='left')

    ax.set_ylabel('Y')
    ax.set_xlabel('X')

axes[1].scatter(x[2], y_outlier[2], s=100, marker='x')
axes[1].text(x[2], y_outlier[2] - 10, 'outlier', color='red', ha='center')

def setbold(txt):
    return ' '.join([r"$\bf{" + item + "}$" for item in txt.split(' ')])

bold_txt = setbold('Separate Medians vs Spatial Medians, ')
plain_txt = r''
fig.suptitle(bold_txt + plain_txt, verticalalignment='top', x=0, horizontalalignment='left', fontsize=12, y=0.96)
yloc = 0.88
ax.annotate('', xy=(0.01, yloc + 0.01), xycoords='figure fraction', xytext=(1.02, yloc + 0.01),
            arrowprops=dict(arrowstyle="-", color='k', lw=0.7))

fig.tight_layout()
            </code>
        </pre>
</div>
</div><p>A convenient library for calculating spatial median is <a href="https://github.com/daleroberts/hdmedians" target="_blank">hdmedians</a>. This library features a Cython implementation that enables faster computation. An example usage is as the following:</p>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In [12]:</div>
<div class="inner_cell">
<div class="input_area">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">hdmedians</span> <span class="k">as</span> <span class="nn">hd</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">3</span><span class="p">,</span> <span class="mi">9</span><span class="p">,</span> <span class="mi">21</span><span class="p">,</span> <span class="mi">25</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">float</span><span class="p">)</span>    <span class="c1"># dtype=float is necessary to avoid TypeError due to conflicts with Cython</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">12</span><span class="p">,</span> <span class="mi">35</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">28</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">float</span><span class="p">)</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">])</span>

<span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">hd</span><span class="o">.</span><span class="n">geomedian</span><span class="p">(</span><span class="n">data</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<div class="output_wrapper">
<div class="output">
<div class="output_area">
<div class="prompt output_prompt">Out[12]:</div>
<div class="output_text output_subarea output_execute_result">
<pre>array([17.06557376, 22.22950821])</pre>
</div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<div id="Advanced: Why use median instead of mean for outliers?" style=""></div><h4 id="2.3.3.-Why-use-median-instead-of-mean-with-outliers?">2.3.3. Why use median instead of mean with outliers?<a class="anchor-link" href="#2.3.3.-Why-use-median-instead-of-mean-with-outliers?">¶</a></h4><p>In <a class="internal-link" href="#fig-15">Figure 15</a>, where 15 slopes and intercepts were fitted, the TheilSen regressor uses the spatial median of these parameters rather than their mean. This choice is informed by the median's superior resilience to outliers, which can be understood deeper from two perspectives:</p>
<div><hr/></div><div id="1. Effect of the squared term"></div><h5 id="2.3.3.1.-Effect-of-the-squared-term">2.3.3.1. Effect of the squared term<a class="anchor-link" href="#2.3.3.1.-Effect-of-the-squared-term">¶</a></h5><p>The squared term in the mean calculation, stemming from the least-squares method (which converges to the mean as previously demonstrated <a class="internal-link" href="#The equation converges to mean, and inherits the statistical properties of mean">above</a>), amplifies the influence of extreme data points. In a 2D or n-dimensional space, the mean, or centroid, is the point that minimizes the squared sum of distances from each point, effectively minimizing the <em>$L_2$-norm squared</em> <a class="internal-link" href="#eq-1">eq-1</a>. This minimization is akin to reducing the average distances from all points. Due to the squaring of distances, outliers significantly impact the mean, as illustrated in <a class="internal-link" href="#fig-3">Figure 3</a>.</p>
<p>In contrast, the spatial median employs the <em>$L_2$-norm</em> <a class="internal-link" href="#eq-2">eq-2</a>. While both equations incorporate a squared term, the spatial median involves taking the square root of the sum of these squared distances, which lessens the impact of extreme values. Note that for 1D array ($k=1$, not applicable to 2D regression), the spatial median is equivalent to 1D median because <a class="internal-link" href="#eq-2">eq-2</a> equates to <a class="internal-link" href="#eq-1">eq-1</a>.</p>
<div id="2. Measure of central tendency"></div><h5 id="2.3.3.2.-Measure-of-central-tendency">2.3.3.2. Measure of central tendency<a class="anchor-link" href="#2.3.3.2.-Measure-of-central-tendency">¶</a></h5><p>Median is a better measure of central tendency than mean in presence of outliers. For simple illustration, consider an array $x = [1, 2, 3, 4, 5]$, where both the mean and median are 3. Introducing an outlier to form $x^{'} = [1,2,3,4,5, 5000]$ shifts the median only slightly to 3.5, while the mean soars to 836. This is not a good representation of the point in which most data points cluster.</p>
<p>For a detailed demonstration, consider a Theil-Sen regression with the same dataset as in the previous RANSAC example <a class="internal-link" href="#RANSAC code snippets">above</a>, featuring $n=50$ points. The dataset, generated from the model $y = 92x + 30$ with some outliers, leads to $\binom{50}{2} = 1225$ unique samples, each yielding a slope and an intercept. The distribution of these fitted parameters is depicted in <a class="internal-link" href="#fig-18">Figure 18</a>.</p>
<p>The distribution of slopes (left plot) exhibits rightward skewness. The median slope, calculated as 74.6, aligns more closely with the true model parameter (92) used to generate the data. This aligns with the theory that the median is less influenced by outliers than the mean, providing a better representation of central tendency, as also illustrated in <a class="internal-link" href="#fig-2">Figure 2</a> above.</p>
<div class="row" id="fig-18" style="">
<div class="col"><img src="jupyter_images/distribution of slopes and intercepts.png"/></div>
<div class="col-12"><p class="image-description"><strong>Figure 18:</strong> Displays the distribution of fitted parameters from the 1225 samples. The model's true parameters are defined by $y=92x + 30$. The left plot, representing the slope distribution, highlights the median as a more accurate representation of the central clustering of data points, underscoring the median's superiority over the mean in outlier-affected scenarios. Conversely, in the intercept distribution (right plot), the mean and median are nearly identical, a result of the intercepts' normal (symmetric) distribution. It's crucial to recognize that the medians depicted here are one-dimensional, used solely for illustrative purposes, and do not represent spatial medians.</p></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<div class="solution_panel closed">
<div class="solution_title">
<p class="solution_title_string">Source Code For Figure (18)</p>
<ul class="nav navbar-right panel_toolbox">
<li><a class="collapse-link"><i class="fa fa-chevron-down"></i></a></li>
</ul>
<div class="clearfix"></div>
</div>
<div class="solution_content">
<pre>
            <code class="language-python">
from itertools import combinations
import random
import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression


###################################### data ######################################

X = np.array([ 0.15, -0.34, 0.32, 0.43, -0.4, -0.04, -0.51, 0.3, 0.47, 0.12, 0.08, 0.04, -0.08,
              -0.23, 0.08, -0.03, 0.03, 0.04, 0.01, 0.06, 0.03, 0., -0.04, -0.18, -0.19, -0.06,
              -0.26, -0.16, 0.13, 0.09, 0.03, -0.03, 0.04, 0.14, -0.01, 0.4, -0.06, 0.15, 0.08,
              0.05, -0.15, -0.09, -0.15, -0.11, -0.07, -0.19, -0.06, 0.17, 0.23, 0.18]).reshape(-1, 1)
y = np.array([17.44, 25.46, 18.61, 26.07, 24.96, -1.22, 26.45, 26.5, 20.57, 3.08, 35.9 , 32.47,
              20.84, 13.37, 42.44, 27.23, 35.65, 29.51, 31.28, 41.34, 32.19, 33.67, 25.64, 9.3,
              14.63, 25.1, 4.69, 14.42, 47.53, 33.82, 32.2 , 24.81, 32.64, 45.11, 26.76, 68.01,
              23.39, 43.49, 37.88, 36.01, 16.32, 19.77, 16.34, 19.57, 29.28, 16.62, 24.39, 43.77,
              50.46, 47.09])

############################# parameter optimization ##################################

n_samples, n_features = X.shape
n_subsamples = 2
fit_intercept = True

# "n choose k" -> n Combination k number of samples.
indices = np.array(list(combinations(range(n_samples), n_subsamples)))

parameters = []
for subset in indices:
    X_subset = X[subset]
    y_subset = y[subset]
    model = LinearRegression(fit_intercept=fit_intercept)
    model.fit(X_subset, y_subset)
    parameters.append([model.intercept_, model.coef_[0]])
parameters = np.vstack(parameters)

intercepts = parameters[:, 0]
slopes = parameters[:, 1]

###################################### plotting ######################################

fig, axes = plt.subplots(1, 2, figsize=(10, 4))

items = [slopes, intercepts]
nbins = [400, 400]

for ax, item, nbin in zip(axes, items, nbins):

    mean = np.mean(item)
    median = np.median(item)

    ax.hist(item, bins=nbin, histtype='stepfilled', edgecolor='k', alpha=0.4, color='grey',)
    ax.axvline(x=median, color='r', alpha=0.7, label='Median=%.1f' % median)
    ax.axvline(x=mean, color='k', alpha=0.7, label='Mean=%.1f' % mean)

    ax.legend(loc='upper left', ncol=1)
    ax.grid(axis='y', linestyle='--', color='#acacac', alpha=0.5)
    ax.set_ylim(0, 340)

    ax.spines['top'].set_visible(False)
    ax.spines['right'].set_visible(False)

    ax.set_ylabel("Occurrences", fontsize=12)
    ax.text(0.99, 0.1, 'aegis4048.github.io', fontsize=10, ha='right', va='center',
        transform=ax.transAxes, color='grey', alpha=0.5)

axes[0].set_xlim(-275, 300)
axes[1].set_xlim(-15, 65)

axes[0].set_xlabel("Slope range", fontsize=12)
axes[1].set_xlabel("Intercept range", fontsize=12)

def setbold(txt):
    return ' '.join([r"$\bf{" + item + "}$" for item in txt.split(' ')])

bold_txt = setbold('Distribution of Slopes & Intercepts, ')
plain_txt = r'comparison of median vs mean, 1225 samples'
fig.suptitle(bold_txt + plain_txt, verticalalignment='top', x=0, horizontalalignment='left', fontsize=14, y=0.98)
yloc = 0.88
ax.annotate('', xy=(0.01, yloc + 0.01), xycoords='figure fraction', xytext=(1.02, yloc + 0.01),
            arrowprops=dict(arrowstyle="-", color='k', lw=0.7))

fig.tight_layout()
            </code>
        </pre>
</div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<div style="margin-top: -15px;"></div><p><a class="internal-link" href="#fig-19">Figure 19</a> compares the performance of mean-based and median-based parameter estimators. The median-based estimator $y=74.6x + 29.5$ is observed to more closely approximate the true population model $y=92x + 30$ than other estimators.</p>
<p>However, it's noteworthy that the spatial median TheilSen regressor does not align perfectly with the true population parameters as RANSAC does, as shown <a class="internal-link" href="#RANSAC code snippets">above</a>. This difference arises because RANSAC identifies and excludes outliers before optimizing parameters, whereas TheilSen attempts to lessen the impact of outliers by taking the square root of the squared sum of residuals. Therefore, if outlier exclusion is preferable to mitigation, RANSAC tends to outperform TheilSen.</p>
<div class="row" id="fig-19" style="">
<div class="col"><img src="jupyter_images/mean vs median regression comparison.png"/></div>
<div class="col-12"><p class="image-description"><strong>Figure 19:</strong> From the original 50 data points, 1225 samples of size $k=2$ are generated, and 1225 lines (light grey) are fitted. The median slope and intercept model (orange line) demonstrates greater robustness to outliers compared to the mean-based (blue) and OLS models (green).</p></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<div class="solution_panel closed">
<div class="solution_title">
<p class="solution_title_string">Source Code For Figure (19)</p>
<ul class="nav navbar-right panel_toolbox">
<li><a class="collapse-link"><i class="fa fa-chevron-down"></i></a></li>
</ul>
<div class="clearfix"></div>
</div>
<div class="solution_content">
<pre>
            <code class="language-python">
from itertools import combinations
import random
import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression, TheilSenRegressor
from scipy.optimize import minimize


###################################### data ######################################

X = np.array([ 0.15, -0.34, 0.32, 0.43, -0.4, -0.04, -0.51, 0.3, 0.47, 0.12, 0.08, 0.04, -0.08,
              -0.23, 0.08, -0.03, 0.03, 0.04, 0.01, 0.06, 0.03, 0., -0.04, -0.18, -0.19, -0.06,
              -0.26, -0.16, 0.13, 0.09, 0.03, -0.03, 0.04, 0.14, -0.01, 0.4, -0.06, 0.15, 0.08,
              0.05, -0.15, -0.09, -0.15, -0.11, -0.07, -0.19, -0.06, 0.17, 0.23, 0.18]).reshape(-1, 1)
y = np.array([17.44, 25.46, 18.61, 26.07, 24.96, -1.22, 26.45, 26.5, 20.57, 3.08, 35.9 , 32.47,
              20.84, 13.37, 42.44, 27.23, 35.65, 29.51, 31.28, 41.34, 32.19, 33.67, 25.64, 9.3,
              14.63, 25.1, 4.69, 14.42, 47.53, 33.82, 32.2 , 24.81, 32.64, 45.11, 26.76, 68.01,
              23.39, 43.49, 37.88, 36.01, 16.32, 19.77, 16.34, 19.57, 29.28, 16.62, 24.39, 43.77,
              50.46, 47.09])

############################# parameter optimization ##################################

n_samples, n_features = X.shape
n_subsamples = 2
fit_intercept = True

# "n choose k" -> n Combination k number of samples.
indices = np.array(list(combinations(range(n_samples), n_subsamples)))

parameters = []
for subset in indices:
    X_subset = X[subset]
    y_subset = y[subset]
    model = LinearRegression(fit_intercept=fit_intercept)
    model.fit(X_subset, y_subset)
    parameters.append([model.intercept_, model.coef_[0]])
parameters = np.vstack(parameters)

intercepts = parameters[:, 0]
slopes = parameters[:, 1]

########################## spatial median approximation ###############################

# L2 loss - euclidean distance
def L2_objective_func(point, _x, _y):
    return np.sum(np.sqrt((point[0] - _x)**2 + (point[1] - _y)**2))

init_guess = [np.mean(intercepts), np.mean(slopes)]  # starting with mean is a good guess to reduce computational cost
result_L2 = minimize(L2_objective_func, init_guess, args=(intercepts, slopes), method='Nelder-Mead')

slope_spatial_median = result_L2.x[1]
intercept_spatial_median = result_L2.x[0]
y_pred_spatial_median = X * slope_spatial_median + intercept_spatial_median

################################### result validation #################################

# Checks that the implemented codes here agree with the sklearn implementation
TS = TheilSenRegressor().fit(X, y)
y_pred_TS = TS.predict(X)

np.testing.assert_almost_equal(slope_spatial_median, TS.coef_[0], decimal=2)
np.testing.assert_almost_equal(intercept_spatial_median, TS.intercept_, decimal=2)

########################################## OLS #######################################

ols = LinearRegression().fit(X, y)
y_pred_ols = ols.predict(X)

##################################### means ##########################################

slope_mean = np.mean(slopes)
intercept_mean = np.mean(intercepts)
y_pred_mean = slope_mean * X + intercept_mean

###################################### plotting ######################################

fig, ax = plt.subplots(figsize=(8, 4.5))

ax.scatter(X, y)
ax.plot(X, y_pred_mean, label='TS Mean     : y = %.1fx + %.1f' % (slope_mean, intercept_mean))
ax.plot(X, y_pred_spatial_median, label='TS Median  : y = %.1fx + %.1f' % (slope_spatial_median, intercept_spatial_median))
ax.plot(X, y_pred_ols, label='OLS            : y = %.1fx + %.1f' % (ols.coef_[0], ols.intercept_), alpha=0.5)

for slope, intercept in zip(slopes, intercepts):
    y_pred_sample = slope * X + intercept
    ax.plot(X, y_pred_sample, alpha=0.03, zorder=-99, color='silver')
ax.plot(X, y_pred_sample, alpha=0.01, zorder=-99, color='lightgrey', label='Random sample fit, $k=2$')

lg = ax.legend(loc='upper left', ncol=1)
for i, lh in enumerate(lg.legendHandles):
    lh.set_alpha(1)

ax.grid(axis='y', linestyle='--', color='#acacac', alpha=0.5)
ax.spines['top'].set_visible(False)
ax.spines['right'].set_visible(False)
ax.set_ylim(-12, 72)

ax.set_xlabel('X', fontsize=13)
ax.set_ylabel('y', fontsize=13)
ax.text(0.99, 0.1, 'aegis4048.github.io', fontsize=10, ha='right', va='center',
    transform=ax.transAxes, color='grey', alpha=0.5)
ax.text(0.02, 0.62, 'True Solution: y = 92x + 30', fontsize=12, ha='left', va='center',
    transform=ax.transAxes, alpha=1, color='r')

def setbold(txt):
    return ' '.join([r"$\bf{" + item + "}$" for item in txt.split(' ')])

bold_txt = setbold('TheilSen Regression')
plain_txt = r', robustness of median to the outliers'
fig.suptitle(bold_txt + plain_txt, verticalalignment='top', x=0, horizontalalignment='left', fontsize=13, y=0.96)
yloc = 0.88
ax.annotate('', xy=(0.01, yloc + 0.01), xycoords='figure fraction', xytext=(1.02, yloc + 0.01),
            arrowprops=dict(arrowstyle="-", color='k', lw=0.7))

fig.tight_layout()
            </code>
        </pre>
</div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<div id="Advanced: Why use median instead of mean for outliers?" style=""></div><h4 id="2.3.4.-Theil-Sen-code-snippets">2.3.4. Theil-Sen code snippets<a class="anchor-link" href="#2.3.4.-Theil-Sen-code-snippets">¶</a></h4><p>For quick copy-paste, replace <code>X</code> and <code>y</code> with your own data. Make sure to reshape your <code>X</code> so that it is a 2D <code>numpy.ndarray</code> object with shape like <code>(13, 1)</code>.</p>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In [9]:</div>
<div class="inner_cell">
<div class="input_area">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">TheilSenRegressor</span>
<span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">linear_model</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="c1"># sample data</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span> <span class="mf">0.15</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.34</span><span class="p">,</span> <span class="mf">0.32</span><span class="p">,</span> <span class="mf">0.43</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.4</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.04</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.51</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">,</span> <span class="mf">0.47</span><span class="p">,</span> <span class="mf">0.12</span><span class="p">,</span> <span class="mf">0.08</span><span class="p">,</span> <span class="mf">0.04</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.08</span><span class="p">,</span> 
              <span class="o">-</span><span class="mf">0.23</span><span class="p">,</span> <span class="mf">0.08</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.03</span><span class="p">,</span> <span class="mf">0.03</span><span class="p">,</span> <span class="mf">0.04</span><span class="p">,</span> <span class="mf">0.01</span><span class="p">,</span> <span class="mf">0.06</span><span class="p">,</span> <span class="mf">0.03</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.04</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.18</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.19</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.06</span><span class="p">,</span> 
              <span class="o">-</span><span class="mf">0.26</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.16</span><span class="p">,</span> <span class="mf">0.13</span><span class="p">,</span> <span class="mf">0.09</span><span class="p">,</span> <span class="mf">0.03</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.03</span><span class="p">,</span> <span class="mf">0.04</span><span class="p">,</span> <span class="mf">0.14</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.01</span><span class="p">,</span> <span class="mf">0.4</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.06</span><span class="p">,</span> <span class="mf">0.15</span><span class="p">,</span> <span class="mf">0.08</span><span class="p">,</span> 
              <span class="mf">0.05</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.15</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.09</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.15</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.11</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.07</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.19</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.06</span><span class="p">,</span> <span class="mf">0.17</span><span class="p">,</span> <span class="mf">0.23</span><span class="p">,</span> <span class="mf">0.18</span><span class="p">])</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">17.44</span><span class="p">,</span> <span class="mf">25.46</span><span class="p">,</span> <span class="mf">18.61</span><span class="p">,</span> <span class="mf">26.07</span><span class="p">,</span> <span class="mf">24.96</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.22</span><span class="p">,</span> <span class="mf">26.45</span><span class="p">,</span> <span class="mf">26.5</span><span class="p">,</span> <span class="mf">20.57</span><span class="p">,</span> <span class="mf">3.08</span><span class="p">,</span> <span class="mf">35.9</span> <span class="p">,</span> <span class="mf">32.47</span><span class="p">,</span> 
              <span class="mf">20.84</span><span class="p">,</span> <span class="mf">13.37</span><span class="p">,</span> <span class="mf">42.44</span><span class="p">,</span> <span class="mf">27.23</span><span class="p">,</span> <span class="mf">35.65</span><span class="p">,</span> <span class="mf">29.51</span><span class="p">,</span> <span class="mf">31.28</span><span class="p">,</span> <span class="mf">41.34</span><span class="p">,</span> <span class="mf">32.19</span><span class="p">,</span> <span class="mf">33.67</span><span class="p">,</span> <span class="mf">25.64</span><span class="p">,</span> <span class="mf">9.3</span><span class="p">,</span> 
              <span class="mf">14.63</span><span class="p">,</span> <span class="mf">25.1</span><span class="p">,</span> <span class="mf">4.69</span><span class="p">,</span> <span class="mf">14.42</span><span class="p">,</span> <span class="mf">47.53</span><span class="p">,</span> <span class="mf">33.82</span><span class="p">,</span> <span class="mf">32.2</span> <span class="p">,</span> <span class="mf">24.81</span><span class="p">,</span> <span class="mf">32.64</span><span class="p">,</span> <span class="mf">45.11</span><span class="p">,</span> <span class="mf">26.76</span><span class="p">,</span> <span class="mf">68.01</span><span class="p">,</span> 
              <span class="mf">23.39</span><span class="p">,</span> <span class="mf">43.49</span><span class="p">,</span> <span class="mf">37.88</span><span class="p">,</span> <span class="mf">36.01</span><span class="p">,</span> <span class="mf">16.32</span><span class="p">,</span> <span class="mf">19.77</span><span class="p">,</span> <span class="mf">16.34</span><span class="p">,</span> <span class="mf">19.57</span><span class="p">,</span> <span class="mf">29.28</span><span class="p">,</span> <span class="mf">16.62</span><span class="p">,</span> <span class="mf">24.39</span><span class="p">,</span> <span class="mf">43.77</span><span class="p">,</span> 
              <span class="mf">50.46</span><span class="p">,</span> <span class="mf">47.09</span><span class="p">])</span>

<span class="c1"># fit and predict: Huber </span>
<span class="n">TS</span> <span class="o">=</span> <span class="n">TheilSenRegressor</span><span class="p">(</span><span class="n">n_subsamples</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>   <span class="c1"># 2 by default for 2D linear regression. </span>
                                                   <span class="c1"># Increasing it will decrease robustness (not recommended)</span>
<span class="n">y_pred_TS</span> <span class="o">=</span> <span class="n">TS</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

<span class="c1"># retrieve the fitted parameters</span>
<span class="n">coefs</span> <span class="o">=</span> <span class="n">TS</span><span class="o">.</span><span class="n">coef_</span>
<span class="n">intercept</span> <span class="o">=</span> <span class="n">TS</span><span class="o">.</span><span class="n">intercept_</span>

<span class="c1"># TheilSen does not explicitly identify outliers from inliers.</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">"TheilSen -----------------------------------</span><span class="se">\n</span><span class="s2">"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"Coefficients         :"</span><span class="p">,</span> <span class="n">coefs</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"Intercept            :"</span><span class="p">,</span> <span class="n">intercept</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"</span><span class="se">\n</span><span class="s2">------------------------------------------"</span><span class="p">)</span>

<span class="c1"># fit and predict: Ordinary Least Squares</span>
<span class="n">ols</span> <span class="o">=</span> <span class="n">linear_model</span><span class="o">.</span><span class="n">LinearRegression</span><span class="p">()</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="n">y_pred_ols</span> <span class="o">=</span> <span class="n">ols</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

<span class="c1"># plot</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s1">'green'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">'Data'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">'TheilSen Regressor'</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y_pred_ols</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">'OLS'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y_pred_TS</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">'TheilSen'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">();</span>
</pre></div>
</div>
</div>
</div>
<div class="output_wrapper">
<div class="output">
<div class="output_area">
<div class="prompt"></div>
<div class="output_subarea output_stream output_stdout output_text">
<pre>TheilSen -----------------------------------

Coefficients         : [73.65998569]
Intercept            : 29.378865864952733

------------------------------------------
</pre>
</div>
</div>
<div class="output_area">
<div class="prompt"></div>
<div class="output_png output_subarea">
<img src="data:image/png;base64,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
"/>
</div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="2.4.-Summary">2.4. Summary<a class="anchor-link" href="#2.4.-Summary">¶</a></h3><div class="highlights" id="key1">
<div class="highlights-title">1. RANSAC regressor</div>
<div class="highlights-content"><a class="internal-link" href="#RANSAC regressor">Quick Scroll:</a> Identifies and excludes outliers prior to fitting the final model. The outliers are defined as the points with residuals that exceed the Median Absolute Deviation (MAD). It repeats random sampling and fitting a test model until enough inliers are detected.</div>
</div><div class="highlights" id="key2">
<div class="highlights-title">2. Huber regressor</div>
<div class="highlights-content"><a class="internal-link" href="#Huber regressor">Quick Scroll:</a> Detects outliers that exceed a certain threshold, and selectively apply different loss functions for outliers vs. inliers.</div>
</div><div class="highlights" id="key3">
<div class="highlights-title">3. Theil-Sen regressor</div>
<div class="highlights-content"><a class="internal-link" href="#Theil-Sen regressor">Quick Scroll:</a> Fits OLS on all possible combinations of points (each of size 2 by default) and returns their spatial median slope and intercept as the final parameters.</div>
</div><p>In the order of robustness:  RANSAC > Theil-Sen > Huber.</p>
<p>RANSAC shows the best robustness because it identifies and excludes outliers prior to fitting the final model, whereas the other two attempts to <em>dampen</em> the effect of outliers instead of excluding them. When in doubt, use RANSAC. However, when reporting a result obtained with RANSAC, make sure to fix the random seed <code>RANSACRegressor(random_state=3)</code>, as explained <a class="internal-link" href="#warning-RANSAC">above</a>.</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="3.-Extension-to-3D+-multivariate-linear-regressions">3. Extension to 3D+ multivariate linear regressions<a class="anchor-link" href="#3.-Extension-to-3D+-multivariate-linear-regressions">¶</a></h2><p>All three of the robust models are applicable to 3D or more n-dimensional regressions. Below is the simple code snippet for multivariate linear regression with 2 features and an intercept.</p>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In [2]:</div>
<div class="inner_cell">
<div class="input_area">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">datasets</span>
<span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LinearRegression</span><span class="p">,</span> <span class="n">RANSACRegressor</span><span class="p">,</span> <span class="n">HuberRegressor</span><span class="p">,</span> <span class="n">TheilSenRegressor</span>

<span class="c1"># generate sample data</span>
<span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">coef</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">make_regression</span><span class="p">(</span>
    <span class="n">n_samples</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">n_features</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">n_informative</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">noise</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">coef</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
<span class="p">)</span>

<span class="c1"># Ordinary Least Squares</span>
<span class="n">ols</span> <span class="o">=</span> <span class="n">linear_model</span><span class="o">.</span><span class="n">LinearRegression</span><span class="p">()</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="n">y_pred_ols</span> <span class="o">=</span> <span class="n">ols</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

<span class="c1"># RANSAC</span>
<span class="n">ransac</span> <span class="o">=</span> <span class="n">RANSACRegressor</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="n">y_pred_ransac</span> <span class="o">=</span> <span class="n">ransac</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

<span class="c1"># Huber</span>
<span class="n">huber</span> <span class="o">=</span> <span class="n">HuberRegressor</span><span class="p">()</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="n">y_pred_huber</span> <span class="o">=</span> <span class="n">huber</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

<span class="c1"># TheilSen</span>
<span class="n">TS</span> <span class="o">=</span> <span class="n">TheilSenRegressor</span><span class="p">()</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="n">y_pred_TS</span> <span class="o">=</span> <span class="n">TS</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">"OLS --------------------------------------"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"Coefficients         :"</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">ols</span><span class="o">.</span><span class="n">coef_</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"Intercept            :"</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">ols</span><span class="o">.</span><span class="n">intercept_</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
<span class="nb">print</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"RANSAC -----------------------------------"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"Coefficients         :"</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">ransac</span><span class="o">.</span><span class="n">estimator_</span><span class="o">.</span><span class="n">coef_</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"Intercept            :"</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">ransac</span><span class="o">.</span><span class="n">estimator_</span><span class="o">.</span><span class="n">intercept_</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
<span class="nb">print</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"Huber ------------------------------------"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"Coefficients         :"</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">huber</span><span class="o">.</span><span class="n">coef_</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"Intercept            :"</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">huber</span><span class="o">.</span><span class="n">intercept_</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
<span class="nb">print</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"TheilSen ---------------------------------"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"Coefficients         :"</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">TS</span><span class="o">.</span><span class="n">coef_</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"Intercept            :"</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">TS</span><span class="o">.</span><span class="n">intercept_</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"------------------------------------------"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<div class="output_wrapper">
<div class="output">
<div class="output_area">
<div class="prompt"></div>
<div class="output_subarea output_stream output_stdout output_text">
<pre>OLS --------------------------------------
Coefficients         : [43.07  1.73]
Intercept            : 1.41

RANSAC -----------------------------------
Coefficients         : [42.62  1.11]
Intercept            : 2.08

Huber ------------------------------------
Coefficients         : [43.13  2.07]
Intercept            : 1.23

TheilSen ---------------------------------
Coefficients         : [42.15  2.95]
Intercept            : 0.71
------------------------------------------

</pre>
</div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="3.1.-Visual-demonstrations">3.1. Visual demonstrations<a class="anchor-link" href="#3.1.-Visual-demonstrations">¶</a></h3><p>(<em>Note that for models with more than 3D features, including intercept, they can't be visualized but the idea of robust regression still extends beyond 3D Cartesian space.</em>)</p>
<p>For demonstrations, a 3D sample data set of 200 points is generated from the true model: $y = 200 x_{1} + 28x_{2} + 300$. Random Gaussian noise is added to the $y$ values to emulate the randomness and variability present in real-life data. Currently, the dataset is free of outliers. Fitting an OLS regression results in <a class="internal-link" href="#fig-20">Figure 20</a>:</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<div class="row" id="fig-20" style="">
<div class="col"><img src="jupyter_images/OLS_3d_free.png"/></div>
<div class="col-12"><p class="image-description"><strong>Figure 20:</strong> This illustration presents the 3D linear model alongside the dataset within a 3D Cartesian space. When an OLS model is applied to this dataset, it closely approximates the true model parameters, particularly in scenarios devoid of outliers. The middle plot highlights how the data points are closely aligned with the 3D model fit (illustrated as a blue plane).</p></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<div class="solution_panel closed">
<div class="solution_title">
<p class="solution_title_string">Source Code For Figure (20)</p>
<ul class="nav navbar-right panel_toolbox">
<li><a class="collapse-link"><i class="fa fa-chevron-down"></i></a></li>
</ul>
<div class="clearfix"></div>
</div>
<div class="solution_content">
<pre>
            <code class="language-python">
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn import linear_model
from mpl_toolkits.mplot3d import Axes3D
from sklearn.linear_model import (LinearRegression, HuberRegressor,
                                RANSACRegressor, TheilSenRegressor)

###################################### Sample Data Generation ###################################

np.random.seed(0)   # for reproducibility

num_samples = 200  

# Porosity
x1_min, x1_max = 5, 15
mean_x1 = (x1_min + x1_max) / 2  
std_dev_x1 = (x1_max - x1_min) / 4 
x1 = np.random.normal(mean_x1, std_dev_x1, num_samples)

# Brittleness
x2_min, x2_max = 20, 85
mean_x2 = (x2_min + x2_max) / 2 
std_dev_x2 = (x2_max - x2_min) / 4  
x2 = np.random.normal(mean_x2, std_dev_x2, num_samples)  

# Reshape X for comptibility with regression models
X = np.vstack((x1, x2)).T

# True model
Y = 200* x1 + 28 * x2 + 300

# Add Gaussian noise to Y
noise = np.random.normal(0, 200, num_samples)  # Mean = 0, Standard deviation = 50
Y = Y + noise

######################## Prepare model data point for visualization ###############################

x = X[:, 0]
y = X[:, 1]
z = Y

x_pred = np.linspace(0, 25, 30)   # range of porosity values
y_pred = np.linspace(0, 100, 30)  # range of brittleness values
xx_pred, yy_pred = np.meshgrid(x_pred, y_pred)
model_viz = np.array([xx_pred.flatten(), yy_pred.flatten()]).T

################################################ Train #############################################

model = LinearRegression()
model.fit(X, Y)
predicted = model.predict(model_viz)

############################################## Plot ################################################

plt.style.use('default')

fig = plt.figure(figsize=(12, 4))

ax1 = fig.add_subplot(131, projection='3d')
ax2 = fig.add_subplot(132, projection='3d')
ax3 = fig.add_subplot(133, projection='3d')

axes = [ax1, ax2, ax3]

for ax in axes:
    ax.plot(x, y, z, color='k', zorder=15, linestyle='none', marker='o', alpha=0.5)
    ax.scatter(xx_pred.flatten(), yy_pred.flatten(), predicted, facecolor=(0,0,0,0), s=20, edgecolor='#70b3f0')
    ax.set_xlabel(r'$X_{1}$', fontsize=12)
    ax.set_ylabel(r'$X_{2}$', fontsize=12)
    ax.set_zlabel('Y', fontsize=12)
    ax.locator_params(nbins=4, axis='x')
    ax.locator_params(nbins=5, axis='x')
    ax.set_xlim(0, 25)
    ax.set_ylim(100, 0)
    ax.set_zlim(0, 8000)


ax1.text2D(0.2, 0.32, 'aegis4048.github.io', fontsize=13, ha='center', va='center',
           transform=ax1.transAxes, color='grey', alpha=0.5)
ax2.text2D(0.5, 0.32, 'aegis4048.github.io', fontsize=13, ha='center', va='center',
           transform=ax2.transAxes, color='grey', alpha=0.5)
ax3.text2D(0.85, 0.85, 'aegis4048.github.io', fontsize=13, ha='center', va='center',
           transform=ax3.transAxes, color='grey', alpha=0.5)

ax1.view_init(elev=30, azim=50)
ax2.view_init(elev=2, azim=60)
ax3.view_init(elev=60, azim=165)

fig.tight_layout()

def setbold(txt):
    return ' '.join([r"$\bf{" + item + "}$" for item in txt.split(' ')])

bold_txt = setbold(' OLS 3D Linear Regression (without outliers), ')
plain_txt = r'fitted model:   $y = $ %.1f$x_{1}$ + %.1f$x_{2}$ + %.1f' % (model.coef_[0], model.coef_[1], model.intercept_)
fig.suptitle(bold_txt + plain_txt, verticalalignment='top', x=0, horizontalalignment='left', fontsize=16, y=1.03)
yloc = 0.95
ax.annotate('', xy=(0.01, yloc + 0.01), xycoords='figure fraction', xytext=(1.02, yloc + 0.01),
            arrowprops=dict(arrowstyle="-", color='k', lw=0.7))

fig.text(0.48, 0.8, 'true model:   $y = $ 200$x_{1}$ + 28$x_{2}$ + 300', ha='center', va='center', fontsize=16, color='grey');
            </code>
        </pre>
</div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Now we will introduce 20 outlier points, constituting 10% of the original dataset, and reapply the OLS regression, as depicted in <a class="internal-link" href="#fig-21">Figure 21</a>. It is noticeable that the OLS parameters fitted in this scenario exhibit a more significant deviation from the population parameters compared to the initial fit that lacked outliers.</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<div class="row" id="fig-21" style="">
<div class="col"><img src="jupyter_images/OLS_3d.png"/></div>
<div class="col-12"><p class="image-description"><strong>Figure 21:</strong> This figure highlights outlier points in red. The introduction of these outliers has caused a noticeable shift in the fitted blue plane, which is now skewed towards these outliers, affecting the accuracy of the model fit. The left plot distinctly shows the misalignment of the model plane with the main dataset. For a comparative analysis, refer to the visual and numerical parameters in <a class="internal-link" href="#fig-20">Figure 20</a>.</p></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<div class="solution_panel closed">
<div class="solution_title">
<p class="solution_title_string">Source Code For Figure (21)</p>
<ul class="nav navbar-right panel_toolbox">
<li><a class="collapse-link"><i class="fa fa-chevron-down"></i></a></li>
</ul>
<div class="clearfix"></div>
</div>
<div class="solution_content">
<pre>
            <code class="language-python">
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn import linear_model
from mpl_toolkits.mplot3d import Axes3D
from sklearn.linear_model import LinearRegression

###################################### Sample Data Generation ###################################

np.random.seed(0)   # for reproducibility

num_samples = 200  

# Porosity
x1_min, x1_max = 5, 15
mean_x1 = (x1_min + x1_max) / 2  
std_dev_x1 = (x1_max - x1_min) / 4 
x1 = np.random.normal(mean_x1, std_dev_x1, num_samples)

# Brittleness
x2_min, x2_max = 20, 85
mean_x2 = (x2_min + x2_max) / 2 
std_dev_x2 = (x2_max - x2_min) / 4  
x2 = np.random.normal(mean_x2, std_dev_x2, num_samples)  

# Reshape X for comptibility with regression models
X = np.vstack((x1, x2)).T

# True model
Y = 200* x1 + 28 * x2 + 300

# Add Gaussian noise to Y
noise = np.random.normal(0, 200, num_samples)  # Mean = 0, Standard deviation = 50
Y_noisy = Y + noise

# Add outliers
num_outliers = int(0.1 * num_samples)                    # define 5% of data to be outliers
outlier_x1 = np.random.uniform(10, 12, num_outliers)     # outlier between range 10 ~ 12
outlier_x2 = np.random.uniform(0, 5, num_outliers)       # outlier between range 0 ~ 5
outlier_Y = np.random.uniform(6500, 7000, num_outliers)  # outlier between range 6500 ~ 7000
X_outliers = np.vstack((outlier_x1, outlier_x2)).T

# Append outliers to the original data
X = np.vstack((X, X_outliers))
Y = np.append(Y_noisy, outlier_Y)

######################## Prepare model data point for visualization ###############################

x = X[:, 0]
y = X[:, 1]
z = Y

x_pred = np.linspace(0, 25, 30)   # range of porosity values
y_pred = np.linspace(0, 100, 30)  # range of brittleness values
xx_pred, yy_pred = np.meshgrid(x_pred, y_pred)
model_viz = np.array([xx_pred.flatten(), yy_pred.flatten()]).T

################################################ Train #############################################

model = LinearRegression()
model.fit(X, Y)
predicted = model.predict(model_viz)

############################################## Plot ################################################

plt.style.use('default')

fig = plt.figure(figsize=(12, 4))

ax1 = fig.add_subplot(131, projection='3d')
ax2 = fig.add_subplot(132, projection='3d')
ax3 = fig.add_subplot(133, projection='3d')

axes = [ax1, ax2, ax3]

for ax in axes:
    ax.plot(x, y, z, color='k', zorder=15, linestyle='none', marker='o', alpha=0.5)
    ax.plot(X_outliers[:, 0], X_outliers[:, 1], outlier_Y, color='r', zorder=15, linestyle='none', marker='x', alpha=1, markersize=10)
    ax.scatter(xx_pred.flatten(), yy_pred.flatten(), predicted, facecolor=(0,0,0,0), s=20, edgecolor='#70b3f0')
    ax.set_xlabel(r'$X_{1}$', fontsize=12)
    ax.set_ylabel(r'$X_{2}$', fontsize=12)
    ax.set_zlabel('Y', fontsize=12)
    ax.locator_params(nbins=4, axis='x')
    ax.locator_params(nbins=5, axis='x')
    ax.set_xlim(0, 25)
    ax.set_ylim(100, 0)
    ax.set_zlim(0, 8000)


ax1.text2D(0.2, 0.32, 'aegis4048.github.io', fontsize=13, ha='center', va='center',
           transform=ax1.transAxes, color='grey', alpha=0.5)
ax2.text2D(0.5, 0.32, 'aegis4048.github.io', fontsize=13, ha='center', va='center',
           transform=ax2.transAxes, color='grey', alpha=0.5)
ax3.text2D(0.85, 0.85, 'aegis4048.github.io', fontsize=13, ha='center', va='center',
           transform=ax3.transAxes, color='grey', alpha=0.5)

ax1.view_init(elev=30, azim=50)
ax2.view_init(elev=2, azim=60)
ax3.view_init(elev=60, azim=165)

fig.tight_layout()

def setbold(txt):
    return ' '.join([r"$\bf{" + item + "}$" for item in txt.split(' ')])

bold_txt = setbold(' OLS 3D Linear Regression (with outliers), ')
plain_txt = r'fitted model:   $y = $ %.1f$x_{1}$ + %.1f$x_{2}$ + %.1f' % (model.coef_[0], model.coef_[1], model.intercept_)
fig.suptitle(bold_txt + plain_txt, verticalalignment='top', x=0, horizontalalignment='left', fontsize=16, y=1.03)
yloc = 0.95
ax.annotate('', xy=(0.01, yloc + 0.01), xycoords='figure fraction', xytext=(1.02, yloc + 0.01),
            arrowprops=dict(arrowstyle="-", color='k', lw=0.7))

fig.text(0.48, 0.8, 'true model:   $y = $ 200$x_{1}$ + 28$x_{2}$ + 300', ha='center', va='center', fontsize=16, color='grey');
            </code>
        </pre>
</div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The below three figures (<a class="internal-link" href="#fig-22">Figure 22</a>, <a class="internal-link" href="#fig-23">Figure 23</a> and <a class="internal-link" href="#fig-24">Figure 24</a>) presents 3D model fits and visualizations for the three robust models: RANSAC, Huber, and TheilSen.</p>
<div class="row" id="fig-22" style="">
<div class="col"><img src="jupyter_images/ransac_3d.png"/></div>
<div class="col-12"><p class="image-description"><strong>Figure 22:</strong> RANSAC robust linear regression on 3D dataset with outliers.</p></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<div class="solution_panel closed">
<div class="solution_title">
<p class="solution_title_string">Source Code For Figure (22)</p>
<ul class="nav navbar-right panel_toolbox">
<li><a class="collapse-link"><i class="fa fa-chevron-down"></i></a></li>
</ul>
<div class="clearfix"></div>
</div>
<div class="solution_content">
<pre>
            <code class="language-python">
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn import linear_model
from mpl_toolkits.mplot3d import Axes3D
from sklearn.linear_model import RANSACRegressor

###################################### Sample Data Generation ###################################

np.random.seed(0)   # for reproducibility

num_samples = 200  

# Porosity
x1_min, x1_max = 5, 15
mean_x1 = (x1_min + x1_max) / 2  
std_dev_x1 = (x1_max - x1_min) / 4 
x1 = np.random.normal(mean_x1, std_dev_x1, num_samples)

# Brittleness
x2_min, x2_max = 20, 85
mean_x2 = (x2_min + x2_max) / 2 
std_dev_x2 = (x2_max - x2_min) / 4  
x2 = np.random.normal(mean_x2, std_dev_x2, num_samples)  

# Reshape X for comptibility with regression models
X = np.vstack((x1, x2)).T

# True model
Y = 200* x1 + 28 * x2 + 300

# Add Gaussian noise to Y
noise = np.random.normal(0, 200, num_samples)  # Mean = 0, Standard deviation = 50
Y_noisy = Y + noise

# Add outliers
num_outliers = int(0.1 * num_samples)                    # define 5% of data to be outliers
outlier_x1 = np.random.uniform(10, 12, num_outliers)     # outlier between range 10 ~ 12
outlier_x2 = np.random.uniform(0, 5, num_outliers)       # outlier between range 0 ~ 5
outlier_Y = np.random.uniform(6500, 7000, num_outliers)  # outlier between range 6500 ~ 7000
X_outliers = np.vstack((outlier_x1, outlier_x2)).T

# Append outliers to the original data
X = np.vstack((X, X_outliers))
Y = np.append(Y_noisy, outlier_Y)

######################## Prepare model data point for visualization ###############################

x = X[:, 0]
y = X[:, 1]
z = Y

x_pred = np.linspace(0, 25, 30)   # range of porosity values
y_pred = np.linspace(0, 100, 30)  # range of brittleness values
xx_pred, yy_pred = np.meshgrid(x_pred, y_pred)
model_viz = np.array([xx_pred.flatten(), yy_pred.flatten()]).T

################################################ Train #############################################

model = RANSACRegressor()
model.fit(X, Y)
predicted = model.predict(model_viz)

############################################## Plot ################################################

plt.style.use('default')

fig = plt.figure(figsize=(12, 4))

ax1 = fig.add_subplot(131, projection='3d')
ax2 = fig.add_subplot(132, projection='3d')
ax3 = fig.add_subplot(133, projection='3d')

axes = [ax1, ax2, ax3]

for ax in axes:
    ax.plot(x, y, z, color='k', zorder=15, linestyle='none', marker='o', alpha=0.5)
    ax.plot(X_outliers[:, 0], X_outliers[:, 1], outlier_Y, color='r', zorder=15, linestyle='none', marker='x', alpha=1, markersize=10)
    ax.scatter(xx_pred.flatten(), yy_pred.flatten(), predicted, facecolor=(0,0,0,0), s=20, edgecolor='#70b3f0')
    ax.set_xlabel(r'$X_{1}$', fontsize=12)
    ax.set_ylabel(r'$X_{2}$', fontsize=12)
    ax.set_zlabel('Y', fontsize=12)
    ax.locator_params(nbins=4, axis='x')
    ax.locator_params(nbins=5, axis='x')
    ax.set_xlim(0, 25)
    ax.set_ylim(100, 0)
    ax.set_zlim(0, 8000)


ax1.text2D(0.2, 0.32, 'aegis4048.github.io', fontsize=13, ha='center', va='center',
           transform=ax1.transAxes, color='grey', alpha=0.5)
ax2.text2D(0.5, 0.32, 'aegis4048.github.io', fontsize=13, ha='center', va='center',
           transform=ax2.transAxes, color='grey', alpha=0.5)
ax3.text2D(0.85, 0.85, 'aegis4048.github.io', fontsize=13, ha='center', va='center',
           transform=ax3.transAxes, color='grey', alpha=0.5)

ax1.view_init(elev=30, azim=50)
ax2.view_init(elev=2, azim=60)
ax3.view_init(elev=60, azim=165)

fig.tight_layout()

def setbold(txt):
    return ' '.join([r"$\bf{" + item + "}$" for item in txt.split(' ')])

bold_txt = setbold(' RANSAC 3D Linear Regression, ')
plain_txt = r'fitted model:   $y = $ %.1f$x_{1}$ + %.1f$x_{2}$ + %.1f' % (
    model.estimator_.coef_[0], model.estimator_.coef_[1], model.estimator_.intercept_
)
fig.suptitle(bold_txt + plain_txt, verticalalignment='top', x=0, horizontalalignment='left', fontsize=16, y=1.03)
yloc = 0.95
ax.annotate('', xy=(0.01, yloc + 0.01), xycoords='figure fraction', xytext=(1.02, yloc + 0.01),
            arrowprops=dict(arrowstyle="-", color='k', lw=0.7))

fig.text(0.48, 0.8, 'true model:   $y = $ 200$x_{1}$ + 28$x_{2}$ + 300', ha='center', va='center', fontsize=16, color='grey');
            </code>
        </pre>
</div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<div class="row" id="fig-23" style="">
<div class="col"><img src="jupyter_images/huber_3d.png"/></div>
<div class="col-12"><p class="image-description"><strong>Figure 23:</strong> Huber robust linear regression on 3D dataset with outliers.</p></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<div class="solution_panel closed">
<div class="solution_title">
<p class="solution_title_string">Source Code For Figure (23)</p>
<ul class="nav navbar-right panel_toolbox">
<li><a class="collapse-link"><i class="fa fa-chevron-down"></i></a></li>
</ul>
<div class="clearfix"></div>
</div>
<div class="solution_content">
<pre>
            <code class="language-python">

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn import linear_model
from mpl_toolkits.mplot3d import Axes3D
from sklearn.linear_model import HuberRegressor

###################################### Sample Data Generation ###################################

np.random.seed(0)   # for reproducibility

num_samples = 200  

# Porosity
x1_min, x1_max = 5, 15
mean_x1 = (x1_min + x1_max) / 2  
std_dev_x1 = (x1_max - x1_min) / 4 
x1 = np.random.normal(mean_x1, std_dev_x1, num_samples)

# Brittleness
x2_min, x2_max = 20, 85
mean_x2 = (x2_min + x2_max) / 2 
std_dev_x2 = (x2_max - x2_min) / 4  
x2 = np.random.normal(mean_x2, std_dev_x2, num_samples)  

# Reshape X for comptibility with regression models
X = np.vstack((x1, x2)).T

# True model
Y = 200* x1 + 28 * x2 + 300

# Add Gaussian noise to Y
noise = np.random.normal(0, 200, num_samples)  # Mean = 0, Standard deviation = 50
Y_noisy = Y + noise

# Add outliers
num_outliers = int(0.1 * num_samples)                    # define 5% of data to be outliers
outlier_x1 = np.random.uniform(10, 12, num_outliers)     # outlier between range 10 ~ 12
outlier_x2 = np.random.uniform(0, 5, num_outliers)       # outlier between range 0 ~ 5
outlier_Y = np.random.uniform(6500, 7000, num_outliers)  # outlier between range 6500 ~ 7000
X_outliers = np.vstack((outlier_x1, outlier_x2)).T

# Append outliers to the original data
X = np.vstack((X, X_outliers))
Y = np.append(Y_noisy, outlier_Y)

######################## Prepare model data point for visualization ###############################

x = X[:, 0]
y = X[:, 1]
z = Y

x_pred = np.linspace(0, 25, 30)   # range of porosity values
y_pred = np.linspace(0, 100, 30)  # range of brittleness values
xx_pred, yy_pred = np.meshgrid(x_pred, y_pred)
model_viz = np.array([xx_pred.flatten(), yy_pred.flatten()]).T

################################################ Train #############################################

model = HuberRegressor()
model.fit(X, Y)
predicted = model.predict(model_viz)

############################################## Plot ################################################

plt.style.use('default')

fig = plt.figure(figsize=(12, 4))

ax1 = fig.add_subplot(131, projection='3d')
ax2 = fig.add_subplot(132, projection='3d')
ax3 = fig.add_subplot(133, projection='3d')

axes = [ax1, ax2, ax3]

for ax in axes:
    ax.plot(x, y, z, color='k', zorder=15, linestyle='none', marker='o', alpha=0.5)
    ax.plot(X_outliers[:, 0], X_outliers[:, 1], outlier_Y, color='r', zorder=15, linestyle='none', marker='x', alpha=1, markersize=10)
    ax.scatter(xx_pred.flatten(), yy_pred.flatten(), predicted, facecolor=(0,0,0,0), s=20, edgecolor='#70b3f0')
    ax.set_xlabel(r'$X_{1}$', fontsize=12)
    ax.set_ylabel(r'$X_{2}$', fontsize=12)
    ax.set_zlabel('Y', fontsize=12)
    ax.locator_params(nbins=4, axis='x')
    ax.locator_params(nbins=5, axis='x')
    ax.set_xlim(0, 25)
    ax.set_ylim(100, 0)
    ax.set_zlim(0, 8000)


ax1.text2D(0.2, 0.32, 'aegis4048.github.io', fontsize=13, ha='center', va='center',
           transform=ax1.transAxes, color='grey', alpha=0.5)
ax2.text2D(0.5, 0.32, 'aegis4048.github.io', fontsize=13, ha='center', va='center',
           transform=ax2.transAxes, color='grey', alpha=0.5)
ax3.text2D(0.85, 0.85, 'aegis4048.github.io', fontsize=13, ha='center', va='center',
           transform=ax3.transAxes, color='grey', alpha=0.5)

ax1.view_init(elev=30, azim=50)
ax2.view_init(elev=2, azim=60)
ax3.view_init(elev=60, azim=165)

fig.tight_layout()

def setbold(txt):
    return ' '.join([r"$\bf{" + item + "}$" for item in txt.split(' ')])

bold_txt = setbold(' Huber 3D Linear Regression, ')
plain_txt = r'fitted model:   $y = $ %.1f$x_{1}$ + %.1f$x_{2}$ + %.1f' % (model.coef_[0], model.coef_[1], model.intercept_)
fig.suptitle(bold_txt + plain_txt, verticalalignment='top', x=0, horizontalalignment='left', fontsize=16, y=1.03)
yloc = 0.95
ax.annotate('', xy=(0.01, yloc + 0.01), xycoords='figure fraction', xytext=(1.02, yloc + 0.01),
            arrowprops=dict(arrowstyle="-", color='k', lw=0.7))

fig.text(0.48, 0.8, 'true model:   $y = $ 200$x_{1}$ + 28$x_{2}$ + 300', ha='center', va='center', fontsize=16, color='grey');            </code>
        </pre>
</div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<div class="row" id="fig-24" style="">
<div class="col"><img src="jupyter_images/TS_3d.png"/></div>
<div class="col-12"><p class="image-description"><strong>Figure 24:</strong> TheilSen robust linear regression on 3D dataset with outliers.</p></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<div class="solution_panel closed">
<div class="solution_title">
<p class="solution_title_string">Source Code For Figure (24)</p>
<ul class="nav navbar-right panel_toolbox">
<li><a class="collapse-link"><i class="fa fa-chevron-down"></i></a></li>
</ul>
<div class="clearfix"></div>
</div>
<div class="solution_content">
<pre>
            <code class="language-python">
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn import linear_model
from mpl_toolkits.mplot3d import Axes3D
from sklearn.linear_model import TheilSenRegressor

###################################### Sample Data Generation ###################################

np.random.seed(0)   # for reproducibility

num_samples = 200  

# Porosity
x1_min, x1_max = 5, 15
mean_x1 = (x1_min + x1_max) / 2  
std_dev_x1 = (x1_max - x1_min) / 4 
x1 = np.random.normal(mean_x1, std_dev_x1, num_samples)

# Brittleness
x2_min, x2_max = 20, 85
mean_x2 = (x2_min + x2_max) / 2 
std_dev_x2 = (x2_max - x2_min) / 4  
x2 = np.random.normal(mean_x2, std_dev_x2, num_samples)  

# Reshape X for comptibility with regression models
X = np.vstack((x1, x2)).T

# True model
Y = 200* x1 + 28 * x2 + 300

# Add Gaussian noise to Y
noise = np.random.normal(0, 200, num_samples)  # Mean = 0, Standard deviation = 50
Y_noisy = Y + noise

# Add outliers
num_outliers = int(0.1 * num_samples)                    # define 5% of data to be outliers
outlier_x1 = np.random.uniform(10, 12, num_outliers)     # outlier between range 10 ~ 12
outlier_x2 = np.random.uniform(0, 5, num_outliers)       # outlier between range 0 ~ 5
outlier_Y = np.random.uniform(6500, 7000, num_outliers)  # outlier between range 6500 ~ 7000
X_outliers = np.vstack((outlier_x1, outlier_x2)).T

# Append outliers to the original data
X = np.vstack((X, X_outliers))
Y = np.append(Y_noisy, outlier_Y)

######################## Prepare model data point for visualization ###############################

x = X[:, 0]
y = X[:, 1]
z = Y

x_pred = np.linspace(0, 25, 30)   # range of porosity values
y_pred = np.linspace(0, 100, 30)  # range of brittleness values
xx_pred, yy_pred = np.meshgrid(x_pred, y_pred)
model_viz = np.array([xx_pred.flatten(), yy_pred.flatten()]).T

################################################ Train #############################################

model = TheilSenRegressor()
model.fit(X, Y)
predicted = model.predict(model_viz)

############################################## Plot ################################################

plt.style.use('default')

fig = plt.figure(figsize=(12, 4))

ax1 = fig.add_subplot(131, projection='3d')
ax2 = fig.add_subplot(132, projection='3d')
ax3 = fig.add_subplot(133, projection='3d')

axes = [ax1, ax2, ax3]

for ax in axes:
    ax.plot(x, y, z, color='k', zorder=15, linestyle='none', marker='o', alpha=0.5)
    ax.plot(X_outliers[:, 0], X_outliers[:, 1], outlier_Y, color='r', zorder=15, linestyle='none', marker='x', alpha=1, markersize=10)
    ax.scatter(xx_pred.flatten(), yy_pred.flatten(), predicted, facecolor=(0,0,0,0), s=20, edgecolor='#70b3f0')
    ax.set_xlabel(r'$X_{1}$', fontsize=12)
    ax.set_ylabel(r'$X_{2}$', fontsize=12)
    ax.set_zlabel('Y', fontsize=12)
    ax.locator_params(nbins=4, axis='x')
    ax.locator_params(nbins=5, axis='x')
    ax.set_xlim(0, 25)
    ax.set_ylim(100, 0)
    ax.set_zlim(0, 8000)


ax1.text2D(0.2, 0.32, 'aegis4048.github.io', fontsize=13, ha='center', va='center',
           transform=ax1.transAxes, color='grey', alpha=0.5)
ax2.text2D(0.5, 0.32, 'aegis4048.github.io', fontsize=13, ha='center', va='center',
           transform=ax2.transAxes, color='grey', alpha=0.5)
ax3.text2D(0.85, 0.85, 'aegis4048.github.io', fontsize=13, ha='center', va='center',
           transform=ax3.transAxes, color='grey', alpha=0.5)

ax1.view_init(elev=30, azim=50)
ax2.view_init(elev=2, azim=60)
ax3.view_init(elev=60, azim=165)

fig.tight_layout()

def setbold(txt):
    return ' '.join([r"$\bf{" + item + "}$" for item in txt.split(' ')])

bold_txt = setbold(' TheilSen 3D Linear Regression, ')
plain_txt = r'fitted model:   $y = $ %.1f$x_{1}$ + %.1f$x_{2}$ + %.1f' % (model.coef_[0], model.coef_[1], model.intercept_)
fig.suptitle(bold_txt + plain_txt, verticalalignment='top', x=0, horizontalalignment='left', fontsize=16, y=1.03)
yloc = 0.95
ax.annotate('', xy=(0.01, yloc + 0.01), xycoords='figure fraction', xytext=(1.02, yloc + 0.01),
            arrowprops=dict(arrowstyle="-", color='k', lw=0.7))

fig.text(0.48, 0.8, 'true model:   $y = $ 200$x_{1}$ + 28$x_{2}$ + 300', ha='center', va='center', fontsize=16, color='grey');
            </code>
        </pre>
</div>
</div>
</div>
</div>
</div>

<script type="text/javascript">if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = '//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: 'center'," +
        "    displayIndent: '0em'," +
        "    showMathMenu: true," +
        "    tex2jax: { " +
        "        inlineMath: [ ['$','$'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'black ! important'} }" +
        "    } " +
        "}); ";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
</script>
        </div>
        <hr/>
        <aside>
        <nav>
        <ul class="articles-timeline">
            <li class="previous-article">« <a href="https://aegis4048.github.io/quantifying_the_impact_of_completion_delay_since_drilled_on_production" title="Previous: Quantifying The Impact Of Completion Delay Since Drilled On Production">Quantifying The Impact Of Completion Delay Since Drilled On Production</a></li>
            <li class="next-article"><a href="https://aegis4048.github.io/change-in-h2s-ppm-levels-at-high-vs-low-pressure-vessels" title="Next: Change In H₂S ppm Levels At High vs Low Pressure Vessels">Change In H₂S ppm Levels At High vs Low Pressure Vessels</a> »</li>
        </ul>
        </nav>
        </aside>
  <section>
    <div id="disqus_thread" aria-live="polite"><noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript></div>
    <script type="text/javascript">
      var disqus_shortname = 'pythonic-excursions';
      var disqus_identifier = '/robust_linear_regressions_in_python';
      var disqus_url = 'https://aegis4048.github.io/robust_linear_regressions_in_python';
      var disqus_title = 'Robust Linear Regressions In Python';
      (function() {
        var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
        dsq.src = "//" + disqus_shortname + '.disqus.com/embed.js';
        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
      })();
    </script>
  </section>
        <hr/>
<section style="margin-top: 30px">
    <h2>Related Posts</h2>
<ul class="related-posts-list related-post row">
    <div class="pop-over col-12 col-md-6 col-lg-4">
        <div class="article-row" onclick="window.location='https://aegis4048.github.io/uncertainty-modeling-with-monte-carlo-simulation';">
            <div class="article-cell-1">
                <img class="article-img" src="images/featured_images/monte-carlo.png">
            </div>
            <div class="article-cell-2">
                <div class="category">Statistics</div>
                <h2 class="">Uncertainty Modeling with Monte-Carlo Simulation</h2>
                <p class="excerpt">How do casinos earn money? The answer is simple - the longer you play, the bigger the chance of you losing the money. Monte-Carlo simulation can construct its profit forecast model.</p>
                <div class="meta">
                    <div class="meta-left">
                        <i class="fa fa-calendar index-comment-icon"></i>
                        <span class="index-reading-time">2019-01-03</span>
                    </div>
                    <div class="meta-right">
                        <i class="far fa-clock index-comment-icon"></i>
                        <span class="index-reading-time">9 min reading</span>
                        <i class="fas fa-comment index-comment-icon" href="https://aegis4048.github.io/uncertainty-modeling-with-monte-carlo-simulation#disqus_thread"></i>
                        <a class="index-reading-time disqus-comment" href="https://aegis4048.github.io/uncertainty-modeling-with-monte-carlo-simulation#disqus_thread"></a>
                    </div>
                </div>
            </div>
        </div>
    </div>
    <div class="pop-over col-12 col-md-6 col-lg-4">
        <div class="article-row" onclick="window.location='https://aegis4048.github.io/non-parametric-confidence-interval-with-bootstrap';">
            <div class="article-cell-1">
                <img class="article-img" src="images/featured_images/bootstrap.png">
            </div>
            <div class="article-cell-2">
                <div class="category">Statistics</div>
                <h2 class="">Non-Parametric Confidence Interval with Bootstrap</h2>
                <p class="excerpt">Bootstrapping is a type of non-parametric re-sampling method used for statistical & machine learning techniques. One application of bootstrapping is that it can compute confidence intervals of any distribution, because it's distribution-free.</p>
                <div class="meta">
                    <div class="meta-left">
                        <i class="fa fa-calendar index-comment-icon"></i>
                        <span class="index-reading-time">2019-01-04</span>
                    </div>
                    <div class="meta-right">
                        <i class="far fa-clock index-comment-icon"></i>
                        <span class="index-reading-time">7 min reading</span>
                        <i class="fas fa-comment index-comment-icon" href="https://aegis4048.github.io/non-parametric-confidence-interval-with-bootstrap#disqus_thread"></i>
                        <a class="index-reading-time disqus-comment" href="https://aegis4048.github.io/non-parametric-confidence-interval-with-bootstrap#disqus_thread"></a>
                    </div>
                </div>
            </div>
        </div>
    </div>
    <div class="pop-over col-12 col-md-6 col-lg-4">
        <div class="article-row" onclick="window.location='https://aegis4048.github.io/transforming-non-normal-distribution-to-normal-distribution';">
            <div class="article-cell-1">
                <img class="article-img" src="images/featured_images/control_chart.png">
            </div>
            <div class="article-cell-2">
                <div class="category">Statistics</div>
                <h2 class="">Transforming Non-Normal Distribution to Normal Distribution</h2>
                <p class="excerpt">Many statistical & machine learning techniques assume normality of data. What are the options you have if your data is not normally distributed? Transforming non-normal data to normal data using Box-Cox transformation is one of them.</p>
                <div class="meta">
                    <div class="meta-left">
                        <i class="fa fa-calendar index-comment-icon"></i>
                        <span class="index-reading-time">2019-02-25</span>
                    </div>
                    <div class="meta-right">
                        <i class="far fa-clock index-comment-icon"></i>
                        <span class="index-reading-time">13 min reading</span>
                        <i class="fas fa-comment index-comment-icon" href="https://aegis4048.github.io/transforming-non-normal-distribution-to-normal-distribution#disqus_thread"></i>
                        <a class="index-reading-time disqus-comment" href="https://aegis4048.github.io/transforming-non-normal-distribution-to-normal-distribution#disqus_thread"></a>
                    </div>
                </div>
            </div>
        </div>
    </div>
    <div class="pop-over col-12 col-md-6 col-lg-4">
        <div class="article-row" onclick="window.location='https://aegis4048.github.io/comprehensive_confidence_intervals_for_python_developers';">
            <div class="article-cell-1">
                <img class="article-img" src="images/featured_images/rock_por_conf.png">
            </div>
            <div class="article-cell-2">
                <div class="category">Statistics</div>
                <h2 class="">Comprehensive Confidence Intervals for Python Developers</h2>
                <p class="excerpt">This post covers everything you need to know about confidence intervals: from the introductory conceptual explanations, to the detailed discussions about the variations of different techniques, their assumptions, strength and weekness, when to use, and when not to use.</p>
                <div class="meta">
                    <div class="meta-left">
                        <i class="fa fa-calendar index-comment-icon"></i>
                        <span class="index-reading-time">2019-09-08</span>
                    </div>
                    <div class="meta-right">
                        <i class="far fa-clock index-comment-icon"></i>
                        <span class="index-reading-time">66 min reading</span>
                        <i class="fas fa-comment index-comment-icon" href="https://aegis4048.github.io/comprehensive_confidence_intervals_for_python_developers#disqus_thread"></i>
                        <a class="index-reading-time disqus-comment" href="https://aegis4048.github.io/comprehensive_confidence_intervals_for_python_developers#disqus_thread"></a>
                    </div>
                </div>
            </div>
        </div>
    </div>
    <div class="pop-over col-12 col-md-6 col-lg-4">
        <div class="article-row" onclick="window.location='https://aegis4048.github.io/mutiple_linear_regression_and_visualization_in_python';">
            <div class="article-cell-1">
                <img class="article-img" src="images/featured_images/multiple_linear_regression_and_visualization.png">
            </div>
            <div class="article-cell-2">
                <div class="category">Machine Learning</div>
                <h2 class="">Multiple Linear Regression and Visualization in Python</h2>
                <p class="excerpt">Data scientists love linear regression for its simplicity. Strengthen your understanding of linear regression in multi-dimensional space through 3D visualization of linear models. This post comes with detailed scikit-learn code snippets for multiple linear regression.</p>
                <div class="meta">
                    <div class="meta-left">
                        <i class="fa fa-calendar index-comment-icon"></i>
                        <span class="index-reading-time">2019-11-18</span>
                    </div>
                    <div class="meta-right">
                        <i class="far fa-clock index-comment-icon"></i>
                        <span class="index-reading-time">10 min reading</span>
                        <i class="fas fa-comment index-comment-icon" href="https://aegis4048.github.io/mutiple_linear_regression_and_visualization_in_python#disqus_thread"></i>
                        <a class="index-reading-time disqus-comment" href="https://aegis4048.github.io/mutiple_linear_regression_and_visualization_in_python#disqus_thread"></a>
                    </div>
                </div>
            </div>
        </div>
    </div>
</ul>
</section>
    </article>
    <div id="article-sidebar" class="col-12 col-xl-3"></div>
</div>
        </div>
<footer class="footer">
   <div class="container bottom_border">
      <div class="row">
         <div class="col">
            <h5 class="headin5_amrc col_white_amrc pt2">ABOUT ERIC</h5>
            <!--headin5_amrc-->
            <p class="mb10"><img id="profile_img" align="left" src="https://aegis4048.github.io/theme/img/profile_photo_footer.jpg">Petroleum engineering analyst at <a href="https://flogistix.com/" target="blank">Flogistix</a>. I am a self-taught Python developer with strong engineering & statistical background. I am good at creating clean, easy-to-read codes for data analysis. I enjoy assisting my fellow engineers by developing accessible and reproducible codes.</p>
            <p><i class="fa fa-envelope mr-2"></i>aegis4048@gmail.com</p>
         </div>
      </div>
   </div>
   <div class="container">
      <ul class="foote_bottom_ul_amrc">
         <li><a href="/">HOME</a></li>
         <li><a href="about.html">ABOUT</a></li>
         <li><a href="archives.html">ARCHIVE</a></li>
      </ul>
      <!--foote_bottom_ul_amrc ends here-->
      <p class="text-center">Handcrafted by me @2018</p>
      <div class="container">
          <div class="row justify-content-center">
              <div class="row" align="center">
                  <div class="footer-icon"><a href="https://www.linkedin.com/in/eric-kim-34318811b/"><i class="fab fa-linkedin-in"></i></a></div>
                  <div class="footer-icon"><a href="https://github.com/aegis4048"><i class="fab fa-github"></i></a></div>
              </div>
          </div>
      </div>

      <!--social_footer_ul ends here-->
   </div>
</footer>
            <script type="text/javascript" src="https://aegis4048.github.io/theme/libs/jquery.min.js"></script>
            <script type="text/javascript" src="https://aegis4048.github.io/theme/libs/bootstrap-4.2.1/dist/js/bootstrap.bundle.min.js"></script>

        <script src="https://aegis4048.github.io/theme/libs/prism.js"></script>
        <script src="https://aegis4048.github.io/theme/libs/Countable.js"></script>

        <script>
            Prism.plugins.NormalizeWhitespace.setDefaults({
                'remove-trailing': true,
                'remove-indent': true,
                'left-trim': true,
                'right-trim': true,
                /*'break-lines': 80,
                'indent': 2,
                'remove-initial-line-feed': false,
                'tabs-to-spaces': 4,
                'spaces-to-tabs': 4*/
            });
        </script>


        <script type="text/javascript" src="https://aegis4048.github.io/theme/js/custom.js"></script>
            <script>
                function validateForm(query)
                {
                    return (query.length > 0);
                }
            </script>
    </body>
</html>

