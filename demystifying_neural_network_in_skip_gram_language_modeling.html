<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">

            <meta name="google-site-verification" content="ZsWFnpirKDgtbmwb1YRymDnSfvnUrpzCbf6LD1F_4TY" />

            <meta name="msvalidate.01" content="8FF1B025212A47B5B27CC47163A042F0" />

            <meta name="author" content="ERIC KIM" />


            <meta name="description" content="        How is neural network used in language modeling to capture relationships among words?
" />

                <meta property="og:type" content="article" />
            <meta name="twitter:card" content="summary"/>

        <meta name="keywords" content="data-mining, nlp, word2vec, co-occurrence matrix, vector space model, word vectors, window size, skip-gram, neural network, negative sampling, stochastic gradient descent, learning rate, Natural Language Processing, "/>

        <link rel="canonical" href="https://aegis4048.github.io/demystifying_neural_network_in_skip_gram_language_modeling">
    <meta property="og:title" content="Demystifying Neural Network in Skip-Gram Language Modeling | Pythonic Excursions"/>
    <meta property="og:url" content="https://aegis4048.github.io/demystifying_neural_network_in_skip_gram_language_modeling" />
    <meta property="og:description" content="How is neural network used in language modeling to capture relationships among words?" />
    <meta property="og:site_name" content="Pythonic Excursions" />
    <meta property="og:article:author" content="ERIC KIM" />
        <meta property="og:article:published_time" content="2019-05-06T00:00:00-07:00" />
    <meta name="twitter:title" content="Demystifying Neural Network in Skip-Gram Language Modeling | Pythonic Excursions">
    <meta name="twitter:description" content="How is neural network used in language modeling to capture relationships among words?">
        <meta property="og:image" content="https://aegis4048.github.io/images/featured_images/skip-gram.png" />
        <meta name="twitter:image" content="https://aegis4048.github.io/images/featured_images/skip-gram.png" >


        <title>    Demystifying Neural Network in Skip-Gram Language Modeling  | Pythonic Excursions
</title>

                <link rel="stylesheet" type="text/css" href="https://aegis4048.github.io/theme/libs/bootstrap-4.2.1/dist/css/bootstrap.min.css">
                <link rel="stylesheet" type="text/css" href="https://aegis4048.github.io/theme/libs/fontawesome-free-5.2.0-web/css/all.min.css">
            <link rel="stylesheet" type="text/css" href="https://aegis4048.github.io/theme/css/custom.css" media="screen">
            <link rel="stylesheet" type="text/css" href="https://aegis4048.github.io/theme/css/ipynb.css" media="screen">

            <style>
                #progressBar::-webkit-progress-value {
                    background-color: #24292e;
                }
                #progressBar::-moz-progress-bar {
                    background-color: #24292e;
                }
            </style>

        <link href="https://aegis4048.github.io/theme/libs/prism.css" rel="stylesheet" />
<script type="text/x-mathjax-config">

MathJax.Hub.Config({
tex2jax: { inlineMath: [["$","$"],["\\(","\\)"]] },
"HTML-CSS": {
  linebreaks: { automatic: true, width: "container" }
}
});

</script>
<script type="text/javascript"
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>

        <link rel="shortcut icon" href="https://aegis4048.github.io/theme/img/favicon.ico" type="image/x-icon" />
        <link rel="icon" href="https://aegis4048.github.io/theme/img/apple-touch-icon-152x152.png" type="image/png" />
        <link rel="apple-touch-icon" href="https://aegis4048.github.io/theme/img/apple-touch-icon.png"  type="image/png" />
        <link rel="apple-touch-icon" sizes="57x57" href="https://aegis4048.github.io/theme/img/apple-touch-icon-57x57.png" type="image/png" />
        <link rel="apple-touch-icon" sizes="72x72" href="https://aegis4048.github.io/theme/img/apple-touch-icon-72x72.png" type="image/png" />
        <link rel="apple-touch-icon" sizes="76x76" href="https://aegis4048.github.io/theme/img/apple-touch-icon-76x76.png" type="image/png" />
        <link rel="apple-touch-icon" sizes="114x114" href="https://aegis4048.github.io/theme/img/apple-touch-icon-114x114.png" type="image/png" />
        <link rel="apple-touch-icon" sizes="120x120" href="https://aegis4048.github.io/theme/img/apple-touch-icon-120x120.png" type="image/png" />
        <link rel="apple-touch-icon" sizes="144x144" href="https://aegis4048.github.io/theme/img/apple-touch-icon-144x144.png" type="image/png" />
        <link rel="apple-touch-icon" sizes="152x152" href="https://aegis4048.github.io/theme/img/apple-touch-icon-152x152.png" type="image/png" />
        <link href="https://aegis4048.github.io/feeds/all.atom.xml" type="application/atom+xml" rel="alternate" title="Pythonic Excursions - Full Atom Feed" />

<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-133310548-1"></script>
<script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-133310548-1');
</script>

    </head>
    <body>
<progress id="progressBar" max="19827" class="flat">
    <div class="progress-container">
        <span class="progress-bar"></span>
    </div>
</progress>        <div class="banner-wrapper row" style="background-color: #24292e;">
            <div class="banner">
                <nav id="navbar" class="navbar navbar-expand-md navbar-light bg-light container">
                    <div class="container navbar-title">
                        <a href="/"><img id="banner-logo" src="https://aegis4048.github.io/theme/img/logo_with_subtitle.svg" style="height: 40px; margin: 6px 0;"></a>
                        <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarSupportedContent" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
                            <span class="navbar-toggler-icon"></span>
                        </button>
                    </div>
                <div class="collapse navbar-collapse justify-content-end" id="navbarSupportedContent">
                    <ul class="navbar-nav">
                        <li class="nav-item active">
                            <a class="nav-link rem_08" href="https://aegis4048.github.io/about.html">About</a>
                        </li>
                        <li id="last-item" class="nav-item">
                            <a class="nav-link rem_08" href="https://aegis4048.github.io/archives.html">Archive</a>
                        </li>
                     </ul>
                    <form id="search-form" class="form-inline my-2 my-lg-0 justify-content-center" action="https://aegis4048.github.io/search.html">
                        <div class="search-box-div" align="center">
                            <input id="tipue_search_input" class="form-control mr-md-2 rem_08 col-9" type="text" name="q" pattern=".{3,}" title="At least 3 characters" required="" aria-label="Search">
                            <button id="search-btn" class="btn btn-search btn-outline-success btn-circle rem_08 col-3" type="submit" for="tipue_search_input">Search</button>
                        </div>
                    </form>
                </div>
                </nav>
            </div>
        </div>
        <div id="wrap">
<div id="post-container" class="container post index">
    <article>
        <header class="pop-over">
            <h1>Demystifying Neural Network in Skip-Gram Language Modeling</h1>
            <div class="row justify-content-between no-margin">
                <h4 class="article-category">Category > <a class="article-category-link" href="https://aegis4048.github.io/archives.html">Natural Language Processing</a></h4>
                <span class="article-date">May 06, 2019</span>
            </div>
            <div class="meta meta-tag no-margin no-border">
                <div>
                        <a href="https://aegis4048.github.io/tag/data-mining.html" class="tag">data-mining</a>
                        <a href="https://aegis4048.github.io/tag/nlp.html" class="tag">nlp</a>
                        <a href="https://aegis4048.github.io/tag/word2vec.html" class="tag">word2vec</a>
                        <a href="https://aegis4048.github.io/tag/co-occurrence-matrix.html" class="tag">co-occurrence matrix</a>
                        <a href="https://aegis4048.github.io/tag/vector-space-model.html" class="tag">vector space model</a>
                        <a href="https://aegis4048.github.io/tag/word-vectors.html" class="tag">word vectors</a>
                        <a href="https://aegis4048.github.io/tag/window-size.html" class="tag">window size</a>
                        <a href="https://aegis4048.github.io/tag/skip-gram.html" class="tag">skip-gram</a>
                        <a href="https://aegis4048.github.io/tag/neural-network.html" class="tag">neural network</a>
                        <a href="https://aegis4048.github.io/tag/negative-sampling.html" class="tag">negative sampling</a>
                        <a href="https://aegis4048.github.io/tag/stochastic-gradient-descent.html" class="tag">stochastic gradient descent</a>
                        <a href="https://aegis4048.github.io/tag/learning-rate.html" class="tag">learning rate</a>
                </div>
            </div>
            <section>
    <div class="row justify-content-end mt-3" style="align-items: center">
        <div class="share-post-intro mr-2">Share This Post :</div>
        <div class="social-share-btns-container">
            <div class="social-share-btns">
                <a class="share-btn share-btn-twitter" href="https://twitter.com/home?status=https://aegis4048.github.io/demystifying_neural_network_in_skip_gram_language_modeling" rel="nofollow" target="_blank">
                    <i class="fab fa-twitter"></i>
                </a>
                <a class="share-btn share-btn-facebook" href="http://www.facebook.com/sharer/sharer.php?u=https://aegis4048.github.io/demystifying_neural_network_in_skip_gram_language_modeling" rel="nofollow" target="_blank">
                    <i class="fab fa-facebook-f"></i>
                </a>
                <a class="share-btn share-btn-linkedin" href="https://www.linkedin.com/shareArticle?mini=true&url=https://aegis4048.github.io/demystifying_neural_network_in_skip_gram_language_modeling" rel="nofollow" target="_blank">
                    <i class="fab fa-linkedin-in"></i>
                </a>
            </div>
        </div>
    </div>
</section>

        </header>
        <div class="article_content">
            
            <style type="text/css">/*!
*
* IPython notebook
*
*/
/* CSS font colors for translated ANSI escape sequences */
/* The color values are a mix of
   http://www.xcolors.net/dl/baskerville-ivorylight and
   http://www.xcolors.net/dl/euphrasia */
.ansi-black-fg {
  color: #3E424D;
}
.ansi-black-bg {
  background-color: #3E424D;
}
.ansi-black-intense-fg {
  color: #282C36;
}
.ansi-black-intense-bg {
  background-color: #282C36;
}
.ansi-red-fg {
  color: #E75C58;
}
.ansi-red-bg {
  background-color: #E75C58;
}
.ansi-red-intense-fg {
  color: #B22B31;
}
.ansi-red-intense-bg {
  background-color: #B22B31;
}
.ansi-green-fg {
  color: #00A250;
}
.ansi-green-bg {
  background-color: #00A250;
}
.ansi-green-intense-fg {
  color: #007427;
}
.ansi-green-intense-bg {
  background-color: #007427;
}
.ansi-yellow-fg {
  color: #DDB62B;
}
.ansi-yellow-bg {
  background-color: #DDB62B;
}
.ansi-yellow-intense-fg {
  color: #B27D12;
}
.ansi-yellow-intense-bg {
  background-color: #B27D12;
}
.ansi-blue-fg {
  color: #208FFB;
}
.ansi-blue-bg {
  background-color: #208FFB;
}
.ansi-blue-intense-fg {
  color: #0065CA;
}
.ansi-blue-intense-bg {
  background-color: #0065CA;
}
.ansi-magenta-fg {
  color: #D160C4;
}
.ansi-magenta-bg {
  background-color: #D160C4;
}
.ansi-magenta-intense-fg {
  color: #A03196;
}
.ansi-magenta-intense-bg {
  background-color: #A03196;
}
.ansi-cyan-fg {
  color: #60C6C8;
}
.ansi-cyan-bg {
  background-color: #60C6C8;
}
.ansi-cyan-intense-fg {
  color: #258F8F;
}
.ansi-cyan-intense-bg {
  background-color: #258F8F;
}
.ansi-white-fg {
  color: #C5C1B4;
}
.ansi-white-bg {
  background-color: #C5C1B4;
}
.ansi-white-intense-fg {
  color: #A1A6B2;
}
.ansi-white-intense-bg {
  background-color: #A1A6B2;
}
.ansi-default-inverse-fg {
  color: #FFFFFF;
}
.ansi-default-inverse-bg {
  background-color: #000000;
}
.ansi-bold {
  font-weight: bold;
}
.ansi-underline {
  text-decoration: underline;
}
/* The following styles are deprecated an will be removed in a future version */
.ansibold {
  font-weight: bold;
}
.ansi-inverse {
  outline: 0.5px dotted;
}
/* use dark versions for foreground, to improve visibility */
.ansiblack {
  color: black;
}
.ansired {
  color: darkred;
}
.ansigreen {
  color: darkgreen;
}
.ansiyellow {
  color: #c4a000;
}
.ansiblue {
  color: darkblue;
}
.ansipurple {
  color: darkviolet;
}
.ansicyan {
  color: steelblue;
}
.ansigray {
  color: gray;
}
/* and light for background, for the same reason */
.ansibgblack {
  background-color: black;
}
.ansibgred {
  background-color: red;
}
.ansibggreen {
  background-color: green;
}
.ansibgyellow {
  background-color: yellow;
}
.ansibgblue {
  background-color: blue;
}
.ansibgpurple {
  background-color: magenta;
}
.ansibgcyan {
  background-color: cyan;
}
.ansibggray {
  background-color: gray;
}
div.cell {
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: vertical;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: vertical;
  -moz-box-align: stretch;
  display: box;
  box-orient: vertical;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: column;
  align-items: stretch;
  border-radius: 2px;
  box-sizing: border-box;
  -moz-box-sizing: border-box;
  -webkit-box-sizing: border-box;
  border-width: 1px;
  border-style: solid;
  border-color: transparent;
  width: 100%;
  padding: 5px;
  /* This acts as a spacer between cells, that is outside the border */
  margin: 0px;
  outline: none;
  position: relative;
  overflow: visible;
}
div.cell:before {
  position: absolute;
  display: block;
  top: -1px;
  left: -1px;
  width: 5px;
  height: calc(100% +  2px);
  content: '';
  background: transparent;
}
div.cell.jupyter-soft-selected {
  border-left-color: #E3F2FD;
  border-left-width: 1px;
  padding-left: 5px;
  border-right-color: #E3F2FD;
  border-right-width: 1px;
  background: #E3F2FD;
}
@media print {
  div.cell.jupyter-soft-selected {
    border-color: transparent;
  }
}
div.cell.selected,
div.cell.selected.jupyter-soft-selected {
  border-color: #ababab;
}
div.cell.selected:before,
div.cell.selected.jupyter-soft-selected:before {
  position: absolute;
  display: block;
  top: -1px;
  left: -1px;
  width: 5px;
  height: calc(100% +  2px);
  content: '';
  background: #42A5F5;
}
@media print {
  div.cell.selected,
  div.cell.selected.jupyter-soft-selected {
    border-color: transparent;
  }
}
.edit_mode div.cell.selected {
  border-color: #66BB6A;
}
.edit_mode div.cell.selected:before {
  position: absolute;
  display: block;
  top: -1px;
  left: -1px;
  width: 5px;
  height: calc(100% +  2px);
  content: '';
  background: #66BB6A;
}
@media print {
  .edit_mode div.cell.selected {
    border-color: transparent;
  }
}
.prompt {
  /* This needs to be wide enough for 3 digit prompt numbers: In[100]: */
  min-width: 14ex;
  /* This padding is tuned to match the padding on the CodeMirror editor. */
  padding: 0.4em;
  margin: 0px;
  font-family: monospace;
  text-align: right;
  /* This has to match that of the the CodeMirror class line-height below */
  line-height: 1.21429em;
  /* Don't highlight prompt number selection */
  -webkit-touch-callout: none;
  -webkit-user-select: none;
  -khtml-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
  /* Use default cursor */
  cursor: default;
}
@media (max-width: 540px) {
  .prompt {
    text-align: left;
  }
}
div.inner_cell {
  min-width: 0;
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: vertical;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: vertical;
  -moz-box-align: stretch;
  display: box;
  box-orient: vertical;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: column;
  align-items: stretch;
  /* Old browsers */
  -webkit-box-flex: 1;
  -moz-box-flex: 1;
  box-flex: 1;
  /* Modern browsers */
  flex: 1;
}
/* input_area and input_prompt must match in top border and margin for alignment */
div.input_area {
  border: 1px solid #cfcfcf;
  border-radius: 2px;
  background: #f7f7f7;
  line-height: 1.21429em;
}
/* This is needed so that empty prompt areas can collapse to zero height when there
   is no content in the output_subarea and the prompt. The main purpose of this is
   to make sure that empty JavaScript output_subareas have no height. */
div.prompt:empty {
  padding-top: 0;
  padding-bottom: 0;
}
div.unrecognized_cell {
  padding: 5px 5px 5px 0px;
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: horizontal;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: horizontal;
  -moz-box-align: stretch;
  display: box;
  box-orient: horizontal;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: row;
  align-items: stretch;
}
div.unrecognized_cell .inner_cell {
  border-radius: 2px;
  padding: 5px;
  font-weight: bold;
  color: red;
  border: 1px solid #cfcfcf;
  background: #eaeaea;
}
div.unrecognized_cell .inner_cell a {
  color: inherit;
  text-decoration: none;
}
div.unrecognized_cell .inner_cell a:hover {
  color: inherit;
  text-decoration: none;
}
@media (max-width: 540px) {
  div.unrecognized_cell > div.prompt {
    display: none;
  }
}
div.code_cell {
  /* avoid page breaking on code cells when printing */
}
@media print {
  div.code_cell {
    page-break-inside: avoid;
  }
}
/* any special styling for code cells that are currently running goes here */
div.input {
  page-break-inside: avoid;
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: horizontal;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: horizontal;
  -moz-box-align: stretch;
  display: box;
  box-orient: horizontal;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: row;
  align-items: stretch;
}
@media (max-width: 540px) {
  div.input {
    /* Old browsers */
    display: -webkit-box;
    -webkit-box-orient: vertical;
    -webkit-box-align: stretch;
    display: -moz-box;
    -moz-box-orient: vertical;
    -moz-box-align: stretch;
    display: box;
    box-orient: vertical;
    box-align: stretch;
    /* Modern browsers */
    display: flex;
    flex-direction: column;
    align-items: stretch;
  }
}
/* input_area and input_prompt must match in top border and margin for alignment */
div.input_prompt {
  color: #303F9F;
  border-top: 1px solid transparent;
}
div.input_area > div.highlight {
  margin: 0.4em;
  border: none;
  padding: 0px;
  background-color: transparent;
}
div.input_area > div.highlight > pre {
  margin: 0px;
  border: none;
  padding: 0px;
  background-color: transparent;
}
/* The following gets added to the <head> if it is detected that the user has a
 * monospace font with inconsistent normal/bold/italic height.  See
 * notebookmain.js.  Such fonts will have keywords vertically offset with
 * respect to the rest of the text.  The user should select a better font.
 * See: https://github.com/ipython/ipython/issues/1503
 *
 * .CodeMirror span {
 *      vertical-align: bottom;
 * }
 */
.CodeMirror {
  line-height: 1.21429em;
  /* Changed from 1em to our global default */
  font-size: 14px;
  height: auto;
  /* Changed to auto to autogrow */
  background: none;
  /* Changed from white to allow our bg to show through */
}
.CodeMirror-scroll {
  /*  The CodeMirror docs are a bit fuzzy on if overflow-y should be hidden or visible.*/
  /*  We have found that if it is visible, vertical scrollbars appear with font size changes.*/
  overflow-y: hidden;
  overflow-x: auto;
}
.CodeMirror-lines {
  /* In CM2, this used to be 0.4em, but in CM3 it went to 4px. We need the em value because */
  /* we have set a different line-height and want this to scale with that. */
  /* Note that this should set vertical padding only, since CodeMirror assumes
       that horizontal padding will be set on CodeMirror pre */
  padding: 0.4em 0;
}
.CodeMirror-linenumber {
  padding: 0 8px 0 4px;
}
.CodeMirror-gutters {
  border-bottom-left-radius: 2px;
  border-top-left-radius: 2px;
}
.CodeMirror pre {
  /* In CM3 this went to 4px from 0 in CM2. This sets horizontal padding only,
    use .CodeMirror-lines for vertical */
  padding: 0 0.4em;
  border: 0;
  border-radius: 0;
}
.CodeMirror-cursor {
  border-left: 1.4px solid black;
}
@media screen and (min-width: 2138px) and (max-width: 4319px) {
  .CodeMirror-cursor {
    border-left: 2px solid black;
  }
}
@media screen and (min-width: 4320px) {
  .CodeMirror-cursor {
    border-left: 4px solid black;
  }
}
/*

Original style from softwaremaniacs.org (c) Ivan Sagalaev <Maniac@SoftwareManiacs.Org>
Adapted from GitHub theme

*/
.highlight-base {
  color: #000;
}
.highlight-variable {
  color: #000;
}
.highlight-variable-2 {
  color: #1a1a1a;
}
.highlight-variable-3 {
  color: #333333;
}
.highlight-string {
  color: #BA2121;
}
.highlight-comment {
  color: #408080;
  font-style: italic;
}
.highlight-number {
  color: #080;
}
.highlight-atom {
  color: #88F;
}
.highlight-keyword {
  color: #008000;
  font-weight: bold;
}
.highlight-builtin {
  color: #008000;
}
.highlight-error {
  color: #f00;
}
.highlight-operator {
  color: #AA22FF;
  font-weight: bold;
}
.highlight-meta {
  color: #AA22FF;
}
/* previously not defined, copying from default codemirror */
.highlight-def {
  color: #00f;
}
.highlight-string-2 {
  color: #f50;
}
.highlight-qualifier {
  color: #555;
}
.highlight-bracket {
  color: #997;
}
.highlight-tag {
  color: #170;
}
.highlight-attribute {
  color: #00c;
}
.highlight-header {
  color: blue;
}
.highlight-quote {
  color: #090;
}
.highlight-link {
  color: #00c;
}
/* apply the same style to codemirror */
.cm-s-ipython span.cm-keyword {
  color: #008000;
  font-weight: bold;
}
.cm-s-ipython span.cm-atom {
  color: #88F;
}
.cm-s-ipython span.cm-number {
  color: #080;
}
.cm-s-ipython span.cm-def {
  color: #00f;
}
.cm-s-ipython span.cm-variable {
  color: #000;
}
.cm-s-ipython span.cm-operator {
  color: #AA22FF;
  font-weight: bold;
}
.cm-s-ipython span.cm-variable-2 {
  color: #1a1a1a;
}
.cm-s-ipython span.cm-variable-3 {
  color: #333333;
}
.cm-s-ipython span.cm-comment {
  color: #408080;
  font-style: italic;
}
.cm-s-ipython span.cm-string {
  color: #BA2121;
}
.cm-s-ipython span.cm-string-2 {
  color: #f50;
}
.cm-s-ipython span.cm-meta {
  color: #AA22FF;
}
.cm-s-ipython span.cm-qualifier {
  color: #555;
}
.cm-s-ipython span.cm-builtin {
  color: #008000;
}
.cm-s-ipython span.cm-bracket {
  color: #997;
}
.cm-s-ipython span.cm-tag {
  color: #170;
}
.cm-s-ipython span.cm-attribute {
  color: #00c;
}
.cm-s-ipython span.cm-header {
  color: blue;
}
.cm-s-ipython span.cm-quote {
  color: #090;
}
.cm-s-ipython span.cm-link {
  color: #00c;
}
.cm-s-ipython span.cm-error {
  color: #f00;
}
.cm-s-ipython span.cm-tab {
  background: url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAADAAAAAMCAYAAAAkuj5RAAAAAXNSR0IArs4c6QAAAGFJREFUSMft1LsRQFAQheHPowAKoACx3IgEKtaEHujDjORSgWTH/ZOdnZOcM/sgk/kFFWY0qV8foQwS4MKBCS3qR6ixBJvElOobYAtivseIE120FaowJPN75GMu8j/LfMwNjh4HUpwg4LUAAAAASUVORK5CYII=);
  background-position: right;
  background-repeat: no-repeat;
}
div.output_wrapper {
  /* this position must be relative to enable descendents to be absolute within it */
  position: relative;
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: vertical;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: vertical;
  -moz-box-align: stretch;
  display: box;
  box-orient: vertical;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: column;
  align-items: stretch;
  z-index: 1;
}
/* class for the output area when it should be height-limited */
div.output_scroll {
  /* ideally, this would be max-height, but FF barfs all over that */
  height: 24em;
  /* FF needs this *and the wrapper* to specify full width, or it will shrinkwrap */
  width: 100%;
  overflow: auto;
  border-radius: 2px;
  -webkit-box-shadow: inset 0 2px 8px rgba(0, 0, 0, 0.8);
  box-shadow: inset 0 2px 8px rgba(0, 0, 0, 0.8);
  display: block;
}
/* output div while it is collapsed */
div.output_collapsed {
  margin: 0px;
  padding: 0px;
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: vertical;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: vertical;
  -moz-box-align: stretch;
  display: box;
  box-orient: vertical;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: column;
  align-items: stretch;
}
div.out_prompt_overlay {
  height: 100%;
  padding: 0px 0.4em;
  position: absolute;
  border-radius: 2px;
}
div.out_prompt_overlay:hover {
  /* use inner shadow to get border that is computed the same on WebKit/FF */
  -webkit-box-shadow: inset 0 0 1px #000;
  box-shadow: inset 0 0 1px #000;
  background: rgba(240, 240, 240, 0.5);
}
div.output_prompt {
  color: #D84315;
}
/* This class is the outer container of all output sections. */
div.output_area {
  padding: 0px;
  page-break-inside: avoid;
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: horizontal;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: horizontal;
  -moz-box-align: stretch;
  display: box;
  box-orient: horizontal;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: row;
  align-items: stretch;
}
div.output_area .MathJax_Display {
  text-align: left !important;
}
div.output_area 
div.output_area 
div.output_area img,
div.output_area svg {
  max-width: 100%;
  height: auto;
}
div.output_area img.unconfined,
div.output_area svg.unconfined {
  max-width: none;
}
div.output_area .mglyph > img {
  max-width: none;
}
/* This is needed to protect the pre formating from global settings such
   as that of bootstrap */
.output {
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: vertical;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: vertical;
  -moz-box-align: stretch;
  display: box;
  box-orient: vertical;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: column;
  align-items: stretch;
}
@media (max-width: 540px) {
  div.output_area {
    /* Old browsers */
    display: -webkit-box;
    -webkit-box-orient: vertical;
    -webkit-box-align: stretch;
    display: -moz-box;
    -moz-box-orient: vertical;
    -moz-box-align: stretch;
    display: box;
    box-orient: vertical;
    box-align: stretch;
    /* Modern browsers */
    display: flex;
    flex-direction: column;
    align-items: stretch;
  }
}
div.output_area pre {
  margin: 0;
  padding: 1px 0 1px 0;
  border: 0;
  vertical-align: baseline;
  color: black;
  background-color: transparent;
  border-radius: 0;
}
/* This class is for the output subarea inside the output_area and after
   the prompt div. */
div.output_subarea {
  overflow-x: auto;
  padding: 0.4em;
  /* Old browsers */
  -webkit-box-flex: 1;
  -moz-box-flex: 1;
  box-flex: 1;
  /* Modern browsers */
  flex: 1;
  max-width: calc(100% - 14ex);
}
div.output_scroll div.output_subarea {
  overflow-x: visible;
}
/* The rest of the output_* classes are for special styling of the different
   output types */
/* all text output has this class: */
div.output_text {
  text-align: left;
  color: #000;
  /* This has to match that of the the CodeMirror class line-height below */
  line-height: 1.21429em;
}
/* stdout/stderr are 'text' as well as 'stream', but execute_result/error are *not* streams */
div.output_stderr {
  background: #fdd;
  /* very light red background for stderr */
}
div.output_latex {
  text-align: left;
}
/* Empty output_javascript divs should have no height */
div.output_javascript:empty {
  padding: 0;
}
.js-error {
  color: darkred;
}
/* raw_input styles */
div.raw_input_container {
  line-height: 1.21429em;
  padding-top: 5px;
}
pre.raw_input_prompt {
  /* nothing needed here. */
}
input.raw_input {
  font-family: monospace;
  font-size: inherit;
  color: inherit;
  width: auto;
  /* make sure input baseline aligns with prompt */
  vertical-align: baseline;
  /* padding + margin = 0.5em between prompt and cursor */
  padding: 0em 0.25em;
  margin: 0em 0.25em;
}
input.raw_input:focus {
  box-shadow: none;
}
p.p-space {
  margin-bottom: 10px;
}
div.output_unrecognized {
  padding: 5px;
  font-weight: bold;
  color: red;
}
div.output_unrecognized a {
  color: inherit;
  text-decoration: none;
}
div.output_unrecognized a:hover {
  color: inherit;
  text-decoration: none;
}
.rendered_html {
  color: #000;
  /* any extras will just be numbers: */
}



.rendered_html :link {
  text-decoration: underline;
}
.rendered_html :visited {
  text-decoration: underline;
}






.rendered_html h1:first-child {
  margin-top: 0.538em;
}
.rendered_html h2:first-child {
  margin-top: 0.636em;
}
.rendered_html h3:first-child {
  margin-top: 0.777em;
}
.rendered_html h4:first-child {
  margin-top: 1em;
}
.rendered_html h5:first-child {
  margin-top: 1em;
}
.rendered_html h6:first-child {
  margin-top: 1em;
}
.rendered_html ul:not(.list-inline),
.rendered_html ol:not(.list-inline) {
  padding-left: 2em;
}








.rendered_html * + ul {
  margin-top: 1em;
}
.rendered_html * + ol {
  margin-top: 1em;
}





.rendered_html pre,




.rendered_html tr,
.rendered_html th,


.rendered_html tbody tr:nth-child(odd) {
  background: #f5f5f5;
}
.rendered_html tbody tr:hover {
  background: rgba(66, 165, 245, 0.2);
}
.rendered_html * + table {
  margin-top: 1em;
}

.rendered_html * + p {
  margin-top: 1em;
}

.rendered_html * + img {
  margin-top: 1em;
}
.rendered_html img,

.rendered_html img.unconfined,


.rendered_html * + .alert {
  margin-top: 1em;
}
[dir="rtl"] 
div.text_cell {
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: horizontal;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: horizontal;
  -moz-box-align: stretch;
  display: box;
  box-orient: horizontal;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: row;
  align-items: stretch;
}
@media (max-width: 540px) {
  div.text_cell > div.prompt {
    display: none;
  }
}
div.text_cell_render {
  /*font-family: "Helvetica Neue", Arial, Helvetica, Geneva, sans-serif;*/
  outline: none;
  resize: none;
  width: inherit;
  border-style: none;
  padding: 0.5em 0.5em 0.5em 0.4em;
  color: #000;
  box-sizing: border-box;
  -moz-box-sizing: border-box;
  -webkit-box-sizing: border-box;
}
a.anchor-link:link {
  text-decoration: none;
  padding: 0px 20px;
  visibility: hidden;
}
h1:hover .anchor-link,
h2:hover .anchor-link,
h3:hover .anchor-link,
h4:hover .anchor-link,
h5:hover .anchor-link,
h6:hover .anchor-link {
  visibility: visible;
}
.text_cell.rendered .input_area {
  display: none;
}
.text_cell.rendered 
.text_cell.rendered .rendered_html tr,
.text_cell.rendered .rendered_html th,
.text_cell.rendered 
.text_cell.unrendered .text_cell_render {
  display: none;
}
.text_cell .dropzone .input_area {
  border: 2px dashed #bababa;
  margin: -1px;
}
.cm-header-1,
.cm-header-2,
.cm-header-3,
.cm-header-4,
.cm-header-5,
.cm-header-6 {
  font-weight: bold;
  font-family: "Helvetica Neue", Helvetica, Arial, sans-serif;
}
.cm-header-1 {
  font-size: 185.7%;
}
.cm-header-2 {
  font-size: 157.1%;
}
.cm-header-3 {
  font-size: 128.6%;
}
.cm-header-4 {
  font-size: 110%;
}
.cm-header-5 {
  font-size: 100%;
  font-style: italic;
}
.cm-header-6 {
  font-size: 100%;
  font-style: italic;
}
</style>
<style type="text/css">.highlight .hll { background-color: #ffffcc }
.highlight  { background: #f8f8f8; }
.highlight .c { color: #408080; font-style: italic } /* Comment */
.highlight .err { border: 1px solid #FF0000 } /* Error */
.highlight .k { color: #008000; font-weight: bold } /* Keyword */
.highlight .o { color: #666666 } /* Operator */
.highlight .ch { color: #408080; font-style: italic } /* Comment.Hashbang */
.highlight .cm { color: #408080; font-style: italic } /* Comment.Multiline */
.highlight .cp { color: #BC7A00 } /* Comment.Preproc */
.highlight .cpf { color: #408080; font-style: italic } /* Comment.PreprocFile */
.highlight .c1 { color: #408080; font-style: italic } /* Comment.Single */
.highlight .cs { color: #408080; font-style: italic } /* Comment.Special */
.highlight .gd { color: #A00000 } /* Generic.Deleted */
.highlight .ge { font-style: italic } /* Generic.Emph */
.highlight .gr { color: #FF0000 } /* Generic.Error */
.highlight .gh { color: #000080; font-weight: bold } /* Generic.Heading */
.highlight .gi { color: #00A000 } /* Generic.Inserted */
.highlight .go { color: #888888 } /* Generic.Output */
.highlight .gp { color: #000080; font-weight: bold } /* Generic.Prompt */
.highlight .gs { font-weight: bold } /* Generic.Strong */
.highlight .gu { color: #800080; font-weight: bold } /* Generic.Subheading */
.highlight .gt { color: #0044DD } /* Generic.Traceback */
.highlight .kc { color: #008000; font-weight: bold } /* Keyword.Constant */
.highlight .kd { color: #008000; font-weight: bold } /* Keyword.Declaration */
.highlight .kn { color: #008000; font-weight: bold } /* Keyword.Namespace */
.highlight .kp { color: #008000 } /* Keyword.Pseudo */
.highlight .kr { color: #008000; font-weight: bold } /* Keyword.Reserved */
.highlight .kt { color: #B00040 } /* Keyword.Type */
.highlight .m { color: #666666 } /* Literal.Number */
.highlight .s { color: #BA2121 } /* Literal.String */
.highlight .na { color: #7D9029 } /* Name.Attribute */
.highlight .nb { color: #008000 } /* Name.Builtin */
.highlight .nc { color: #0000FF; font-weight: bold } /* Name.Class */
.highlight .no { color: #880000 } /* Name.Constant */
.highlight .nd { color: #AA22FF } /* Name.Decorator */
.highlight .ni { color: #999999; font-weight: bold } /* Name.Entity */
.highlight .ne { color: #D2413A; font-weight: bold } /* Name.Exception */
.highlight .nf { color: #0000FF } /* Name.Function */
.highlight .nl { color: #A0A000 } /* Name.Label */
.highlight .nn { color: #0000FF; font-weight: bold } /* Name.Namespace */
.highlight .nt { color: #008000; font-weight: bold } /* Name.Tag */
.highlight .nv { color: #19177C } /* Name.Variable */
.highlight .ow { color: #AA22FF; font-weight: bold } /* Operator.Word */
.highlight .w { color: #bbbbbb } /* Text.Whitespace */
.highlight .mb { color: #666666 } /* Literal.Number.Bin */
.highlight .mf { color: #666666 } /* Literal.Number.Float */
.highlight .mh { color: #666666 } /* Literal.Number.Hex */
.highlight .mi { color: #666666 } /* Literal.Number.Integer */
.highlight .mo { color: #666666 } /* Literal.Number.Oct */
.highlight .sa { color: #BA2121 } /* Literal.String.Affix */
.highlight .sb { color: #BA2121 } /* Literal.String.Backtick */
.highlight .sc { color: #BA2121 } /* Literal.String.Char */
.highlight .dl { color: #BA2121 } /* Literal.String.Delimiter */
.highlight .sd { color: #BA2121; font-style: italic } /* Literal.String.Doc */
.highlight .s2 { color: #BA2121 } /* Literal.String.Double */
.highlight .se { color: #BB6622; font-weight: bold } /* Literal.String.Escape */
.highlight .sh { color: #BA2121 } /* Literal.String.Heredoc */
.highlight .si { color: #BB6688; font-weight: bold } /* Literal.String.Interpol */
.highlight .sx { color: #008000 } /* Literal.String.Other */
.highlight .sr { color: #BB6688 } /* Literal.String.Regex */
.highlight .s1 { color: #BA2121 } /* Literal.String.Single */
.highlight .ss { color: #19177C } /* Literal.String.Symbol */
.highlight .bp { color: #008000 } /* Name.Builtin.Pseudo */
.highlight .fm { color: #0000FF } /* Name.Function.Magic */
.highlight .vc { color: #19177C } /* Name.Variable.Class */
.highlight .vg { color: #19177C } /* Name.Variable.Global */
.highlight .vi { color: #19177C } /* Name.Variable.Instance */
.highlight .vm { color: #19177C } /* Name.Variable.Magic */
.highlight .il { color: #666666 } /* Literal.Number.Integer.Long */</style><div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<div class="alert alert-info">
<h4>Acknowledgement</h4>
<p>The materials on this post are based the on two NLP papers, <a href="https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf">Distributed Representations of Words and Phrases and their Compositionality</a> (Mikolov et al., 2013) and <a href="https://arxiv.org/pdf/1411.2738.pdf">word2vec Parameter Learning Explained</a> (Rong, 2014).</p>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Paradigm-Shift-in-Word-Embedding:-Count-Based-to-Prediction-Based">Paradigm Shift in Word Embedding: Count-Based to Prediction-Based<a class="anchor-link" href="#Paradigm-Shift-in-Word-Embedding:-Count-Based-to-Prediction-Based">¶</a></h2><p>Up until 2013, the traditional models for NLP tasks were count-based models. They mainly involve computing a co-occurence matrix to capture meaningful relationships among words (If you are interested in how co-occurrence matrix is used for language modeling, check out <a href="https://aegis4048.github.io/understanding_multi-dimensionality_in_vector_space_modeling">Understanding Multi-Dimensionality in Vector Space Modeling</a>). For example:</p>
<p><em>Document 1: "all that glitters is not gold"</em></p>
<p><em>Document 2: "all is well that ends well"</em></p>
<p><table>
<thead>
<tr>
<th>*</th>
<th>START</th>
<th>all</th>
<th>that</th>
<th>glitters</th>
<th>is</th>
<th>not</th>
<th>gold</th>
<th>well</th>
<th>ends</th>
<th>END</th>
</tr>
</thead>
<tbody>
<tr>
<td>START</td>
<td>0</td>
<td>2</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
</tr>
<tr>
<td>all</td>
<td>2</td>
<td>0</td>
<td>1</td>
<td>0</td>
<td>1</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
</tr>
<tr>
<td>that</td>
<td>0</td>
<td>1</td>
<td>0</td>
<td>1</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>1</td>
<td>1</td>
<td>0</td>
</tr>
<tr>
<td>glitters</td>
<td>0</td>
<td>0</td>
<td>1</td>
<td>0</td>
<td>1</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
</tr>
<tr>
<td>is</td>
<td>0</td>
<td>1</td>
<td>0</td>
<td>1</td>
<td>0</td>
<td>1</td>
<td>0</td>
<td>1</td>
<td>0</td>
<td>0</td>
</tr>
<tr>
<td>not</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>1</td>
<td>0</td>
<td>1</td>
<td>0</td>
<td>0</td>
<td>0</td>
</tr>
<tr>
<td>gold</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>1</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>1</td>
</tr>
<tr>
<td>well</td>
<td>0</td>
<td>0</td>
<td>1</td>
<td>0</td>
<td>1</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>1</td>
<td>1</td>
</tr>
<tr>
<td>ends</td>
<td>0</td>
<td>0</td>
<td>1</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>1</td>
<td>0</td>
<td>0</td>
</tr>
<tr>
<td>END</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>1</td>
<td>1</td>
<td>0</td>
<td>0</td>
</tr>
</tbody>
</table></p>
<div class="col-12"><p class="image-description">Table 1: Co-Occurence Matrix</p></div><p>Count-based language modeling is easy to comprehend — related words are observed (counted) together more often than unrelated words. Many attempts were made to improve the performance of the model to the state-of-art, using SVD, ramped window, and non-negative matrix factorization (<a href="https://pdfs.semanticscholar.org/73e6/351a8fb61afc810a8bb3feaa44c41e5c5d7b.pdf">Rohde et al. ms., 2005</a>), but the model did not do well in capturing complex relationships among words.</p>
<p>Then, the paradigm started to change in 2013, when Thomas Mikolov proposed the prediction-based modeling technique, called Word2Vec, in his famous paper, <a href="https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf">Distributed Representations of Words and Phrases and their Compositionality</a>. Unlike counting word co-occurrences, the model uses neural networks to learn intelligent representation of words in a vector space. Then, the paper submitted to ACL in 2014, <a href="http://www.aclweb.org/anthology/P/P14/P14-1023.pdf" target="_blank">Don’t count, predict! A systematic comparison of context-counting vs. context-predicting semantic vectors</a>, quantified & compared the performances of count-based vs prediction-based models.</p>
<div class="row give-margin-inline-big-plot mobile_responsive_plot_full_width" id="fig1">
<div class="col"><img src="jupyter_images/countpredictresults2.png"/></div>
<div class="col-12"><p class="image-description">Figure 1: Performance comparison of models (<a href="http://www.marekrei.com/blog/dont-count-predict/">source</a>)</p></div>
</div><p>The blue bars represent the count-based models, and the red bars are for prediction-based models. The full summary of the paper and more detailed description about the result graph can be found <a href="http://www.marekrei.com/blog/dont-count-predict/">here</a>. Long story short, <strong>prediction-based models outperformed count-based models</strong> by a large margin on various language tasks.</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Prediction-based-word-embedding:-Word2Vec-Skip-Gram">Prediction-based word-embedding: Word2Vec Skip-Gram<a class="anchor-link" href="#Prediction-based-word-embedding:-Word2Vec-Skip-Gram">¶</a></h2><p>One of the prediction-based language model introduced by Mikolov is Skip-Gram:</p>
<div class="row give-margin-inline-big-plot mobile_responsive_plot_full_width" id="fig2">
<div class="col"><img src="jupyter_images/skip-gram-paper.png" style="height: 400px;"/></div>
<div class="col-12"><p class="image-description">Figure 2: Original Skip-gram model architecture</p></div>
</div><p><a href="#fig2">Figure 2</a> is a diagram presented in the original Word2Vec paper. It is essentially describing that the model uses a neural network of one hidden (projection) layer to correctly predict context words ($w(t-2)$, $w(t-1)$, $w(t+1)$, $w(t+2)$) of an input word ($w(t)$). In the other words, the model attempts to maximize the probability of observing all four context words together, given a center word. Mathematically, it can be denoted as <a href="#eq-1">eq (1)</a>. The training objective is to learn word vector representations that are good at predicting the nearby words.</p>
<div class="alert alert-info">
<h4>Notes: CBOW and Skip-Gram</h4>
<p>There are two models for Word2Vec: <i>Continous Bag Of Words (CBOW)</i> and <i>Skip-Gram</i>. While Skip-Gram model predicts context words given a center word, CBOW model predicts a center word given context words. According to Mikolov:</p>
<p><u>Skip-gram</u>: works well with small amount of the training data, represents well even rare words or phrases</p>
<p><u>CBOW</u>: several times faster to train than the skip-gram, slightly better accuracy for the frequent words</p>
<p>Skip-Gram model is a better choice most of the time due to its ability to predict infrequent words, but this comes at the price of increased computational cost. If training time is a big concern, and you have large enough data to overcome the issue of predicting infrequent words, CBOW model may be a more viable choice. The details of CBOW model won't be covered in this post.</p>
</div><p><strong>Why predict context words?</strong></p>
<p>A natural question is, why do we predict context words? One must understand that the ultimate goal of Skip-Gram model is not to predict context words, but to learn intelligent vector representation of words. It just happens that predicting context words inevitably results in good vector representations of words, because of the neural network structure of Skip-Gram. Neural network at its essence is just optimizing weight marices ($\theta$) to correctly predict output. In Word2Vec Skip-Gram, the weight matrices are, in fact, the vector representations of words. Therefore, optimizing weight matrix = good vector representations of words. This is described in detail <a href="#weight_matrix">below</a>.</p>
<p><strong>What is the application of vector representations of words?</strong></p>
<p>In Word2Vec, words are represented as vectors, and related words are placed closed to each other on a vector space. Mathematically, this means that the vector distance between related words are smaller than the vector distance between unrelated words.</p>
<div class="row give-margin-inline-big-plot mobile_responsive_plot_full_width" id="fig3">
<div class="col"><img src="jupyter_images/vector_distance.png"/></div>
<div class="col-12"><p class="image-description">Figure 3: Vector distance between two words</p></div>
</div><p>For example in <a href="#fig3">figure 3</a>, correlation between <em>"success"</em> and <em>"achieve"</em> can be quantified by computing the vector distance between them (Notes: For illustration purpose, three-dimensional word vectors are assumed in the figure, because higher dimensional vectors can't be visualized. Also, distance annotated in the figure is Euclidean, but in real-life, we use Cosine distance to evaluate vector correlations).</p>
<p>One interesting application of vector representaion of words is that it can be used to solve analogy tasks. Let's assume the following word vectors for <em>"Germany"</em>, <em>"capital"</em>, and <em>"Berlin"</em>.</p>
$$
\begin{align*} 
vec(\text{Germany}) & = [1.22 \quad 0.34 \quad -3.82] \\ 
vec(\text{capital}) & = [3.02 \quad -0.93 \quad 1.82] \\
vec(\text{Berlin})  & = [4.09 \quad -0.58 \quad 2.01]
\end{align*}
$$<p>To find out the capital of Germany, the word vector of <em>"capital"</em> can be added to the word vector of <em>"Germany"</em>.</p>
$$
\begin{align*}
    vec(\text{Germany}) + vec(\text{capital}) &= [1.22 \quad 0.34 \quad -3.82] + [3.02 \quad -0.93 \quad 1.82] \\
                                              &= [4.24 \quad -0.59 \quad -2.00]
\end{align*}
$$<p>Since the sum of the word vectors of <em>"Germany"</em> and <em>"capital"</em> is similar to the word vector of <em>"Berlin"</em>, the model may conclude that the capital of Germany is Berlin.</p>
$$
\begin{align*} 
[4.24 \quad -0.59 \quad -2.00] & \cong [4.09 \quad -0.58 \quad 2.01] \\ 
vec(\text{Germany}) + vec(\text{capital}) & \cong vec(\text{Berlin})
\end{align*}
$$<div class="alert alert-info">
<h4>Notes: Analogy tasks don't always work</h4>
<p>Not all analogy tasks can be solved like this. The above illustration works like a magic, but there are many analogy problems that can't be solved with Word2Vec. Think of the above illustration as just one use case of Word2Vec.</p>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Derivation-of-Cost-Function">Derivation of Cost Function<a class="anchor-link" href="#Derivation-of-Cost-Function">¶</a></h2><p>Skip-Gram model seeks to optimize the word weight (embedding) matrix by correctly predicting context words, given a center word. In the other words, the model wants to maximize the probability of correctly predicting all context words at the same time, given a center word. Maximizing the probability of predicting context words leads to optimizing the weight matrix ($\theta$) that best represents words in a vector space. $\theta$ is a concatenation of input and output weight matrices — $[W_{input} \quad W_{output}]$, as described <a href="#theta_in_cost">below</a>. It is passed into the cost function ($J$) as a variable and optimized. Mathematically, it can be expressed as:</p>
<div id="eq-1" style="font-size: 1rem;">$$ \underset{\theta}{\text{argmax}} \,\, p(w_{1}, w_{2}, ... , w_{C}|w_{center}; \, \theta) \tag{1} $$</div><p>where $C$ is the window size. Recall that in statistics, the probability of $A$ given $B$ is expressed as $P(A|B)$. Then, natural log is taken on <a href="#eq-1">eq (1)</a> to simplify taking derivatives.</p>
<div id="eq-2" style="font-size: 1rem;">$$ \underset{\theta}{\text{argmax}} \,\, log \, p(w_{1}, w_{2}, ... , w_{C}|w_{center}; \, \theta) \tag{2} $$</div><div id="take_natural_log"></div>
<div class="alert alert-info">
<h4>Notes: Why take a natural log?</h4>
<p>In machine learning, it is a common practice to take a natural log to the objective function to simplify taking derivatives. For example, a multinomial regression classifer called Softmax (details explained <a href="#Softmax-Output-Layer-($y_{pred}$)">below</a>) has the following probability function:</p>
<p><center style="font-size: 1rem; margin-top: 20px;">$p(x_i) = \frac{e^{x_i}}{\sum_{j=1}e^{x_{j}}}$</center></p>
<p>Taking a log simplifies the function:</p>
<p><center style="font-size: 1rem; margin-top: 20px;">$log \, p(x_i) = x_i - log \, {\sum_{j=1}e^{x_{j}}}$</center></p>
<p>Depending on a model, the argument ($x_i$) passed into the probability function ($p$) can be complicated, and simplifying the original softmax function helps with taking the derivatives in the future.</p>
<p>Taking a log does not affect the optimized weights ($\theta$), because natural log is a <u>monotonically increasing</u> function. This means that increasing the value of $x$-axis results in increasing the value of $y$-axis. This is important because it ensures that the maximum value of the original probability function occurs at the same point as the log probability function. Therefore:</p>
<p><center style="font-size: 1rem; margin-top: 20px;">$\underset{\theta}{\text{argmax}} \,\, p(x_i) = \underset{\theta}{\text{argmax}} \,\, log \, p(x_i)$</center></p>
</div><p>In Skip-Gram, softmax function is used for context words classfication. The details are explained <a href="#Softmax-Output-Layer-($y_{pred}$)">below</a>. Softmax in Skip-Gram has the following equation:</p>
<div id="eq-3" style="font-size: 1rem;">$$ p(w_{context}|w_{center}; \, \theta) = \frac{exp(W_{output_{(context)}} \cdot h)}{\sum^V_{i=1}exp(W_{output_{(i)}} \cdot h)} \tag{3} $$</div><p>$W_{output_{(context)}}$ is a row vector for a context word from the output embedding matrix (see <a href="#weight_matrix">below</a>), and $h$ is the hidden (projection) layer word vector for a center word (see <a href="#hidden_layer">below</a>). Softmax function is then plugged into the <a href="#eq-2">eq (2)</a> to yield a new objective function that maximizes the probability of observing all $C$ context words, given a center word:</p>
<div id="eq-4" style="font-size: 1rem;">$$ \underset{\theta}{\text{argmax}} \,\, log \, \prod_{c=1}^{C} \frac{exp(W_{output_{(c)}} \cdot h)}{\sum^V_{i=1}exp(W_{output_{(i)}} \cdot h)} \tag{4} $$</div><div class="alert alert-info">
<h4>Notes: Probability Product</h4>
<p>In statistics, probability of observing $C$ multiple events at the same time is computed by the product of each event's probability.</p>
<p><center style="font-size: 1rem; margin-top: 20px;">$$p(x_{1}, x_{2} ... x_{C}) = p(x_{1}) \times p(x_{2}) \, \times \, ... \, \times \, p(x_{C})$$</center></p>
<p>This can be shortened with a product notation:</p>
<p><center style="font-size: 1rem; margin-top: 20px;">$$p(x_{1}, x_{2} ... x_{C}) = \prod_{c=1}^{C}p(x_{c})$$</center></p>
</div><p>However, in machine learning, the convention is to minimize the cost function, not to maximize it. To stick to the convention, we add a negative sign to <a href="#eq-4">eq (4)</a>. This can be done because minimizing a negative log-likelihood is equivalent to maximizing a positive log-likelihood. Therefore, the cost function we want to minimize becomes:</p>
<div id="eq-5" style="font-size: 1rem;">$$ J(\theta; w^{(t)}) = -log \, \prod_{c=1}^{C} \frac{exp(W_{output_{(c)}} \cdot h)}{\sum^V_{i=1}exp(W_{output_{(i)}} \cdot h)} \tag{5} $$</div><p>where $c$ is the index of the context word around the center word ($w_{t}$). $t$ is the index of the center word within a corpus of size $T$. Using the property of log, it can be changed to:</p>
<div id="eq-6" style="font-size: 1rem;">
$$J(\theta; w^{(t)}) = -
\sum_{c=1}^{C}
log 
\frac{exp(W_{output_{(c)}} \cdot h)}{\sum^V_{i=1}exp(W_{output_{(i)}} \cdot h)} \tag{6}$$
</div><p>Taking a log to the softmax function allows us to simplify the expression into simpler forms because we can split the fraction into addtion of the numerator and the denominator:</p>
<div id="eq-7" style="font-size: 1rem;">
$$
J(\theta; w^{(t)}) = - \sum_{c=1}^{C}(W_{output_{(c)}} \cdot h) + C \cdot log \sum^V_{i=1}exp(W_{output_{(i)}} \cdot h) \tag{7}
$$
</div><p>Different paper uses different notations for the cost function. To stick to the notation used in the <a href="https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf">Word2Vec original paper</a>, some of the notations in <a href="#eq-7">eq (7)</a> can be changed. However, they are all equivalent:</p>
<div id="eq-8" style="font-size: 1rem;">$$J(\theta;w^{(t)}) = -\sum_{-c\leq j \leq c,j\neq 0} \log p(w_{t+j} \mid w_t ; \, \theta) \tag{8}$$</div><p>Note that <a href="#eq-7">eq (7)</a> and <a href="#eq-8">eq (8)</a> are equivalent. They both assume <strong>stochastic gradient descent</strong>, which means that for each training sample ($w^{(t)}$) in the corpus of size ($T$), one update is made to the weight matrix ($\theta$). The cost function expressed in the paper shows <strong>batch gradient descent</strong> <a href="#eq-9">eq (9)</a>, which means that only one update is made for all $T$ training samples:</p>
<div id="eq-9" style="font-size: 1rem;">$$J(\theta) = -\frac{1}{T} \sum^T_{t=1} \sum_{-c\leq j \leq c,j\neq 0} \log p(w_{t+j} \mid w_t ;\, \theta) \tag{9}$$</div><p>However, in Word2Vec, batch gradient descent is almost never used due to its high computational cost. The author of the paper stated that he used stochastic gradient descent for training. Read the below <a href="#stochastic">notes</a> for more information about stochastic gradient descent.</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<div id="window_size"></div><h3 id="Window-Size-of-Skip-Gram">Window Size of Skip-Gram<a class="anchor-link" href="#Window-Size-of-Skip-Gram">¶</a></h3><p>Softmax regression (or multinomial logistic regression) is a generalization of logistic regression to the case where we want to handle multiple classes. A general form of the softmax regression looks like this:</p>
<div id="eq-10" style="font-size: 1rem;">
$$J(\theta) = 
-\frac{1}{T} 
\sum^T_{t=1}
\sum^K_{k=1}
log 
\frac
{exp(\theta^{(k)\top}x^{(t)})}
{\sum^K_{i=1}
exp(\theta^{(i)\top}x^{(t)})} \tag{10}$$</div><p>where $T$ is the number of training samples, and $K$ is the number of labels to classify. In NLP applications, $K = V$, because there are $V$ unique vocabulary we need to classify in a vector space. $V$ can easily exceed tens of thousands. Skip-Gram tweaks this a little, and replaces $K$ with a variable called <strong>window size</strong> $C$. Window size is a hyper parameter of the model with a typical range of $[1, 10]$ (see <a href="#fig4">figure 4</a>). Recall that Skip-Gram is a model that attempts to predict neighboring words of a center word. It doesn't have to predict all $V$ vocab in the corpus that may be 100 or more words away from it, but instead predict only a few, 1~10 neighboring context words. This is also intuitive, considering how words that are far away carry less information about each another. Thus, the adapted form of the softmax regression equation for Skip-Gram becomes:</p>
<div id="eq-11" style="font-size: 1rem;">
$$J(\theta) = 
-\frac{1}{T} 
\sum^T_{t=1}
\sum_{-c\leq j \leq c,j\neq 0}
log 
\frac
{exp(\theta^{(t+j)\top}x^{(t)})}
{\sum^K_{i=1}
exp(\theta^{(i)\top}x^{(t)})} \tag{11}$$</div><p>This is equivalent to <a href="#eq-9">eq (9)</a>. Note that the $K$ in the denominator is still equal to $V$, because the denominator acts as a normalization factor, as described <a href="#output_layer">below</a>. However, the size of $K$ in the denominator can still be reduced to smaller size using <a href="#negsample">negative sampling</a>.</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="Neural-Network-Structure-of-Skip-Gram">Neural Network Structure of Skip-Gram<a class="anchor-link" href="#Neural-Network-Structure-of-Skip-Gram">¶</a></h1><p>How is neural network used to minimize the cost functoin described in <a href="#eq-11">eq (11)</a>? One needs to look into the structure of the Skip-Gram model to gain insights about their correlation.</p>
<p>For illustration purpose, let's assume that the entire corpus is composed of the quote from the Game of Thrones, <i>"The man who passes the sentence should swing the sword"</i>, by Ned Stark. There are 10 words ($T = 10$), and 8 unique words ($V = 8$).</p>
<p>Note that in real life, the corpus is much bigger than just one sentence.</p>
<blockquote class="quote">
  The man who passes the sentence should swing the sword.
  <span>- Ned Stark</span>
</blockquote><div id="window_id"></div><p>We will use <code>window=1</code>, and assume that <i>'passes'</i> is the current center word, making <i>'who'</i> and <i>'the'</i> context words. <code>window</code> is a hyper-parameter that can be empirically tuned. It typically has a range of $[1, 10]$.</p>
<div class="row give-margin-inline-big-plot" id="fig4">
<div class="col"><img src="jupyter_images/quote_ned_stark.png"/></div>
<div class="col-12"><p class="image-description">Figure 4: Training Window</p></div>
</div><div id="size_3"></div>
For illustration purpose, a three-dimensional neural net will be constructed. In *gensim*, this can be implemented by setting <code>size=3</code>. This makes $N = 3$. Note that <code>size</code> is also a hyper-parameter that can be empirically tuned. In real life, a typical Word2Vec model has 200-600 neurons. 

<pre>
    <code class="language-python">
        from gensim.models import Word2Vec

        model = Word2Vec(corpus, size=3, window=1)
    </code>
</pre>

This means that the input weight matrix ($W_{input}$) will have a size of $8 \times 3$, and output weight matrix ($W_{output}^T$) will have a size of $3 \times 8$. Recall that the corpus, <i>"The man who passes the sentence should swing the sword"</i>, has 8 unique vocabularies ($V = 8$). 

<div class="row" id="fig5">
<div class="col"><img src="jupyter_images/word2vec_skip-gram.png" style="margin: 0;"/></div>
<div class="col-12"><p class="image-description">Figure 5: Skip-Gram model structure</p></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Training:-Forward-Propagation">Training: Forward Propagation<a class="anchor-link" href="#Training:-Forward-Propagation">¶</a></h2><p>The word embedding matrices ($W_{input}$, $W_{output}$) in Skip-Gram are optimized through forward and backward propagations. For each iteration of forward + backward propagations, the model learns to reduce prediction error by optimizing the weight matrix ($\theta$), thus acquiring higher quality embedding matrices that better capture relationships among words.</p>
<p>Forward propagation includes obtaining the probability distribution of words ($y_{pred}$ in <a href="#fig5">figure 5</a>) given a center word, and backward propagation includes calculating the prediction error, and updating the weight (embedding) matrices to minimize the prediction error.</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<div id="input_layer"></div><h3 id="Input-Layer-($x$)">Input Layer ($x$)<a class="anchor-link" href="#Input-Layer-($x$)">¶</a></h3><p>The input layer is a $V$-dim one-hot encoded vector. Every element in the vector is 0 except one element that corresponds to the center (input) word. Input vector is multiplied with the input weight matrix ($W_{input}$) of size $V \times N$, and yields a hidden (projection) layer ($h$) of $N$-dim vector. Because the input layer is one-hot encoded, it makes the input weight matrix ($W_{input}$) to behave like a <em>look-up table</em> for the center word. Assuming epoch number of 1 (<code>iter=1</code> in <em>gensim</em> Word2Vec implementation) and stochastic gradient descent, the input vector is injected into the network $T$ times for every word in the corpus and makes $T$ updates to the weight matrix ($\theta$) to learn from the training samples. Derivation of the stochasitc update equations are explained <a href="#backward">below</a>.</p>
<div class="row">
<div class="col"><img src="jupyter_images/one-hot-vector.png"/></div>
<div class="col-12"><p class="image-description">Figure 6: One-hot encoded input vector and parameter update</p></div>
</div><div id="stochastic"></div>
<div class="alert alert-info">
<h4>Notes: Stochastic Gradient Descent</h4>
<p>The goal of any machine learning model is to find the optimal values of a weight matrix ($\theta$) to minimize prediction error. A general update equation for weight matrix looks like the following:</p>
<p><center style="font-size: 1rem; margin-top: 20px;">$\theta^{(new)}=\theta^{(old)}-\eta\cdot\nabla_{J(\theta)}$</center></p>
<p>$\eta$ is learning rate, $\nabla_{J(\theta)}$ is gradient for the weight matrix, and $J(\theta)$ is the cost function that has different forms for each model. The cost function for the Skip-Gram model proposed in the <a href="https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf">Word2Vec original paper</a> has the following equation:</p>
<p><center style="font-size: 1rem; margin-top: 20px;">$$J(\theta) = -\frac{1}{T} \sum^T_{t=1} \sum_{-c\leq j \leq c,j\neq 0} \log p(w_{t+j} \mid w_t ; \theta)$$</center></p>
<p>Here, what gives us headache is the expression, $\frac{1}{T} \sum^T_{t=1}$, because $T$ can be larger than billions or more in many NLP applications. It is basically telling us that billions of iterations need to be computed to make just one update to the weight matrix ($\theta$). In order to mitigate this computational burden, the author of the paper states that Stochastic Gradient Descent (SGD) was used for parameter optimization. SGD removes the expression, $\frac{1}{T} \sum^T_{t=1}$, from the cost function and performs parameter update for each training example, $w^{(t)}$:</p>
<p><center style="font-size: 1rem; margin-top: 20px;">$$J(\theta;w^{(t)}) = -\sum_{-c\leq j \leq c,j\neq 0} \log p(w_{t+j} \mid w_t ; \theta)$$</center></p>
<p>Then, the new parameter update equation for SGD becomes:</p>
<p><center style="font-size: 1rem; margin-top: 20px;">$\theta^{(new)}=\theta^{(old)}-\eta\cdot\nabla_{J(\theta;w^{(t)})}$</center></p>
<p>The original vanilla graident descent makes $1$ parameter update for $T$ training samples, but the new update equation using SGD makes $T$ parameter update for $T$ training samples. However, this comes at the price of higher fluctuation (or variance) in minimizing prediction error.</p>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<div id="weight_matrix"></div><h3 id="Input-and-Output-Weight-Matrix--($W_{input}$,-$W_{output}$)">Input and Output Weight Matrix  ($W_{input}$, $W_{output}$)<a class="anchor-link" href="#Input-and-Output-Weight-Matrix--($W_{input}$,-$W_{output}$)">¶</a></h3><p>Why does Skip-Gram model attempt to predict context words given a center word? How does predicting context words help with quantifying words and representing them in a vector space? In fact, the ultimate goal of the model is not to predict context words, but to construct the word embedding matrices ($W_{input}$, $W_{output}$) that best caputure relationship among words in a vector space. Skip-Gram achieves this by using a neural net — it optimizes the weight (embedding) matrices by adjusting the weight matrix to minimize the <a href="#predict_error">prediction error</a> ($y_{pred} - y_{true}$). This will make more sense once you understand how the embedding matrix behaves like a <em>look-up table</em>.</p>
<p>Each row in a word-embedding matrix is a word-vector for each word. Consider the following word-embedding matrix, $W_{input}$.</p>
<div class="row give-margin-inline-plot mobile_responsive_plot_full_width">
<div class="col"><img src="jupyter_images/embedding_matrix_input.png" style="height: 300px;"/></div>
<div class="col-12"><p class="image-description">Figure 7: Word-embedding matrix, $W_{input}$</p></div>
</div><p>The words of our interest are <em>"passes"</em> and <em>"should"</em>. <em>"passes"</em> has a word vector of $[0.1 \quad 0.2 \quad 0.7]$ and <em>"should"</em> has $[-2 \quad 0.2 \quad 0.8]$. Since we set the size of the weight matrix to be <code>size=3</code> <a href="#size_3">above</a>, the matrix is three-dimensional, and can be visualized in a 3D vector space:</p>
<div class="row full_screen_margin_md mobile_responsive_plot_full_width">
<div class="col"><img src="jupyter_images/word2vec_3d.png" style="height: 300px;"/></div>
<div class="col-12"><p class="image-description">Figure 8: 3D visualization of word vectors in embedding matrix</p></div>
</div><p>Optimizing the embedding (weight) matrices ($\theta$) results in representing words in a high quality vector space, and the model will be able to capture meaningful relationships among words.</p>
<div id="theta_in_cost"></div>
<div class="alert alert-info">
<h4>Notes: $\theta$ in cost function</h4>
<p>There are two weight matrices that need to be optimized in Skip-Gram model: $W_{input}$ and $W_{output}$. Often times in neural net, the weights are expressed as $\theta$. In Skip-Gram, $\theta$ is a concatenation of input and output weight matrices — $[W_{input} \quad W_{output}]$.</p>
<div style="font-size: 1rem; margin-top: 20px;">$$ \theta = [W_{input} \quad W_{output}] = \left[ \begin{array}{l} u_{the} \\ u_{passes} \\ \vdots \\ u_{who} \\  v_{the} \\ v_{passes} \\ \vdots \\ v_{who} \end{array} \right] \in \mathbb{R}^{2NV}$$</div>
<p>$\theta$ has a size of $2V \times N$, where $V$ is the number of unique vocab in a corpus, and $N$ is the dimension of word vectors in the embedding matrices. $2$ is multipled to $V$ because there are two weight matrices, $W_{input}$ and $W_{output}$. $u$ is a word vector from $W_{input}$ and $v$ is a word vector from $W_{output}$. Each word vectors are $N$-dim row vectors from input and output embedding matrices.</p>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<div id="hidden_layer"></div><h3 id="Hidden-(Projection)-Layer-($h$)">Hidden (Projection) Layer ($h$)<a class="anchor-link" href="#Hidden-(Projection)-Layer-($h$)">¶</a></h3><p>Skip-Gram uses a neural net with one hidden layer. In the context of natural language processing, hidden layer is often referred to as a <em>projection</em> layer, because $h$ is essentially an 1D vector projected by the one-hot encoded input vector.</p>
<div class="row full_screen_margin mobile_responsive_plot_full_width">
<div class="col"><img src="jupyter_images/skip-gram_lookup.png"/></div>
<div class="col-12"><p class="image-description">Figure 9: Computing projection layer</p></div>
</div><p>$h$ is obtained by multiplying the input word embedding matrix with the $V$-dim input vector.</p>
<div id="eq-12" style="font-size: 1rem;">$$h = W_{input}^T \cdot x  \in \mathbb{R}^{N} \tag{12}$$</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<div id="output_layer"></div><h3 id="Softmax-Output-Layer-($y_{pred}$)">Softmax Output Layer ($y_{pred}$)<a class="anchor-link" href="#Softmax-Output-Layer-($y_{pred}$)">¶</a></h3><p>The output layer is a $V$-dim probability distribution of all unique words in the corpus, given a center word. In statistics, the conditional probability of $A$ given $B$ is denoted as $p(A|B)$. In Skip-Gram, we use the notation, $p(w_{context}| w_{center})$, to denote the conditional probability of observing a context word given a center word. It is obtained by using the softmax function,</p>
<div id="eq-13" style="font-size: 1rem;">$$ p(w_{context}|w_{center}) = \frac{exp(W_{output_{(context)}} \cdot h)}{\sum^V_{i=1}exp(W_{output_{(i)}} \cdot h)} \in \mathbb{R}^{1} \tag{13} $$</div><p>where $W_{output_{(i)}}$ is the $i$-th row vector of size $1 \times N$ from the output embedding matrix, $W_{output_{context}}$ is also a row vector of size $1 \times N$ from the output embedding matrix corresponding to the context word. $V$ is the size of unique vocab in the corpus, and $h$ is the hidden (projection) layer of size ($N \times 1$). The output is an $1 \times 1$ scalar value of probability of range $[0, 1)$.</p>
<p>This probability is computed $V$ times to obtain a conditional probability distribution of observing each unique vocabs in the corpus, given a center word.</p>
<div id="eq-14" style="font-size: 1rem;">$$ \left[ \begin{array}{c} p(w_{1}|w_{center}) \\ p(w_{2}|w_{center}) \\ p(w_{3}|w_{center}) \\ \vdots \\ p(w_{V}|w_{center}) \end{array} \right] = \frac{exp(W_{output} \cdot h)}{\sum^V_{i=1}exp(W_{output_{(i)}} \cdot h)} \in \mathbb{R}^{V}\tag{14} $$</div><p>$W_{output}$ in the denominator of eq 13 has size $V \times N$. Multiplying $W_{output}$ with $h$ of size $N \times 1$ will yield a dot product vector of size $V \times 1$. This dot product vector goes through the softmax function:</p>
<div class="row full_screen_margin_md mobile_responsive_plot_full_width">
<div class="col"><img src="jupyter_images/softmax_function.png"/></div>
<div class="col-12"><p class="image-description">Figure 10: softmax function transformation</p></div>
</div><p>The exponentiation ensures that the transformed values are positive, and the normalization factor in the denominator ensures that the values have a range of $[0, 1)$. The result is a conditional probability distribution of observing each unique vocabs in the corpus, given a center word.</p>
<div class="alert alert-info" id="negsample">
<h4>Notes: Negative Sampling</h4>
<p>Softmax function in Skip-Gram has the following equation:</p>
<p><div style="font-size: 1rem; margin-top: 20px;">$$ P = \frac{exp(W_{output} \cdot h)}{\sum^V_{i=1}exp(W_{output_{(i)}} \cdot h)} \in \mathbb{R}^{V}$$</div></p>
<p>There is an issue with softmax in Skip-Gram — it is computationally very expensive, as it requires scanning through the entire output embedding matrix ($W_{output}$) to compute the probability distribution of all $V$ words, where $V$ can be millions or more. Furtheremore, the normalization factor in the denominator also requires $V$ iterations. When implemented in codes, the normalization factor is computed only once and cached as a Python variable, making the alogrithm complexity = $O(V+V)\approx O(V)$.</p>
<p>Due to this computational inefficiency, <b>softmax is not used in most implementaions of Skip-Gram</b>. Instead we use an alternative called negative sampling with sigmoid function, which rephrases the problem into a set of independent binary classification task of algorithm complexity = $O(K+1)$, where $K$ typically has a range of $[5,20]$. Then, the new probability distribution is defined as:</p>
<p><div style="font-size: 1rem; margin-top: 20px;">$$ P = \frac{1}{1+exp(-(\{c_{pos}\} \cup W_{neg}) \cdot h)}
\in \mathbb{R}^{K+1}$$</div></p>
<p>$K=20$ is used for small corpus, and $K=5$ is used for big corpus. Negative sampling is much cheaper than vanilla Skip-Gram with softmax, because $K$ is between 5 ~ 20, whereas $V$ can be millions. Moreover, no extra iterations are necessary to compute the normalization factor in the denominator, because sigmoid function is a binary regression classifier. The algorithm complexity of the probability distribution of vanilla Skip-Gram is $O(V)$, whereas negative sampling's is $O(K+1)$. This shows why negative sampling saves a significant amount of computational cost per iteration.</p>
<p>In <i>gensim</i>, negative sampling is applied by default with <code>Word2Vec(negative=5, ns_exponent=0.75)</code>, where <code>negative</code> is the number of $K$-negative samples, and <code>ns_exponent</code> is a hyperparameter related to negative sampling, of range $(0, 1)$. The details of the methodology behind negative sampling deserves another fully devoted post, and as such, covered in a <a href="https://aegis4048.github.io/optimize_computational_efficiency_of_skip-gram_with_negative_sampling" target="_blank">different post</a>.</p>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<div id="backward"></div><h2 id="Training:-Backward-Propagation">Training: Backward Propagation<a class="anchor-link" href="#Training:-Backward-Propagation">¶</a></h2><p>Backward propagation involves computing prediction errors, and updating the weight matrix ($\theta$) to optimize vector representation of words. Assuming <a href="#stochastic">stochastic gradient descent</a>, we have the following general update equations for the weight matrix ($\theta$):</p>
<div style="font-size: 1rem;">$$ \theta_{new}=\theta_{old}-\eta\cdot\nabla_{J(\theta;w^{(t)}}) \tag{15} $$</div><p>$\eta$ is learning rate, $\nabla_{J(\theta;w^{(t)})}$ is gradient for the weight matrix, and $J(\theta;w^{(t)})$ is the cost function defined in <a href="#eq-6">eq (6)</a>. Since the $\theta$ is a concatenation of input and output weight matrices ($[W_{input} \quad W_{output}]$) as described <a href="#theta_in_cost">above</a>, there are two update equations for each embedding matrix:</p>
<div id="eq-16" style="font-size: 1rem;">$$ W_{input}^{(new)}=W_{input}^{(old)}- \eta \cdot \frac{\partial J}{\partial W_{input}} \tag{16} $$</div>
<div id="eq-17" style="font-size: 1rem;">$$ W_{output}^{(new)}=W_{output}^{(old)}- \eta \cdot \frac{\partial J}{\partial W_{output}} \tag{17} $$</div><p>Mathematically, it can be shown that the gradients of $W_{input}$ $W_{output}$ have the following forms:</p>
<div id="eq-18" style="font-size: 1rem;">$$ \frac{\partial J}{\partial W_{input}}  = x \cdot (W_{output}^T \sum^C_{c=1} e_c) \tag{18}$$</div>
<div id="eq-19" style="font-size: 1rem;">$$ \frac{\partial J}{\partial W_{output}} = h \cdot \sum^C_{c=1} e_c \tag{19}$$</div><p>The gradients can be substitued into <a href="#eq-16">eq (16)</a> and <a href="#eq-17">eq (17)</a>:</p>
<div id="eq-20" style="font-size: 1rem;">$$ W_{input}^{(new)}=W_{input}^{(old)}- \eta \cdot x \cdot (W_{output}^T \sum^C_{c=1} e_c)  \tag{20} $$</div>
<div id="eq-21" style="font-size: 1rem;">$$ W_{output}^{(new)}=W_{output}^{(old)}- \eta \cdot h \cdot \sum^C_{c=1} e_c \tag{21} $$</div><p>$W_{input}$ is <a href="#weight_matrix">input weight matrix</a>, $W_{output}$ is <a href="#weight_matrix">output weight matrix</a>, $x$ is one-hot encoded <a href="#input_layer">input layer</a>, $C$ is <a href="#window_size">window size</a>, and $e_{c}$ is <a href="#predict_error">prediction error</a> for $c$-th context word in the window. Note that $h$ (hidden layer) is equivalent to $W_{input}^T x$.</p>
<div class="alert alert-info">
<h4>Notes: Applying softmax</h4>
<p>Although <a href="#eq-21">eq (21)</a> does not explicitly show it, softmax function is applied in the prediction error ($e_c$). Prediction error is the difference between the predicted and true probability ($y_{pred} - y_{true}$) as illustrated <a href="#forward_softmax">below</a>. The predicted probability $y_{pred}$ is computed using softmax function using <a href="#eq-13">eq (13)</a>.</p>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<div id="predict_error"></div><h3 id="Prediction-Error-($y_{pred}---y_{true}$)">Prediction Error ($y_{pred} - y_{true}$)<a class="anchor-link" href="#Prediction-Error-($y_{pred}---y_{true}$)">¶</a></h3><p>Skip-Gram model optimizes the weight matrix ($\theta$) to reduce the prediction error. Prediction error is the difference between the probability distribution of words computed from the <a href="#output_layer">softmax output layer</a> ($y_{pred}$) and the true probability distribution ($y_{true}$) of the $c$-th context word. Just like the input layer, $y_{true}$ is one-hot encoded vector, in which only one element in the vector that corresponds to the $c$-th context word is $1$, and the rest is all $0$.</p>
<div class="row give-margin-inline-plot">
<div class="col"><img src="jupyter_images/error_window.png" style="height: 400px;"/></div>
<div class="col-12"><p class="image-description">Figure 11: Prediction error window</p></div>
</div><p>The figure has a window size of $2$, so two prediction errors were computed. Recall from the above notes about the <a href="#window_size">window size</a> that the original softmax regression classifier (<a href="#eq-10">eq (10)</a>) has $K$ labels to classify, in which $K = V$ in NLP applications because there are $V$ words to classify. Employing window size transforms <a href="#eq-10">eq (10)</a> into <a href="#eq-11">eq (11)</a> and significantly reduces the algorithm complexity because the model only needs to compute prediction errors for $[1, 10]$ neighboring words, instead of computing all $V$-prediction errors for all vocabs that can be millions or more.</p>
<p>Then, prediction errors for all $C$ context words are summed up to compute weight gradients to the update weight matrices.</p>
<div class="row full_screen_margin mobile_responsive_plot_full_width">
<div class="col"><img src="jupyter_images/prediction_error_sum.png" style="height: 230px;"/></div>
<div class="col-12"><p class="image-description">Figure 12: Sum of prediction errors</p></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Numerical-Demonstration">Numerical Demonstration<a class="anchor-link" href="#Numerical-Demonstration">¶</a></h2><p>For the ease of illustration, screenshots from Excel will be used to demonstrate the concept of updating weight matrices through forward and backward propagations.</p>
<p><strong>Forward Propagation: Computing hidden (projection) layer</strong></p>
<p>Center word is <em>"passes"</em>. Window size is <code>size=1</code>, making <em>"the"</em> and <em>"who"</em> context words. Hidden layer ($h$) is <i>looked up</i> from the input weight matrix. It is computed with <a href="#eq-12">eq (12)</a>.</p>
<div class="row">
<div class="col"><img src="jupyter_images/forward_1.png"/></div>
<div class="col-12"><p class="image-description">Figure 13: Computing hidden (projection) layer</p></div>
</div><div id="forward_softmax"></div><p><strong>Forward Propagation: Softmax output layer</strong></p>
<p>Output layer is a probability distribution of all words, given a center word. It is computed with <a href="#eq-14">eq (14)</a>. Note that all context windows share the same output layer ($y_{pred}$). Only the errors ($e_c$) are different.</p>
<div class="row">
<div class="col"><img src="jupyter_images/forward_2.png"/></div>
<div class="col-12"><p class="image-description">Figure 14: Softmax output layer</p></div>
</div><p><strong>Backward Propagation: Sum of Prediction Errors</strong></p>
<p>$C$ different prediction errors are computed, then summed up. In this case, since we set <code>window=1</code> <a href="#window_id">above</a>, only two errors are computed.</p>
<div class="row">
<div class="col"><img src="jupyter_images/backward_1.png"/></div>
<div class="col-12"><p class="image-description">Figure 15: Prediction errors of context words</p></div>
</div><p><strong>Backward Propagation: Computing $\nabla W_{input}$</strong></p>
<p>Gradients of input weight matrix ($\frac{\partial J}{\partial W_{input}}$) are computed using <a href="#eq-18">eq (18)</a>. Note that multiplying $W_{output}^T \sum^C_{c=1} e_c$ with the one-hot-encoded input vector ($x$) makes the neural net to <u>update only one word vector</u> that corresponds to the input (center) word.</p>
<div class="row">
<div class="col"><img src="jupyter_images/backward_2.png"/></div>
<div class="col-12"><p class="image-description">Figure 16: Computing input weight matrix gradient $\nabla W_{input}$</p></div>
</div><p><strong>Backward Propagation: Computing $\nabla W_{output}$</strong></p>
<p>Gradients of output weight matrix ($\frac{\partial J}{\partial W_{output}}$) are computed using <a href="#eq-19">eq (19)</a>. Unlike the input weight matrix ($W_{input}$), all word vectors in the output weight matrix ($W_{output}$) are updated.</p>
<div class="row">
<div class="col"><img src="jupyter_images/backward_3.png"/></div>
<div class="col-12"><p class="image-description">Figure 17: Computing output weight matrix gradient $\nabla W_{output}$</p></div>
</div><div id="weight_update"></div><p><strong>Backward Propagation: Updating Weight matrices</strong></p>
<p>Input and output weight matrices ($[W_{input} \quad W_{output}]$) are updated using <a href="#eq-20">eq (20)</a> and <a href="#eq-21">eq (21)</a>.</p>
<div class="row">
<div class="col"><img src="jupyter_images/backward_4.png"/></div>
<div class="col-12"><p class="image-description">Figure 18: Updating $W_{input}$</p></div>
</div><div class="row">
<div class="col"><img src="jupyter_images/backward_5.png"/></div>
<div class="col-12"><p class="image-description">Figure 19: Updating $W_{output}$</p></div>
</div><p>Note that for each iteration in the learning process, all weights in $W_{output}$ are updated, but only one row vector that corresponds to the center word is updated in $W_{input}$. When the model finishes updating both of the weight matrices, then one iteration is completed. The model then moves to the next iteration with the next center word. However, remember that this uses <a href="#eq-8">eq (8)</a> as the cost function and assumes <a href="#stochastic">stochastic gradient descent</a>. This means that one update is made for each training example. If <a href="#eq-9">eq (9)</a> is used as a cost function instead (which is almost never the case), then one update is made for all $T$ training examples in the corpus.</p>
</div>
</div>
</div>

<script type="text/javascript">if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = '//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: 'center'," +
        "    displayIndent: '0em'," +
        "    showMathMenu: true," +
        "    tex2jax: { " +
        "        inlineMath: [ ['$','$'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'black ! important'} }" +
        "    } " +
        "}); ";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
</script>
        </div>
        <hr/>
        <aside>
        <nav>
        <ul class="articles-timeline">
            <li class="previous-article">« <a href="https://aegis4048.github.io/understanding_multi-dimensionality_in_vector_space_modeling" title="Previous: Understanding Multi-Dimensionality in Vector Space Modeling">Understanding Multi-Dimensionality in Vector Space Modeling</a></li>
            <li class="next-article"><a href="https://aegis4048.github.io/optimize_computational_efficiency_of_skip-gram_with_negative_sampling" title="Next: Optimize Computational Efficiency of Skip-Gram with Negative Sampling">Optimize Computational Efficiency of Skip-Gram with Negative Sampling</a> »</li>
        </ul>
        </nav>
        </aside>
  <section>
    <div id="disqus_thread" aria-live="polite"><noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript></div>
    <script type="text/javascript">
      var disqus_shortname = 'pythonic-excursions';
      var disqus_identifier = '/demystifying_neural_network_in_skip_gram_language_modeling';
      var disqus_url = 'https://aegis4048.github.io/demystifying_neural_network_in_skip_gram_language_modeling';
      var disqus_title = 'Demystifying Neural Network in Skip-Gram Language Modeling';
      (function() {
        var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
        dsq.src = "//" + disqus_shortname + '.disqus.com/embed.js';
        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
      })();
    </script>
  </section>
        <hr/>
<section style="margin-top: 30px">
    <h2>Related Posts</h2>
<ul class="related-posts-list">
<li><a href="https://aegis4048.github.io/parse-pdf-files-while-retaining-structure-with-tabula-py" title="Parse PDF Files While Retaining Structure with Tabula-py">Parse PDF Files While Retaining Structure with Tabula-py</a></li>
<li><a href="https://aegis4048.github.io/understanding_multi-dimensionality_in_vector_space_modeling" title="Understanding Multi-Dimensionality in Vector Space Modeling">Understanding Multi-Dimensionality in Vector Space Modeling</a></li>
<li><a href="https://aegis4048.github.io/optimize_computational_efficiency_of_skip-gram_with_negative_sampling" title="Optimize Computational Efficiency of Skip-Gram with Negative Sampling">Optimize Computational Efficiency of Skip-Gram with Negative Sampling</a></li>
</ul>
</section>
    </article>
</div>
        </div>
<footer class="footer">
   <div class="container bottom_border">
      <div class="row">
         <div class="col">
            <h5 class="headin5_amrc col_white_amrc pt2">ABOUT ERIC</h5>
            <!--headin5_amrc-->
            <p class="mb10"><img id="profile_img" align="left" src="https://aegis4048.github.io/theme/img/profile_photo_footer.jpg">Senior undergraduate student at the Univeristy of Texas at Austin, Hildebrand Department of Petroleum Engineering, the #1 petroleum engineering school in the US. I am a self-taught Python developer with strong engineering & statistical background. I am good at creating clean, easy-to-read codes for data analysis. I enjoy assisting my fellow engineers by developing accessible and reproducible codes.</p>
            <p><i class="fa fa-envelope mr-2"></i>aegis4048@gmail.com</p>
         </div>
      </div>
   </div>
   <div class="container">
      <ul class="foote_bottom_ul_amrc">
         <li><a href="/">HOME</a></li>
         <li><a href="about.html">ABOUT</a></li>
         <li><a href="archives.html">ARCHIVE</a></li>
      </ul>
      <!--foote_bottom_ul_amrc ends here-->
      <p class="text-center">Handcrafted by me @2018</p>
      <div class="container">
          <div class="row justify-content-center">
              <div class="row" align="center">
                  <div class="footer-icon"><a href="https://www.linkedin.com/in/eric-kim-34318811b/"><i class="fab fa-linkedin-in"></i></a></div>
                  <div class="footer-icon"><a href="https://github.com/aegis4048"><i class="fab fa-github"></i></a></div>
              </div>
          </div>
      </div>

      <!--social_footer_ul ends here-->
   </div>
</footer>
            <script type="text/javascript" src="https://aegis4048.github.io/theme/libs/jquery.min.js"></script>
            <script type="text/javascript" src="https://aegis4048.github.io/theme/libs/bootstrap-4.2.1/dist/js/bootstrap.bundle.min.js"></script>

        <script src="https://aegis4048.github.io/theme/libs/prism.js"></script>
        <script src="https://aegis4048.github.io/theme/libs/Countable.js"></script>

        <script>
            Prism.plugins.NormalizeWhitespace.setDefaults({
                'remove-trailing': true,
                'remove-indent': true,
                'left-trim': true,
                'right-trim': true,
                /*'break-lines': 80,
                'indent': 2,
                'remove-initial-line-feed': false,
                'tabs-to-spaces': 4,
                'spaces-to-tabs': 4*/
            });
        </script>


        <script type="text/javascript" src="https://aegis4048.github.io/theme/js/custom.js"></script>
            <script>
                function validateForm(query)
                {
                    return (query.length > 0);
                }
            </script>
    </body>
</html>

