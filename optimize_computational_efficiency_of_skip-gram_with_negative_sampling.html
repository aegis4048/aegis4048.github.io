<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">

            <meta name="google-site-verification" content="ZsWFnpirKDgtbmwb1YRymDnSfvnUrpzCbf6LD1F_4TY" />

            <meta name="msvalidate.01" content="8FF1B025212A47B5B27CC47163A042F0" />

            <meta name="author" content="ERIC KIM" />


            <meta name="description" content="        How is negative sampling used to improve computational efficieny of Word2Vec?
" />

                <meta property="og:type" content="article" />
            <meta name="twitter:card" content="summary"/>

        <meta name="keywords" content="data-mining, nlp, word2vec, word vectors, window size, skip-gram, neural network, negative sampling, stochastic gradient descent, learning rate, sigmoid, softmax, algorithm complexity, noise distribution, Natural Language Processing, "/>

        <link rel="canonical" href="https://aegis4048.github.io/optimize_computational_efficiency_of_skip-gram_with_negative_sampling">
    <meta property="og:title" content="Optimize Computational Efficiency of Skip-Gram with Negative Sampling | Pythonic Excursions"/>
    <meta property="og:url" content="https://aegis4048.github.io/optimize_computational_efficiency_of_skip-gram_with_negative_sampling" />
    <meta property="og:description" content="How is negative sampling used to improve computational efficieny of Word2Vec?" />
    <meta property="og:site_name" content="Pythonic Excursions" />
    <meta property="og:article:author" content="ERIC KIM" />
        <meta property="og:article:published_time" content="2019-05-26T09:00:00-07:00" />
    <meta name="twitter:title" content="Optimize Computational Efficiency of Skip-Gram with Negative Sampling | Pythonic Excursions">
    <meta name="twitter:description" content="How is negative sampling used to improve computational efficieny of Word2Vec?">
        <meta property="og:image" content="https://aegis4048.github.io/images/featured_images/negative_sampling.png" />
        <meta name="twitter:image" content="https://aegis4048.github.io/images/featured_images/negative_sampling.png" >


        <title>    Optimize Computational Efficiency of Skip-Gram with Negative Sampling  | Pythonic Excursions
</title>

                <link rel="stylesheet" type="text/css" href="https://aegis4048.github.io/theme/libs/bootstrap-4.2.1/dist/css/bootstrap.min.css">
                <link rel="stylesheet" type="text/css" href="https://aegis4048.github.io/theme/libs/fontawesome-free-5.2.0-web/css/all.min.css">
            <link rel="stylesheet" type="text/css" href="https://aegis4048.github.io/theme/css/custom.css" media="screen">
            <link rel="stylesheet" type="text/css" href="https://aegis4048.github.io/theme/css/ipynb.css" media="screen">

            <style>
                #progressBar::-webkit-progress-value {
                    background-color: #24292e;
                }
                #progressBar::-moz-progress-bar {
                    background-color: #24292e;
                }
            </style>

        <link href="https://aegis4048.github.io/theme/libs/prism.css" rel="stylesheet" />
<script type="text/x-mathjax-config">

MathJax.Hub.Config({
tex2jax: { inlineMath: [["$","$"],["\\(","\\)"]] },
"HTML-CSS": {
  linebreaks: { automatic: true, width: "container" }
}
});

</script>
<script type="text/javascript"
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>

        <link rel="shortcut icon" href="https://aegis4048.github.io/theme/img/favicon.ico" type="image/x-icon" />
        <link rel="icon" href="https://aegis4048.github.io/theme/img/apple-touch-icon-152x152.png" type="image/png" />
        <link rel="apple-touch-icon" href="https://aegis4048.github.io/theme/img/apple-touch-icon.png"  type="image/png" />
        <link rel="apple-touch-icon" sizes="57x57" href="https://aegis4048.github.io/theme/img/apple-touch-icon-57x57.png" type="image/png" />
        <link rel="apple-touch-icon" sizes="72x72" href="https://aegis4048.github.io/theme/img/apple-touch-icon-72x72.png" type="image/png" />
        <link rel="apple-touch-icon" sizes="76x76" href="https://aegis4048.github.io/theme/img/apple-touch-icon-76x76.png" type="image/png" />
        <link rel="apple-touch-icon" sizes="114x114" href="https://aegis4048.github.io/theme/img/apple-touch-icon-114x114.png" type="image/png" />
        <link rel="apple-touch-icon" sizes="120x120" href="https://aegis4048.github.io/theme/img/apple-touch-icon-120x120.png" type="image/png" />
        <link rel="apple-touch-icon" sizes="144x144" href="https://aegis4048.github.io/theme/img/apple-touch-icon-144x144.png" type="image/png" />
        <link rel="apple-touch-icon" sizes="152x152" href="https://aegis4048.github.io/theme/img/apple-touch-icon-152x152.png" type="image/png" />
        <link href="https://aegis4048.github.io/feeds/all.atom.xml" type="application/atom+xml" rel="alternate" title="Pythonic Excursions - Full Atom Feed" />

<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-133310548-1"></script>
<script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-133310548-1');
</script>

    </head>
    <body>
<progress id="progressBar" max="19827" class="flat">
    <div class="progress-container">
        <span class="progress-bar"></span>
    </div>
</progress>        <div class="banner-wrapper row" style="background-color: #24292e;">
            <div class="banner">
                <nav id="navbar" class="navbar navbar-expand-md navbar-light bg-light container">
                    <div class="container navbar-title">
                        <a href="/"><img id="banner-logo" src="https://aegis4048.github.io/theme/img/logo_with_subtitle.svg" style="height: 40px; margin: 6px 0;"></a>
                        <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarSupportedContent" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
                            <span class="navbar-toggler-icon"></span>
                        </button>
                    </div>
                <div class="collapse navbar-collapse justify-content-end" id="navbarSupportedContent">
                    <ul class="navbar-nav">
                        <li class="nav-item active">
                            <a class="nav-link rem_08" href="https://aegis4048.github.io/about.html">About</a>
                        </li>
                        <li id="last-item" class="nav-item">
                            <a class="nav-link rem_08" href="https://aegis4048.github.io/archives.html">Archive</a>
                        </li>
                     </ul>
                    <form id="search-form" class="form-inline my-2 my-lg-0 justify-content-center" action="https://aegis4048.github.io/search.html">
                        <div class="search-box-div" align="center">
                            <input id="tipue_search_input" class="form-control mr-md-2 rem_08 col-9" type="text" name="q" pattern=".{3,}" title="At least 3 characters" required="" aria-label="Search">
                            <button id="search-btn" class="btn btn-search btn-outline-success btn-circle rem_08 col-3" type="submit" for="tipue_search_input">Search</button>
                        </div>
                    </form>
                </div>
                </nav>
            </div>
        </div>
        <div id="wrap">
<div id="post-container" class="container post index">
    <article>
        <header>
            <h1>Optimize Computational Efficiency of Skip-Gram with Negative Sampling</h1>
            <div class="row justify-content-between no-margin">
                <h4 class="article-category">Category > <a class="article-category-link" href="https://aegis4048.github.io/archives.html">Natural Language Processing</a></h4>
                <span class="article-date">May 26, 2019</span>
            </div>
            <div class="meta meta-tag no-margin no-border">
                <div>
                        <a href="https://aegis4048.github.io/tag/data-mining.html" class="tag">data-mining</a>
                        <a href="https://aegis4048.github.io/tag/nlp.html" class="tag">nlp</a>
                        <a href="https://aegis4048.github.io/tag/word2vec.html" class="tag">word2vec</a>
                        <a href="https://aegis4048.github.io/tag/word-vectors.html" class="tag">word vectors</a>
                        <a href="https://aegis4048.github.io/tag/window-size.html" class="tag">window size</a>
                        <a href="https://aegis4048.github.io/tag/skip-gram.html" class="tag">skip-gram</a>
                        <a href="https://aegis4048.github.io/tag/neural-network.html" class="tag">neural network</a>
                        <a href="https://aegis4048.github.io/tag/negative-sampling.html" class="tag">negative sampling</a>
                        <a href="https://aegis4048.github.io/tag/stochastic-gradient-descent.html" class="tag">stochastic gradient descent</a>
                        <a href="https://aegis4048.github.io/tag/learning-rate.html" class="tag">learning rate</a>
                        <a href="https://aegis4048.github.io/tag/sigmoid.html" class="tag">sigmoid</a>
                        <a href="https://aegis4048.github.io/tag/softmax.html" class="tag">softmax</a>
                        <a href="https://aegis4048.github.io/tag/algorithm-complexity.html" class="tag">algorithm complexity</a>
                        <a href="https://aegis4048.github.io/tag/noise-distribution.html" class="tag">noise distribution</a>
                </div>
            </div>
            <section>
    <div class="row justify-content-end mt-3" style="align-items: center">
        <div class="share-post-intro mr-2">Share This Post :</div>
        <div class="social-share-btns-container">
            <div class="social-share-btns">
                <a class="share-btn share-btn-twitter" href="https://twitter.com/home?status=https://aegis4048.github.io/optimize_computational_efficiency_of_skip-gram_with_negative_sampling" rel="nofollow" target="_blank">
                    <i class="fab fa-twitter"></i>
                </a>
                <a class="share-btn share-btn-facebook" href="http://www.facebook.com/sharer/sharer.php?u=https://aegis4048.github.io/optimize_computational_efficiency_of_skip-gram_with_negative_sampling" rel="nofollow" target="_blank">
                    <i class="fab fa-facebook-f"></i>
                </a>
                <a class="share-btn share-btn-linkedin" href="https://www.linkedin.com/shareArticle?mini=true&url=https://aegis4048.github.io/optimize_computational_efficiency_of_skip-gram_with_negative_sampling" rel="nofollow" target="_blank">
                    <i class="fab fa-linkedin-in"></i>
                </a>
            </div>
        </div>
    </div>
</section>

        </header>
        <div class="article_content">
            
            <style type="text/css">/*!
*
* IPython notebook
*
*/
/* CSS font colors for translated ANSI escape sequences */
/* The color values are a mix of
   http://www.xcolors.net/dl/baskerville-ivorylight and
   http://www.xcolors.net/dl/euphrasia */
.ansi-black-fg {
  color: #3E424D;
}
.ansi-black-bg {
  background-color: #3E424D;
}
.ansi-black-intense-fg {
  color: #282C36;
}
.ansi-black-intense-bg {
  background-color: #282C36;
}
.ansi-red-fg {
  color: #E75C58;
}
.ansi-red-bg {
  background-color: #E75C58;
}
.ansi-red-intense-fg {
  color: #B22B31;
}
.ansi-red-intense-bg {
  background-color: #B22B31;
}
.ansi-green-fg {
  color: #00A250;
}
.ansi-green-bg {
  background-color: #00A250;
}
.ansi-green-intense-fg {
  color: #007427;
}
.ansi-green-intense-bg {
  background-color: #007427;
}
.ansi-yellow-fg {
  color: #DDB62B;
}
.ansi-yellow-bg {
  background-color: #DDB62B;
}
.ansi-yellow-intense-fg {
  color: #B27D12;
}
.ansi-yellow-intense-bg {
  background-color: #B27D12;
}
.ansi-blue-fg {
  color: #208FFB;
}
.ansi-blue-bg {
  background-color: #208FFB;
}
.ansi-blue-intense-fg {
  color: #0065CA;
}
.ansi-blue-intense-bg {
  background-color: #0065CA;
}
.ansi-magenta-fg {
  color: #D160C4;
}
.ansi-magenta-bg {
  background-color: #D160C4;
}
.ansi-magenta-intense-fg {
  color: #A03196;
}
.ansi-magenta-intense-bg {
  background-color: #A03196;
}
.ansi-cyan-fg {
  color: #60C6C8;
}
.ansi-cyan-bg {
  background-color: #60C6C8;
}
.ansi-cyan-intense-fg {
  color: #258F8F;
}
.ansi-cyan-intense-bg {
  background-color: #258F8F;
}
.ansi-white-fg {
  color: #C5C1B4;
}
.ansi-white-bg {
  background-color: #C5C1B4;
}
.ansi-white-intense-fg {
  color: #A1A6B2;
}
.ansi-white-intense-bg {
  background-color: #A1A6B2;
}
.ansi-default-inverse-fg {
  color: #FFFFFF;
}
.ansi-default-inverse-bg {
  background-color: #000000;
}
.ansi-bold {
  font-weight: bold;
}
.ansi-underline {
  text-decoration: underline;
}
/* The following styles are deprecated an will be removed in a future version */
.ansibold {
  font-weight: bold;
}
.ansi-inverse {
  outline: 0.5px dotted;
}
/* use dark versions for foreground, to improve visibility */
.ansiblack {
  color: black;
}
.ansired {
  color: darkred;
}
.ansigreen {
  color: darkgreen;
}
.ansiyellow {
  color: #c4a000;
}
.ansiblue {
  color: darkblue;
}
.ansipurple {
  color: darkviolet;
}
.ansicyan {
  color: steelblue;
}
.ansigray {
  color: gray;
}
/* and light for background, for the same reason */
.ansibgblack {
  background-color: black;
}
.ansibgred {
  background-color: red;
}
.ansibggreen {
  background-color: green;
}
.ansibgyellow {
  background-color: yellow;
}
.ansibgblue {
  background-color: blue;
}
.ansibgpurple {
  background-color: magenta;
}
.ansibgcyan {
  background-color: cyan;
}
.ansibggray {
  background-color: gray;
}
div.cell {
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: vertical;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: vertical;
  -moz-box-align: stretch;
  display: box;
  box-orient: vertical;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: column;
  align-items: stretch;
  border-radius: 2px;
  box-sizing: border-box;
  -moz-box-sizing: border-box;
  -webkit-box-sizing: border-box;
  border-width: 1px;
  border-style: solid;
  border-color: transparent;
  width: 100%;
  padding: 5px;
  /* This acts as a spacer between cells, that is outside the border */
  margin: 0px;
  outline: none;
  position: relative;
  overflow: visible;
}
div.cell:before {
  position: absolute;
  display: block;
  top: -1px;
  left: -1px;
  width: 5px;
  height: calc(100% +  2px);
  content: '';
  background: transparent;
}
div.cell.jupyter-soft-selected {
  border-left-color: #E3F2FD;
  border-left-width: 1px;
  padding-left: 5px;
  border-right-color: #E3F2FD;
  border-right-width: 1px;
  background: #E3F2FD;
}
@media print {
  div.cell.jupyter-soft-selected {
    border-color: transparent;
  }
}
div.cell.selected,
div.cell.selected.jupyter-soft-selected {
  border-color: #ababab;
}
div.cell.selected:before,
div.cell.selected.jupyter-soft-selected:before {
  position: absolute;
  display: block;
  top: -1px;
  left: -1px;
  width: 5px;
  height: calc(100% +  2px);
  content: '';
  background: #42A5F5;
}
@media print {
  div.cell.selected,
  div.cell.selected.jupyter-soft-selected {
    border-color: transparent;
  }
}
.edit_mode div.cell.selected {
  border-color: #66BB6A;
}
.edit_mode div.cell.selected:before {
  position: absolute;
  display: block;
  top: -1px;
  left: -1px;
  width: 5px;
  height: calc(100% +  2px);
  content: '';
  background: #66BB6A;
}
@media print {
  .edit_mode div.cell.selected {
    border-color: transparent;
  }
}
.prompt {
  /* This needs to be wide enough for 3 digit prompt numbers: In[100]: */
  min-width: 14ex;
  /* This padding is tuned to match the padding on the CodeMirror editor. */
  padding: 0.4em;
  margin: 0px;
  font-family: monospace;
  text-align: right;
  /* This has to match that of the the CodeMirror class line-height below */
  line-height: 1.21429em;
  /* Don't highlight prompt number selection */
  -webkit-touch-callout: none;
  -webkit-user-select: none;
  -khtml-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
  /* Use default cursor */
  cursor: default;
}
@media (max-width: 540px) {
  .prompt {
    text-align: left;
  }
}
div.inner_cell {
  min-width: 0;
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: vertical;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: vertical;
  -moz-box-align: stretch;
  display: box;
  box-orient: vertical;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: column;
  align-items: stretch;
  /* Old browsers */
  -webkit-box-flex: 1;
  -moz-box-flex: 1;
  box-flex: 1;
  /* Modern browsers */
  flex: 1;
}
/* input_area and input_prompt must match in top border and margin for alignment */
div.input_area {
  border: 1px solid #cfcfcf;
  border-radius: 2px;
  background: #f7f7f7;
  line-height: 1.21429em;
}
/* This is needed so that empty prompt areas can collapse to zero height when there
   is no content in the output_subarea and the prompt. The main purpose of this is
   to make sure that empty JavaScript output_subareas have no height. */
div.prompt:empty {
  padding-top: 0;
  padding-bottom: 0;
}
div.unrecognized_cell {
  padding: 5px 5px 5px 0px;
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: horizontal;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: horizontal;
  -moz-box-align: stretch;
  display: box;
  box-orient: horizontal;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: row;
  align-items: stretch;
}
div.unrecognized_cell .inner_cell {
  border-radius: 2px;
  padding: 5px;
  font-weight: bold;
  color: red;
  border: 1px solid #cfcfcf;
  background: #eaeaea;
}
div.unrecognized_cell .inner_cell a {
  color: inherit;
  text-decoration: none;
}
div.unrecognized_cell .inner_cell a:hover {
  color: inherit;
  text-decoration: none;
}
@media (max-width: 540px) {
  div.unrecognized_cell > div.prompt {
    display: none;
  }
}
div.code_cell {
  /* avoid page breaking on code cells when printing */
}
@media print {
  div.code_cell {
    page-break-inside: avoid;
  }
}
/* any special styling for code cells that are currently running goes here */
div.input {
  page-break-inside: avoid;
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: horizontal;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: horizontal;
  -moz-box-align: stretch;
  display: box;
  box-orient: horizontal;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: row;
  align-items: stretch;
}
@media (max-width: 540px) {
  div.input {
    /* Old browsers */
    display: -webkit-box;
    -webkit-box-orient: vertical;
    -webkit-box-align: stretch;
    display: -moz-box;
    -moz-box-orient: vertical;
    -moz-box-align: stretch;
    display: box;
    box-orient: vertical;
    box-align: stretch;
    /* Modern browsers */
    display: flex;
    flex-direction: column;
    align-items: stretch;
  }
}
/* input_area and input_prompt must match in top border and margin for alignment */
div.input_prompt {
  color: #303F9F;
  border-top: 1px solid transparent;
}
div.input_area > div.highlight {
  margin: 0.4em;
  border: none;
  padding: 0px;
  background-color: transparent;
}
div.input_area > div.highlight > pre {
  margin: 0px;
  border: none;
  padding: 0px;
  background-color: transparent;
}
/* The following gets added to the <head> if it is detected that the user has a
 * monospace font with inconsistent normal/bold/italic height.  See
 * notebookmain.js.  Such fonts will have keywords vertically offset with
 * respect to the rest of the text.  The user should select a better font.
 * See: https://github.com/ipython/ipython/issues/1503
 *
 * .CodeMirror span {
 *      vertical-align: bottom;
 * }
 */
.CodeMirror {
  line-height: 1.21429em;
  /* Changed from 1em to our global default */
  font-size: 14px;
  height: auto;
  /* Changed to auto to autogrow */
  background: none;
  /* Changed from white to allow our bg to show through */
}
.CodeMirror-scroll {
  /*  The CodeMirror docs are a bit fuzzy on if overflow-y should be hidden or visible.*/
  /*  We have found that if it is visible, vertical scrollbars appear with font size changes.*/
  overflow-y: hidden;
  overflow-x: auto;
}
.CodeMirror-lines {
  /* In CM2, this used to be 0.4em, but in CM3 it went to 4px. We need the em value because */
  /* we have set a different line-height and want this to scale with that. */
  /* Note that this should set vertical padding only, since CodeMirror assumes
       that horizontal padding will be set on CodeMirror pre */
  padding: 0.4em 0;
}
.CodeMirror-linenumber {
  padding: 0 8px 0 4px;
}
.CodeMirror-gutters {
  border-bottom-left-radius: 2px;
  border-top-left-radius: 2px;
}
.CodeMirror pre {
  /* In CM3 this went to 4px from 0 in CM2. This sets horizontal padding only,
    use .CodeMirror-lines for vertical */
  padding: 0 0.4em;
  border: 0;
  border-radius: 0;
}
.CodeMirror-cursor {
  border-left: 1.4px solid black;
}
@media screen and (min-width: 2138px) and (max-width: 4319px) {
  .CodeMirror-cursor {
    border-left: 2px solid black;
  }
}
@media screen and (min-width: 4320px) {
  .CodeMirror-cursor {
    border-left: 4px solid black;
  }
}
/*

Original style from softwaremaniacs.org (c) Ivan Sagalaev <Maniac@SoftwareManiacs.Org>
Adapted from GitHub theme

*/
.highlight-base {
  color: #000;
}
.highlight-variable {
  color: #000;
}
.highlight-variable-2 {
  color: #1a1a1a;
}
.highlight-variable-3 {
  color: #333333;
}
.highlight-string {
  color: #BA2121;
}
.highlight-comment {
  color: #408080;
  font-style: italic;
}
.highlight-number {
  color: #080;
}
.highlight-atom {
  color: #88F;
}
.highlight-keyword {
  color: #008000;
  font-weight: bold;
}
.highlight-builtin {
  color: #008000;
}
.highlight-error {
  color: #f00;
}
.highlight-operator {
  color: #AA22FF;
  font-weight: bold;
}
.highlight-meta {
  color: #AA22FF;
}
/* previously not defined, copying from default codemirror */
.highlight-def {
  color: #00f;
}
.highlight-string-2 {
  color: #f50;
}
.highlight-qualifier {
  color: #555;
}
.highlight-bracket {
  color: #997;
}
.highlight-tag {
  color: #170;
}
.highlight-attribute {
  color: #00c;
}
.highlight-header {
  color: blue;
}
.highlight-quote {
  color: #090;
}
.highlight-link {
  color: #00c;
}
/* apply the same style to codemirror */
.cm-s-ipython span.cm-keyword {
  color: #008000;
  font-weight: bold;
}
.cm-s-ipython span.cm-atom {
  color: #88F;
}
.cm-s-ipython span.cm-number {
  color: #080;
}
.cm-s-ipython span.cm-def {
  color: #00f;
}
.cm-s-ipython span.cm-variable {
  color: #000;
}
.cm-s-ipython span.cm-operator {
  color: #AA22FF;
  font-weight: bold;
}
.cm-s-ipython span.cm-variable-2 {
  color: #1a1a1a;
}
.cm-s-ipython span.cm-variable-3 {
  color: #333333;
}
.cm-s-ipython span.cm-comment {
  color: #408080;
  font-style: italic;
}
.cm-s-ipython span.cm-string {
  color: #BA2121;
}
.cm-s-ipython span.cm-string-2 {
  color: #f50;
}
.cm-s-ipython span.cm-meta {
  color: #AA22FF;
}
.cm-s-ipython span.cm-qualifier {
  color: #555;
}
.cm-s-ipython span.cm-builtin {
  color: #008000;
}
.cm-s-ipython span.cm-bracket {
  color: #997;
}
.cm-s-ipython span.cm-tag {
  color: #170;
}
.cm-s-ipython span.cm-attribute {
  color: #00c;
}
.cm-s-ipython span.cm-header {
  color: blue;
}
.cm-s-ipython span.cm-quote {
  color: #090;
}
.cm-s-ipython span.cm-link {
  color: #00c;
}
.cm-s-ipython span.cm-error {
  color: #f00;
}
.cm-s-ipython span.cm-tab {
  background: url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAADAAAAAMCAYAAAAkuj5RAAAAAXNSR0IArs4c6QAAAGFJREFUSMft1LsRQFAQheHPowAKoACx3IgEKtaEHujDjORSgWTH/ZOdnZOcM/sgk/kFFWY0qV8foQwS4MKBCS3qR6ixBJvElOobYAtivseIE120FaowJPN75GMu8j/LfMwNjh4HUpwg4LUAAAAASUVORK5CYII=);
  background-position: right;
  background-repeat: no-repeat;
}
div.output_wrapper {
  /* this position must be relative to enable descendents to be absolute within it */
  position: relative;
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: vertical;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: vertical;
  -moz-box-align: stretch;
  display: box;
  box-orient: vertical;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: column;
  align-items: stretch;
  z-index: 1;
}
/* class for the output area when it should be height-limited */
div.output_scroll {
  /* ideally, this would be max-height, but FF barfs all over that */
  height: 24em;
  /* FF needs this *and the wrapper* to specify full width, or it will shrinkwrap */
  width: 100%;
  overflow: auto;
  border-radius: 2px;
  -webkit-box-shadow: inset 0 2px 8px rgba(0, 0, 0, 0.8);
  box-shadow: inset 0 2px 8px rgba(0, 0, 0, 0.8);
  display: block;
}
/* output div while it is collapsed */
div.output_collapsed {
  margin: 0px;
  padding: 0px;
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: vertical;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: vertical;
  -moz-box-align: stretch;
  display: box;
  box-orient: vertical;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: column;
  align-items: stretch;
}
div.out_prompt_overlay {
  height: 100%;
  padding: 0px 0.4em;
  position: absolute;
  border-radius: 2px;
}
div.out_prompt_overlay:hover {
  /* use inner shadow to get border that is computed the same on WebKit/FF */
  -webkit-box-shadow: inset 0 0 1px #000;
  box-shadow: inset 0 0 1px #000;
  background: rgba(240, 240, 240, 0.5);
}
div.output_prompt {
  color: #D84315;
}
/* This class is the outer container of all output sections. */
div.output_area {
  padding: 0px;
  page-break-inside: avoid;
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: horizontal;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: horizontal;
  -moz-box-align: stretch;
  display: box;
  box-orient: horizontal;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: row;
  align-items: stretch;
}
div.output_area .MathJax_Display {
  text-align: left !important;
}
div.output_area 
div.output_area 
div.output_area img,
div.output_area svg {
  max-width: 100%;
  height: auto;
}
div.output_area img.unconfined,
div.output_area svg.unconfined {
  max-width: none;
}
div.output_area .mglyph > img {
  max-width: none;
}
/* This is needed to protect the pre formating from global settings such
   as that of bootstrap */
.output {
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: vertical;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: vertical;
  -moz-box-align: stretch;
  display: box;
  box-orient: vertical;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: column;
  align-items: stretch;
}
@media (max-width: 540px) {
  div.output_area {
    /* Old browsers */
    display: -webkit-box;
    -webkit-box-orient: vertical;
    -webkit-box-align: stretch;
    display: -moz-box;
    -moz-box-orient: vertical;
    -moz-box-align: stretch;
    display: box;
    box-orient: vertical;
    box-align: stretch;
    /* Modern browsers */
    display: flex;
    flex-direction: column;
    align-items: stretch;
  }
}
div.output_area pre {
  margin: 0;
  padding: 1px 0 1px 0;
  border: 0;
  vertical-align: baseline;
  color: black;
  background-color: transparent;
  border-radius: 0;
}
/* This class is for the output subarea inside the output_area and after
   the prompt div. */
div.output_subarea {
  overflow-x: auto;
  padding: 0.4em;
  /* Old browsers */
  -webkit-box-flex: 1;
  -moz-box-flex: 1;
  box-flex: 1;
  /* Modern browsers */
  flex: 1;
  max-width: calc(100% - 14ex);
}
div.output_scroll div.output_subarea {
  overflow-x: visible;
}
/* The rest of the output_* classes are for special styling of the different
   output types */
/* all text output has this class: */
div.output_text {
  text-align: left;
  color: #000;
  /* This has to match that of the the CodeMirror class line-height below */
  line-height: 1.21429em;
}
/* stdout/stderr are 'text' as well as 'stream', but execute_result/error are *not* streams */
div.output_stderr {
  background: #fdd;
  /* very light red background for stderr */
}
div.output_latex {
  text-align: left;
}
/* Empty output_javascript divs should have no height */
div.output_javascript:empty {
  padding: 0;
}
.js-error {
  color: darkred;
}
/* raw_input styles */
div.raw_input_container {
  line-height: 1.21429em;
  padding-top: 5px;
}
pre.raw_input_prompt {
  /* nothing needed here. */
}
input.raw_input {
  font-family: monospace;
  font-size: inherit;
  color: inherit;
  width: auto;
  /* make sure input baseline aligns with prompt */
  vertical-align: baseline;
  /* padding + margin = 0.5em between prompt and cursor */
  padding: 0em 0.25em;
  margin: 0em 0.25em;
}
input.raw_input:focus {
  box-shadow: none;
}
p.p-space {
  margin-bottom: 10px;
}
div.output_unrecognized {
  padding: 5px;
  font-weight: bold;
  color: red;
}
div.output_unrecognized a {
  color: inherit;
  text-decoration: none;
}
div.output_unrecognized a:hover {
  color: inherit;
  text-decoration: none;
}
.rendered_html {
  color: #000;
  /* any extras will just be numbers: */
}



.rendered_html :link {
  text-decoration: underline;
}
.rendered_html :visited {
  text-decoration: underline;
}






.rendered_html h1:first-child {
  margin-top: 0.538em;
}
.rendered_html h2:first-child {
  margin-top: 0.636em;
}
.rendered_html h3:first-child {
  margin-top: 0.777em;
}
.rendered_html h4:first-child {
  margin-top: 1em;
}
.rendered_html h5:first-child {
  margin-top: 1em;
}
.rendered_html h6:first-child {
  margin-top: 1em;
}
.rendered_html ul:not(.list-inline),
.rendered_html ol:not(.list-inline) {
  padding-left: 2em;
}








.rendered_html * + ul {
  margin-top: 1em;
}
.rendered_html * + ol {
  margin-top: 1em;
}





.rendered_html pre,




.rendered_html tr,
.rendered_html th,


.rendered_html tbody tr:nth-child(odd) {
  background: #f5f5f5;
}
.rendered_html tbody tr:hover {
  background: rgba(66, 165, 245, 0.2);
}
.rendered_html * + table {
  margin-top: 1em;
}

.rendered_html * + p {
  margin-top: 1em;
}

.rendered_html * + img {
  margin-top: 1em;
}
.rendered_html img,

.rendered_html img.unconfined,


.rendered_html * + .alert {
  margin-top: 1em;
}
[dir="rtl"] 
div.text_cell {
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: horizontal;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: horizontal;
  -moz-box-align: stretch;
  display: box;
  box-orient: horizontal;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: row;
  align-items: stretch;
}
@media (max-width: 540px) {
  div.text_cell > div.prompt {
    display: none;
  }
}
div.text_cell_render {
  /*font-family: "Helvetica Neue", Arial, Helvetica, Geneva, sans-serif;*/
  outline: none;
  resize: none;
  width: inherit;
  border-style: none;
  padding: 0.5em 0.5em 0.5em 0.4em;
  color: #000;
  box-sizing: border-box;
  -moz-box-sizing: border-box;
  -webkit-box-sizing: border-box;
}
a.anchor-link:link {
  text-decoration: none;
  padding: 0px 20px;
  visibility: hidden;
}
h1:hover .anchor-link,
h2:hover .anchor-link,
h3:hover .anchor-link,
h4:hover .anchor-link,
h5:hover .anchor-link,
h6:hover .anchor-link {
  visibility: visible;
}
.text_cell.rendered .input_area {
  display: none;
}
.text_cell.rendered 
.text_cell.rendered .rendered_html tr,
.text_cell.rendered .rendered_html th,
.text_cell.rendered 
.text_cell.unrendered .text_cell_render {
  display: none;
}
.text_cell .dropzone .input_area {
  border: 2px dashed #bababa;
  margin: -1px;
}
.cm-header-1,
.cm-header-2,
.cm-header-3,
.cm-header-4,
.cm-header-5,
.cm-header-6 {
  font-weight: bold;
  font-family: "Helvetica Neue", Helvetica, Arial, sans-serif;
}
.cm-header-1 {
  font-size: 185.7%;
}
.cm-header-2 {
  font-size: 157.1%;
}
.cm-header-3 {
  font-size: 128.6%;
}
.cm-header-4 {
  font-size: 110%;
}
.cm-header-5 {
  font-size: 100%;
  font-style: italic;
}
.cm-header-6 {
  font-size: 100%;
  font-style: italic;
}
</style>
<style type="text/css">.highlight .hll { background-color: #ffffcc }
.highlight  { background: #f8f8f8; }
.highlight .c { color: #408080; font-style: italic } /* Comment */
.highlight .err { border: 1px solid #FF0000 } /* Error */
.highlight .k { color: #008000; font-weight: bold } /* Keyword */
.highlight .o { color: #666666 } /* Operator */
.highlight .ch { color: #408080; font-style: italic } /* Comment.Hashbang */
.highlight .cm { color: #408080; font-style: italic } /* Comment.Multiline */
.highlight .cp { color: #BC7A00 } /* Comment.Preproc */
.highlight .cpf { color: #408080; font-style: italic } /* Comment.PreprocFile */
.highlight .c1 { color: #408080; font-style: italic } /* Comment.Single */
.highlight .cs { color: #408080; font-style: italic } /* Comment.Special */
.highlight .gd { color: #A00000 } /* Generic.Deleted */
.highlight .ge { font-style: italic } /* Generic.Emph */
.highlight .gr { color: #FF0000 } /* Generic.Error */
.highlight .gh { color: #000080; font-weight: bold } /* Generic.Heading */
.highlight .gi { color: #00A000 } /* Generic.Inserted */
.highlight .go { color: #888888 } /* Generic.Output */
.highlight .gp { color: #000080; font-weight: bold } /* Generic.Prompt */
.highlight .gs { font-weight: bold } /* Generic.Strong */
.highlight .gu { color: #800080; font-weight: bold } /* Generic.Subheading */
.highlight .gt { color: #0044DD } /* Generic.Traceback */
.highlight .kc { color: #008000; font-weight: bold } /* Keyword.Constant */
.highlight .kd { color: #008000; font-weight: bold } /* Keyword.Declaration */
.highlight .kn { color: #008000; font-weight: bold } /* Keyword.Namespace */
.highlight .kp { color: #008000 } /* Keyword.Pseudo */
.highlight .kr { color: #008000; font-weight: bold } /* Keyword.Reserved */
.highlight .kt { color: #B00040 } /* Keyword.Type */
.highlight .m { color: #666666 } /* Literal.Number */
.highlight .s { color: #BA2121 } /* Literal.String */
.highlight .na { color: #7D9029 } /* Name.Attribute */
.highlight .nb { color: #008000 } /* Name.Builtin */
.highlight .nc { color: #0000FF; font-weight: bold } /* Name.Class */
.highlight .no { color: #880000 } /* Name.Constant */
.highlight .nd { color: #AA22FF } /* Name.Decorator */
.highlight .ni { color: #999999; font-weight: bold } /* Name.Entity */
.highlight .ne { color: #D2413A; font-weight: bold } /* Name.Exception */
.highlight .nf { color: #0000FF } /* Name.Function */
.highlight .nl { color: #A0A000 } /* Name.Label */
.highlight .nn { color: #0000FF; font-weight: bold } /* Name.Namespace */
.highlight .nt { color: #008000; font-weight: bold } /* Name.Tag */
.highlight .nv { color: #19177C } /* Name.Variable */
.highlight .ow { color: #AA22FF; font-weight: bold } /* Operator.Word */
.highlight .w { color: #bbbbbb } /* Text.Whitespace */
.highlight .mb { color: #666666 } /* Literal.Number.Bin */
.highlight .mf { color: #666666 } /* Literal.Number.Float */
.highlight .mh { color: #666666 } /* Literal.Number.Hex */
.highlight .mi { color: #666666 } /* Literal.Number.Integer */
.highlight .mo { color: #666666 } /* Literal.Number.Oct */
.highlight .sa { color: #BA2121 } /* Literal.String.Affix */
.highlight .sb { color: #BA2121 } /* Literal.String.Backtick */
.highlight .sc { color: #BA2121 } /* Literal.String.Char */
.highlight .dl { color: #BA2121 } /* Literal.String.Delimiter */
.highlight .sd { color: #BA2121; font-style: italic } /* Literal.String.Doc */
.highlight .s2 { color: #BA2121 } /* Literal.String.Double */
.highlight .se { color: #BB6622; font-weight: bold } /* Literal.String.Escape */
.highlight .sh { color: #BA2121 } /* Literal.String.Heredoc */
.highlight .si { color: #BB6688; font-weight: bold } /* Literal.String.Interpol */
.highlight .sx { color: #008000 } /* Literal.String.Other */
.highlight .sr { color: #BB6688 } /* Literal.String.Regex */
.highlight .s1 { color: #BA2121 } /* Literal.String.Single */
.highlight .ss { color: #19177C } /* Literal.String.Symbol */
.highlight .bp { color: #008000 } /* Name.Builtin.Pseudo */
.highlight .fm { color: #0000FF } /* Name.Function.Magic */
.highlight .vc { color: #19177C } /* Name.Variable.Class */
.highlight .vg { color: #19177C } /* Name.Variable.Global */
.highlight .vi { color: #19177C } /* Name.Variable.Instance */
.highlight .vm { color: #19177C } /* Name.Variable.Magic */
.highlight .il { color: #666666 } /* Literal.Number.Integer.Long */</style><div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><em>The code snippet assumes Anaconda 5.2.0 version of Python virtual environment</em></p>
<div class="alert alert-info">
<h4>Acknowledgement</h4>
<p>The materials on this post are based the on five NLP papers, <a href="https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf" target="_blank">Distributed Representations of Words and Phrases and their Compositionality</a> (Mikolov et al., 2013), <a href="https://arxiv.org/pdf/1411.2738.pdf" target="_blank">word2vec Parameter Learning Explained</a> (Rong, 2014), <a href="https://www.aaai.org/ocs/index.php/AAAI/AAAI17/paper/viewFile/14956/14446" target="_blank">Distributed Negative Sampling for Word Embeddings</a> (Stergiou et al., 2017), <a href="https://aclweb.org/anthology/D17-1037" target="_blank">Incremental Skip-gram Model with Negative Sampling</a> (Kaji and Kobayashi, 2017), and <a href="https://arxiv.org/pdf/1402.3722.pdf" target="_blank">word2vec Explained: Deriving Mikolov et al.’s
Negative-Sampling Word-Embedding Method</a> (Goldberg and Levy, 2014).</p>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Review-on-Word2Vec-Skip-Gram">Review on Word2Vec Skip-Gram<a class="anchor-link" href="#Review-on-Word2Vec-Skip-Gram">¶</a></h2><p>In my <a href="https://aegis4048.github.io/demystifying_neural_network_in_skip_gram_language_modeling" target="_blank">previous post</a>, I illustrated the neural network structure of Skip-Gram Word2Vec model that represents words in a vector space.</p>
<div class="row" id="fig1">
<div class="col"><img src="jupyter_images/word2vec_skip-gram.png" style="margin: 0;"/></div>
<div class="col-12"><p class="image-description">Figure 1: Neural network structure of Skip-Gram</p></div>
</div><p>I also derived the cost function of Skip-Gram in <a href="https://aegis4048.github.io/demystifying_neural_network_in_skip_gram_language_modeling#Derivation-of-Cost-Function" target="_blank">Derivation of Cost Function</a>:</p>
<div id="eq-1" style="font-size: 1rem;">
$$\begin{align}
  J(\theta) = & - \frac{1}{T} \sum_{t=1}^{T} \sum_{-c\leq j \leq c,j\neq 0} \log p(w_{t+j} \mid w_t ; \, \theta) \\
\end{align}
\tag{1} $$
</div><p>where $p(w_{t+j} \mid w_t ; \, \theta)$ is a probability of observing $w_{t+j}$ given $w_{t}$ with parameters $\theta$. In vanilla Skip-Gram, the probability is computed with softmax. I also noted that <a href="https://aegis4048.github.io/demystifying_neural_network_in_skip_gram_language_modeling#stochastic" target="_blank">stochastic gradient descent</a> (SGD) is used to mitigate computational burden — the size of $T$ in $\frac{1}{T} \sum^T_{t=1}$ can be billions or more in NLP applications. The new cost function using SGD is:</p>
<div id="eq-2" style="font-size: 1rem;">
$$J(\theta; w^{(t)}) = -
\sum_{c=1}^{C}
log 
\frac{exp(W_{output_{(c)}} \cdot h)}{\sum^V_{i=1}exp(W_{output_{(i)}} \cdot h)} \tag{2}$$
</div><p>where $T$ is the size of training samples, $C$ is the <a href="https://aegis4048.github.io/demystifying_neural_network_in_skip_gram_language_modeling#Window-Size-of-Skip-Gram" target="_blank">window size</a>, $V$ is the size of unique vocab in the corpus, and $W_{input}$, $W_{output}$ and $h$ are illustrated in <a href="#fig1">figure 1</a>.</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Review-on-Softmax">Review on Softmax<a class="anchor-link" href="#Review-on-Softmax">¶</a></h3><p>Softmax is a multinomial regression classifier. It means that it classifies multiple labels, such as predicting if an hand-written digit is $0,\,1,\,2,\,...\,8\,$ or $9$. In case of binary classification (True or False), such as classifying fraud or not-fraud in bank transactions, binomial regression classifier called Sigmoid function is used.</p>
<p>In <a href="#eq-2">eq (2)</a>, the fraction inside the summation of log yields the probability distribution of all $V$-vocabs in the corpus, given the input word. In statistics, the conditional probability of $A$ given $B$ is denoted as $p(A|B)$. In Skip-Gram, we use the notation, $p(w_{context}| w_{center})$, to denote the conditional probability of observing a context word given a center word. It is obtained by using the softmax function:</p>
<div id="eq-3" style="font-size: 1rem;">$$ p(w_{context}|w_{center}) = \frac{exp(W_{output_{(context)}} \cdot h)}{\sum^V_{i=1}exp(W_{output_{(i)}} \cdot h)} \in \mathbb{R}^{1} \tag{3} $$</div><p>Exponentiation ensures that the transformed values are positive, and the normalization factor in the denominator ensures that the values have a range of $[0, 1)$ and their sum equals $1$.</p>
<div class="row full_screen_margin mobile_responsive_plot_full_width" id="fig2">
<div class="col"><img src="jupyter_images/softmax_function.png" style="height: 300px;"/></div>
<div class="col-12"><p class="image-description">Figure 2: softmax function transformation</p></div>
</div><p>The probability is computed $V$ times using <a href="#eq-3">eq (3)</a> to obtain a conditional probability distribution of observing all $V$-unique vocabs in the corpus, given a center word ($w^{(t)}$).</p>
<div id="eq-4" style="font-size: 1rem;">$$ \left[ \begin{array}{c} p(w_{1}|w^{(t)}) \\ p(w_{2}|w^{(t)}) \\ p(w_{3}|w^{(t)}) \\ \vdots \\ p(w_{V}|w^{(t)}) \end{array} \right] = \frac{exp(W_{output} \cdot h)}{\sum^V_{i=1}exp(W_{output_{(i)}} \cdot h)} \in \mathbb{R}^{V}\tag{4} $$</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Softmax-is-computationally-very-expensive">Softmax is computationally very expensive<a class="anchor-link" href="#Softmax-is-computationally-very-expensive">¶</a></h3><p>There is an issue with the vanilla Skip-Gram — softmax is computationally very expensive, as it requires scanning through the entire output embedding matrix ($W_{output}$) to compute the probability distribution of all $V$ words, where $V$ can be millions or more.</p>
<div class="row full_screen_margin mobile_responsive_plot_full_width">
<div class="col"><img src="jupyter_images/vanilla-skip-gram-complexity.png"/></div>
<div class="col-12"><p class="image-description">Figure 3: Algorithm complexity of vanilla Skip-Gram</p></div>
</div><p>Furtheremore, the normalization factor in the denominator also requires $V$ iterations. In mathematical context, the normalization factor needs to be computed for each probability p($w_{context}| w_{center}$), making the alogrithm complexity = $O(V \times V)$. However, when implemented on code, the normalization factor is computed only once and cached as a Python variable, making the alogrithm complexity = $O(V + V) \approx O(V)$. This is possible because normalization factor is the same for all words.</p>
<p>Due to this computational inefficiency, <strong>softmax is not used in most implementaions of Skip-Gram</strong>. Instead we use an alternative called <em>negative sampling</em> with <a href="#sigmoid">sigmoid function</a>, which rephrases the problem into a set of independent binary classification task of algorithm complexity = $O(K \, + \, 1)$, where $K$ typically has a range of $[5, 20]$.</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Skip-Gram-Negative-Sampling">Skip-Gram Negative Sampling<a class="anchor-link" href="#Skip-Gram-Negative-Sampling">¶</a></h2><p>In Skip-Gram, assuming <a href="https://aegis4048.github.io/demystifying_neural_network_in_skip_gram_language_modeling#stochastic" target="_blank">stochastic gradient descent</a>, weight marices in the neural network are updated for each training sample to correctly predict output. Let's assume that the training corpus has 10,000 unique vocabs ($V$ = 10000) and the hidden layer is 300-dimensional ($N$ = 300). This means that there are 3,000,000 neurons in the output weight matrix ($W_{output}$) that need to be updated for each training sample (Notes: for the input weight matrix ($W_{input}$), only 300 neurons are updated for each training sample. This is illustrated in <em>figure 18</em> of my <a href="https://aegis4048.github.io/demystifying_neural_network_in_skip_gram_language_modeling#weight_update" target="_blank">previous post</a>.) Since the size of the training corpus ($T$) is very large, updating 3M neurons for each training sample is unrealistic in terms of computational efficiency. Negative sampling addresses this issue by updating only a small fraction of the output weight neurons for each training sample.</p>
<p>In negative sampling, $K$ <a href="#neg_word">negative samples</a> are <a href="#neg_drawn">randomly drawn</a> from a <a href="#noise_dist">noise distribution</a>. $K$ is a hyper-parameter that can be empirically tuned, with a typical range of $[5,\, 20]$. For each training sample (positive pair: $w$ and $c_{pos}$), you randomly draw $K$ number of negative samples from a noise distribution $P_n(w)$, and the model will update $(K+1) \times N$ neurons in the output weight matrix ($W_{output}$). $N$ is the dimension of the hidden layer ($h$), or the size of a word vector. $+1$ accounts for a <a href="#pos_word">positive sample</a>.</p>
<p>With the above assumption, if you set <code>K=9</code>, the model will update $(9 + 1) \times 300 = 3000$ neurons, which is only 0.1% of the 3M neurons in $W_{output}$. This is computationally much cheaper than the original Skip-Gram, and yet maintains a good quality of word vectors.</p>
<p>The below figure has 3-dimensional hidden layer ($N=3$), 11 vocabs ($V=11$), and 3 negative samples ($K=3$).</p>
<div class="row" id="fig4" style="margin-top: 20px;">
<div class="col"><img src="jupyter_images/neg_vs_skip.png" style="margin: 0;"/></div>
<div class="col-12"><p class="image-description">Figure 4: Skip-Gram model structure</p></div>
</div><div class="alert alert-info" id="negsample">
<h4>Notes: Choice of $K$</h4>
<p>The <a href="https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf" target="_blank">paper</a> (Mikolov et al., 2013) says that K=2 ~ 5 works for large data sets, and K=5 ~ 20 for small data sets.
    </p>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="How-does-negative-sampling-work?">How does negative sampling work?<a class="anchor-link" href="#How-does-negative-sampling-work?">¶</a></h3><p>With negative sampling, word vectors are no longer learned by predicting context words of a center word. Instead of using softmax to compute the $V$-dimensional probability distribution of observing an output word given an input word, $p(w_O|w_I)$, the model uses sigmoid function to learn to differentiate the actual context words (<em>positive</em>) from randomly drawn words (<em>negative</em>) from the <a href="#noise_dist">noise distribution</a> $P_n(w)$.</p>
<p>Assume that the center word is <em>"regression"</em>. It is likely to observe <em>"regression"</em> + {<em>"logistic"</em>, <em>"machine"</em>, <em>"sigmoid"</em>, <em>"supervised"</em>, <em>"neural"</em>} pairs, but it is unlikely to observe <em>"regression"</em> + {<em>"zebra"</em>, <em>"pimples"</em>, <em>"Gangnam-Style"</em>, <em>"toothpaste"</em>, <em>"idiot"</em>}. The model maximizes the probability $p(D=1|w,c_{pos})$ of observing positive pairs, while minimizing the probability $p(D=1|w,c_{neg})$ of observing negative pairs. The idea is that <b>if the model can distinguish between the likely (positive) pairs vs unlikely (negative) pairs, good word vectors will be learned.</b></p>
<div class="row full_screen_margin_md mobile_responsive_plot_full_width" id="fig5" style="margin-top: 20px;">
<div class="col"><img src="jupyter_images/neg_binomial.png"/></div>
<div class="col-12"><p class="image-description">Figure 5: Binomial classification of negative sampling</p></div>
</div><p>Negative sampling converts multi-classification task into binary-classification task. The new objective is to predict, for any given word-context pair ($w,\,c$), whether the word ($c$) is in the context window of the the center word ($w$) or not. Since the goal is to identify a given word as True (<em>positive</em>, $D=1$) or False (<em>negative</em>, $D=0$), we use sigmoid function instead of softmax function. The probability of a word ($c$) appearing within the context of the center word ($w$) can be defined as the following:</p>
<div id="eq-5" style="font-size: 1rem;">
$$
p(D=1|w,c;\theta)=\frac{1}{1+exp(-\bar{c}_{output_{(j)}} \cdot w)}
\in \mathbb{R}^{1}
\tag{5} 
$$</div><p>where $c$ is the word you want to know whether it came from the context window or the noise distribution. $w$ is the input (center) word, and $\theta$ is the <a href="https://aegis4048.github.io/demystifying_neural_network_in_skip_gram_language_modeling#theta_in_cost" target="_blank">weight matrix</a> passed into the model. Note that $w$ is equivalent to the hidden layer ($h$). $\bar{c}_{output_{(j)}}$ is the word vector from the output weight matrix ($W_{output}$) of <a href="#fig1">figure 1</a>.</p>
<p><a href="#eq-5">Eq (5)</a> computes the probability that the given word ($c$) is a <a href="#pos_word">positive word</a> ($D=1$). It only needs to be applied $K + 1$ times instead of $V$ times for every word in the vocabulary, because $\bar{c}_{output_{(j)}}$ comes from the concatenation of a true context word ($c_{pos}$) and $K$ <a href="#neg_word">negative words</a> ($\bar{W}_{neg} = \{ \bar{c}_{neg, j}|j=1,\cdots,K \}$):</p>
<div id="eq-6" style="font-size: 1rem;">
$$
\bar{c}_{output{(j)}} \in \{\bar{c}_{pos}\} \cup \bar{W}_{neg}
\tag{6} 
$$
</div><p>This probability is computed $K + 1$ times to obtain a probability distribution of a true context word and $K$ negative samples:</p>
<div id="eq-7" style="font-size: 1rem;">
$$
\left[ \begin{array}{c} p(D=1|w,c_{pos}) \\ p(D=1|w,c_{neg, 1}) \\ p(D=1|w,c_{neg, 2}) \\ p(D=1|w,c_{neg, 3}) \\ \vdots \\ p(D=1|w,c_{neg, K}) \end{array} \right] 
= 
\frac{1}{1+exp(-(\{\bar{c}_{pos}\} \cup \bar{W}_{neg}) \cdot h)}
\in \mathbb{R}^{K+1}\tag{7} 
$$
</div><p>Compare this equation with <a href="#eq-4">eq (4)</a> — you will notice that <a href="#eq-7">eq (7)</a> is <strong>computationally much cheaper because $K$ is between 5 ~ 20, whereas $V$ can be millions. Moreover, no extra iterations are necessary to compute the normalization factor</strong> in the denominator of <a href="#eq-4">eq (4)</a>, because sigmoid function is a binary regression classifier. The algorithm complexity for probability distribution of vanilla Skip-Gram is $O(V)$, whereas negative sampling's is $O(K+1)$. This shows why negative sampling saves a significant amount of computational cost per iteration.</p>
<div class="alert alert-info" id="sigmoid">
<h4>Notes: Sigmoid function $\sigma(x)$</h4>
<p>Sigmoid function is used for two-class logistic regression.</p>
<p><div style="font-size: 1rem; margin-top: 20px;">$$ \sigma(x) = \frac{1}{1+exp(-x))}$$</div></p>
<p>It is used to classify if a given sample is True or False based on the computed probability. The sample is classified as True if the value is greater then 0.5, and vice versa. For example, if you want to classify if a certain bank transaction is fraud or not, you will use sigmoid for binary classification.</p>
<img class="admonition-image" src="jupyter_images/sigmoid.png" style="border: 1px solid #ddd;">
<p>If you are working on a multi-class task, such as hand-written digit classification, you will use softmax regression classifier.</p>
</img></div><div style="display: none">
from matplotlib import pylab
import pylab as plt
import numpy as np
%matplotlib notebook

#sigmoid = lambda x: 1 / (1 + np.exp(-x))
def sigmoid(x):
    return (1 / (1 + np.exp(-x)))

mySamples = []
mySigmoid = []

x = plt.linspace(-10,10,10)
y = plt.linspace(-10,10,100)

plt.plot(y, sigmoid(y))
plt.title('Sigmoid Function $\sigma(x)$')
plt.text(4, 0.8, r'$\sigma(x)=\frac{1}{1+e^{-x}}$', fontsize=15)
plt.xlabel('X', fontsize=12)
plt.ylabel('$\sigma(x)$', fontsize=14)
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<div><hr/></div>
<div class="row give-margin-inline-big-plot mobile_responsive_plot_full_width" id="fig6">
<div class="col"><img src="jupyter_images/negative_sample_text.png"/></div>
<div class="col-12"><p class="image-description">Figure 6: Choice of negative samples (Text source: <a href="https://petrowiki.org/Drilling_induced_formation_damage" target="_blank">Petrowiki</a>)</p></div>
</div><p>For the purpose of illustration, consider the above paragraphs. Assume that our center word ($w$) is <code>drilling</code>, window size is $3$, and the number of negative samples ($K$) is $5$. With the window size of $3$, the contexts words are: <em>"engineer"</em>, <em>"traditionally"</em>, <em>"designs"</em>, <em>"fluids"</em>, <em>"with"</em>, and <em>"two"</em>. These context words are considered as positive labels ($D = 1$). Our current context word ($c_{pos}$) is <code>engineer</code>. We also need negative words. We randomly pick $5$-words from the <a href="#noise_dist">noise distribution</a> $P_n(w)$ of the corpus for each context word, and consider them as negative samples ($D = 0$). For the current context word, <code>engineer</code>, the 5 randomly drawn negative words ($c_{neg}$) are: <em>"minimized"</em>, <em>"primary"</em>, <em>"concerns"</em>, <em>"led"</em>, and <em>"page"</em>.</p>
<p>The idea of negative sampling is that it is more likely to observe positive word pairs ($w$, $c_{pos}$) together than negative word pairs ($w$, $c_{neg}$) together in the corpus. The model attempts to maximize the the probability of observing positive pairs $p(c_{pos}|w) \rightarrow 1$ and minimize the probability of observing negative pairs $p(c_{neg}|w) \rightarrow 0$ simultaneously by iterating through the training samples and updating the weights ($\theta$). Note that the sum of the probability distribution obtained by sigmoid function (<a href="#eq-7">eq (7)</a>) does not need to equal $1$, unlike softmax (<a href="#eq-4">eq (4)</a>).</p>
<div class="row" id="fig7">
<div class="col"><img src="jupyter_images/neg_opt_1.png"/></div>
<div class="col-12"><p class="image-description">Figure 7: maximizing postive pairs and minimizing negative pairs</p></div>
</div><p>By the time the output probability distribution is nearly one-hot-encoded as in $iter = 4$ of the above figure, weight matrices $\theta$ are optimized and good word vectors are learned. This optimization is achieved by maximizing the dot product of positive pairs ($\bar{c}_{pos}\cdot \bar{w}$) and minimizing the dot product of negative pairs ($c_{neg}\cdot w$) in <a href="#eq-18">eq (18)</a>.</p>
<div class="alert alert-info" id="negsample">
<h4>Notes: Drawing random negative samples</h4>
<p>For each positive word-context pair ($w,\,c_{pos}$), $K$ new negative samples are <a href="#neg_drawn">randomly drawn</a> from a <a href="#noise_dist">noise distribution</a>. In <a href="#fig6">figure 6</a>, there are $6$ positive context words (<i>"engineer"</i>, <i>"traditionally"</i>, <i>"designs"</i>, <i>"fluids"</i>, <i>"with"</i>, and <i>"two"</i>) for one center word (<i>"drilling"</i>), and $K$ is $5$. This means that a total of $6 \times 5 = 30$ word vectors are updated for each center word $w$.</p>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<div id="pos_word"></div><h4 id="What-is-a-positive-word-$c_{pos}$?">What is a positive word $c_{pos}$?<a class="anchor-link" href="#What-is-a-positive-word-$c_{pos}$?">¶</a></h4><p>Words that actually appear within the context window of the center word ($w$). After the model is optimized, the probability computed with <a href="#eq-5">eq (5)</a> for positive words $c_{pos}$ will output $\approx$ 1 as shown in <a href="#fig7">fig 7</a>. A word vector of a center word ($w$) will be more similar to a word vector of positive word ($c_{pos}$) than of randomly drawn negative words ($c_{neg}$). This is because words that frequently appear together show strong correlation with each other. Therefore, once the model is optimized: $p(D=1|w,c_{pos})\approx1$.</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<div id="neg_word"></div><h4 id="What-is-a-negative-word-$c_{neg}$?">What is a negative word $c_{neg}$?<a class="anchor-link" href="#What-is-a-negative-word-$c_{neg}$?">¶</a></h4><p>Words that are randomly drawn from a noise distribution $P_n(w)$. After the model is optimized, the probability computed with <a href="#eq-5">eq (5)</a> for negative words $c_{neg}$ will output $\approx$ 0 as shown in <a href="#fig7">fig 7</a>. When training an Word2Vec model, the vocab size ($V$) easily exceeds tens of thousands. When 5 ~ 20 negative samples are randomly drawn among the vocabs, it is unlikely to observe the random word with a center word together in the corpus. Therefore, once the model is optimized: $p(D=1|w,c_{neg})\approx0$.</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<div id="noise_dist"></div><h4 id="What-is-a-noise-distribution-$P_n(w)$?">What is a noise distribution $P_n(w)$?<a class="anchor-link" href="#What-is-a-noise-distribution-$P_n(w)$?">¶</a></h4><p>Imagine a distribution of words based on how many times each word appeared in a corpus, denoted as $U(w)$ (this is called unigram distribution). For each word $w$, divide the number of times it appeared in a corpus by a normalization factor $Z$ so that the distribution becomes a probability distribution of range $[0, 1)$ and sums up to $1$. Raise the normalized distribution to the power of $\alpha$ so that the distribution is "smoothed-out". Then this becomes your noise distribution $P_n(w)$ — normalized frequency distribution of words raised to the power of $\alpha$. Mathematically, it can be expressed as:</p>
<div id="eq-8" style="font-size: 1rem;">
$$
P_n(w) = \left(\frac{U(w)}{Z}\right)^{\alpha}
\tag{8} 
$$</div><p>Raising the unigram distribution $U(w)$ to the power of $\alpha$ has an effect of smoothing out the distribution. <strong>It attempts to combat the imbalance between common words and rare words</strong> by decreasing the probability of drawing common words, and increasing the probability drawing rare words.</p>
<p>$\alpha$ is a hyper-parameter that can be empirically tuned. The authors of the original Word2Vec <a href="https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf" target="_blank">paper</a> claims that the unigram distribution $U(w)$ raised to the $3/4$rd power (i.e., $U(w)^{3/4}/Z$) yielded the best result.</p>
<div class="row" id="fig8">
<div class="col"><img src="jupyter_images/noise_dist.png"/></div>
<div class="col-12"><p class="image-description">Figure 8: Effect of raising power of unigram distribution $U(w)$</p></div>
</div><div class="alert alert-info" id="incremental_noise_dist">
<h4>Notes: Incremental noise distribution</h4>
<p>Noise distribution $P_n(w)$ needs to be pre-computed by iterating through the entire corpus to obtain word frequency $U(w)$ and normalization factor $Z$ of the distribution. If an additional training data is added to the corpus, $P_n(w)$ needs to be computed all over again. To address this problem, a simple incremental extension of negative sampling was provided in this <a href="https://aclweb.org/anthology/D17-1037" target="_blank">paper</a> (Kaji and Kobayashi, 2017).</p>
</div><div style="display: none">

import numpy as np
import matplotlib.pyplot as plt
from scipy import stats
%matplotlib notebook

# sample data generation
data = sorted(stats.norm.rvs(size=1000) + 5)

# fit normal distribution
mean, std = stats.norm.fit(data, loc=0)
pdf_norm = stats.norm.pdf(data, mean, std)

temp = np.power(data, 3/4)
temp_mean, temp_std = stats.norm.fit(temp, loc=0)
temp_pdf_norm = stats.norm.pdf(temp, temp_mean, temp_std)

fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(8, 4), sharey=True)
ax1.set_title('$U(w)$')
ax1.hist(temp, bins='auto', density=True)
ax1.set_xticklabels(['apple', 'desk', 'cup', 'chair', 'zebra', 'room', 'pencil', 'water', 'coin'])
ax1.plot(temp, temp_pdf_norm, label='norm')
ax2.set_title('$U(w)^{3/4}$')
ax2.hist(data, bins='auto', density=True)
ax2.plot(data, pdf_norm, label='norm')
ax2.set_xticklabels(['apple', 'desk', 'cup', 'chair', 'zebra', 'room', 'pencil', 'water', 'coin'])
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><div id="neg_drawn"></div></p>
<h4 id="How-are-negative-samples-drawn?">How are negative samples drawn?<a class="anchor-link" href="#How-are-negative-samples-drawn?">¶</a></h4><p>$K$-negative samples are randomly drawn from a noise distribution $P_n(w)$. The noise distribution is generated with <a href="#eq-8">eq (8)</a> and the random samples are drawn with <a href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.random.choice.html" target="_blank">np.random.choice</a>.</p>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In [2]:</div>
<div class="inner_cell">
<div class="input_area">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In [7]:</div>
<div class="inner_cell">
<div class="input_area">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">unig_dist</span>  <span class="o">=</span> <span class="p">{</span><span class="s1">'apple'</span><span class="p">:</span> <span class="mf">0.023</span><span class="p">,</span> <span class="s1">'bee'</span><span class="p">:</span> <span class="mf">0.12</span><span class="p">,</span> <span class="s1">'desk'</span><span class="p">:</span> <span class="mf">0.34</span><span class="p">,</span> <span class="s1">'chair'</span><span class="p">:</span> <span class="mf">0.517</span><span class="p">}</span>

<span class="nb">sum</span><span class="p">(</span><span class="n">unig_dist</span><span class="o">.</span><span class="n">values</span><span class="p">())</span>
</pre></div>
</div>
</div>
</div>
<div class="output_wrapper">
<div class="output">
<div class="output_area">
<div class="prompt output_prompt">Out[7]:</div>
<div class="output_text output_subarea output_execute_result">
<pre>1.0</pre>
</div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In [5]:</div>
<div class="inner_cell">
<div class="input_area">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">alpha</span>      <span class="o">=</span> <span class="mi">3</span> <span class="o">/</span> <span class="mi">4</span>

<span class="n">noise_dist</span> <span class="o">=</span> <span class="p">{</span><span class="n">key</span><span class="p">:</span> <span class="n">val</span> <span class="o">**</span> <span class="n">alpha</span> <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">val</span> <span class="ow">in</span> <span class="n">unig_dist</span><span class="o">.</span><span class="n">items</span><span class="p">()}</span>
<span class="n">Z</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="n">noise_dist</span><span class="o">.</span><span class="n">values</span><span class="p">())</span>
<span class="n">noise_dist_normalized</span> <span class="o">=</span> <span class="p">{</span><span class="n">key</span><span class="p">:</span> <span class="n">val</span> <span class="o">/</span> <span class="n">Z</span> <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">val</span> <span class="ow">in</span> <span class="n">noise_dist</span><span class="o">.</span><span class="n">items</span><span class="p">()}</span>
<span class="n">noise_dist_normalized</span>
</pre></div>
</div>
</div>
</div>
<div class="output_wrapper">
<div class="output">
<div class="output_area">
<div class="prompt output_prompt">Out[5]:</div>
<div class="output_text output_subarea output_execute_result">
<pre>{'apple': 0.044813853132981724,
 'bee': 0.15470428538870049,
 'desk': 0.33785130228003507,
 'chair': 0.4626305591982827}</pre>
</div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In [8]:</div>
<div class="inner_cell">
<div class="input_area">
<div class="highlight hl-ipython3"><pre><span></span><span class="nb">sum</span><span class="p">(</span><span class="n">noise_dist_normalized</span><span class="o">.</span><span class="n">values</span><span class="p">())</span>
</pre></div>
</div>
</div>
</div>
<div class="output_wrapper">
<div class="output">
<div class="output_area">
<div class="prompt output_prompt">Out[8]:</div>
<div class="output_text output_subarea output_execute_result">
<pre>1.0</pre>
</div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<div style="margin-top: -25px;"><hr/></div><p>In the initial unigram distribution, <code>chair</code> appeared the most in the corpus, and had <code>0.517</code> chance of being drawn as a negative sample. However, the unigram distribution is raised to the power of $3/4$rd to combat the imbalance between common vs rare words, as shown in <a href="#fig8">figure 8</a>. After <code>unig_dist</code> was raised to the power of <code>alpha = 3/4</code> and normalized, <code>chair</code> now has <code>0.463</code> chance of being drawn.</p>
<p>On the other hand, <code>apple</code> had the lowest probability (<code>0.023</code>) of being drawn. After the transformation, it now has a bit higher probability (<code>0.049</code>) of being drawn. The imbalance between the most common word (<code>chair</code>) and the least common word (<code>apple</code>) was mitigated.</p>
<p>Once the noise distribution (<code>noise_dist_normalized</code>) is generated, you randomly draw $K$ negative samples according to each word's probability.</p>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In [6]:</div>
<div class="inner_cell">
<div class="input_area">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">K</span> <span class="o">=</span> <span class="mi">10</span>

<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">noise_dist_normalized</span><span class="o">.</span><span class="n">keys</span><span class="p">()),</span> <span class="n">size</span><span class="o">=</span><span class="n">K</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="nb">list</span><span class="p">(</span><span class="n">noise_dist_normalized</span><span class="o">.</span><span class="n">values</span><span class="p">()))</span>
</pre></div>
</div>
</div>
</div>
<div class="output_wrapper">
<div class="output">
<div class="output_area">
<div class="prompt output_prompt">Out[6]:</div>
<div class="output_text output_subarea output_execute_result">
<pre>array(['apple', 'chair', 'bee', 'desk', 'chair', 'bee', 'bee', 'chair',
       'desk', 'chair'], dtype='<U5')</pre>
</div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Derivation-of-Cost-Function-in-Negative-Sampling">Derivation of Cost Function in Negative Sampling<a class="anchor-link" href="#Derivation-of-Cost-Function-in-Negative-Sampling">¶</a></h2><p>The derivations written here are based on the work of <a href="https://arxiv.org/pdf/1402.3722.pdf" target="_blank">word2vec Explained: Deriving Mikolov et al.’s
Negative-Sampling Word-Embedding Method</a> (Goldberg and Levy, 2014).</p>
<p>Consider a pair ($w,\,c$) of center word and its context. Did this pair come from the context window or the <a href="#noise_dist">noise distribution</a>? Let $p(D=1|w,c)$ be the probability that ($w,\,c$) is observed in the true corpus, and $p(D=0|w,c) = 1 - p(D=1|w,c)$ the probability that ($w,\,c$) is non-observed. There are <a href="https://aegis4048.github.io/demystifying_neural_network_in_skip_gram_language_modeling#theta_in_cost" target="_blank">parameters</a> $\theta$ controlling the probability distribution: $p(D=1|w,c;\theta)$.</p>
<p>Negative sampling attempts to optimize the parameters $\theta$ by maximizing the probability of observing positive pairs ($w,\,c_{pos}$) while minimizing the probability of observing negative pairs ($w,\,c_{neg}$). For each positive pair, the model randomly draws $K$ negative words $W_{neg} = \{ c_{neg, j}|j=1,\cdots,K \}$. Our objective function is then:</p>
<div id="eq-9" style="font-size: 1rem;">
$$
\begin{align}
     & \underset{\theta}{\text{argmax}} \ p(D=1|w,c_{pos};\theta) \prod_{c_{neg} \in W_{neg}} p(D=0|w,c_{neg};\theta) \label{}\tag{9}\\ 
   =\quad & \underset{\theta}{\text{argmax}} \ p(D=1|w,c_{pos};\theta) \prod_{c_{neg} \in W_{neg}} (1 - p(D=1|w,c_{neg};\theta))\label{}\tag{10}
\end{align}
$$</div><div class="alert alert-info">
<h4>Notes: Probability Product</h4>
<p>In statistics, probability of observing $C$ multiple events at the same time is computed by the product of each event's probability.</p>
<p><center style="font-size: 1rem; margin-top: 20px;">$$p(x_{1}, x_{2} ... x_{C}) = p(x_{1}) \times p(x_{2}) \, \times \, ... \, \times \, p(x_{C})$$</center></p>
<p>This can be shortened with a product notation:</p>
<p><center style="font-size: 1rem; margin-top: 20px;">$$p(x_{1}, x_{2} ... x_{C}) = \prod_{c=1}^{C}p(x_{c})$$</center></p>
</div><p>It is a common practice in machine learning to <a href="https://aegis4048.github.io/demystifying_neural_network_in_skip_gram_language_modeling#take_natural_log" target="_blank">take a natural log to the objective function</a> to simplify derivations. This does not affect the optimized weights ($\theta$) because natural log is a monotonically increasing function. It ensures that the maximum value of the original probability function occurs at the same point as the log probability function. Therefore:</p>
<div id="eq-11" style="font-size: 1rem;">
$$
\begin{align}
   =\quad & \underset{\theta}{\text{argmax}} \ log \, (p(D=1|w,c_{pos};\theta) \prod_{c_{neg} \in W_{neg}} (1 - p(D=1|w,c_{neg};\theta)) \label{}\tag{11}
\end{align}
$$</div><p>Using the property of logs, the objective function can be simplified:</p>
<div id="eq-12" style="font-size: 1rem;">
$$
\begin{align}
   =\quad  & \underset{\theta}{\text{argmax}} \ log \, p(D=1|w,c_{pos};\theta) + log \, \prod_{c_{neg} \in W_{neg}} (1 - p(D=1|w,c_{neg};\theta)) \label{}\tag{12}\\ 
   =\quad & \underset{\theta}{\text{argmax}} \ log \, p(D=1|w,c_{pos};\theta) + \sum_{c_{neg} \in W_{neg}} log \, (1 - p(D=1|w,c_{neg};\theta)) \label{}\tag{13}
\end{align} 
$$</div><p>The binomial probability $p(D=1|w,c;\theta)$ can be replaced with <a href="#eq-5">eq (5)</a>:</p>
<div id="eq-14" style="font-size: 1rem;">
$$
\begin{align}
   =\quad  & \underset{\theta}{\text{argmax}} \ log \, \frac{1}{1+exp(-\bar{c}_{pos} \cdot \bar{w})} + \sum_{c_{pos} \in W_{neg}} log \, (1 - \frac{1}{1+exp(-\bar{c}_{neg} \cdot \bar{w})}) \label{}\tag{14}\\ 
   =\quad  & \underset{\theta}{\text{argmax}} \ log \, \frac{1}{1+exp(-\bar{c}_{pos} \cdot \bar{w})} + \sum_{c_{pos} \in W_{neg}} log \, \frac{1}{1+exp(\bar{c}_{neg} \cdot \bar{w})} \label{}\tag{15}\\ 
\end{align}
$$</div><p>Using the definition of <a href="#sigmoid">sigmoid</a> function $\sigma(x)=\frac{1}{1+exp(-x)}$:</p>
<div id="eq-16" style="font-size: 1rem;">
$$
\begin{align}
   =\quad  & \underset{\theta}{\text{argmax}} \ log \, \sigma(\bar{c}_{pos} \cdot \bar{w}) + \sum_{c_{pos} \in W_{neg}} log \, \sigma(-\bar{c}_{neg} \cdot \bar{w}) \label{}\tag{16}  
\end{align} 
$$</div><p>According to Mikolov, <a href="#eq-16">eq (16)</a> replaces every $log \, p(w_O|w_I)$ in the vanilla Skip-Gram cost function defined in <a href="#eq-1">eq (1)</a>. Then, the cost function we want to minimize becomes:</p>
<div id="eq-17" style="font-size: 1rem;">$$J(\theta) = - \frac{1}{T} \sum_{i = 1}^T \sum_{-c\leq j \leq c,j\neq 0}( log \, \sigma(\bar{c}_{pos} \cdot \bar{w}) + \sum_{c_{neg} \in W_{neg}} log \, \sigma(-\bar{c}_{neg} \cdot \bar{w})) \tag{17}$$</div><p>However, when implemented in codes, batch gradient descent (making one update after iterating through the entire $T$ corpus) is almost never used due to its high computational cost. Instead, we use <a href="https://aegis4048.github.io/demystifying_neural_network_in_skip_gram_language_modeling#stochastic" target="_blank">stochastic gradient descent</a>. Also, for negative sampling, gradients are calculated and weights are updated for each positive training pairs ($w,\, c_{pos}$). In the other words, one update for each word within the context window of a center word $w$. This is shown <a href="#Negative-Sampling-Algorithm">below</a>. The new cost function is then:</p>
<div id="eq-18" style="font-size: 1rem;">$$J(\theta; w,c_{pos}) = - log \, \sigma(\bar{c}_{pos} \cdot \bar{w}) - \sum_{c_{neg} \in W_{neg}} log \, \sigma(-\bar{c}_{neg} \cdot \bar{w}) \tag{18}$$</div><p>Note that $w$ is a word vector for an input word, and that it is equivalent to a <a href="https://aegis4048.github.io/demystifying_neural_network_in_skip_gram_language_modeling#hidden_layer" target="_blank">hidden layer</a> ($w = h$). For clarification:</p>
<div id="eq-19" style="font-size: 1rem;">$$J(\theta; w,c_{pos}) = - log \, \sigma(\bar{c}_{pos} \cdot h) - \sum_{c_{neg} \in W_{neg}} log \, \sigma(-\bar{c}_{neg} \cdot h)\tag{19}$$</div><div class="alert alert-info">
<h4>Notes: Notations used in different papers</h4>
<p>Different papers use different notations. In <a href="https://arxiv.org/pdf/1411.2738.pdf" target="_blank">word2vec Parameter Learning Explained</a> (Rong, 2014):</p>
<p>$$J(\theta; w_I, w_O) = - log \, \sigma(v^{'}_{w_{O}} \cdot h) - \sum_{w_j \in W_{neg}} log \, \sigma(-v^{'}_{w_{j}} \cdot h)$$</p>
<p>where $w_I$ is the input (center) word in the corpus. $w_{O}$ is a word found in the context window of $w_I$ and is a positive word. $v^{'}$ is a word vector in the output weight matrix ($v^{'} \in W_{output}$), $w_j$ is a randomly drawn negative word from the <a href="#noise_dist">noise distribution</a>, and $v^{'}_{w_{j}}$ is a $j$-th word vector in $W_{output}$ that corresponds to the negative word $w_j$. $h$ is a <a href="https://aegis4048.github.io/demystifying_neural_network_in_skip_gram_language_modeling#hidden_layer" target="_blank">hidden layer</a>.</p>
<p>In the original Word2Vec paper, <a href="https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf" target="_blank">Distributed Representations of Words and Phrases and their Compositionality</a> (Mikolov et al., 2013):</p>
<p>$$J(\theta; w_I, w_O) = - log \, \sigma(v^{'}_{w_{O}} \top v_I) - \sum^k_{i=1} \mathbb{E}_{w_{i} \sim P_n(w)}\big[ log \, \sigma(-v^{'}_{w_{i}} \top v_I) \big]$$</p>
<p>$k$ is the number of negative samples and $w_i$ is an $i$-th negative word drawn from the noise distribution $P_n(w)$. $v_{w_{I}}$ is a word vector in the input weight matrix $W_{input}$ for the input word $w_I$ and is equivalent to the hidden layer $h$. These are all equivalent to <a href="#eq-19">eq (19)</a>.</p>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Derivation-of-gradients">Derivation of gradients<a class="anchor-link" href="#Derivation-of-gradients">¶</a></h3><p>The goal of any machine learning model is to find the optimal values of a weight matrix ($\theta$) to minimize prediction error. A general update equation for weight matrix looks like the following:</p>
<div id="eq-20" style="font-size: 1rem;">$$ \theta^{(new)}=\theta^{(old)}-\eta\cdot\frac{\partial J}{\partial \theta} \tag{20}$$</div><p>$\theta$ is a parameter that needs to be optimized, and $\eta$ is a learning rate. In negative sampling, we take the derivative to the cost function $J(\theta; w, c_{pos})$ defined in <a href="#eq-19">eq (19)</a> with respect to $\theta$. Note that the derivative of a sigmoid function is $\frac{\partial \sigma}{\partial x} = \sigma(x)(1–\sigma(x))$.</p>
<div id="eq-21" style="font-size: 1rem;">$$ \frac{\partial J}{\partial \theta} = (\sigma(\bar{c}_{pos} \cdot h)) - 1)\frac{\partial \bar{c}_{pos} \cdot h}{\partial \theta} + \sum_{c_{neg} \in W_{neg}} \sigma(\bar{c}_{neg} \cdot h) \frac{\partial \bar{c}_{neg} \cdot h}{\partial \theta} \tag{21}$$</div><p>Since the parameter $\theta$ is a <a href="https://aegis4048.github.io/demystifying_neural_network_in_skip_gram_language_modeling#theta_in_cost">concatenation</a> of the input and output weight matrix $[W_{input} \quad W_{output}]$, the cost function needs to be differentiated with respect to the both matrices — $[\frac{\partial J}{\partial W_{input}} \quad \frac{\partial J}{\partial W_{output}}]$.</p>
<div class="alert alert-info">
<h4>Notes: Clarification on notation</h4>
<p>$\bar{c}$ represents a word vector in the output weight matrix ($\bar{c} \in W_{output}$) for a context word. The context word can be a positive word ($c_{pos}$) from a context window or a negative word ($c_{neg} \in W_{neg}$) from a <a href="#noise_dist">noise distribution</a>.</p>
<p>$h$ is a <a href="https://aegis4048.github.io/demystifying_neural_network_in_skip_gram_language_modeling#hidden_layer" target="_blank">hidden layer</a>. Recall that the hidden layer is essentially a word vector for the input word that is <i>looked up</i> from the input weight matrix $W_{input}$.</p>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<div id="grad_W_output"></div><p><strong>Gradients with respect to output weight matrix $\frac{\partial J}{\partial W_{output}}$</strong></p>
<p>With negative sampling, we do not update the entire output weight matrix $W_{output}$, but only a fraction of it. We update $K + 1$ word vectors in the output weight matrix — $\bar{c}_{pos}$, $\bar{c}_{neg,1},\,...\,\bar{c}_{neg,K}$. We take partial derivatives to the cost function defined in <a href="#eq-19">eq (19)</a> with respect to positive words and negative words. This can be done by replacing $\theta$ in <a href="#eq-21">eq (21)</a> with $\bar{c}_{pos}$ and $\bar{c}_{neg}$ each.</p>
<div id="eq-22" style="font-size: 1rem;">
$$
\begin{align}
   \frac{\partial J}{\partial \bar{c}_{pos}} &= (\sigma(\bar{c}_{pos} \cdot h) - 1)\cdot h \label{}\tag{22}\\[5pt]
   \frac{\partial J}{\partial \bar{c}_{eng}} &= \sigma(\bar{c}_{neg} \cdot h)\cdot h \label{}\tag{23}
\end{align}
$$</div><p>The update equations are then:</p>
<div id="eq-24" style="font-size: 1rem;">
$$
\begin{align}
   \bar{c}_{pos}^{(new)} &= \bar{c}_{pos}^{(old)} - \eta \cdot (\sigma(\bar{c}_{pos} \cdot h) - 1)\cdot h \label{}\tag{24}\\[6pt]
   \bar{c}_{neg}^{(new)} &= \bar{c}_{neg}^{(old)} - \eta \cdot \sigma(\bar{c}_{neg} \cdot h) \label{}\tag{25}
\end{align}
$$</div><p>The gradients for positive and negative words can be merged for brevity:</p>
<div id="eq-26" style="font-size: 1rem;">$$ \bar{c}_{j}^{(new)} = \bar{c}_{j}^{(old)}-\eta\cdot (\sigma(\bar{c}_{j} \cdot h) - t_j) \cdot h \tag{26}$$</div><p>where $t_j = 1$ for positive words ($c_j = c_{pos}$) and $t_j = 0$ for negative words ($c_j = c_{neg} \in W_{neg}$). $\bar{c}_{j}$ is the $j$-th word vector in the output word matrix ($\bar{c}_j \in W_{output}$). For each positive pairs ($w$, $c_{pos}$), <a href="#eq-26">eq (26)</a> is applied to $K + 1$ word vectors in $W_{output}$ as shown in <a href="#fig4">figure 4</a>.</p>
<div id="pred_error"></div><div class="alert alert-info">
<h4>Notes: Prediction Error</h4>
<p>In <a href="#eq-26">eq (26)</a>, $\sigma(\bar{c}_{j} \cdot h) - t_j$ is called a <i>prediction error</i>. Recall that negative sampling attempts to maximize the the probability of observing positive pairs $p(c_{pos}|w) \rightarrow 1$ while minimizing the probability of observing negative pairs $p(c_{neg}|w) \rightarrow 0$. If good word vectors are learned, $\sigma(\bar{c}_{pos}\cdot h) \approx 1$ for positive pairs, and $\sigma(\bar{c}_{neg}\cdot h) \approx 0$ for negative pairs as shown in <a href="#fig7">figure 7</a>.<p>
<p>The prediction error will gradually approach zero $\sigma(\bar{c}_{pos}\cdot h) - t_j \approx 0$, as the model iterates through the training samples (positive pairs) and optimizes the weights.</p>
</p></p></div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<div id="grad_W_input"></div><p><strong>Gradients with respect to input weight matrix $\frac{\partial J}{\partial W_{input}}$</strong></p>
<p>Just like vanilla Skip-Gram, only one word vector that corresponds to the input word $w$ in $W_{input}$ is updated with negative sampling. This is because the <a href="https://aegis4048.github.io/demystifying_neural_network_in_skip_gram_language_modeling#input_layer" target="_blank">input layer is an one-hot-encoded vector</a> (however, the equation for the gradient descent is different.) Therefore, taking the derivative for the input weight matrix is equivalent to taking the derivative to the hidden layer ($\frac{\partial J}{\partial W_{input}} = \frac{\partial J}{\partial h}$). We replace $\theta$ in <a href="#eq-21">eq (21)</a> with $h$, and differentiate it:</p>
<div id="eq-27" style="font-size: 1rem;">
$$
\begin{align}
   \frac{\partial J}{\partial h} &= (\sigma(\bar{c}_{pos} \cdot h) - 1) \cdot \bar{c}_{pos} + \sum_{c_{eng} \in W_{neg}} \sigma(\bar{c}_{neg} \cdot h) \cdot \bar{c}_{neg} \label{}\tag{27}\\
   &= \sum_{c_j \in \{c_{pos}\} \cup W_{neg}} (\sigma(\bar{c}_{j} \cdot h) - t_j) \cdot \bar{c}_{j} \label{}\tag{28}
\end{align}
$$</div><p>Same as <a href="#eq-26">eq (26)</a>, $t_j = 1$ for positive words ($c_j = c_{pos}$) and $t_j = 0$ for negative words ($c_j = c_{neg} \in W_{neg}$). The update equation is then:</p>
<div id="eq-29" style="font-size: 1rem;">$$ \bar{w}^{(new)} = \bar{w}^{(old)}-\eta\cdot \sum_{c_j \in \{c_{pos}\} \cup W_{neg}} (\sigma(\bar{c}_{j} \cdot h) - t_j) \cdot \bar{c}_{j} \tag{29}$$</div><p>Recall that $w$ is a word vector in the input weight matrix ($\bar{w} \in W_{input}$) and $\bar{c}_j$ is a $j$-th word vector in the output weight matrix ($\bar{c}_j \in W_{output}$).</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<div id="algorithm"></div><h2 id="Negative-Sampling-Algorithm">Negative Sampling Algorithm<a class="anchor-link" href="#Negative-Sampling-Algorithm">¶</a></h2><p>For each positive word-context pair ($w, c_{pos}$), $1$ word vector that corresponds to the center word ($w$) is updated in the input weight matrix $W_{input}$ as shown in <a href="#fig15">figure 15</a>. In the output weight matrix $W_{output}$, $1+K$ word vectors that correspond to the positive and negative words ($\{\bar{c}_{pos}\} \cup \bar{W}_{neg} \in \mathbb{R}^{1+K} $) are updated as shown in <a href="#fig4">figure 4</a>.</p>
<p>Since Skip-Gram uses SGD to reduce computational cost, negative sampling also uses SGD too. Then, the training for Skip-Gram negative sampling has the following algorithm structure:</p>
<p><div id="sgns_algorithm"></div></p>
<p><p><u>Algorithm 1: Skip-Gram Negative Sampling</u></p></p>
<pre>
    <code class="language-python">
        P_nw = # generate noise distribution
        for word in corpus:
            for context in context_window:

                # draw K negative samples from P_nw
                W_neg = np.random.choice(Pn_w.keys(), size=K, p=Pn_w.values())

                # compute gradients. w is a input word vector = hidden layer
                grad_V_output_pos = (sigmoid(c_pos * h) - 1) * w
                grad_V_input = (sigmoid(c_pos * h) - 1) * c_pos
                grad_V_output_neg_list = []
                for c_neg in W_neg:
                    grad_V_output_neg_list.append(sigmoid(c_neg * h) * h)
                    grad_V_input += sigmoid(c_neg * h) * c_neg

                # use SGD to update w, c_pos, and c_neg_1, ... , c_neg_K
                V_output_pos = V_output_pos - alpha * grad_V_output_pos
                V_input = V_input - alpha * grad_V_input
                for grad_V_output_neg in grad_V_output_neg_list:
                    V_output_neg = V_output_neg - alpha * grad_V_output_neg
    </code>
</pre><p>The Python implementation of negative sampling here is based on the interpretation of <em>Algorithm 1 SGNS Word2Vec</em> in <a href="https://www.aaai.org/ocs/index.php/AAAI/AAAI17/paper/viewFile/14956/14446" target="_blank">Distributed Negative Sampling for Word Embeddings</a> (Stergiou et al., 2017).</p>
<p>In vanilla Skip-Gram, one update is made for the entire weight matrices $[W_{input} \quad W_{output}]$ for each input word. Each update involves summing up dot products for all context words within the context window of size $C$ as shown in eq (20) and eq (21) of my <a href="https://aegis4048.github.io/demystifying_neural_network_in_skip_gram_language_modeling#eq-20" target="_blank">previous post</a>.</p>
<p>In negative sampling, $C$ updates are made for a fraction of the weights for each input word. This is because negative sampling treats each positive pair ($w$, $c_{pos}$) as one training sample, whereas vanilla Skip-Gram treats a center word ($w$) and its $C$ neighboring context words ($c_{pos, 1}$, $...$, $c_{pos,C}$) all together as one training sample for SGD.</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Numerical-Demonstration">Numerical Demonstration<a class="anchor-link" href="#Numerical-Demonstration">¶</a></h2><p>For the ease of illustration, screenshots from Excel will be used to demonstrate the concept of updating weight matrices through forward and backward propagations.</p>
<p><strong>Description of the Corpus</strong></p>
<p>Assume that the training corpus is the entire text in the book, "<a href="https://en.wikipedia.org/wiki/A_Song_of_Ice_and_Fire" target="_black">A Song of Ice and Fire</a>." Unlike the simple one-sentence training corpus used in my <a href="https://aegis4048.github.io/demystifying_neural_network_in_skip_gram_language_modeling#window_id" target="_blank">previous post</a>, the training corpus needs to be much bigger to illustrate negative sampling, because we need to randomly draw negative samples that are unlikely to be observed in a pair with a center word. The sentence that has the current center word is <em>"Ned Stark is the most honorable man"</em>.</p>
<p>Center (input) word is <code>Ned</code>, and window size is $C = 2$, making <code>Stark</code> and <code>is</code> context words. Number of negative samples drawn from the <a href="#noise_dist">noise distribution</a> for each positive pair is $K = 3$.</p>
<div class="row give-margin-inline-big-plot mobile_responsive_plot_full_width">
<div class="col"><img src="jupyter_images/neg_training.png"/></div>
<div class="col-12"><p class="image-description">Figure 9: Training corpus for negative sampling</p></div>
</div><p>For your information, Ned Stark is a fictional character from the book, <em>A Song of Ice and Fire</em>. The book also has a TV show adaptation, known as the <em>Game of Thrones</em> (GoT). Ned Stark is a noble lord of his land and has a reputation of being the most honorable man in the kingdom.</p>
<p>So here's the idea. The center word <code>Ned</code> will be observed in a pair with context words (postive) like <code>Stark</code>, because it is his last name. The same thing goes for <code>is</code> too, because <code>is</code> is a verb tense used to describe a singular object.</p>
<p>However, <code>Ned</code> most likely won't be observed in a pair with random words (negative) like <code>pimples</code>, <code>zebra</code>, <code>donkey</code> within the book. If the model can differentiate between positive pairs and negative pairs as shown in <a href="#fig5">figure 5</a>, good word vectors will be learned.</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="1.-Positive-word-pair:-(Ned,-Stark)">1. Positive word pair: (<code>Ned</code>, <code>Stark</code>)<a class="anchor-link" href="#1.-Positive-word-pair:-(Ned,-Stark)">¶</a></h3><p>Recall that in negative sampling, one update is made for each of the positive training pairs. This means that $C$ weight updates are made for each input (center) word, where $C$ is the window size.</p>
<p>Our current positive word pair is (<code>Ned</code>, <code>Stark</code>). For the current positive pair, we <a href="#neg_drawn">randomly draw</a> $K=3$ negative words from the noise distribution: <code>pimples</code>, <code>zebra</code>, <code>idiot</code></p>
<p><strong>Forward Propagation: Computing hidden (projection) layer</strong></p>
<p>Hidden layer ($h$) is <i>looked up</i> from $W_{input}$ by multiplying the <a href="https://aegis4048.github.io/demystifying_neural_network_in_skip_gram_language_modeling#input_layer" target="_blank">one-hot-encoded input vector</a> with the <a href="https://aegis4048.github.io/demystifying_neural_network_in_skip_gram_language_modeling#weight_matrix" target="_blank">input weight matrix</a> $W_{input}$.</p>
<div class="row">
<div class="col"><img src="jupyter_images/neg_hidden.png"/></div>
<div class="col-12"><p class="image-description">Figure 10: Computing hidden (projection) layer</p></div>
</div><p><strong>Forward Propagation: Sigmoid output layer</strong></p>
<p>Output layer is a probability distribution of positive and negative words ($c_{pos} \cup W_{neg}$), given a center word ($w$). It is computed with <a href="#eq-7">eq (7)</a>. Recall that <a href="#sigmoid">sigmoid function</a> has $\sigma(x) = \frac{1}{1+exp(-x)}$.</p>
<div class="row">
<div class="col"><img src="jupyter_images/neg_forward_2.png"/></div>
<div class="col-12"><p class="image-description">Figure 11: Sigmoid output layer</p></div>
</div><p><strong>Backward Propagation: Prediction Error</strong></p>
<p>The details about the prediction error is described <a href="#pred_error">above</a>. Since our current positive word is <code>Stark</code>, $t_j = 1$ for <code>Stark</code> and $t_j=0$ for other negative words (<code>pimples</code>, <code>zebra</code>, <code>idiot</code>).</p>
<div class="row">
<div class="col"><img src="jupyter_images/neg_backward_1.png"/></div>
<div class="col-12"><p class="image-description">Figure 12: Prediction errors of positive and negative words</p></div>
</div><p><strong>Backward Propagation: Computing $\nabla W_{input}$</strong></p>
<p>Gradients of input weight matrix ($\frac{\partial J}{\partial W_{input}}$) are computed using <a href="#eq-27">eq (28)</a>. Just like vanilla Skip-Gram, only the word vector in the input weight matrix $W_{input}$ that corresponds to the input (center) word $w$ is updated.</p>
<div class="row">
<div class="col"><img src="jupyter_images/neg_backward_2.png"/></div>
<div class="col-12"><p class="image-description">Figure 13: Computing input weight matrix gradient $\nabla W_{input}$</p></div>
</div><p><strong>Backward Propagation: Computing $\nabla W_{output}$</strong></p>
<p>With negative sampling, only a fraction of word vectors in the output weight matrix $W_{output}$ is updated. Gradients for $K + 1$ word vectors for positive and negative words in the $W_{output}$ are computed using <a href="#eq-22">eq (22)</a> and <a href="#eq-22">eq (23)</a>. Recall that $K$ is the number of negative samples drawn from a noise distribution, and that $K = 3$ in our example.</p>
<div class="row">
<div class="col"><img src="jupyter_images/neg_backward_3.png"/></div>
<div class="col-12"><p class="image-description">Figure 14: Computing output weight matrix gradient $\nabla W_{output}$</p></div>
</div><p><strong>Backward Propagation: Updating Weight matrices</strong></p>
<p>Input and output weight matrices ($[W_{input} \quad W_{output}]$) are updated using <a href="#eq-26">eq (26)</a> and <a href="#eq-29">eq (29)</a>.</p>
<div class="row" id="fig15">
<div class="col"><img src="jupyter_images/neg_backward_4.png"/></div>
<div class="col-12"><p class="image-description">Figure 15: Updating $W_{input}$</p></div>
</div><div class="row">
<div class="col"><img src="jupyter_images/neg_backward_5.png"/></div>
<div class="col-12"><p class="image-description">Figure 16: Updating $W_{output}$</p></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="2.-Positive-word-pair:-(Ned,-is)">2. Positive word pair: (<code>Ned</code>, <code>is</code>)<a class="anchor-link" href="#2.-Positive-word-pair:-(Ned,-is)">¶</a></h3><p>The center word <code>Ned</code> has two context words: <code>Stark</code> and <code>is</code>. This means that we have two positive pairs = two updates to make. Since we already update the matrices $[W_{input} \quad W_{output}]$ using (<code>Ned</code>, <code>Stark</code>), we will use (<code>Ned</code>, <code>is</code>) to update weight matrices this time.</p>
<p>In negative sampling, we draw new $K$ negative words for each positive pairs. Assume that we randomly drew <code>coins</code>, <code>donkey</code>, and <code>machine</code> as our negative words this time.</p>
<p><strong>Forward Propagation: Computing hidden (projection) layer</strong></p>
<div class="row">
<div class="col"><img src="jupyter_images/neg_hidden_2.png"/></div>
<div class="col-12"><p class="image-description">Figure 17: Computing hidden (projection) layer</p></div>
</div><p><strong>Forward Propagation: Sigmoid output layer</strong></p>
<div class="row">
<div class="col"><img src="jupyter_images/neg_forward_2_2.png"/></div>
<div class="col-12"><p class="image-description">Figure 18 Sigmoid output layer</p></div>
</div><p><strong>Backward Propagation: Prediction Error</strong></p>
<p>Our current positive word is <code>Stark</code>: $t_j = 1$ for <code>Stark</code> and $t_j=0$ for other negative words (<code>coins</code>, <code>donkey</code>, and <code>machine</code>).</p>
<div class="row">
<div class="col"><img src="jupyter_images/neg_backward_1_2.png"/></div>
<div class="col-12"><p class="image-description">Figure 19: Prediction errors of positive and negative words</p></div>
</div><p><strong>Backward Propagation: Computing $\nabla W_{input}$</strong></p>
<div class="row">
<div class="col"><img src="jupyter_images/neg_backward_2_2.png"/></div>
<div class="col-12"><p class="image-description">Figure 20: Computing input weight matrix gradient $\nabla W_{input}$</p></div>
</div><p><strong>Backward Propagation: Computing $\nabla W_{output}$</strong></p>
<div class="row">
<div class="col"><img src="jupyter_images/neg_backward_3_2.png"/></div>
<div class="col-12"><p class="image-description">Figure 21: Computing output weight matrix gradient $\nabla W_{output}$</p></div>
</div><p><strong>Backward Propagation: Updating Weight matrices</strong></p>
<div class="row">
<div class="col"><img src="jupyter_images/neg_backward_4_2.png"/></div>
<div class="col-12"><p class="image-description">Figure 22: Updating $W_{input}$</p></div>
</div><div class="row">
<div class="col"><img src="jupyter_images/neg_backward_5_2.png"/></div>
<div class="col-12"><p class="image-description">Figure 23: Updating $W_{output}$</p></div>
</div>
</div>
</div>
</div>

<script type="text/javascript">if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = '//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: 'center'," +
        "    displayIndent: '0em'," +
        "    showMathMenu: true," +
        "    tex2jax: { " +
        "        inlineMath: [ ['$','$'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'black ! important'} }" +
        "    } " +
        "}); ";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
</script>
        </div>
        <hr/>
        <aside>
        <nav>
        <ul class="articles-timeline">
            <li class="previous-article">« <a href="https://aegis4048.github.io/demystifying_neural_network_in_skip_gram_language_modeling" title="Previous: Demystifying Neural Network in Skip-Gram Language Modeling">Demystifying Neural Network in Skip-Gram Language Modeling</a></li>
            <li class="next-article"><a href="https://aegis4048.github.io/comprehensive_confidence_intervals_for_python_developers" title="Next: Comprehensive Confidence Intervals for Python Developers">Comprehensive Confidence Intervals for Python Developers</a> »</li>
        </ul>
        </nav>
        </aside>
  <section>
    <div id="disqus_thread" aria-live="polite"><noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript></div>
    <script type="text/javascript">
      var disqus_shortname = 'pythonic-excursions';
      var disqus_identifier = '/optimize_computational_efficiency_of_skip-gram_with_negative_sampling';
      var disqus_url = 'https://aegis4048.github.io/optimize_computational_efficiency_of_skip-gram_with_negative_sampling';
      var disqus_title = 'Optimize Computational Efficiency of Skip-Gram with Negative Sampling';
      (function() {
        var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
        dsq.src = "//" + disqus_shortname + '.disqus.com/embed.js';
        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
      })();
    </script>
  </section>
        <hr/>
<section style="margin-top: 30px">
    <h2>Related Posts</h2>
<ul class="related-posts-list">
<li><a href="https://aegis4048.github.io/parse-pdf-files-while-retaining-structure-with-tabula-py" title="Parse PDF Files While Retaining Structure with Tabula-py">Parse PDF Files While Retaining Structure with Tabula-py</a></li>
<li><a href="https://aegis4048.github.io/understanding_multi-dimensionality_in_vector_space_modeling" title="Understanding Multi-Dimensionality in Vector Space Modeling">Understanding Multi-Dimensionality in Vector Space Modeling</a></li>
<li><a href="https://aegis4048.github.io/demystifying_neural_network_in_skip_gram_language_modeling" title="Demystifying Neural Network in Skip-Gram Language Modeling">Demystifying Neural Network in Skip-Gram Language Modeling</a></li>
</ul>
</section>
    </article>
</div>
        </div>
<footer class="footer">
   <div class="container bottom_border">
      <div class="row">
         <div class="col">
            <h5 class="headin5_amrc col_white_amrc pt2">ABOUT ERIC</h5>
            <!--headin5_amrc-->
            <p class="mb10"><img id="profile_img" align="left" src="https://aegis4048.github.io/theme/img/profile_photo_footer.jpg">Senior undergraduate student at the Univeristy of Texas at Austin, Hildebrand Department of Petroleum Engineering, the #1 petroleum engineering school in the US. I am a self-taught Python developer with strong engineering & statistical background. I am good at creating clean, easy-to-read codes for data analysis. I enjoy assisting my fellow engineers by developing accessible and reproducible codes.</p>
            <p><i class="fa fa-envelope mr-2"></i>aegis4048@gmail.com</p>
         </div>
      </div>
   </div>
   <div class="container">
      <ul class="foote_bottom_ul_amrc">
         <li><a href="/">HOME</a></li>
         <li><a href="about.html">ABOUT</a></li>
         <li><a href="archives.html">ARCHIVE</a></li>
      </ul>
      <!--foote_bottom_ul_amrc ends here-->
      <p class="text-center">Handcrafted by me @2018</p>
      <div class="container">
          <div class="row justify-content-center">
              <div class="row" align="center">
                  <div class="footer-icon"><a href="https://www.linkedin.com/in/eric-kim-34318811b/"><i class="fab fa-linkedin-in"></i></a></div>
                  <div class="footer-icon"><a href="https://github.com/aegis4048"><i class="fab fa-github"></i></a></div>
              </div>
          </div>
      </div>

      <!--social_footer_ul ends here-->
   </div>
</footer>
            <script type="text/javascript" src="https://aegis4048.github.io/theme/libs/jquery.min.js"></script>
            <script type="text/javascript" src="https://aegis4048.github.io/theme/libs/bootstrap-4.2.1/dist/js/bootstrap.bundle.min.js"></script>

        <script src="https://aegis4048.github.io/theme/libs/prism.js"></script>
        <script src="https://aegis4048.github.io/theme/libs/Countable.js"></script>

        <script>
            Prism.plugins.NormalizeWhitespace.setDefaults({
                'remove-trailing': true,
                'remove-indent': true,
                'left-trim': true,
                'right-trim': true,
                /*'break-lines': 80,
                'indent': 2,
                'remove-initial-line-feed': false,
                'tabs-to-spaces': 4,
                'spaces-to-tabs': 4*/
            });
        </script>


        <script type="text/javascript" src="https://aegis4048.github.io/theme/js/custom.js"></script>
            <script>
                function validateForm(query)
                {
                    return (query.length > 0);
                }
            </script>
    </body>
</html>

